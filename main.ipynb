{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from matrix_factorization.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nbimporter \n",
    "import matrix_factorization\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  9 4808k    9  451k    0     0   267k      0  0:00:17  0:00:01  0:00:16  267k\n",
      " 40 4808k   40 1930k    0     0   701k      0  0:00:06  0:00:02  0:00:04  701k\n",
      "100 4808k  100 4808k    0     0  1733k      0  0:00:02  0:00:02 --:--:-- 1732k\n",
      "\"unzip\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "# Downloading Movielens-100k\n",
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip ml-100k.zip\n",
    "!cd ml-100k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    print (str(n_users) + ' users')\n",
    "    print (str(n_items) + ' items')\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n",
      "Sparsity: 6.30%\n"
     ]
    }
   ],
   "source": [
    "print ('Sparsity: {:4.2f}%'.format(get_sparsity(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], size=10, replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.9179628673300955\n",
      "Test mse: 1.0113053267188021\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(train, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "iter_array = [10]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.304669364224531"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.710139043178159"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5945303210463734"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ####################################### GANS ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as t_data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_some_noise(batch_size):\n",
    "    return torch.rand(batch_size,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6460, 0.9845, 0.7296, 0.4430, 0.1759, 0.3234, 0.7032, 0.6447, 0.2641,\n",
       "         0.9395, 0.2441, 0.0141, 0.6784, 0.1234, 0.6130, 0.4340, 0.6513, 0.7498,\n",
       "         0.2748, 0.2782, 0.7485, 0.7293, 0.5898, 0.2039, 0.1539, 0.1619, 0.1513,\n",
       "         0.6915, 0.6329, 0.6814, 0.9938, 0.3703, 0.4236, 0.1557, 0.4175, 0.9782,\n",
       "         0.7575, 0.3240, 0.8149, 0.8862, 0.7526, 0.6839, 0.9553, 0.4930, 0.3651,\n",
       "         0.0131, 0.4318, 0.7300, 0.9287, 0.2212, 0.3266, 0.3109, 0.6550, 0.4028,\n",
       "         0.1070, 0.5430, 0.3236, 0.7261, 0.9952, 0.9886, 0.4024, 0.5292, 0.6700,\n",
       "         0.5248, 0.1665, 0.1725, 0.0162, 0.9185, 0.2202, 0.7755, 0.3687, 0.4191,\n",
       "         0.0083, 0.1148, 0.9622, 0.1604, 0.9545, 0.3251, 0.6385, 0.7165, 0.6565,\n",
       "         0.9522, 0.5257, 0.1520, 0.4725, 0.9770, 0.9492, 0.3033, 0.4193, 0.4662,\n",
       "         0.1588, 0.9027, 0.1344, 0.5052, 0.6971, 0.0013, 0.8680, 0.2243, 0.9634,\n",
       "         0.3286],\n",
       "        [0.5406, 0.2043, 0.7063, 0.6196, 0.6062, 0.4251, 0.0231, 0.9926, 0.6023,\n",
       "         0.1554, 0.7290, 0.6610, 0.0690, 0.2844, 0.3395, 0.2064, 0.8830, 0.6817,\n",
       "         0.1130, 0.0695, 0.2054, 0.5804, 0.8204, 0.0482, 0.9798, 0.0865, 0.7624,\n",
       "         0.4474, 0.7931, 0.6424, 0.1851, 0.0015, 0.8960, 0.8260, 0.1752, 0.9146,\n",
       "         0.2915, 0.5142, 0.5604, 0.6209, 0.3151, 0.6707, 0.9559, 0.5265, 0.4623,\n",
       "         0.0344, 0.5371, 0.7100, 0.3589, 0.8893, 0.7821, 0.7385, 0.3392, 0.9403,\n",
       "         0.7072, 0.0785, 0.8586, 0.4682, 0.0695, 0.3742, 0.5033, 0.5724, 0.3734,\n",
       "         0.3388, 0.8994, 0.1429, 0.2150, 0.4085, 0.0874, 0.5738, 0.3579, 0.8841,\n",
       "         0.8253, 0.3669, 0.1616, 0.2499, 0.0057, 0.7770, 0.6495, 0.5564, 0.6442,\n",
       "         0.0612, 0.4697, 0.5441, 0.0740, 0.8606, 0.3152, 0.8444, 0.0364, 0.3769,\n",
       "         0.6513, 0.6483, 0.5456, 0.5872, 0.1286, 0.4899, 0.6469, 0.4084, 0.8316,\n",
       "         0.3770],\n",
       "        [0.7276, 0.8017, 0.2461, 0.7376, 0.2406, 0.2685, 0.0040, 0.4869, 0.4094,\n",
       "         0.1877, 0.1004, 0.3065, 0.4525, 0.0575, 0.0668, 0.7570, 0.3856, 0.6415,\n",
       "         0.5082, 0.4412, 0.6330, 0.5593, 0.0711, 0.9028, 0.5035, 0.5059, 0.2583,\n",
       "         0.8162, 0.4005, 0.2488, 0.0639, 0.2718, 0.8064, 0.4006, 0.8964, 0.2527,\n",
       "         0.5127, 0.2928, 0.1588, 0.9119, 0.9811, 0.2153, 0.1847, 0.1000, 0.4073,\n",
       "         0.8096, 0.7254, 0.9613, 0.4020, 0.0801, 0.1204, 0.7875, 0.0650, 0.4569,\n",
       "         0.8102, 0.4819, 0.9842, 0.1283, 0.5111, 0.1626, 0.6960, 0.2332, 0.1216,\n",
       "         0.3682, 0.0529, 0.2395, 0.0135, 0.4644, 0.2319, 0.8938, 0.3535, 0.1782,\n",
       "         0.8977, 0.7602, 0.9717, 0.6801, 0.2187, 0.4578, 0.8596, 0.0932, 0.1979,\n",
       "         0.2150, 0.6763, 0.6911, 0.5009, 0.0594, 0.2383, 0.0862, 0.6142, 0.2923,\n",
       "         0.1197, 0.1220, 0.8413, 0.7736, 0.2215, 0.0015, 0.5551, 0.7582, 0.5750,\n",
       "         0.4919],\n",
       "        [0.2043, 0.0392, 0.2425, 0.8306, 0.2040, 0.3574, 0.1873, 0.4186, 0.6068,\n",
       "         0.7150, 0.2407, 0.8467, 0.7315, 0.5806, 0.6516, 0.8024, 0.2339, 0.1339,\n",
       "         0.1169, 0.6241, 0.1202, 0.9311, 0.2040, 0.4573, 0.1630, 0.7625, 0.2120,\n",
       "         0.5355, 0.6675, 0.7193, 0.1057, 0.1687, 0.4245, 0.6987, 0.3160, 0.3573,\n",
       "         0.7058, 0.4549, 0.7494, 0.1383, 0.2429, 0.5524, 0.0744, 0.0265, 0.0252,\n",
       "         0.5015, 0.3427, 0.9504, 0.3263, 0.2667, 0.8620, 0.8656, 0.5292, 0.4759,\n",
       "         0.7705, 0.1813, 0.1947, 0.2820, 0.1351, 0.1884, 0.7636, 0.8790, 0.4661,\n",
       "         0.5983, 0.4314, 0.4998, 0.1228, 0.6850, 0.5949, 0.5725, 0.4200, 0.1483,\n",
       "         0.0078, 0.7964, 0.1246, 0.5193, 0.4361, 0.0089, 0.0636, 0.8698, 0.9362,\n",
       "         0.9612, 0.3682, 0.0955, 0.6113, 0.2994, 0.4742, 0.8209, 0.0578, 0.7454,\n",
       "         0.3051, 0.4610, 0.9685, 0.1279, 0.0873, 0.0223, 0.3348, 0.6855, 0.5502,\n",
       "         0.0888],\n",
       "        [0.1364, 0.3800, 0.9017, 0.0666, 0.9249, 0.9634, 0.5995, 0.5323, 0.6829,\n",
       "         0.9367, 0.9247, 0.8014, 0.6438, 0.7265, 0.2963, 0.6557, 0.6090, 0.3735,\n",
       "         0.5243, 0.3533, 0.5192, 0.5193, 0.6064, 0.9922, 0.4708, 0.0656, 0.1899,\n",
       "         0.5220, 0.2897, 0.5715, 0.5684, 0.8537, 0.5654, 0.8165, 0.6468, 0.7843,\n",
       "         0.3901, 0.5380, 0.2594, 0.7276, 0.1769, 0.8171, 0.2804, 0.8985, 0.9216,\n",
       "         0.0347, 0.7663, 0.4640, 0.4909, 0.0473, 0.4322, 0.2288, 0.1731, 0.6248,\n",
       "         0.0157, 0.9676, 0.0446, 0.2237, 0.6712, 0.6455, 0.5450, 0.1490, 0.6129,\n",
       "         0.7587, 0.3011, 0.2213, 0.7745, 0.1208, 0.5203, 0.0809, 0.0905, 0.1727,\n",
       "         0.7066, 0.3902, 0.5481, 0.0084, 0.3048, 0.2881, 0.9976, 0.1803, 0.5677,\n",
       "         0.2828, 0.1095, 0.1781, 0.5544, 0.6400, 0.5716, 0.6962, 0.2639, 0.4701,\n",
       "         0.7495, 0.6615, 0.6564, 0.3093, 0.1101, 0.7006, 0.2764, 0.2565, 0.2148,\n",
       "         0.2026]])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_some_noise(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining generator class\n",
    "\n",
    "class generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, out):\n",
    "        \n",
    "        super(generator, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                 nn.Linear(inp,300),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(300,1000),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(1000,800),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(800,out),\n",
    "                                 nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining discriminator class\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, out):\n",
    "        \n",
    "        super(discriminator, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                 nn.Linear(inp,300),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(300,300),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(300,200),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(200,out),\n",
    "                                 nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = discriminator(ratings.shape[1], 1)\n",
    "gen = generator(100, ratings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=1682, out_features=300, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=300, out_features=200, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=200, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=300, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=300, out_features=1000, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=1000, out_features=800, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=800, out_features=1682, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_steps = 300\n",
    "g_steps = 300\n",
    "\n",
    "criteriond1 = nn.BCELoss()\n",
    "optimizerd1 = optim.SGD(dis.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "criteriond2 = nn.BCELoss()\n",
    "optimizerd2 = optim.SGD(gen.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# printing_steps = 200\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]\n",
    "    \n",
    "get_random_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1. MSE distance between random real and fake samples 1286161.0\n",
      "Epoch number 2. MSE distance between random real and fake samples 1297580.625\n",
      "Epoch number 3. MSE distance between random real and fake samples 1257361.875\n",
      "Epoch number 4. MSE distance between random real and fake samples 1239742.75\n",
      "Epoch number 5. MSE distance between random real and fake samples 1117233.5\n",
      "Epoch number 6. MSE distance between random real and fake samples 981990.3125\n",
      "Epoch number 7. MSE distance between random real and fake samples 813074.25\n",
      "Epoch number 8. MSE distance between random real and fake samples 682110.9375\n",
      "Epoch number 9. MSE distance between random real and fake samples 566130.5625\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "eval_losses = []\n",
    "for epoch in range(10):\n",
    "#     print (epoch)\n",
    "\n",
    "    # training discriminator\n",
    "    gen.train()\n",
    "    dis.train()\n",
    "    for d_step in range(d_steps):\n",
    "        dis.zero_grad()\n",
    "        \n",
    "        # training discriminator on real data\n",
    "        real_rows = get_random_batch(train, batch_size)\n",
    "        discriminator_real_outputs = dis(real_rows)\n",
    "   \n",
    "        dis_real_loss = criteriond1(discriminator_real_outputs, Variable(torch.ones(batch_size,1)))\n",
    "    \n",
    "        dis_real_loss.backward()\n",
    "\n",
    "        # training discriminator on data produced by generator\n",
    "        z_vector = make_some_noise(batch_size)\n",
    "        #output from generator is generated        \n",
    "        fake_rows = gen(z_vector).detach()\n",
    "#         print(fake_rows[:20])\n",
    "        dis_fake_out = dis(fake_rows)\n",
    "        dis_fake_loss = criteriond1(dis_fake_out, Variable(torch.zeros(batch_size,1)))\n",
    "        dis_fake_loss.backward()\n",
    "\n",
    "        optimizerd1.step()\n",
    "        \n",
    "    # training generator\n",
    "    for g_step in range(g_steps):\n",
    "        gen.zero_grad()\n",
    "        \n",
    "        #generating data for input for generator\n",
    "        z_vector = make_some_noise(batch_size)\n",
    "        \n",
    "        fake_rows = gen(z_vector)\n",
    "#         print(fake_rows.shape, z_vector.shape)\n",
    "#         print(fake_rows[:20])\n",
    "        dis_out_gen_training = dis(fake_rows)\n",
    "        gen_loss = criteriond2(dis_out_gen_training, Variable(torch.ones(batch_size,1)))\n",
    "        gen_loss.backward()\n",
    "        \n",
    "        optimizerd2.step()\n",
    "\n",
    "    # evaluation\n",
    "    if epoch % 10: # todo- to change\n",
    "        gen.eval()\n",
    "        z_vector_eval = make_some_noise(128)\n",
    "        fake_rows_eval = gen(z_vector_eval)\n",
    "        real_rows_eval = get_random_batch(train, 128)\n",
    "#         print(fake_rows[0][:10]) enable to see some results\n",
    "        eval_loss = F.mse_loss(fake_rows_eval, real_rows_eval, reduction='sum')\n",
    "        eval_losses.append(eval_loss)\n",
    "        print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))\n",
    "#         print('Epoch number {}. L1 distance between random real and fake samples {}'.format(epoch, torch.sum(torch.abs(fake_rows_eval - real_rows_eval))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vector = make_some_noise(16)\n",
    "fake_rows = gen(z_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9384, 2.0510, 2.0211, 4.0023, 0.6754, 0.1114, 4.9678, 4.7714, 4.8160],\n",
       "        [4.9397, 2.0485, 2.0191, 4.0074, 0.6688, 0.1109, 4.9686, 4.7733, 4.8177],\n",
       "        [4.9389, 2.0423, 2.0070, 4.0117, 0.6723, 0.1116, 4.9681, 4.7724, 4.8184],\n",
       "        [4.9389, 2.0523, 2.0147, 4.0046, 0.6737, 0.1110, 4.9678, 4.7735, 4.8175],\n",
       "        [4.9396, 2.0421, 2.0103, 4.0109, 0.6721, 0.1100, 4.9689, 4.7744, 4.8187],\n",
       "        [4.9393, 2.0508, 2.0170, 4.0109, 0.6668, 0.1112, 4.9684, 4.7729, 4.8177],\n",
       "        [4.9398, 2.0380, 2.0102, 4.0165, 0.6683, 0.1093, 4.9688, 4.7744, 4.8201],\n",
       "        [4.9397, 2.0419, 2.0164, 4.0165, 0.6701, 0.1106, 4.9689, 4.7755, 4.8203],\n",
       "        [4.9387, 2.0582, 2.0154, 4.0064, 0.6702, 0.1116, 4.9678, 4.7723, 4.8177],\n",
       "        [4.9377, 2.0616, 2.0197, 4.0116, 0.6763, 0.1131, 4.9671, 4.7697, 4.8149],\n",
       "        [4.9390, 2.0456, 2.0093, 4.0066, 0.6786, 0.1118, 4.9679, 4.7713, 4.8184],\n",
       "        [4.9383, 2.0484, 2.0099, 4.0102, 0.6814, 0.1126, 4.9678, 4.7715, 4.8152],\n",
       "        [4.9384, 2.0467, 2.0242, 4.0071, 0.6754, 0.1114, 4.9679, 4.7722, 4.8171],\n",
       "        [4.9394, 2.0328, 2.0167, 4.0151, 0.6719, 0.1098, 4.9686, 4.7734, 4.8198],\n",
       "        [4.9391, 2.0451, 2.0228, 4.0117, 0.6713, 0.1115, 4.9681, 4.7735, 4.8175],\n",
       "        [4.9388, 2.0450, 2.0164, 4.0118, 0.6747, 0.1118, 4.9683, 4.7703, 4.8167]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we see generator produces very similar vectors \n",
    "fake_rows[:,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from os.path import isfile, isdir, join\n",
    "import os\n",
    "# from tensorboard_logger import configure, log_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20c79f43930>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 100\n",
    "cuda = True\n",
    "epochs = 1000\n",
    "device = 5\n",
    "seed = 42\n",
    "nz = 100\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 1e-2 # constant for L2 penalty (diversity)\n",
    "name = \"mnist-experiment\"\n",
    "# configure(\"runs/run-\" + args.name, flush_secs=5)\n",
    "torch.manual_seed(seed)\n",
    "# if cuda:\n",
    "# #     torch.cuda.set_device('cuda')\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "# datasets.MNIST('../data', train=True, download=True,\n",
    "# transform=transforms.Compose([transforms.ToTensor(),])), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=6\n",
    "batch_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        print(\"##########\"*40)\n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "#         if (filt == xr * xf).all():\n",
    "#             print('AAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "            \n",
    "#         print('xr & xf', xr * xf)\n",
    "#         print('filt', filt)\n",
    "#         print('xr.shape, xf.shape', xr.shape, xf.shape)\n",
    "#         print('filt.shape', filt.shape)\n",
    "        print('xr', xr)\n",
    "        print('xf', xf)\n",
    "        print('filt', filt)\n",
    "#         return filt\n",
    "#         # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "#         print(idr.shape)\n",
    "        print('idr', idr)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        print('idrx', idrx)\n",
    "#         print('idrx.shape', idrx.shape)\n",
    "#         print('idrx', idrx[10:20, 100:200])\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "#         print('xr.shape', xr.shape)\n",
    "#         print('xr', xr[10:20, 100:200])\n",
    "#         print('xr*idrx.shape', (xr*idrx).shape)\n",
    "#         print('xr*idrx', (xr*idrx)[10:20, 100:200])\n",
    "#         print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', xr * idrx == xr)\n",
    "#         for c in xr * idrx == xr:\n",
    "#             print(c)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        print('xt', xt)\n",
    "        print('xb', xb)\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "#         print(xt.shape, xb.shape)\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        print('x', x)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "#         print('xxxx', x.shape)\n",
    "#         print('x[:, :10]', x[:10, :10])\n",
    "#         print('filt', filt[:10, :10])\n",
    "        # apply filter, aggregate\n",
    "#         print('x[:, :10]', x[:10, :10])\n",
    "#         print('filt', filt[:10, :10])\n",
    "        x = filt * x\n",
    "        print('x', x)\n",
    "#         print('x[:, :10]', x[:10, :10])\n",
    "#         print('xxxx', x.shape)\n",
    "#         print('x', x[:10, :10])\n",
    "#         print(x.mean(dim = 1).shape, x.mean(dim = 1))\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        print('x', x)\n",
    "#         print('xxxx', x.shape)\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        print('x', x)\n",
    "        print(\"##########\"*40)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_vec_size = 100\n",
    "# vec_size = 1000\n",
    "\n",
    "netG = torch.nn.Sequential(\n",
    "    torch.nn.Linear(nz, 1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1024, length),\n",
    "    torch.nn.Sigmoid()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=1024, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n",
      "NetD(\n",
      "  (t1): Linear(in_features=6, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=6, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD = NetD()\n",
    "print(netG)\n",
    "print(netD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "if cuda is True:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    one, mone = one.cuda(), mone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in netD.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #\n",
    "    \n",
    "for p in netG.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRealSample(length=6):\n",
    "     return Variable(torch.IntTensor(np.random.choice([0, 1], size=(batch_size, length))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 0],\n",
       "        [1, 0, 1, 0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRealSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6]) torch.Size([5, 6])\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "xr tensor([[1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 1., 1., 1.]], device='cuda:0')\n",
      "xf tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "filt tensor([[1., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 1., 0., 1.]], device='cuda:0')\n",
      "idr tensor([1, 0, 0, 0, 1])\n",
      "idrx tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "xt tensor([[1., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 1.]], device='cuda:0')\n",
      "xb tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "x tensor([[0.0000, 0.0000, 1.1909,  ..., 0.0000, 0.0523, 0.2128],\n",
      "        [0.0000, 0.0000, 0.9391,  ..., 0.0000, 0.1477, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3715,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0646, 0.0000, 0.1769,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4637,  ..., 0.0000, 0.0609, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n",
      "x tensor([[ 0.8516, -0.8205,  0.0000, -0.0000, -0.0000, -0.5008],\n",
      "        [ 0.0000, -0.9268,  0.7131, -0.0000, -0.9805, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.4230, -0.0000, -0.9413, -0.0000],\n",
      "        [ 0.0000, -0.9588,  0.0000, -0.8685, -0.9914, -0.0000],\n",
      "        [ 0.0000, -0.9435,  0.0000, -0.5974, -0.0000, -0.8545]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "x tensor([-0.0783, -0.1990, -0.0864, -0.4698, -0.3993], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "x tensor([-0.0783,  0.1990,  0.0864,  0.4698, -0.3993], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "torch.Size([5, 6]) torch.Size([5, 6]) torch.Size([5])\n",
      "outputD.shape tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gen_iterations = 0\n",
    "for epoch in range(epochs):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter and i < len(data_loader):\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = getRealSample()\n",
    "            X = X.view(X.size(0), -1).float()\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = (Variable(netG(noisev), requires_grad=True) > 0.5).float()\n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "            print(real.shape, fake.shape)\n",
    "            out = netD(real, fake)\n",
    "            print(real.shape, fake.shape, out.shape)\n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            print('outputD.shape', outputD)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "\n",
    "#             g_iter = g_iter\n",
    "#             j = 0\n",
    "#             while j < g_iter and i < len(data_loader):\n",
    "#                 j += 1\n",
    "#             for p in netD.parameters():\n",
    "#                 p.requires_grad = False # to avoid computation\n",
    "#                 netG.zero_grad()\n",
    "#                 # load real data\n",
    "#                 i += 1\n",
    "#                 try:\n",
    "#                     X = getRealSample()\n",
    "#                 except:\n",
    "#                     continue\n",
    "#                 X = X.view(X.size(0), -1)\n",
    "#                 X = (X >= 0.5).float()\n",
    "#                 if cuda: \n",
    "#                     X = X.cuda()\n",
    "#                 real = Variable(X)\n",
    "#                 # update generator\n",
    "#                 noise = torch.randn(batch_size, nz)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "#                 noisev = Variable(noise)\n",
    "#                 fake = netG(noisev)\n",
    "#                 print(real.shape, fake.shape)\n",
    "#                 out = netD(real, fake)\n",
    "#                 outputG = torch.mean(out) + lamba * out.norm()\n",
    "#                 stdG = torch.std(out)\n",
    "#                 outputG.backward(one)\n",
    "#                 optimizerG.step()\n",
    "#                 gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f ' % (epoch, epochs, i, len(data_loader), gen_iterations, outputD.data.item(), outputG.data.item()))\n",
    "#             print('output_D', outputD.data.item(), gen_iterations)\n",
    "#             print('output_G', outputG.data.item(), gen_iterations)\n",
    "#             print('std_D', stdD.data.item(), gen_iterations)\n",
    "#             print('std_G', stdG.data.item(), gen_iterations)\n",
    "#             if gen_iterations % 100 == 0:\n",
    "#                 if not isdir('./images/{0}'.format(name)):\n",
    "#                     os.mkdir('./images/{0}'.format(name))\n",
    "#                 real = real.data[0:100,:]\n",
    "#                 real = real.view(real.size(0), 1, 28, 28)\n",
    "#                 vutils.save_image(real, './images/{0}/real_samples.png'.format(name, gen_iterations))\n",
    "#                 noise = torch.randn(min(100, batch_size), nz)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "#                 fake = netG(Variable(noise, volatile=True))\n",
    "#                 # fake = (fake.data >= 0.5).float()\n",
    "#                 R = torch.rand(fake.size())\n",
    "#                 fake = (fake.data.cpu() >= R).float()\n",
    "#                 fake = fake.view(fake.size(0), 1, 28, 28)\n",
    "#                 vutils.save_image(fake, './images/{0}/fake_samples_{1}.png'.format(name, gen_iterations))\n",
    "\n",
    "#             # do checkpointing\n",
    "#             if not isdir('./checkpoint/{0}'.format(name)):\n",
    "#                 os.mkdir('./checkpoint/{0}'.format(name))\n",
    "#             torch.save(netG.state_dict(), './checkpoint/{0}/netG_epoch_{1}.pth'.format(name, epoch))\n",
    "#             torch.save(netD.state_dict(), './checkpoint/{0}/netD_epoch_{1}.pth'.format(name, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
