{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import sys\n",
    "from dataLoader import loadData\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 5.011397123336792 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from os.path import isfile, isdir, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 0 #change\n",
    "nz = 10\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 1e-3 # constant for L2 penalty (diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available()==True:\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device =\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Dropout(p=0.5)\n",
      "    (5): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Dropout(p=0.6)\n",
      "  )\n",
      ")\n",
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "#         filt = (torch.abs((real > 0.3).float() * fake - real))/real.shape[0]\n",
    "        filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())/(fake == 0).sum()\n",
    "#         filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())\n",
    "\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "        x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz, 1024), \n",
    "                                torch.nn.ReLU(), \n",
    "                                torch.nn.Linear(1024, 1024), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.5),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.6)\n",
    "                                )\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#                                  nn.Linear(nz,1024),\n",
    "# #                                  nn.Dropout(0.3)\n",
    "#                                  nn.ReLU(),\n",
    "#                                  nn.Linear(1024,2048),\n",
    "#                                  nn.Sigmoid(),\n",
    "#                                  nn.Dropout(0.3),\n",
    "#                                  nn.Linear(2048,features_length),\n",
    "# #                                  nn.Sigmoid()\n",
    "#                                  nn.Dropout(0.5)\n",
    "#                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "#         return x\n",
    "        return x*5 # to get values in range [0,5]\n",
    "    \n",
    "# networks\n",
    "netD = NetD().to(device)\n",
    "netG = NetG().to(device)\n",
    "print(netG)\n",
    "print(netD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (one * -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in netD.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #\n",
    "    \n",
    "for p in netG.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3706)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_batch(train, batch_size=batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g): # custom loss -todo\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 300\n",
    "gen_iterations = 0\n",
    "eval_losses = []\n",
    "for epoch in range(0):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < steps_per_epoch:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter*5:\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "#             X, _ = data_iter.next()\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             print(X >= 0.5)\n",
    "# #             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             real + fake * (real == 0).float()\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            fake.requires_grad = False\n",
    "#             print(real.shape, fake.shape)\n",
    "    \n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "#             print('real', real[:10, :20])\n",
    "#             print('fake', fake[:10, :20])\n",
    "#             print(real.type(), fake.type())\n",
    "#             print(fake)\n",
    "            out = netD(real, fake)\n",
    "            \n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "#             print(out.shape)\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        g_iter = g_iter\n",
    "        j = 0\n",
    "        while j < g_iter*5:\n",
    "            j += 1\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            \n",
    "            # update generator\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            noisev = Variable(noise)\n",
    "            \n",
    "            fake = netG(noisev)\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake.requires_grad = False\n",
    "#             fake = Variable(netG(noisev)).data\n",
    "#             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "#             fake.requires_grad = True\n",
    "            \n",
    "            out = netD(real, fake)\n",
    "            outputG = torch.mean(out) + lamba * out.norm()\n",
    "            stdG = torch.std(out)\n",
    "            outputG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "#             print('output_D', outputD.item(), gen_iterations)\n",
    "#             print('output_G', outputG.item(), gen_iterations)\n",
    "#             print('std_D', stdD.item(), gen_iterations)\n",
    "#             print('std_G', stdG.item(), gen_iterations)\n",
    "            torch.save(netG.state_dict(), './netG-1m')\n",
    "            torch.save(netD.state_dict(), './netD-1m')\n",
    "            # evaluation\n",
    "            if gen_iterations % 100 == 0: # todo- to change\n",
    "#                 gen.eval()\n",
    "#                 z_vector_eval = make_some_noise(128)\n",
    "#                 fake_rows_eval = gen(z_vector_eval)\n",
    "#                 real_rows_eval = get_random_batch(train, 128)\n",
    "        #         print(fake_rows[0][:10]) enable to see some results\n",
    "#                 fake = Variable(netG(noisev).data).round()\n",
    "#                 fake = ((real != 0) & (fake != 0))\n",
    "#                 print(fake)\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                eval_losses.append(eval_loss)\n",
    "                print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(netG.state_dict(), './netG-1m')\n",
    "# torch.save(netD.state_dict(), './netD-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1787729 226310\n",
      "4 634312 348971\n",
      "3 94038 261197\n",
      "2 2991 107557\n",
      "1 5 56174\n",
      "0 19865165 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train > 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.934857739195076e-06"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[0,:] > 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [3., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.Tensor(tr.copy()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback_mask = (train > 3).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_feedback_mask = ((train < 4).to(device).float() * (1 - zero_mask)).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((positive_feedback_mask + negative_feedback_mask) != zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7090908603553212"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(negative_feedback_mask.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_autoencoder(nn.Module):\n",
    "    def __init__(self, n_users, input_size, z=512):\n",
    "        super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "#         self.V = torch.FloatTensor(z).to(device)\n",
    "        self.V = torch.FloatTensor(n_users, z).to(device)\n",
    "        self.b = torch.FloatTensor(n_users, 1).to(device)\n",
    "        self.b_shtrix = torch.FloatTensor(n_users, 1).to(device)\n",
    "    \n",
    "        self.encoder = nn.Linear(input_size, z)\n",
    "        self.decoder = nn.Linear(z, input_size)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        \n",
    "#         torch.nn.init.xavier_uniform(self.V)\n",
    "#         torch.nn.init.xavier_uniform(self.b.weight)\n",
    "#         torch.nn.init.xavier_uniform(self.b_shtrix.weight)\n",
    "\n",
    "#         self.encoder=nn.Sequential(\n",
    "#                       nn.Linear(input_size, 1024),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(1024,512),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(512, z),\n",
    "# #                       nn.Sigmoid()\n",
    "#                       )\n",
    "\n",
    "#         self.decoder=nn.Sequential(\n",
    "#                       nn.Linear(z, 512),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(512, 1024),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(1024, input_size),\n",
    "# #                       nn.Sigmoid(),\n",
    "#                       )\n",
    " \n",
    "    def forward(self, x, i):\n",
    "        z = self.encoder(x)\n",
    "#         print(z.t().shape, x.shape, self.V.shape, self.V[i, :].shape)\n",
    "#         torch.Size([32, 64]) torch.Size([64, 3706]) torch.Size([6040, 32]) torch.Size([64, 32])\n",
    "# #         torch.Size([64, 32]) torch.Size([64, 3706]) torch.Size([3706])\n",
    "#         print(self.V)\n",
    "#         print(z.t + self.V)\n",
    "#         print(self.b[i, :].shape, z.shape, self.V[i, :].shape)\n",
    "        z = z + self.V[i, :] + self.b[i, :] \n",
    "#         z = z + self.V[i, :]\n",
    "        z = torch.nn.functional.tanh(z)\n",
    "#         print(z)\n",
    "        x = self.decoder(z)\n",
    "#     torch.Size([64, 3706]) torch.Size([64, 32]) torch.Size([64, 32])\n",
    "#         print(x.shape, z.shape, self.b_shtrix[i, :].shape)\n",
    "#         print(x.t().shape, self.b_shtrix[i, :].shape)\n",
    "        x = x + self.b_shtrix[i, :]\n",
    "    \n",
    "#         return x\n",
    "    \n",
    "        return torch.nn.functional.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(10, 32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = denoising_autoencoder(input_size=train[0,:].shape[0], n_users=train.shape[0]).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_masked_batch(mat, batch_size = 32, p=0.5):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = mat[rand_rows].clone()\n",
    "    corrupted = mat[rand_rows].clone()\n",
    "    mask_arr = torch.FloatTensor((np.random.rand(orig.shape[0], orig.shape[1]) > p)).to(device)\n",
    "\n",
    "    return orig, corrupted*mask_arr, rand_rows\n",
    "\n",
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.296107663248786"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(orig.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1459457636265515"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(masked.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1459457636265515"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(torch.nn.functional.dropout(masked, training=False).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.functional.dropout(orig, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering Epoch:  0\n",
      "======> epoch: 0/400, Loss:0.5000854134559631\n",
      "======> epoch: 0/400, Loss:0.3810594975948334\n",
      "======> epoch: 0/400, Loss:0.29062819480895996\n",
      "======> epoch: 0/400, Loss:0.2653268873691559\n",
      "======> epoch: 0/400, Loss:0.22323191165924072\n",
      "======> epoch: 0/400, Loss:0.2283015251159668\n",
      "======> epoch: 0/400, Loss:0.1793147325515747\n",
      "======> epoch: 0/400, Loss:0.14210015535354614\n",
      "======> epoch: 0/400, Loss:0.145160973072052\n",
      "======> epoch: 0/400, Loss:0.12610886991024017\n",
      "Entering Epoch:  1\n",
      "======> epoch: 1/400, Loss:0.15251320600509644\n",
      "======> epoch: 1/400, Loss:0.14511314034461975\n",
      "======> epoch: 1/400, Loss:0.11607272922992706\n",
      "======> epoch: 1/400, Loss:0.13640323281288147\n",
      "======> epoch: 1/400, Loss:0.11901631951332092\n",
      "======> epoch: 1/400, Loss:0.09775959700345993\n",
      "======> epoch: 1/400, Loss:0.10340878367424011\n",
      "======> epoch: 1/400, Loss:0.10527709126472473\n",
      "======> epoch: 1/400, Loss:0.08812107145786285\n",
      "======> epoch: 1/400, Loss:0.07319626212120056\n",
      "Entering Epoch:  2\n",
      "======> epoch: 2/400, Loss:0.08121225237846375\n",
      "======> epoch: 2/400, Loss:0.07936378568410873\n",
      "======> epoch: 2/400, Loss:0.10110518336296082\n",
      "======> epoch: 2/400, Loss:0.07336869090795517\n",
      "======> epoch: 2/400, Loss:0.0939389169216156\n",
      "======> epoch: 2/400, Loss:0.06450068205595016\n",
      "======> epoch: 2/400, Loss:0.07181805372238159\n",
      "======> epoch: 2/400, Loss:0.08106161653995514\n",
      "======> epoch: 2/400, Loss:0.07311572879552841\n",
      "======> epoch: 2/400, Loss:0.062320347875356674\n",
      "Entering Epoch:  3\n",
      "======> epoch: 3/400, Loss:0.07534720003604889\n",
      "======> epoch: 3/400, Loss:0.0581309050321579\n",
      "======> epoch: 3/400, Loss:0.05996723100543022\n",
      "======> epoch: 3/400, Loss:0.06002684682607651\n",
      "======> epoch: 3/400, Loss:0.07281389832496643\n",
      "======> epoch: 3/400, Loss:0.06543552130460739\n",
      "======> epoch: 3/400, Loss:0.05618622526526451\n",
      "======> epoch: 3/400, Loss:0.054721079766750336\n",
      "======> epoch: 3/400, Loss:0.06674040853977203\n",
      "======> epoch: 3/400, Loss:0.06817206740379333\n",
      "Entering Epoch:  4\n",
      "======> epoch: 4/400, Loss:0.06451233476400375\n",
      "======> epoch: 4/400, Loss:0.054568320512771606\n",
      "======> epoch: 4/400, Loss:0.05419371649622917\n",
      "======> epoch: 4/400, Loss:0.03479919955134392\n",
      "======> epoch: 4/400, Loss:0.06230650097131729\n",
      "======> epoch: 4/400, Loss:0.041012637317180634\n",
      "======> epoch: 4/400, Loss:0.04331178963184357\n",
      "======> epoch: 4/400, Loss:0.042635172605514526\n",
      "======> epoch: 4/400, Loss:0.05189434811472893\n",
      "======> epoch: 4/400, Loss:0.04103337973356247\n",
      "Entering Epoch:  5\n",
      "======> epoch: 5/400, Loss:0.04478880763053894\n",
      "======> epoch: 5/400, Loss:0.045137885957956314\n",
      "======> epoch: 5/400, Loss:0.053164735436439514\n",
      "======> epoch: 5/400, Loss:0.034614916890859604\n",
      "======> epoch: 5/400, Loss:0.05626900866627693\n",
      "======> epoch: 5/400, Loss:0.042546723037958145\n",
      "======> epoch: 5/400, Loss:0.03855348378419876\n",
      "======> epoch: 5/400, Loss:0.04836365953087807\n",
      "======> epoch: 5/400, Loss:0.036463119089603424\n",
      "======> epoch: 5/400, Loss:0.047038573771715164\n",
      "Entering Epoch:  6\n",
      "======> epoch: 6/400, Loss:0.04343230277299881\n",
      "======> epoch: 6/400, Loss:0.04106421396136284\n",
      "======> epoch: 6/400, Loss:0.04179614409804344\n",
      "======> epoch: 6/400, Loss:0.038039010018110275\n",
      "======> epoch: 6/400, Loss:0.04509910196065903\n",
      "======> epoch: 6/400, Loss:0.05287010595202446\n",
      "======> epoch: 6/400, Loss:0.04153958708047867\n",
      "======> epoch: 6/400, Loss:0.04357358440756798\n",
      "======> epoch: 6/400, Loss:0.034842781722545624\n",
      "======> epoch: 6/400, Loss:0.038896843791007996\n",
      "Entering Epoch:  7\n",
      "======> epoch: 7/400, Loss:0.03285041078925133\n",
      "======> epoch: 7/400, Loss:0.04755468666553497\n",
      "======> epoch: 7/400, Loss:0.03644919395446777\n",
      "======> epoch: 7/400, Loss:0.03909171000123024\n",
      "======> epoch: 7/400, Loss:0.03799038380384445\n",
      "======> epoch: 7/400, Loss:0.04056090861558914\n",
      "======> epoch: 7/400, Loss:0.04053175076842308\n",
      "======> epoch: 7/400, Loss:0.03205987811088562\n",
      "======> epoch: 7/400, Loss:0.042944565415382385\n",
      "======> epoch: 7/400, Loss:0.03385024517774582\n",
      "Entering Epoch:  8\n",
      "======> epoch: 8/400, Loss:0.036740001291036606\n",
      "======> epoch: 8/400, Loss:0.03551323711872101\n",
      "======> epoch: 8/400, Loss:0.03230317309498787\n",
      "======> epoch: 8/400, Loss:0.033877525478601456\n",
      "======> epoch: 8/400, Loss:0.035172298550605774\n",
      "======> epoch: 8/400, Loss:0.03972673788666725\n",
      "======> epoch: 8/400, Loss:0.03061240166425705\n",
      "======> epoch: 8/400, Loss:0.03980641812086105\n",
      "======> epoch: 8/400, Loss:0.02691975235939026\n",
      "======> epoch: 8/400, Loss:0.044763676822185516\n",
      "Entering Epoch:  9\n",
      "======> epoch: 9/400, Loss:0.03059227392077446\n",
      "======> epoch: 9/400, Loss:0.02424764074385166\n",
      "======> epoch: 9/400, Loss:0.035718709230422974\n",
      "======> epoch: 9/400, Loss:0.03299212455749512\n",
      "======> epoch: 9/400, Loss:0.029160741716623306\n",
      "======> epoch: 9/400, Loss:0.03367229551076889\n",
      "======> epoch: 9/400, Loss:0.030308442190289497\n",
      "======> epoch: 9/400, Loss:0.035593681037425995\n",
      "======> epoch: 9/400, Loss:0.036561813205480576\n",
      "======> epoch: 9/400, Loss:0.033010177314281464\n",
      "Entering Epoch:  10\n",
      "======> epoch: 10/400, Loss:0.0319889597594738\n",
      "======> epoch: 10/400, Loss:0.04285350441932678\n",
      "======> epoch: 10/400, Loss:0.02792629785835743\n",
      "======> epoch: 10/400, Loss:0.030844205990433693\n",
      "======> epoch: 10/400, Loss:0.03351660817861557\n",
      "======> epoch: 10/400, Loss:0.033546943217515945\n",
      "======> epoch: 10/400, Loss:0.03301995247602463\n",
      "======> epoch: 10/400, Loss:0.032658450305461884\n",
      "======> epoch: 10/400, Loss:0.025539204478263855\n",
      "======> epoch: 10/400, Loss:0.033862028270959854\n",
      "Entering Epoch:  11\n",
      "======> epoch: 11/400, Loss:0.024933552369475365\n",
      "======> epoch: 11/400, Loss:0.03322076052427292\n",
      "======> epoch: 11/400, Loss:0.026042208075523376\n",
      "======> epoch: 11/400, Loss:0.029577599838376045\n",
      "======> epoch: 11/400, Loss:0.0337362103164196\n",
      "======> epoch: 11/400, Loss:0.027892204001545906\n",
      "======> epoch: 11/400, Loss:0.031141560524702072\n",
      "======> epoch: 11/400, Loss:0.03207994997501373\n",
      "======> epoch: 11/400, Loss:0.03919157013297081\n",
      "======> epoch: 11/400, Loss:0.030434096232056618\n",
      "Entering Epoch:  12\n",
      "======> epoch: 12/400, Loss:0.03871509060263634\n",
      "======> epoch: 12/400, Loss:0.027101753279566765\n",
      "======> epoch: 12/400, Loss:0.029079418629407883\n",
      "======> epoch: 12/400, Loss:0.03025357984006405\n",
      "======> epoch: 12/400, Loss:0.026421304792165756\n",
      "======> epoch: 12/400, Loss:0.03269098326563835\n",
      "======> epoch: 12/400, Loss:0.025051521137356758\n",
      "======> epoch: 12/400, Loss:0.025168471038341522\n",
      "======> epoch: 12/400, Loss:0.01966494508087635\n",
      "======> epoch: 12/400, Loss:0.02545640803873539\n",
      "Entering Epoch:  13\n",
      "======> epoch: 13/400, Loss:0.029816677793860435\n",
      "======> epoch: 13/400, Loss:0.02817833423614502\n",
      "======> epoch: 13/400, Loss:0.02455938793718815\n",
      "======> epoch: 13/400, Loss:0.030855737626552582\n",
      "======> epoch: 13/400, Loss:0.021541617810726166\n",
      "======> epoch: 13/400, Loss:0.021742692217230797\n",
      "======> epoch: 13/400, Loss:0.02048630453646183\n",
      "======> epoch: 13/400, Loss:0.02686443366110325\n",
      "======> epoch: 13/400, Loss:0.03329864889383316\n",
      "======> epoch: 13/400, Loss:0.02769569121301174\n",
      "Entering Epoch:  14\n",
      "======> epoch: 14/400, Loss:0.023839762434363365\n",
      "======> epoch: 14/400, Loss:0.028495971113443375\n",
      "======> epoch: 14/400, Loss:0.033771105110645294\n",
      "======> epoch: 14/400, Loss:0.03787560760974884\n",
      "======> epoch: 14/400, Loss:0.028571538627147675\n",
      "======> epoch: 14/400, Loss:0.03466685861349106\n",
      "======> epoch: 14/400, Loss:0.02783619984984398\n",
      "======> epoch: 14/400, Loss:0.0294679943472147\n",
      "======> epoch: 14/400, Loss:0.034755557775497437\n",
      "======> epoch: 14/400, Loss:0.03134869039058685\n",
      "Entering Epoch:  15\n",
      "======> epoch: 15/400, Loss:0.02579445205628872\n",
      "======> epoch: 15/400, Loss:0.029065769165754318\n",
      "======> epoch: 15/400, Loss:0.026395633816719055\n",
      "======> epoch: 15/400, Loss:0.02515501156449318\n",
      "======> epoch: 15/400, Loss:0.028070276603102684\n",
      "======> epoch: 15/400, Loss:0.02819579839706421\n",
      "======> epoch: 15/400, Loss:0.030207790434360504\n",
      "======> epoch: 15/400, Loss:0.022916877642273903\n",
      "======> epoch: 15/400, Loss:0.02373110130429268\n",
      "======> epoch: 15/400, Loss:0.023561453446745872\n",
      "Entering Epoch:  16\n",
      "======> epoch: 16/400, Loss:0.02596316859126091\n",
      "======> epoch: 16/400, Loss:0.030670087784528732\n",
      "======> epoch: 16/400, Loss:0.032550085335969925\n",
      "======> epoch: 16/400, Loss:0.03433592617511749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 16/400, Loss:0.02950158342719078\n",
      "======> epoch: 16/400, Loss:0.026425769552588463\n",
      "======> epoch: 16/400, Loss:0.025863241404294968\n",
      "======> epoch: 16/400, Loss:0.02730412222445011\n",
      "======> epoch: 16/400, Loss:0.029536927118897438\n",
      "======> epoch: 16/400, Loss:0.02263629250228405\n",
      "Entering Epoch:  17\n",
      "======> epoch: 17/400, Loss:0.029974181205034256\n",
      "======> epoch: 17/400, Loss:0.020587721839547157\n",
      "======> epoch: 17/400, Loss:0.022550029680132866\n",
      "======> epoch: 17/400, Loss:0.02379276230931282\n",
      "======> epoch: 17/400, Loss:0.02510671503841877\n",
      "======> epoch: 17/400, Loss:0.026822732761502266\n",
      "======> epoch: 17/400, Loss:0.025463605299592018\n",
      "======> epoch: 17/400, Loss:0.022749627009034157\n",
      "======> epoch: 17/400, Loss:0.02617516554892063\n",
      "======> epoch: 17/400, Loss:0.021802935749292374\n",
      "Entering Epoch:  18\n",
      "======> epoch: 18/400, Loss:0.026199379935860634\n",
      "======> epoch: 18/400, Loss:0.02502358704805374\n",
      "======> epoch: 18/400, Loss:0.01830233260989189\n",
      "======> epoch: 18/400, Loss:0.028837472200393677\n",
      "======> epoch: 18/400, Loss:0.023757126182317734\n",
      "======> epoch: 18/400, Loss:0.027729906141757965\n",
      "======> epoch: 18/400, Loss:0.022118009626865387\n",
      "======> epoch: 18/400, Loss:0.01915125921368599\n",
      "======> epoch: 18/400, Loss:0.020533310249447823\n",
      "======> epoch: 18/400, Loss:0.026201432570815086\n",
      "Entering Epoch:  19\n",
      "======> epoch: 19/400, Loss:0.028605274856090546\n",
      "======> epoch: 19/400, Loss:0.028343183919787407\n",
      "======> epoch: 19/400, Loss:0.02954125590622425\n",
      "======> epoch: 19/400, Loss:0.02709699049592018\n",
      "======> epoch: 19/400, Loss:0.0289364755153656\n",
      "======> epoch: 19/400, Loss:0.029460454359650612\n",
      "======> epoch: 19/400, Loss:0.026274247094988823\n",
      "======> epoch: 19/400, Loss:0.03259006887674332\n",
      "======> epoch: 19/400, Loss:0.029066404327750206\n",
      "======> epoch: 19/400, Loss:0.0173366516828537\n",
      "Entering Epoch:  20\n",
      "======> epoch: 20/400, Loss:0.024791618809103966\n",
      "======> epoch: 20/400, Loss:0.02176705002784729\n",
      "======> epoch: 20/400, Loss:0.02145436778664589\n",
      "======> epoch: 20/400, Loss:0.023288628086447716\n",
      "======> epoch: 20/400, Loss:0.025669634342193604\n",
      "======> epoch: 20/400, Loss:0.02054859697818756\n",
      "======> epoch: 20/400, Loss:0.02325393632054329\n",
      "======> epoch: 20/400, Loss:0.031902726739645004\n",
      "======> epoch: 20/400, Loss:0.020366651937365532\n",
      "======> epoch: 20/400, Loss:0.022132735699415207\n",
      "Entering Epoch:  21\n",
      "======> epoch: 21/400, Loss:0.026266321539878845\n",
      "======> epoch: 21/400, Loss:0.030148884281516075\n",
      "======> epoch: 21/400, Loss:0.028848355636000633\n",
      "======> epoch: 21/400, Loss:0.020893514156341553\n",
      "======> epoch: 21/400, Loss:0.030054880306124687\n",
      "======> epoch: 21/400, Loss:0.025982514023780823\n",
      "======> epoch: 21/400, Loss:0.022893955931067467\n",
      "======> epoch: 21/400, Loss:0.02371666021645069\n",
      "======> epoch: 21/400, Loss:0.02156423218548298\n",
      "======> epoch: 21/400, Loss:0.024907946586608887\n",
      "Entering Epoch:  22\n",
      "======> epoch: 22/400, Loss:0.024776270613074303\n",
      "======> epoch: 22/400, Loss:0.028345119208097458\n",
      "======> epoch: 22/400, Loss:0.028322551399469376\n",
      "======> epoch: 22/400, Loss:0.027699565514922142\n",
      "======> epoch: 22/400, Loss:0.02420984022319317\n",
      "======> epoch: 22/400, Loss:0.019247813150286674\n",
      "======> epoch: 22/400, Loss:0.030997855588793755\n",
      "======> epoch: 22/400, Loss:0.01619253121316433\n",
      "======> epoch: 22/400, Loss:0.021493857726454735\n",
      "======> epoch: 22/400, Loss:0.02283761091530323\n",
      "Entering Epoch:  23\n",
      "======> epoch: 23/400, Loss:0.02168208174407482\n",
      "======> epoch: 23/400, Loss:0.020315108820796013\n",
      "======> epoch: 23/400, Loss:0.027431271970272064\n",
      "======> epoch: 23/400, Loss:0.021003367379307747\n",
      "======> epoch: 23/400, Loss:0.03244645148515701\n",
      "======> epoch: 23/400, Loss:0.016351910308003426\n",
      "======> epoch: 23/400, Loss:0.02036009170114994\n",
      "======> epoch: 23/400, Loss:0.02991553768515587\n",
      "======> epoch: 23/400, Loss:0.028332248330116272\n",
      "======> epoch: 23/400, Loss:0.024085063487291336\n",
      "Entering Epoch:  24\n",
      "======> epoch: 24/400, Loss:0.017757736146450043\n",
      "======> epoch: 24/400, Loss:0.021441075950860977\n",
      "======> epoch: 24/400, Loss:0.02205745317041874\n",
      "======> epoch: 24/400, Loss:0.017392808571457863\n",
      "======> epoch: 24/400, Loss:0.033001624047756195\n",
      "======> epoch: 24/400, Loss:0.023968443274497986\n",
      "======> epoch: 24/400, Loss:0.02232290990650654\n",
      "======> epoch: 24/400, Loss:0.024479826912283897\n",
      "======> epoch: 24/400, Loss:0.020853260532021523\n",
      "======> epoch: 24/400, Loss:0.025930255651474\n",
      "Entering Epoch:  25\n",
      "======> epoch: 25/400, Loss:0.032103948295116425\n",
      "======> epoch: 25/400, Loss:0.023685773834586143\n",
      "======> epoch: 25/400, Loss:0.03011842630803585\n",
      "======> epoch: 25/400, Loss:0.02169732190668583\n",
      "======> epoch: 25/400, Loss:0.022254306823015213\n",
      "======> epoch: 25/400, Loss:0.022925568744540215\n",
      "======> epoch: 25/400, Loss:0.022512849420309067\n",
      "======> epoch: 25/400, Loss:0.021918004378676414\n",
      "======> epoch: 25/400, Loss:0.022471429780125618\n",
      "======> epoch: 25/400, Loss:0.022140322253108025\n",
      "Entering Epoch:  26\n",
      "======> epoch: 26/400, Loss:0.02161104418337345\n",
      "======> epoch: 26/400, Loss:0.020285379141569138\n",
      "======> epoch: 26/400, Loss:0.02339753322303295\n",
      "======> epoch: 26/400, Loss:0.024053139612078667\n",
      "======> epoch: 26/400, Loss:0.022763866931200027\n",
      "======> epoch: 26/400, Loss:0.01792646385729313\n",
      "======> epoch: 26/400, Loss:0.027327803894877434\n",
      "======> epoch: 26/400, Loss:0.023481344804167747\n",
      "======> epoch: 26/400, Loss:0.0331454798579216\n",
      "======> epoch: 26/400, Loss:0.018765388056635857\n",
      "Entering Epoch:  27\n",
      "======> epoch: 27/400, Loss:0.025839896872639656\n",
      "======> epoch: 27/400, Loss:0.016810016706585884\n",
      "======> epoch: 27/400, Loss:0.027189157903194427\n",
      "======> epoch: 27/400, Loss:0.022327085956931114\n",
      "======> epoch: 27/400, Loss:0.022183837369084358\n",
      "======> epoch: 27/400, Loss:0.024994991719722748\n",
      "======> epoch: 27/400, Loss:0.017939554527401924\n",
      "======> epoch: 27/400, Loss:0.025447983294725418\n",
      "======> epoch: 27/400, Loss:0.022912051528692245\n",
      "======> epoch: 27/400, Loss:0.026519911363720894\n",
      "Entering Epoch:  28\n",
      "======> epoch: 28/400, Loss:0.018412180244922638\n",
      "======> epoch: 28/400, Loss:0.024132337421178818\n",
      "======> epoch: 28/400, Loss:0.021612530574202538\n",
      "======> epoch: 28/400, Loss:0.02500881627202034\n",
      "======> epoch: 28/400, Loss:0.024666907265782356\n",
      "======> epoch: 28/400, Loss:0.018441393971443176\n",
      "======> epoch: 28/400, Loss:0.02200869657099247\n",
      "======> epoch: 28/400, Loss:0.027939448133111\n",
      "======> epoch: 28/400, Loss:0.025033563375473022\n",
      "======> epoch: 28/400, Loss:0.02289906144142151\n",
      "Entering Epoch:  29\n",
      "======> epoch: 29/400, Loss:0.021556122228503227\n",
      "======> epoch: 29/400, Loss:0.02880118414759636\n",
      "======> epoch: 29/400, Loss:0.02347038872539997\n",
      "======> epoch: 29/400, Loss:0.021973608061671257\n",
      "======> epoch: 29/400, Loss:0.022031575441360474\n",
      "======> epoch: 29/400, Loss:0.017930185422301292\n",
      "======> epoch: 29/400, Loss:0.023987125605344772\n",
      "======> epoch: 29/400, Loss:0.021886009722948074\n",
      "======> epoch: 29/400, Loss:0.018063589930534363\n",
      "======> epoch: 29/400, Loss:0.020311489701271057\n",
      "Entering Epoch:  30\n",
      "======> epoch: 30/400, Loss:0.016053417697548866\n",
      "======> epoch: 30/400, Loss:0.02323342114686966\n",
      "======> epoch: 30/400, Loss:0.013800625689327717\n",
      "======> epoch: 30/400, Loss:0.022612018510699272\n",
      "======> epoch: 30/400, Loss:0.02032683975994587\n",
      "======> epoch: 30/400, Loss:0.019913310185074806\n",
      "======> epoch: 30/400, Loss:0.015075583942234516\n",
      "======> epoch: 30/400, Loss:0.019853273406624794\n",
      "======> epoch: 30/400, Loss:0.027271296828985214\n",
      "======> epoch: 30/400, Loss:0.019143663346767426\n",
      "Entering Epoch:  31\n",
      "======> epoch: 31/400, Loss:0.019957229495048523\n",
      "======> epoch: 31/400, Loss:0.013860264793038368\n",
      "======> epoch: 31/400, Loss:0.02461634762585163\n",
      "======> epoch: 31/400, Loss:0.02057400718331337\n",
      "======> epoch: 31/400, Loss:0.0277514960616827\n",
      "======> epoch: 31/400, Loss:0.022723905742168427\n",
      "======> epoch: 31/400, Loss:0.02439918741583824\n",
      "======> epoch: 31/400, Loss:0.021384047344326973\n",
      "======> epoch: 31/400, Loss:0.024702824652194977\n",
      "======> epoch: 31/400, Loss:0.02389000542461872\n",
      "Entering Epoch:  32\n",
      "======> epoch: 32/400, Loss:0.022031759843230247\n",
      "======> epoch: 32/400, Loss:0.019153865054249763\n",
      "======> epoch: 32/400, Loss:0.02298460341989994\n",
      "======> epoch: 32/400, Loss:0.01716449297964573\n",
      "======> epoch: 32/400, Loss:0.025802131742239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 32/400, Loss:0.017857424914836884\n",
      "======> epoch: 32/400, Loss:0.026990648359060287\n",
      "======> epoch: 32/400, Loss:0.019545577466487885\n",
      "======> epoch: 32/400, Loss:0.02297106198966503\n",
      "======> epoch: 32/400, Loss:0.023300353437662125\n",
      "Entering Epoch:  33\n",
      "======> epoch: 33/400, Loss:0.018538210541009903\n",
      "======> epoch: 33/400, Loss:0.02129737101495266\n",
      "======> epoch: 33/400, Loss:0.02286544442176819\n",
      "======> epoch: 33/400, Loss:0.016625847667455673\n",
      "======> epoch: 33/400, Loss:0.020380644127726555\n",
      "======> epoch: 33/400, Loss:0.013837490230798721\n",
      "======> epoch: 33/400, Loss:0.019706420600414276\n",
      "======> epoch: 33/400, Loss:0.016730381175875664\n",
      "======> epoch: 33/400, Loss:0.015835268422961235\n",
      "======> epoch: 33/400, Loss:0.02181277796626091\n",
      "Entering Epoch:  34\n",
      "======> epoch: 34/400, Loss:0.019986776635050774\n",
      "======> epoch: 34/400, Loss:0.020810186862945557\n",
      "======> epoch: 34/400, Loss:0.024726631119847298\n",
      "======> epoch: 34/400, Loss:0.016817623749375343\n",
      "======> epoch: 34/400, Loss:0.016743836924433708\n",
      "======> epoch: 34/400, Loss:0.019850214943289757\n",
      "======> epoch: 34/400, Loss:0.021061353385448456\n",
      "======> epoch: 34/400, Loss:0.020690998062491417\n",
      "======> epoch: 34/400, Loss:0.023595159873366356\n",
      "======> epoch: 34/400, Loss:0.026366086676716805\n",
      "Entering Epoch:  35\n",
      "======> epoch: 35/400, Loss:0.019314639270305634\n",
      "======> epoch: 35/400, Loss:0.025430332869291306\n",
      "======> epoch: 35/400, Loss:0.02721101976931095\n",
      "======> epoch: 35/400, Loss:0.025261852890253067\n",
      "======> epoch: 35/400, Loss:0.016479317098855972\n",
      "======> epoch: 35/400, Loss:0.022164618596434593\n",
      "======> epoch: 35/400, Loss:0.02128850854933262\n",
      "======> epoch: 35/400, Loss:0.02398945763707161\n",
      "======> epoch: 35/400, Loss:0.016213376075029373\n",
      "======> epoch: 35/400, Loss:0.020819511264562607\n",
      "Entering Epoch:  36\n",
      "======> epoch: 36/400, Loss:0.019656728953123093\n",
      "======> epoch: 36/400, Loss:0.018547454848885536\n",
      "======> epoch: 36/400, Loss:0.021300358697772026\n",
      "======> epoch: 36/400, Loss:0.020743200555443764\n",
      "======> epoch: 36/400, Loss:0.02341376803815365\n",
      "======> epoch: 36/400, Loss:0.018502995371818542\n",
      "======> epoch: 36/400, Loss:0.025927310809493065\n",
      "======> epoch: 36/400, Loss:0.021422982215881348\n",
      "======> epoch: 36/400, Loss:0.02170655131340027\n",
      "======> epoch: 36/400, Loss:0.019018201157450676\n",
      "Entering Epoch:  37\n",
      "======> epoch: 37/400, Loss:0.019617561250925064\n",
      "======> epoch: 37/400, Loss:0.022129345685243607\n",
      "======> epoch: 37/400, Loss:0.025332927703857422\n",
      "======> epoch: 37/400, Loss:0.019296256825327873\n",
      "======> epoch: 37/400, Loss:0.018988393247127533\n",
      "======> epoch: 37/400, Loss:0.02424316480755806\n",
      "======> epoch: 37/400, Loss:0.018300043419003487\n",
      "======> epoch: 37/400, Loss:0.021794985979795456\n",
      "======> epoch: 37/400, Loss:0.017154579982161522\n",
      "======> epoch: 37/400, Loss:0.02188008278608322\n",
      "Entering Epoch:  38\n",
      "======> epoch: 38/400, Loss:0.022199712693691254\n",
      "======> epoch: 38/400, Loss:0.023719942197203636\n",
      "======> epoch: 38/400, Loss:0.01801781915128231\n",
      "======> epoch: 38/400, Loss:0.020713943988084793\n",
      "======> epoch: 38/400, Loss:0.01867137849330902\n",
      "======> epoch: 38/400, Loss:0.01911335252225399\n",
      "======> epoch: 38/400, Loss:0.018304364755749702\n",
      "======> epoch: 38/400, Loss:0.01617441698908806\n",
      "======> epoch: 38/400, Loss:0.019485075026750565\n",
      "======> epoch: 38/400, Loss:0.023612968623638153\n",
      "Entering Epoch:  39\n",
      "======> epoch: 39/400, Loss:0.020509839057922363\n",
      "======> epoch: 39/400, Loss:0.02037150226533413\n",
      "======> epoch: 39/400, Loss:0.026625704020261765\n",
      "======> epoch: 39/400, Loss:0.018626222386956215\n",
      "======> epoch: 39/400, Loss:0.02660493552684784\n",
      "======> epoch: 39/400, Loss:0.023033451288938522\n",
      "======> epoch: 39/400, Loss:0.019534004852175713\n",
      "======> epoch: 39/400, Loss:0.02241814136505127\n",
      "======> epoch: 39/400, Loss:0.019991504028439522\n",
      "======> epoch: 39/400, Loss:0.024016976356506348\n",
      "Entering Epoch:  40\n",
      "======> epoch: 40/400, Loss:0.02518749237060547\n",
      "======> epoch: 40/400, Loss:0.022459743544459343\n",
      "======> epoch: 40/400, Loss:0.02304857224225998\n",
      "======> epoch: 40/400, Loss:0.021633844822645187\n",
      "======> epoch: 40/400, Loss:0.022459423169493675\n",
      "======> epoch: 40/400, Loss:0.020574653521180153\n",
      "======> epoch: 40/400, Loss:0.020632658153772354\n",
      "======> epoch: 40/400, Loss:0.020924605429172516\n",
      "======> epoch: 40/400, Loss:0.02076282724738121\n",
      "======> epoch: 40/400, Loss:0.016650451347231865\n",
      "Entering Epoch:  41\n",
      "======> epoch: 41/400, Loss:0.013774174265563488\n",
      "======> epoch: 41/400, Loss:0.018969610333442688\n",
      "======> epoch: 41/400, Loss:0.017802460119128227\n",
      "======> epoch: 41/400, Loss:0.021027909591794014\n",
      "======> epoch: 41/400, Loss:0.025483902543783188\n",
      "======> epoch: 41/400, Loss:0.01575113832950592\n",
      "======> epoch: 41/400, Loss:0.023263758048415184\n",
      "======> epoch: 41/400, Loss:0.02585260011255741\n",
      "======> epoch: 41/400, Loss:0.01765022799372673\n",
      "======> epoch: 41/400, Loss:0.014418618753552437\n",
      "Entering Epoch:  42\n",
      "======> epoch: 42/400, Loss:0.016022834926843643\n",
      "======> epoch: 42/400, Loss:0.020368313416838646\n",
      "======> epoch: 42/400, Loss:0.01900128275156021\n",
      "======> epoch: 42/400, Loss:0.0230878833681345\n",
      "======> epoch: 42/400, Loss:0.02085736021399498\n",
      "======> epoch: 42/400, Loss:0.017377667129039764\n",
      "======> epoch: 42/400, Loss:0.019493501633405685\n",
      "======> epoch: 42/400, Loss:0.015871450304985046\n",
      "======> epoch: 42/400, Loss:0.02010928839445114\n",
      "======> epoch: 42/400, Loss:0.0169058907777071\n",
      "Entering Epoch:  43\n",
      "======> epoch: 43/400, Loss:0.01809282787144184\n",
      "======> epoch: 43/400, Loss:0.021336279809474945\n",
      "======> epoch: 43/400, Loss:0.021516941487789154\n",
      "======> epoch: 43/400, Loss:0.019107449799776077\n",
      "======> epoch: 43/400, Loss:0.017661286517977715\n",
      "======> epoch: 43/400, Loss:0.02217978984117508\n",
      "======> epoch: 43/400, Loss:0.02005794085562229\n",
      "======> epoch: 43/400, Loss:0.020612232387065887\n",
      "======> epoch: 43/400, Loss:0.018168598413467407\n",
      "======> epoch: 43/400, Loss:0.02200692892074585\n",
      "Entering Epoch:  44\n",
      "======> epoch: 44/400, Loss:0.02043871209025383\n",
      "======> epoch: 44/400, Loss:0.016858357936143875\n",
      "======> epoch: 44/400, Loss:0.01807759515941143\n",
      "======> epoch: 44/400, Loss:0.018624255433678627\n",
      "======> epoch: 44/400, Loss:0.018411900848150253\n",
      "======> epoch: 44/400, Loss:0.016882995143532753\n",
      "======> epoch: 44/400, Loss:0.021074946969747543\n",
      "======> epoch: 44/400, Loss:0.01803337037563324\n",
      "======> epoch: 44/400, Loss:0.01974879391491413\n",
      "======> epoch: 44/400, Loss:0.023756274953484535\n",
      "Entering Epoch:  45\n",
      "======> epoch: 45/400, Loss:0.024943817406892776\n",
      "======> epoch: 45/400, Loss:0.01785932295024395\n",
      "======> epoch: 45/400, Loss:0.022490117698907852\n",
      "======> epoch: 45/400, Loss:0.020655956119298935\n",
      "======> epoch: 45/400, Loss:0.020155401900410652\n",
      "======> epoch: 45/400, Loss:0.01920551061630249\n",
      "======> epoch: 45/400, Loss:0.023747077211737633\n",
      "======> epoch: 45/400, Loss:0.024806205183267593\n",
      "======> epoch: 45/400, Loss:0.018704326823353767\n",
      "======> epoch: 45/400, Loss:0.018691411241889\n",
      "Entering Epoch:  46\n",
      "======> epoch: 46/400, Loss:0.01861410029232502\n",
      "======> epoch: 46/400, Loss:0.018013469874858856\n",
      "======> epoch: 46/400, Loss:0.02425428293645382\n",
      "======> epoch: 46/400, Loss:0.019336916506290436\n",
      "======> epoch: 46/400, Loss:0.014535734429955482\n",
      "======> epoch: 46/400, Loss:0.02852761559188366\n",
      "======> epoch: 46/400, Loss:0.017107926309108734\n",
      "======> epoch: 46/400, Loss:0.01338141318410635\n",
      "======> epoch: 46/400, Loss:0.0188042800873518\n",
      "======> epoch: 46/400, Loss:0.01611088588833809\n",
      "Entering Epoch:  47\n",
      "======> epoch: 47/400, Loss:0.017133362591266632\n",
      "======> epoch: 47/400, Loss:0.021909276023507118\n",
      "======> epoch: 47/400, Loss:0.02431749552488327\n",
      "======> epoch: 47/400, Loss:0.02027069963514805\n",
      "======> epoch: 47/400, Loss:0.019468095153570175\n",
      "======> epoch: 47/400, Loss:0.02198321372270584\n",
      "======> epoch: 47/400, Loss:0.01566854864358902\n",
      "======> epoch: 47/400, Loss:0.019998105242848396\n",
      "======> epoch: 47/400, Loss:0.019427139312028885\n",
      "======> epoch: 47/400, Loss:0.020950207486748695\n",
      "Entering Epoch:  48\n",
      "======> epoch: 48/400, Loss:0.02154645323753357\n",
      "======> epoch: 48/400, Loss:0.018063776195049286\n",
      "======> epoch: 48/400, Loss:0.016056738793849945\n",
      "======> epoch: 48/400, Loss:0.01961219310760498\n",
      "======> epoch: 48/400, Loss:0.02583819068968296\n",
      "======> epoch: 48/400, Loss:0.024773450568318367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 48/400, Loss:0.020661883056163788\n",
      "======> epoch: 48/400, Loss:0.01754726655781269\n",
      "======> epoch: 48/400, Loss:0.021356675773859024\n",
      "======> epoch: 48/400, Loss:0.0193482618778944\n",
      "Entering Epoch:  49\n",
      "======> epoch: 49/400, Loss:0.01975620537996292\n",
      "======> epoch: 49/400, Loss:0.01974296197295189\n",
      "======> epoch: 49/400, Loss:0.014862345531582832\n",
      "======> epoch: 49/400, Loss:0.01541930716484785\n",
      "======> epoch: 49/400, Loss:0.021488787606358528\n",
      "======> epoch: 49/400, Loss:0.01629195734858513\n",
      "======> epoch: 49/400, Loss:0.018661094829440117\n",
      "======> epoch: 49/400, Loss:0.01970416121184826\n",
      "======> epoch: 49/400, Loss:0.019727010279893875\n",
      "======> epoch: 49/400, Loss:0.015828795731067657\n",
      "Entering Epoch:  50\n",
      "======> epoch: 50/400, Loss:0.01853027008473873\n",
      "======> epoch: 50/400, Loss:0.016844268888235092\n",
      "======> epoch: 50/400, Loss:0.022691182792186737\n",
      "======> epoch: 50/400, Loss:0.021857349202036858\n",
      "======> epoch: 50/400, Loss:0.015884224325418472\n",
      "======> epoch: 50/400, Loss:0.01853015646338463\n",
      "======> epoch: 50/400, Loss:0.019105689600110054\n",
      "======> epoch: 50/400, Loss:0.01963937096297741\n",
      "======> epoch: 50/400, Loss:0.02150878496468067\n",
      "======> epoch: 50/400, Loss:0.01432232279330492\n",
      "Entering Epoch:  51\n",
      "======> epoch: 51/400, Loss:0.02290262281894684\n",
      "======> epoch: 51/400, Loss:0.022493699565529823\n",
      "======> epoch: 51/400, Loss:0.015709485858678818\n",
      "======> epoch: 51/400, Loss:0.015671342611312866\n",
      "======> epoch: 51/400, Loss:0.020113036036491394\n",
      "======> epoch: 51/400, Loss:0.019771402701735497\n",
      "======> epoch: 51/400, Loss:0.029930101707577705\n",
      "======> epoch: 51/400, Loss:0.02074354887008667\n",
      "======> epoch: 51/400, Loss:0.015682941302657127\n",
      "======> epoch: 51/400, Loss:0.02767217345535755\n",
      "Entering Epoch:  52\n",
      "======> epoch: 52/400, Loss:0.014699704945087433\n",
      "======> epoch: 52/400, Loss:0.01812945120036602\n",
      "======> epoch: 52/400, Loss:0.01806539297103882\n",
      "======> epoch: 52/400, Loss:0.014984989538788795\n",
      "======> epoch: 52/400, Loss:0.022953413426876068\n",
      "======> epoch: 52/400, Loss:0.019246315583586693\n",
      "======> epoch: 52/400, Loss:0.015819162130355835\n",
      "======> epoch: 52/400, Loss:0.0196385458111763\n",
      "======> epoch: 52/400, Loss:0.01967005990445614\n",
      "======> epoch: 52/400, Loss:0.018959801644086838\n",
      "Entering Epoch:  53\n",
      "======> epoch: 53/400, Loss:0.020634008571505547\n",
      "======> epoch: 53/400, Loss:0.015238805674016476\n",
      "======> epoch: 53/400, Loss:0.025438278913497925\n",
      "======> epoch: 53/400, Loss:0.016893932595849037\n",
      "======> epoch: 53/400, Loss:0.020153772085905075\n",
      "======> epoch: 53/400, Loss:0.020234448835253716\n",
      "======> epoch: 53/400, Loss:0.023075789213180542\n",
      "======> epoch: 53/400, Loss:0.02421501651406288\n",
      "======> epoch: 53/400, Loss:0.023302452638745308\n",
      "======> epoch: 53/400, Loss:0.02146349661052227\n",
      "Entering Epoch:  54\n",
      "======> epoch: 54/400, Loss:0.021556107327342033\n",
      "======> epoch: 54/400, Loss:0.021101322025060654\n",
      "======> epoch: 54/400, Loss:0.02366263046860695\n",
      "======> epoch: 54/400, Loss:0.01596846617758274\n",
      "======> epoch: 54/400, Loss:0.018353113904595375\n",
      "======> epoch: 54/400, Loss:0.0211680568754673\n",
      "======> epoch: 54/400, Loss:0.01911303400993347\n",
      "======> epoch: 54/400, Loss:0.017906086519360542\n",
      "======> epoch: 54/400, Loss:0.01721438765525818\n",
      "======> epoch: 54/400, Loss:0.018262090161442757\n",
      "Entering Epoch:  55\n",
      "======> epoch: 55/400, Loss:0.020332669839262962\n",
      "======> epoch: 55/400, Loss:0.016691187396645546\n",
      "======> epoch: 55/400, Loss:0.019982103258371353\n",
      "======> epoch: 55/400, Loss:0.02526720054447651\n",
      "======> epoch: 55/400, Loss:0.019928839057683945\n",
      "======> epoch: 55/400, Loss:0.016829876229166985\n",
      "======> epoch: 55/400, Loss:0.019641468301415443\n",
      "======> epoch: 55/400, Loss:0.023708129301667213\n",
      "======> epoch: 55/400, Loss:0.022288627922534943\n",
      "======> epoch: 55/400, Loss:0.017592597752809525\n",
      "Entering Epoch:  56\n",
      "======> epoch: 56/400, Loss:0.017341971397399902\n",
      "======> epoch: 56/400, Loss:0.019805822521448135\n",
      "======> epoch: 56/400, Loss:0.024320978671312332\n",
      "======> epoch: 56/400, Loss:0.017452577129006386\n",
      "======> epoch: 56/400, Loss:0.02067635767161846\n",
      "======> epoch: 56/400, Loss:0.025547876954078674\n",
      "======> epoch: 56/400, Loss:0.01577606610953808\n",
      "======> epoch: 56/400, Loss:0.02248358726501465\n",
      "======> epoch: 56/400, Loss:0.016687626019120216\n",
      "======> epoch: 56/400, Loss:0.017534038051962852\n",
      "Entering Epoch:  57\n",
      "======> epoch: 57/400, Loss:0.02408490888774395\n",
      "======> epoch: 57/400, Loss:0.015620321035385132\n",
      "======> epoch: 57/400, Loss:0.01581551879644394\n",
      "======> epoch: 57/400, Loss:0.017892224714159966\n",
      "======> epoch: 57/400, Loss:0.018207691609859467\n",
      "======> epoch: 57/400, Loss:0.019443565979599953\n",
      "======> epoch: 57/400, Loss:0.018993370234966278\n",
      "======> epoch: 57/400, Loss:0.017536120489239693\n",
      "======> epoch: 57/400, Loss:0.021817781031131744\n",
      "======> epoch: 57/400, Loss:0.01969899795949459\n",
      "Entering Epoch:  58\n",
      "======> epoch: 58/400, Loss:0.025138817727565765\n",
      "======> epoch: 58/400, Loss:0.02025015465915203\n",
      "======> epoch: 58/400, Loss:0.02140652947127819\n",
      "======> epoch: 58/400, Loss:0.01892094500362873\n",
      "======> epoch: 58/400, Loss:0.023046864196658134\n",
      "======> epoch: 58/400, Loss:0.015341127291321754\n",
      "======> epoch: 58/400, Loss:0.028497707098722458\n",
      "======> epoch: 58/400, Loss:0.016846049576997757\n",
      "======> epoch: 58/400, Loss:0.021398143842816353\n",
      "======> epoch: 58/400, Loss:0.013310864567756653\n",
      "Entering Epoch:  59\n",
      "======> epoch: 59/400, Loss:0.01986067183315754\n",
      "======> epoch: 59/400, Loss:0.020483950152993202\n",
      "======> epoch: 59/400, Loss:0.01757405884563923\n",
      "======> epoch: 59/400, Loss:0.020394766703248024\n",
      "======> epoch: 59/400, Loss:0.01852291077375412\n",
      "======> epoch: 59/400, Loss:0.025049801915884018\n",
      "======> epoch: 59/400, Loss:0.023113902658224106\n",
      "======> epoch: 59/400, Loss:0.020125554874539375\n",
      "======> epoch: 59/400, Loss:0.019937297329306602\n",
      "======> epoch: 59/400, Loss:0.018585724756121635\n",
      "Entering Epoch:  60\n",
      "======> epoch: 60/400, Loss:0.02048850990831852\n",
      "======> epoch: 60/400, Loss:0.017804579809308052\n",
      "======> epoch: 60/400, Loss:0.023502059280872345\n",
      "======> epoch: 60/400, Loss:0.018056565895676613\n",
      "======> epoch: 60/400, Loss:0.019865863025188446\n",
      "======> epoch: 60/400, Loss:0.019048966467380524\n",
      "======> epoch: 60/400, Loss:0.019020184874534607\n",
      "======> epoch: 60/400, Loss:0.0164343249052763\n",
      "======> epoch: 60/400, Loss:0.018062839284539223\n",
      "======> epoch: 60/400, Loss:0.02083284966647625\n",
      "Entering Epoch:  61\n",
      "======> epoch: 61/400, Loss:0.019117841497063637\n",
      "======> epoch: 61/400, Loss:0.019266074523329735\n",
      "======> epoch: 61/400, Loss:0.02520889975130558\n",
      "======> epoch: 61/400, Loss:0.02611365355551243\n",
      "======> epoch: 61/400, Loss:0.019395016133785248\n",
      "======> epoch: 61/400, Loss:0.01894015073776245\n",
      "======> epoch: 61/400, Loss:0.02196579799056053\n",
      "======> epoch: 61/400, Loss:0.017759641632437706\n",
      "======> epoch: 61/400, Loss:0.01960911974310875\n",
      "======> epoch: 61/400, Loss:0.017855703830718994\n",
      "Entering Epoch:  62\n",
      "======> epoch: 62/400, Loss:0.022141363471746445\n",
      "======> epoch: 62/400, Loss:0.017613274976611137\n",
      "======> epoch: 62/400, Loss:0.01839963160455227\n",
      "======> epoch: 62/400, Loss:0.019531644880771637\n",
      "======> epoch: 62/400, Loss:0.018244585022330284\n",
      "======> epoch: 62/400, Loss:0.017347225919365883\n",
      "======> epoch: 62/400, Loss:0.013394723646342754\n",
      "======> epoch: 62/400, Loss:0.01477820985019207\n",
      "======> epoch: 62/400, Loss:0.024836979806423187\n",
      "======> epoch: 62/400, Loss:0.017569029703736305\n",
      "Entering Epoch:  63\n",
      "======> epoch: 63/400, Loss:0.023217223584651947\n",
      "======> epoch: 63/400, Loss:0.018784288316965103\n",
      "======> epoch: 63/400, Loss:0.019535159692168236\n",
      "======> epoch: 63/400, Loss:0.024367215111851692\n",
      "======> epoch: 63/400, Loss:0.018335143104195595\n",
      "======> epoch: 63/400, Loss:0.019839661195874214\n",
      "======> epoch: 63/400, Loss:0.01889171451330185\n",
      "======> epoch: 63/400, Loss:0.016431741416454315\n",
      "======> epoch: 63/400, Loss:0.016115067526698112\n",
      "======> epoch: 63/400, Loss:0.013529067859053612\n",
      "Entering Epoch:  64\n",
      "======> epoch: 64/400, Loss:0.02037559635937214\n",
      "======> epoch: 64/400, Loss:0.018260100856423378\n",
      "======> epoch: 64/400, Loss:0.017828918993473053\n",
      "======> epoch: 64/400, Loss:0.015229959972202778\n",
      "======> epoch: 64/400, Loss:0.02305651642382145\n",
      "======> epoch: 64/400, Loss:0.01678258553147316\n",
      "======> epoch: 64/400, Loss:0.017188621684908867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 64/400, Loss:0.018535686656832695\n",
      "======> epoch: 64/400, Loss:0.013869349844753742\n",
      "======> epoch: 64/400, Loss:0.016330722719430923\n",
      "Entering Epoch:  65\n",
      "======> epoch: 65/400, Loss:0.022015321999788284\n",
      "======> epoch: 65/400, Loss:0.01525786705315113\n",
      "======> epoch: 65/400, Loss:0.019906239584088326\n",
      "======> epoch: 65/400, Loss:0.015927156433463097\n",
      "======> epoch: 65/400, Loss:0.020155180245637894\n",
      "======> epoch: 65/400, Loss:0.019876394420862198\n",
      "======> epoch: 65/400, Loss:0.01913352683186531\n",
      "======> epoch: 65/400, Loss:0.019365763291716576\n",
      "======> epoch: 65/400, Loss:0.017134789377450943\n",
      "======> epoch: 65/400, Loss:0.015120752155780792\n",
      "Entering Epoch:  66\n",
      "======> epoch: 66/400, Loss:0.021307317540049553\n",
      "======> epoch: 66/400, Loss:0.022595297545194626\n",
      "======> epoch: 66/400, Loss:0.02034151367843151\n",
      "======> epoch: 66/400, Loss:0.015959786251187325\n",
      "======> epoch: 66/400, Loss:0.01923551969230175\n",
      "======> epoch: 66/400, Loss:0.01655387505888939\n",
      "======> epoch: 66/400, Loss:0.01996767707169056\n",
      "======> epoch: 66/400, Loss:0.02300860546529293\n",
      "======> epoch: 66/400, Loss:0.014608651399612427\n",
      "======> epoch: 66/400, Loss:0.017686011269688606\n",
      "Entering Epoch:  67\n",
      "======> epoch: 67/400, Loss:0.018502140417695045\n",
      "======> epoch: 67/400, Loss:0.020900486037135124\n",
      "======> epoch: 67/400, Loss:0.02289215661585331\n",
      "======> epoch: 67/400, Loss:0.022323003038764\n",
      "======> epoch: 67/400, Loss:0.01669018343091011\n",
      "======> epoch: 67/400, Loss:0.018073979765176773\n",
      "======> epoch: 67/400, Loss:0.01564049907028675\n",
      "======> epoch: 67/400, Loss:0.01718681864440441\n",
      "======> epoch: 67/400, Loss:0.011411298997700214\n",
      "======> epoch: 67/400, Loss:0.01779881678521633\n",
      "Entering Epoch:  68\n",
      "======> epoch: 68/400, Loss:0.017996540293097496\n",
      "======> epoch: 68/400, Loss:0.021638885140419006\n",
      "======> epoch: 68/400, Loss:0.01644182577729225\n",
      "======> epoch: 68/400, Loss:0.022913232445716858\n",
      "======> epoch: 68/400, Loss:0.01584266684949398\n",
      "======> epoch: 68/400, Loss:0.020692773163318634\n",
      "======> epoch: 68/400, Loss:0.018446847796440125\n",
      "======> epoch: 68/400, Loss:0.0214985404163599\n",
      "======> epoch: 68/400, Loss:0.01909790374338627\n",
      "======> epoch: 68/400, Loss:0.021788349375128746\n",
      "Entering Epoch:  69\n",
      "======> epoch: 69/400, Loss:0.015285642817616463\n",
      "======> epoch: 69/400, Loss:0.018799439072608948\n",
      "======> epoch: 69/400, Loss:0.019825883209705353\n",
      "======> epoch: 69/400, Loss:0.026809178292751312\n",
      "======> epoch: 69/400, Loss:0.020026803016662598\n",
      "======> epoch: 69/400, Loss:0.018119968473911285\n",
      "======> epoch: 69/400, Loss:0.019623462110757828\n",
      "======> epoch: 69/400, Loss:0.019059496000409126\n",
      "======> epoch: 69/400, Loss:0.014994139783084393\n",
      "======> epoch: 69/400, Loss:0.0184127539396286\n",
      "Entering Epoch:  70\n",
      "======> epoch: 70/400, Loss:0.024169960990548134\n",
      "======> epoch: 70/400, Loss:0.017679937183856964\n",
      "======> epoch: 70/400, Loss:0.020438896492123604\n",
      "======> epoch: 70/400, Loss:0.01716521754860878\n",
      "======> epoch: 70/400, Loss:0.01816272735595703\n",
      "======> epoch: 70/400, Loss:0.020610429346561432\n",
      "======> epoch: 70/400, Loss:0.017772268503904343\n",
      "======> epoch: 70/400, Loss:0.020070530474185944\n",
      "======> epoch: 70/400, Loss:0.014561755582690239\n",
      "======> epoch: 70/400, Loss:0.01641741953790188\n",
      "Entering Epoch:  71\n",
      "======> epoch: 71/400, Loss:0.02216443419456482\n",
      "======> epoch: 71/400, Loss:0.013024859130382538\n",
      "======> epoch: 71/400, Loss:0.01889811083674431\n",
      "======> epoch: 71/400, Loss:0.017856169492006302\n",
      "======> epoch: 71/400, Loss:0.014475658535957336\n",
      "======> epoch: 71/400, Loss:0.018075892701745033\n",
      "======> epoch: 71/400, Loss:0.013687827624380589\n",
      "======> epoch: 71/400, Loss:0.017313700169324875\n",
      "======> epoch: 71/400, Loss:0.018915295600891113\n",
      "======> epoch: 71/400, Loss:0.022535592317581177\n",
      "Entering Epoch:  72\n",
      "======> epoch: 72/400, Loss:0.01744588278234005\n",
      "======> epoch: 72/400, Loss:0.01851603575050831\n",
      "======> epoch: 72/400, Loss:0.02268078550696373\n",
      "======> epoch: 72/400, Loss:0.01782984845340252\n",
      "======> epoch: 72/400, Loss:0.020738866180181503\n",
      "======> epoch: 72/400, Loss:0.02023547887802124\n",
      "======> epoch: 72/400, Loss:0.0236880574375391\n",
      "======> epoch: 72/400, Loss:0.01733677089214325\n",
      "======> epoch: 72/400, Loss:0.018936483189463615\n",
      "======> epoch: 72/400, Loss:0.012872721999883652\n",
      "Entering Epoch:  73\n",
      "======> epoch: 73/400, Loss:0.016820931807160378\n",
      "======> epoch: 73/400, Loss:0.022784369066357613\n",
      "======> epoch: 73/400, Loss:0.013734755106270313\n",
      "======> epoch: 73/400, Loss:0.0206848606467247\n",
      "======> epoch: 73/400, Loss:0.017833618447184563\n",
      "======> epoch: 73/400, Loss:0.020625779405236244\n",
      "======> epoch: 73/400, Loss:0.01798781007528305\n",
      "======> epoch: 73/400, Loss:0.019408883526921272\n",
      "======> epoch: 73/400, Loss:0.015870211645960808\n",
      "======> epoch: 73/400, Loss:0.0181371308863163\n",
      "Entering Epoch:  74\n",
      "======> epoch: 74/400, Loss:0.020488785579800606\n",
      "======> epoch: 74/400, Loss:0.016477031633257866\n",
      "======> epoch: 74/400, Loss:0.016942515969276428\n",
      "======> epoch: 74/400, Loss:0.01644783839583397\n",
      "======> epoch: 74/400, Loss:0.01580733433365822\n",
      "======> epoch: 74/400, Loss:0.019096577540040016\n",
      "======> epoch: 74/400, Loss:0.018295107409358025\n",
      "======> epoch: 74/400, Loss:0.01327261608093977\n",
      "======> epoch: 74/400, Loss:0.014358816668391228\n",
      "======> epoch: 74/400, Loss:0.016390858218073845\n",
      "Entering Epoch:  75\n",
      "======> epoch: 75/400, Loss:0.022191055119037628\n",
      "======> epoch: 75/400, Loss:0.019467106088995934\n",
      "======> epoch: 75/400, Loss:0.019232863560318947\n",
      "======> epoch: 75/400, Loss:0.014965709298849106\n",
      "======> epoch: 75/400, Loss:0.020554354414343834\n",
      "======> epoch: 75/400, Loss:0.023743484169244766\n",
      "======> epoch: 75/400, Loss:0.018282771110534668\n",
      "======> epoch: 75/400, Loss:0.01909269206225872\n",
      "======> epoch: 75/400, Loss:0.016579443588852882\n",
      "======> epoch: 75/400, Loss:0.0196516253054142\n",
      "Entering Epoch:  76\n",
      "======> epoch: 76/400, Loss:0.01518962997943163\n",
      "======> epoch: 76/400, Loss:0.016505597159266472\n",
      "======> epoch: 76/400, Loss:0.01692890003323555\n",
      "======> epoch: 76/400, Loss:0.01888377219438553\n",
      "======> epoch: 76/400, Loss:0.017358005046844482\n",
      "======> epoch: 76/400, Loss:0.012947560288012028\n",
      "======> epoch: 76/400, Loss:0.020128516480326653\n",
      "======> epoch: 76/400, Loss:0.018483160063624382\n",
      "======> epoch: 76/400, Loss:0.01757415197789669\n",
      "======> epoch: 76/400, Loss:0.014177885837852955\n",
      "Entering Epoch:  77\n",
      "======> epoch: 77/400, Loss:0.016484210267663002\n",
      "======> epoch: 77/400, Loss:0.012547646649181843\n",
      "======> epoch: 77/400, Loss:0.013388504274189472\n",
      "======> epoch: 77/400, Loss:0.019376298412680626\n",
      "======> epoch: 77/400, Loss:0.017881620675325394\n",
      "======> epoch: 77/400, Loss:0.022798852995038033\n",
      "======> epoch: 77/400, Loss:0.01244966872036457\n",
      "======> epoch: 77/400, Loss:0.014582432806491852\n",
      "======> epoch: 77/400, Loss:0.019655274227261543\n",
      "======> epoch: 77/400, Loss:0.017873669043183327\n",
      "Entering Epoch:  78\n",
      "======> epoch: 78/400, Loss:0.018077362328767776\n",
      "======> epoch: 78/400, Loss:0.01609981432557106\n",
      "======> epoch: 78/400, Loss:0.0223273653537035\n",
      "======> epoch: 78/400, Loss:0.022464359179139137\n",
      "======> epoch: 78/400, Loss:0.02149418741464615\n",
      "======> epoch: 78/400, Loss:0.020655138418078423\n",
      "======> epoch: 78/400, Loss:0.020893266424536705\n",
      "======> epoch: 78/400, Loss:0.018992086872458458\n",
      "======> epoch: 78/400, Loss:0.015014934353530407\n",
      "======> epoch: 78/400, Loss:0.020191218703985214\n",
      "Entering Epoch:  79\n",
      "======> epoch: 79/400, Loss:0.017053375020623207\n",
      "======> epoch: 79/400, Loss:0.013319136574864388\n",
      "======> epoch: 79/400, Loss:0.01649590954184532\n",
      "======> epoch: 79/400, Loss:0.016247840598225594\n",
      "======> epoch: 79/400, Loss:0.021049709990620613\n",
      "======> epoch: 79/400, Loss:0.016369197517633438\n",
      "======> epoch: 79/400, Loss:0.017248040065169334\n",
      "======> epoch: 79/400, Loss:0.019722603261470795\n",
      "======> epoch: 79/400, Loss:0.01779526099562645\n",
      "======> epoch: 79/400, Loss:0.019299214705824852\n",
      "Entering Epoch:  80\n",
      "======> epoch: 80/400, Loss:0.01597747765481472\n",
      "======> epoch: 80/400, Loss:0.01710651069879532\n",
      "======> epoch: 80/400, Loss:0.017696984112262726\n",
      "======> epoch: 80/400, Loss:0.02539278380572796\n",
      "======> epoch: 80/400, Loss:0.015412739478051662\n",
      "======> epoch: 80/400, Loss:0.022015074267983437\n",
      "======> epoch: 80/400, Loss:0.017533065751194954\n",
      "======> epoch: 80/400, Loss:0.01857662945985794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 80/400, Loss:0.017851388081908226\n",
      "======> epoch: 80/400, Loss:0.014501258730888367\n",
      "Entering Epoch:  81\n",
      "======> epoch: 81/400, Loss:0.016386602073907852\n",
      "======> epoch: 81/400, Loss:0.022988807410001755\n",
      "======> epoch: 81/400, Loss:0.016930310055613518\n",
      "======> epoch: 81/400, Loss:0.0154305100440979\n",
      "======> epoch: 81/400, Loss:0.014631709083914757\n",
      "======> epoch: 81/400, Loss:0.020216079428792\n",
      "======> epoch: 81/400, Loss:0.017854023724794388\n",
      "======> epoch: 81/400, Loss:0.02129402756690979\n",
      "======> epoch: 81/400, Loss:0.0183247160166502\n",
      "======> epoch: 81/400, Loss:0.01719151996076107\n",
      "Entering Epoch:  82\n",
      "======> epoch: 82/400, Loss:0.019302653148770332\n",
      "======> epoch: 82/400, Loss:0.020352568477392197\n",
      "======> epoch: 82/400, Loss:0.01906086690723896\n",
      "======> epoch: 82/400, Loss:0.021814001724123955\n",
      "======> epoch: 82/400, Loss:0.015227258205413818\n",
      "======> epoch: 82/400, Loss:0.017687175422906876\n",
      "======> epoch: 82/400, Loss:0.01589428074657917\n",
      "======> epoch: 82/400, Loss:0.014735325239598751\n",
      "======> epoch: 82/400, Loss:0.01890534721314907\n",
      "======> epoch: 82/400, Loss:0.01239098608493805\n",
      "Entering Epoch:  83\n",
      "======> epoch: 83/400, Loss:0.019312962889671326\n",
      "======> epoch: 83/400, Loss:0.017318127676844597\n",
      "======> epoch: 83/400, Loss:0.019294584169983864\n",
      "======> epoch: 83/400, Loss:0.013695895671844482\n",
      "======> epoch: 83/400, Loss:0.02602599374949932\n",
      "======> epoch: 83/400, Loss:0.015195504762232304\n",
      "======> epoch: 83/400, Loss:0.019165489822626114\n",
      "======> epoch: 83/400, Loss:0.015740739181637764\n",
      "======> epoch: 83/400, Loss:0.017575480043888092\n",
      "======> epoch: 83/400, Loss:0.013042785227298737\n",
      "Entering Epoch:  84\n",
      "======> epoch: 84/400, Loss:0.014901525340974331\n",
      "======> epoch: 84/400, Loss:0.0236116461455822\n",
      "======> epoch: 84/400, Loss:0.016920993104577065\n",
      "======> epoch: 84/400, Loss:0.021183863282203674\n",
      "======> epoch: 84/400, Loss:0.01346477773040533\n",
      "======> epoch: 84/400, Loss:0.018428701907396317\n",
      "======> epoch: 84/400, Loss:0.016060875728726387\n",
      "======> epoch: 84/400, Loss:0.01757933758199215\n",
      "======> epoch: 84/400, Loss:0.017999639734625816\n",
      "======> epoch: 84/400, Loss:0.020819587633013725\n",
      "Entering Epoch:  85\n",
      "======> epoch: 85/400, Loss:0.022401534020900726\n",
      "======> epoch: 85/400, Loss:0.01809784211218357\n",
      "======> epoch: 85/400, Loss:0.02402820810675621\n",
      "======> epoch: 85/400, Loss:0.02036764845252037\n",
      "======> epoch: 85/400, Loss:0.020361758768558502\n",
      "======> epoch: 85/400, Loss:0.01570320688188076\n",
      "======> epoch: 85/400, Loss:0.02172376960515976\n",
      "======> epoch: 85/400, Loss:0.01931373029947281\n",
      "======> epoch: 85/400, Loss:0.01920360140502453\n",
      "======> epoch: 85/400, Loss:0.017581669613718987\n",
      "Entering Epoch:  86\n",
      "======> epoch: 86/400, Loss:0.023474587127566338\n",
      "======> epoch: 86/400, Loss:0.023414194583892822\n",
      "======> epoch: 86/400, Loss:0.01978902332484722\n",
      "======> epoch: 86/400, Loss:0.01977858692407608\n",
      "======> epoch: 86/400, Loss:0.018832338973879814\n",
      "======> epoch: 86/400, Loss:0.015890726819634438\n",
      "======> epoch: 86/400, Loss:0.0158456452190876\n",
      "======> epoch: 86/400, Loss:0.014032389037311077\n",
      "======> epoch: 86/400, Loss:0.015599091537296772\n",
      "======> epoch: 86/400, Loss:0.024393945932388306\n",
      "Entering Epoch:  87\n",
      "======> epoch: 87/400, Loss:0.015588382259011269\n",
      "======> epoch: 87/400, Loss:0.01822289265692234\n",
      "======> epoch: 87/400, Loss:0.018338337540626526\n",
      "======> epoch: 87/400, Loss:0.01801116205751896\n",
      "======> epoch: 87/400, Loss:0.018590684980154037\n",
      "======> epoch: 87/400, Loss:0.02176615223288536\n",
      "======> epoch: 87/400, Loss:0.018354816362261772\n",
      "======> epoch: 87/400, Loss:0.020687023177742958\n",
      "======> epoch: 87/400, Loss:0.016623934730887413\n",
      "======> epoch: 87/400, Loss:0.01937861740589142\n",
      "Entering Epoch:  88\n",
      "======> epoch: 88/400, Loss:0.017110366374254227\n",
      "======> epoch: 88/400, Loss:0.018211480230093002\n",
      "======> epoch: 88/400, Loss:0.013346775434911251\n",
      "======> epoch: 88/400, Loss:0.02613259106874466\n",
      "======> epoch: 88/400, Loss:0.015357887372374535\n",
      "======> epoch: 88/400, Loss:0.017931770533323288\n",
      "======> epoch: 88/400, Loss:0.016763929277658463\n",
      "======> epoch: 88/400, Loss:0.014474529772996902\n",
      "======> epoch: 88/400, Loss:0.018846189603209496\n",
      "======> epoch: 88/400, Loss:0.020437661558389664\n",
      "Entering Epoch:  89\n",
      "======> epoch: 89/400, Loss:0.016737444326281548\n",
      "======> epoch: 89/400, Loss:0.012087198905646801\n",
      "======> epoch: 89/400, Loss:0.025303006172180176\n",
      "======> epoch: 89/400, Loss:0.01346014067530632\n",
      "======> epoch: 89/400, Loss:0.020947575569152832\n",
      "======> epoch: 89/400, Loss:0.02395719103515148\n",
      "======> epoch: 89/400, Loss:0.012935501523315907\n",
      "======> epoch: 89/400, Loss:0.016312478110194206\n",
      "======> epoch: 89/400, Loss:0.020427821204066277\n",
      "======> epoch: 89/400, Loss:0.021090343594551086\n",
      "Entering Epoch:  90\n",
      "======> epoch: 90/400, Loss:0.01968272216618061\n",
      "======> epoch: 90/400, Loss:0.01411205343902111\n",
      "======> epoch: 90/400, Loss:0.01624550297856331\n",
      "======> epoch: 90/400, Loss:0.013316820375621319\n",
      "======> epoch: 90/400, Loss:0.020564131438732147\n",
      "======> epoch: 90/400, Loss:0.019557053223252296\n",
      "======> epoch: 90/400, Loss:0.022564023733139038\n",
      "======> epoch: 90/400, Loss:0.022888723760843277\n",
      "======> epoch: 90/400, Loss:0.019163843244314194\n",
      "======> epoch: 90/400, Loss:0.027076873928308487\n",
      "Entering Epoch:  91\n",
      "======> epoch: 91/400, Loss:0.023884858936071396\n",
      "======> epoch: 91/400, Loss:0.016627689823508263\n",
      "======> epoch: 91/400, Loss:0.015061946585774422\n",
      "======> epoch: 91/400, Loss:0.01503936666995287\n",
      "======> epoch: 91/400, Loss:0.011636227369308472\n",
      "======> epoch: 91/400, Loss:0.023527584969997406\n",
      "======> epoch: 91/400, Loss:0.01940980739891529\n",
      "======> epoch: 91/400, Loss:0.01949678733944893\n",
      "======> epoch: 91/400, Loss:0.016218220815062523\n",
      "======> epoch: 91/400, Loss:0.015899695456027985\n",
      "Entering Epoch:  92\n",
      "======> epoch: 92/400, Loss:0.02120588719844818\n",
      "======> epoch: 92/400, Loss:0.01511310413479805\n",
      "======> epoch: 92/400, Loss:0.017878776416182518\n",
      "======> epoch: 92/400, Loss:0.01131406705826521\n",
      "======> epoch: 92/400, Loss:0.018673038110136986\n",
      "======> epoch: 92/400, Loss:0.015615997835993767\n",
      "======> epoch: 92/400, Loss:0.01974784955382347\n",
      "======> epoch: 92/400, Loss:0.02430928871035576\n",
      "======> epoch: 92/400, Loss:0.025266868993639946\n",
      "======> epoch: 92/400, Loss:0.013983438722789288\n",
      "Entering Epoch:  93\n",
      "======> epoch: 93/400, Loss:0.0161381047219038\n",
      "======> epoch: 93/400, Loss:0.016997383907437325\n",
      "======> epoch: 93/400, Loss:0.03211893513798714\n",
      "======> epoch: 93/400, Loss:0.014956972561776638\n",
      "======> epoch: 93/400, Loss:0.018785245716571808\n",
      "======> epoch: 93/400, Loss:0.012753316201269627\n",
      "======> epoch: 93/400, Loss:0.021306851878762245\n",
      "======> epoch: 93/400, Loss:0.02082504704594612\n",
      "======> epoch: 93/400, Loss:0.018641745671629906\n",
      "======> epoch: 93/400, Loss:0.019512776285409927\n",
      "Entering Epoch:  94\n",
      "======> epoch: 94/400, Loss:0.0153470728546381\n",
      "======> epoch: 94/400, Loss:0.020042547956109047\n",
      "======> epoch: 94/400, Loss:0.02226213924586773\n",
      "======> epoch: 94/400, Loss:0.01564173959195614\n",
      "======> epoch: 94/400, Loss:0.01776847057044506\n",
      "======> epoch: 94/400, Loss:0.013665108010172844\n",
      "======> epoch: 94/400, Loss:0.017024008557200432\n",
      "======> epoch: 94/400, Loss:0.01805504597723484\n",
      "======> epoch: 94/400, Loss:0.016834205016493797\n",
      "======> epoch: 94/400, Loss:0.028425943106412888\n",
      "Entering Epoch:  95\n",
      "======> epoch: 95/400, Loss:0.01548804622143507\n",
      "======> epoch: 95/400, Loss:0.019970940425992012\n",
      "======> epoch: 95/400, Loss:0.017676005139946938\n",
      "======> epoch: 95/400, Loss:0.016096245497465134\n",
      "======> epoch: 95/400, Loss:0.01588996686041355\n",
      "======> epoch: 95/400, Loss:0.014388944022357464\n",
      "======> epoch: 95/400, Loss:0.017528189346194267\n",
      "======> epoch: 95/400, Loss:0.015011877752840519\n",
      "======> epoch: 95/400, Loss:0.015778442844748497\n",
      "======> epoch: 95/400, Loss:0.017790138721466064\n",
      "Entering Epoch:  96\n",
      "======> epoch: 96/400, Loss:0.015390707179903984\n",
      "======> epoch: 96/400, Loss:0.0163493100553751\n",
      "======> epoch: 96/400, Loss:0.021864255890250206\n",
      "======> epoch: 96/400, Loss:0.02002764120697975\n",
      "======> epoch: 96/400, Loss:0.020879046991467476\n",
      "======> epoch: 96/400, Loss:0.016842147335410118\n",
      "======> epoch: 96/400, Loss:0.017769645899534225\n",
      "======> epoch: 96/400, Loss:0.013876683078706264\n",
      "======> epoch: 96/400, Loss:0.014385359361767769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 96/400, Loss:0.017391296103596687\n",
      "Entering Epoch:  97\n",
      "======> epoch: 97/400, Loss:0.016079816967248917\n",
      "======> epoch: 97/400, Loss:0.02281658910214901\n",
      "======> epoch: 97/400, Loss:0.023669321089982986\n",
      "======> epoch: 97/400, Loss:0.02360018715262413\n",
      "======> epoch: 97/400, Loss:0.022699180990457535\n",
      "======> epoch: 97/400, Loss:0.01877468451857567\n",
      "======> epoch: 97/400, Loss:0.019385257735848427\n",
      "======> epoch: 97/400, Loss:0.012530379928648472\n",
      "======> epoch: 97/400, Loss:0.015665292739868164\n",
      "======> epoch: 97/400, Loss:0.014508950524032116\n",
      "Entering Epoch:  98\n",
      "======> epoch: 98/400, Loss:0.01882060058414936\n",
      "======> epoch: 98/400, Loss:0.02447262406349182\n",
      "======> epoch: 98/400, Loss:0.01904415898025036\n",
      "======> epoch: 98/400, Loss:0.017113614827394485\n",
      "======> epoch: 98/400, Loss:0.020751183852553368\n",
      "======> epoch: 98/400, Loss:0.01208078395575285\n",
      "======> epoch: 98/400, Loss:0.022310756146907806\n",
      "======> epoch: 98/400, Loss:0.01675846241414547\n",
      "======> epoch: 98/400, Loss:0.014236378483474255\n",
      "======> epoch: 98/400, Loss:0.016973908990621567\n",
      "Entering Epoch:  99\n",
      "======> epoch: 99/400, Loss:0.020678281784057617\n",
      "======> epoch: 99/400, Loss:0.018903017044067383\n",
      "======> epoch: 99/400, Loss:0.01773134060204029\n",
      "======> epoch: 99/400, Loss:0.011935685761272907\n",
      "======> epoch: 99/400, Loss:0.015590225346386433\n",
      "======> epoch: 99/400, Loss:0.017033781856298447\n",
      "======> epoch: 99/400, Loss:0.017556607723236084\n",
      "======> epoch: 99/400, Loss:0.015291474759578705\n",
      "======> epoch: 99/400, Loss:0.019856510683894157\n",
      "======> epoch: 99/400, Loss:0.017871957272291183\n",
      "Entering Epoch:  100\n",
      "======> epoch: 100/400, Loss:0.015754466876387596\n",
      "======> epoch: 100/400, Loss:0.019272804260253906\n",
      "======> epoch: 100/400, Loss:0.011665504425764084\n",
      "======> epoch: 100/400, Loss:0.018675610423088074\n",
      "======> epoch: 100/400, Loss:0.021527107805013657\n",
      "======> epoch: 100/400, Loss:0.016593066975474358\n",
      "======> epoch: 100/400, Loss:0.013158321380615234\n",
      "======> epoch: 100/400, Loss:0.019827621057629585\n",
      "======> epoch: 100/400, Loss:0.017861803993582726\n",
      "======> epoch: 100/400, Loss:0.014163722284138203\n",
      "Entering Epoch:  101\n",
      "======> epoch: 101/400, Loss:0.013491612859070301\n",
      "======> epoch: 101/400, Loss:0.01708338037133217\n",
      "======> epoch: 101/400, Loss:0.015660667791962624\n",
      "======> epoch: 101/400, Loss:0.020485203713178635\n",
      "======> epoch: 101/400, Loss:0.015554390847682953\n",
      "======> epoch: 101/400, Loss:0.01777258887887001\n",
      "======> epoch: 101/400, Loss:0.018930276855826378\n",
      "======> epoch: 101/400, Loss:0.027206825092434883\n",
      "======> epoch: 101/400, Loss:0.022897422313690186\n",
      "======> epoch: 101/400, Loss:0.02208012156188488\n",
      "Entering Epoch:  102\n",
      "======> epoch: 102/400, Loss:0.018825965002179146\n",
      "======> epoch: 102/400, Loss:0.01646442338824272\n",
      "======> epoch: 102/400, Loss:0.023237116634845734\n",
      "======> epoch: 102/400, Loss:0.011900205165147781\n",
      "======> epoch: 102/400, Loss:0.016631944105029106\n",
      "======> epoch: 102/400, Loss:0.022154737263917923\n",
      "======> epoch: 102/400, Loss:0.018209507688879967\n",
      "======> epoch: 102/400, Loss:0.016897769644856453\n",
      "======> epoch: 102/400, Loss:0.016596650704741478\n",
      "======> epoch: 102/400, Loss:0.019444745033979416\n",
      "Entering Epoch:  103\n",
      "======> epoch: 103/400, Loss:0.016092555597424507\n",
      "======> epoch: 103/400, Loss:0.02062080055475235\n",
      "======> epoch: 103/400, Loss:0.014521417208015919\n",
      "======> epoch: 103/400, Loss:0.01904614083468914\n",
      "======> epoch: 103/400, Loss:0.016237733885645866\n",
      "======> epoch: 103/400, Loss:0.01553930900990963\n",
      "======> epoch: 103/400, Loss:0.019018515944480896\n",
      "======> epoch: 103/400, Loss:0.014344325289130211\n",
      "======> epoch: 103/400, Loss:0.015043731778860092\n",
      "======> epoch: 103/400, Loss:0.020900117233395576\n",
      "Entering Epoch:  104\n",
      "======> epoch: 104/400, Loss:0.020029256120324135\n",
      "======> epoch: 104/400, Loss:0.015561037696897984\n",
      "======> epoch: 104/400, Loss:0.01409873180091381\n",
      "======> epoch: 104/400, Loss:0.02142925187945366\n",
      "======> epoch: 104/400, Loss:0.02018478326499462\n",
      "======> epoch: 104/400, Loss:0.014326387085020542\n",
      "======> epoch: 104/400, Loss:0.01999490149319172\n",
      "======> epoch: 104/400, Loss:0.016384145244956017\n",
      "======> epoch: 104/400, Loss:0.013201442547142506\n",
      "======> epoch: 104/400, Loss:0.022102361544966698\n",
      "Entering Epoch:  105\n",
      "======> epoch: 105/400, Loss:0.022587772458791733\n",
      "======> epoch: 105/400, Loss:0.016040373593568802\n",
      "======> epoch: 105/400, Loss:0.014202818274497986\n",
      "======> epoch: 105/400, Loss:0.01750306971371174\n",
      "======> epoch: 105/400, Loss:0.019136609509587288\n",
      "======> epoch: 105/400, Loss:0.017670990899205208\n",
      "======> epoch: 105/400, Loss:0.020756687968969345\n",
      "======> epoch: 105/400, Loss:0.012405837886035442\n",
      "======> epoch: 105/400, Loss:0.014687065966427326\n",
      "======> epoch: 105/400, Loss:0.017629068344831467\n",
      "Entering Epoch:  106\n",
      "======> epoch: 106/400, Loss:0.016988597810268402\n",
      "======> epoch: 106/400, Loss:0.018590101972222328\n",
      "======> epoch: 106/400, Loss:0.018761059269309044\n",
      "======> epoch: 106/400, Loss:0.02294156514108181\n",
      "======> epoch: 106/400, Loss:0.022059878334403038\n",
      "======> epoch: 106/400, Loss:0.017402177676558495\n",
      "======> epoch: 106/400, Loss:0.01620154269039631\n",
      "======> epoch: 106/400, Loss:0.014338372275233269\n",
      "======> epoch: 106/400, Loss:0.019782934337854385\n",
      "======> epoch: 106/400, Loss:0.020024772733449936\n",
      "Entering Epoch:  107\n",
      "======> epoch: 107/400, Loss:0.02003471553325653\n",
      "======> epoch: 107/400, Loss:0.025756577029824257\n",
      "======> epoch: 107/400, Loss:0.01866912469267845\n",
      "======> epoch: 107/400, Loss:0.013998182490468025\n",
      "======> epoch: 107/400, Loss:0.02197153866291046\n",
      "======> epoch: 107/400, Loss:0.021225830540060997\n",
      "======> epoch: 107/400, Loss:0.016014350578188896\n",
      "======> epoch: 107/400, Loss:0.016883522272109985\n",
      "======> epoch: 107/400, Loss:0.022806137800216675\n",
      "======> epoch: 107/400, Loss:0.017567103728652\n",
      "Entering Epoch:  108\n",
      "======> epoch: 108/400, Loss:0.017262078821659088\n",
      "======> epoch: 108/400, Loss:0.026779163628816605\n",
      "======> epoch: 108/400, Loss:0.018830815330147743\n",
      "======> epoch: 108/400, Loss:0.01752796769142151\n",
      "======> epoch: 108/400, Loss:0.014987514354288578\n",
      "======> epoch: 108/400, Loss:0.01620405912399292\n",
      "======> epoch: 108/400, Loss:0.015226301737129688\n",
      "======> epoch: 108/400, Loss:0.02079913020133972\n",
      "======> epoch: 108/400, Loss:0.016100723296403885\n",
      "======> epoch: 108/400, Loss:0.016469961032271385\n",
      "Entering Epoch:  109\n",
      "======> epoch: 109/400, Loss:0.01881861686706543\n",
      "======> epoch: 109/400, Loss:0.022031858563423157\n",
      "======> epoch: 109/400, Loss:0.020137518644332886\n",
      "======> epoch: 109/400, Loss:0.016107406467199326\n",
      "======> epoch: 109/400, Loss:0.014677985571324825\n",
      "======> epoch: 109/400, Loss:0.017461305484175682\n",
      "======> epoch: 109/400, Loss:0.018902253359556198\n",
      "======> epoch: 109/400, Loss:0.01698843017220497\n",
      "======> epoch: 109/400, Loss:0.024983251467347145\n",
      "======> epoch: 109/400, Loss:0.015983382239937782\n",
      "Entering Epoch:  110\n",
      "======> epoch: 110/400, Loss:0.01860150508582592\n",
      "======> epoch: 110/400, Loss:0.01711122691631317\n",
      "======> epoch: 110/400, Loss:0.01700063981115818\n",
      "======> epoch: 110/400, Loss:0.01334359496831894\n",
      "======> epoch: 110/400, Loss:0.019348423928022385\n",
      "======> epoch: 110/400, Loss:0.018922561779618263\n",
      "======> epoch: 110/400, Loss:0.022184044122695923\n",
      "======> epoch: 110/400, Loss:0.016787810251116753\n",
      "======> epoch: 110/400, Loss:0.01693415828049183\n",
      "======> epoch: 110/400, Loss:0.015797818079590797\n",
      "Entering Epoch:  111\n",
      "======> epoch: 111/400, Loss:0.018995512276887894\n",
      "======> epoch: 111/400, Loss:0.02835959941148758\n",
      "======> epoch: 111/400, Loss:0.017435215413570404\n",
      "======> epoch: 111/400, Loss:0.0178246907889843\n",
      "======> epoch: 111/400, Loss:0.019663432613015175\n",
      "======> epoch: 111/400, Loss:0.019961098209023476\n",
      "======> epoch: 111/400, Loss:0.013503520749509335\n",
      "======> epoch: 111/400, Loss:0.01717999018728733\n",
      "======> epoch: 111/400, Loss:0.016168737784028053\n",
      "======> epoch: 111/400, Loss:0.015383893623948097\n",
      "Entering Epoch:  112\n",
      "======> epoch: 112/400, Loss:0.02232525683939457\n",
      "======> epoch: 112/400, Loss:0.022884098812937737\n",
      "======> epoch: 112/400, Loss:0.016168542206287384\n",
      "======> epoch: 112/400, Loss:0.014719519764184952\n",
      "======> epoch: 112/400, Loss:0.01603075861930847\n",
      "======> epoch: 112/400, Loss:0.014641067944467068\n",
      "======> epoch: 112/400, Loss:0.01522712130099535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 112/400, Loss:0.015319057740271091\n",
      "======> epoch: 112/400, Loss:0.02404891885817051\n",
      "======> epoch: 112/400, Loss:0.019610397517681122\n",
      "Entering Epoch:  113\n",
      "======> epoch: 113/400, Loss:0.01621892675757408\n",
      "======> epoch: 113/400, Loss:0.02064918726682663\n",
      "======> epoch: 113/400, Loss:0.01905568689107895\n",
      "======> epoch: 113/400, Loss:0.02145419642329216\n",
      "======> epoch: 113/400, Loss:0.016779504716396332\n",
      "======> epoch: 113/400, Loss:0.021859413012862206\n",
      "======> epoch: 113/400, Loss:0.020196007564663887\n",
      "======> epoch: 113/400, Loss:0.01753128133714199\n",
      "======> epoch: 113/400, Loss:0.018511217087507248\n",
      "======> epoch: 113/400, Loss:0.016896337270736694\n",
      "Entering Epoch:  114\n",
      "======> epoch: 114/400, Loss:0.015355240553617477\n",
      "======> epoch: 114/400, Loss:0.01782008819282055\n",
      "======> epoch: 114/400, Loss:0.015834083780646324\n",
      "======> epoch: 114/400, Loss:0.022264594212174416\n",
      "======> epoch: 114/400, Loss:0.02022489719092846\n",
      "======> epoch: 114/400, Loss:0.014115061610937119\n",
      "======> epoch: 114/400, Loss:0.017186865210533142\n",
      "======> epoch: 114/400, Loss:0.020196521654725075\n",
      "======> epoch: 114/400, Loss:0.014246448874473572\n",
      "======> epoch: 114/400, Loss:0.01868904009461403\n",
      "Entering Epoch:  115\n",
      "======> epoch: 115/400, Loss:0.01779731921851635\n",
      "======> epoch: 115/400, Loss:0.019186807796359062\n",
      "======> epoch: 115/400, Loss:0.01653011329472065\n",
      "======> epoch: 115/400, Loss:0.021233707666397095\n",
      "======> epoch: 115/400, Loss:0.020186906680464745\n",
      "======> epoch: 115/400, Loss:0.01551720593124628\n",
      "======> epoch: 115/400, Loss:0.01834253780543804\n",
      "======> epoch: 115/400, Loss:0.01878354512155056\n",
      "======> epoch: 115/400, Loss:0.013179121538996696\n",
      "======> epoch: 115/400, Loss:0.020707188174128532\n",
      "Entering Epoch:  116\n",
      "======> epoch: 116/400, Loss:0.02042507566511631\n",
      "======> epoch: 116/400, Loss:0.024907538667321205\n",
      "======> epoch: 116/400, Loss:0.02085348218679428\n",
      "======> epoch: 116/400, Loss:0.017219722270965576\n",
      "======> epoch: 116/400, Loss:0.01459461823105812\n",
      "======> epoch: 116/400, Loss:0.022315720096230507\n",
      "======> epoch: 116/400, Loss:0.018855039030313492\n",
      "======> epoch: 116/400, Loss:0.025463715195655823\n",
      "======> epoch: 116/400, Loss:0.018235454335808754\n",
      "======> epoch: 116/400, Loss:0.017746947705745697\n",
      "Entering Epoch:  117\n",
      "======> epoch: 117/400, Loss:0.02164720743894577\n",
      "======> epoch: 117/400, Loss:0.019570624455809593\n",
      "======> epoch: 117/400, Loss:0.013643727637827396\n",
      "======> epoch: 117/400, Loss:0.015812400728464127\n",
      "======> epoch: 117/400, Loss:0.016729742288589478\n",
      "======> epoch: 117/400, Loss:0.02079187147319317\n",
      "======> epoch: 117/400, Loss:0.01583521068096161\n",
      "======> epoch: 117/400, Loss:0.019091853871941566\n",
      "======> epoch: 117/400, Loss:0.01582254469394684\n",
      "======> epoch: 117/400, Loss:0.015689939260482788\n",
      "Entering Epoch:  118\n",
      "======> epoch: 118/400, Loss:0.014555944129824638\n",
      "======> epoch: 118/400, Loss:0.01703951507806778\n",
      "======> epoch: 118/400, Loss:0.01753322221338749\n",
      "======> epoch: 118/400, Loss:0.016729746013879776\n",
      "======> epoch: 118/400, Loss:0.019657202064990997\n",
      "======> epoch: 118/400, Loss:0.019788706675171852\n",
      "======> epoch: 118/400, Loss:0.01550058089196682\n",
      "======> epoch: 118/400, Loss:0.011369970627129078\n",
      "======> epoch: 118/400, Loss:0.0204375758767128\n",
      "======> epoch: 118/400, Loss:0.01981240138411522\n",
      "Entering Epoch:  119\n",
      "======> epoch: 119/400, Loss:0.019421681761741638\n",
      "======> epoch: 119/400, Loss:0.017461711540818214\n",
      "======> epoch: 119/400, Loss:0.022461654618382454\n",
      "======> epoch: 119/400, Loss:0.02016308344900608\n",
      "======> epoch: 119/400, Loss:0.01659645140171051\n",
      "======> epoch: 119/400, Loss:0.023518256843090057\n",
      "======> epoch: 119/400, Loss:0.020353946834802628\n",
      "======> epoch: 119/400, Loss:0.020287005230784416\n",
      "======> epoch: 119/400, Loss:0.015529925003647804\n",
      "======> epoch: 119/400, Loss:0.013524061068892479\n",
      "Entering Epoch:  120\n",
      "======> epoch: 120/400, Loss:0.018147505819797516\n",
      "======> epoch: 120/400, Loss:0.015444234944880009\n",
      "======> epoch: 120/400, Loss:0.020542321726679802\n",
      "======> epoch: 120/400, Loss:0.013021579943597317\n",
      "======> epoch: 120/400, Loss:0.014774921350181103\n",
      "======> epoch: 120/400, Loss:0.014711371622979641\n",
      "======> epoch: 120/400, Loss:0.012434977106750011\n",
      "======> epoch: 120/400, Loss:0.01916820928454399\n",
      "======> epoch: 120/400, Loss:0.018518833443522453\n",
      "======> epoch: 120/400, Loss:0.01863446831703186\n",
      "Entering Epoch:  121\n",
      "======> epoch: 121/400, Loss:0.01782247982919216\n",
      "======> epoch: 121/400, Loss:0.019642921164631844\n",
      "======> epoch: 121/400, Loss:0.01890077441930771\n",
      "======> epoch: 121/400, Loss:0.01704251393675804\n",
      "======> epoch: 121/400, Loss:0.01740282215178013\n",
      "======> epoch: 121/400, Loss:0.01858520694077015\n",
      "======> epoch: 121/400, Loss:0.020178772509098053\n",
      "======> epoch: 121/400, Loss:0.014525906182825565\n",
      "======> epoch: 121/400, Loss:0.01579088531434536\n",
      "======> epoch: 121/400, Loss:0.01799588091671467\n",
      "Entering Epoch:  122\n",
      "======> epoch: 122/400, Loss:0.02203822135925293\n",
      "======> epoch: 122/400, Loss:0.01688540354371071\n",
      "======> epoch: 122/400, Loss:0.019717518240213394\n",
      "======> epoch: 122/400, Loss:0.0189724899828434\n",
      "======> epoch: 122/400, Loss:0.021414773538708687\n",
      "======> epoch: 122/400, Loss:0.019811347126960754\n",
      "======> epoch: 122/400, Loss:0.02157900668680668\n",
      "======> epoch: 122/400, Loss:0.01957947015762329\n",
      "======> epoch: 122/400, Loss:0.021135766059160233\n",
      "======> epoch: 122/400, Loss:0.016347704455256462\n",
      "Entering Epoch:  123\n",
      "======> epoch: 123/400, Loss:0.01900716871023178\n",
      "======> epoch: 123/400, Loss:0.017138775438070297\n",
      "======> epoch: 123/400, Loss:0.01529424637556076\n",
      "======> epoch: 123/400, Loss:0.01650273986160755\n",
      "======> epoch: 123/400, Loss:0.02520068921148777\n",
      "======> epoch: 123/400, Loss:0.01678188517689705\n",
      "======> epoch: 123/400, Loss:0.017055321484804153\n",
      "======> epoch: 123/400, Loss:0.013436906039714813\n",
      "======> epoch: 123/400, Loss:0.01981869339942932\n",
      "======> epoch: 123/400, Loss:0.014144590124487877\n",
      "Entering Epoch:  124\n",
      "======> epoch: 124/400, Loss:0.014576558955013752\n",
      "======> epoch: 124/400, Loss:0.017540114000439644\n",
      "======> epoch: 124/400, Loss:0.019204331561923027\n",
      "======> epoch: 124/400, Loss:0.012402721680700779\n",
      "======> epoch: 124/400, Loss:0.014685594476759434\n",
      "======> epoch: 124/400, Loss:0.020748091861605644\n",
      "======> epoch: 124/400, Loss:0.0199492946267128\n",
      "======> epoch: 124/400, Loss:0.01788429729640484\n",
      "======> epoch: 124/400, Loss:0.019592437893152237\n",
      "======> epoch: 124/400, Loss:0.012629625387489796\n",
      "Entering Epoch:  125\n",
      "======> epoch: 125/400, Loss:0.02163633145391941\n",
      "======> epoch: 125/400, Loss:0.014346035197377205\n",
      "======> epoch: 125/400, Loss:0.015173682942986488\n",
      "======> epoch: 125/400, Loss:0.018599938601255417\n",
      "======> epoch: 125/400, Loss:0.014087403193116188\n",
      "======> epoch: 125/400, Loss:0.02174030803143978\n",
      "======> epoch: 125/400, Loss:0.017083680257201195\n",
      "======> epoch: 125/400, Loss:0.016376882791519165\n",
      "======> epoch: 125/400, Loss:0.016363853588700294\n",
      "======> epoch: 125/400, Loss:0.014119391329586506\n",
      "Entering Epoch:  126\n",
      "======> epoch: 126/400, Loss:0.019378483295440674\n",
      "======> epoch: 126/400, Loss:0.020382078364491463\n",
      "======> epoch: 126/400, Loss:0.016620781272649765\n",
      "======> epoch: 126/400, Loss:0.019580665975809097\n",
      "======> epoch: 126/400, Loss:0.018458465114235878\n",
      "======> epoch: 126/400, Loss:0.022663675248622894\n",
      "======> epoch: 126/400, Loss:0.01952512189745903\n",
      "======> epoch: 126/400, Loss:0.012979494407773018\n",
      "======> epoch: 126/400, Loss:0.015802515670657158\n",
      "======> epoch: 126/400, Loss:0.022644415497779846\n",
      "Entering Epoch:  127\n",
      "======> epoch: 127/400, Loss:0.018992379307746887\n",
      "======> epoch: 127/400, Loss:0.014564352110028267\n",
      "======> epoch: 127/400, Loss:0.017980655655264854\n",
      "======> epoch: 127/400, Loss:0.01783907227218151\n",
      "======> epoch: 127/400, Loss:0.024631531909108162\n",
      "======> epoch: 127/400, Loss:0.016646888107061386\n",
      "======> epoch: 127/400, Loss:0.020216334611177444\n",
      "======> epoch: 127/400, Loss:0.012367036193609238\n",
      "======> epoch: 127/400, Loss:0.02199655957520008\n",
      "======> epoch: 127/400, Loss:0.016076525673270226\n",
      "Entering Epoch:  128\n",
      "======> epoch: 128/400, Loss:0.014940529130399227\n",
      "======> epoch: 128/400, Loss:0.0181101206690073\n",
      "======> epoch: 128/400, Loss:0.017845742404460907\n",
      "======> epoch: 128/400, Loss:0.01737699843943119\n",
      "======> epoch: 128/400, Loss:0.02030806802213192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 128/400, Loss:0.020086318254470825\n",
      "======> epoch: 128/400, Loss:0.0210579764097929\n",
      "======> epoch: 128/400, Loss:0.018759699538350105\n",
      "======> epoch: 128/400, Loss:0.013461739756166935\n",
      "======> epoch: 128/400, Loss:0.015179182402789593\n",
      "Entering Epoch:  129\n",
      "======> epoch: 129/400, Loss:0.01689883880317211\n",
      "======> epoch: 129/400, Loss:0.017024466767907143\n",
      "======> epoch: 129/400, Loss:0.015964502468705177\n",
      "======> epoch: 129/400, Loss:0.020581135526299477\n",
      "======> epoch: 129/400, Loss:0.023015422746539116\n",
      "======> epoch: 129/400, Loss:0.01483611948788166\n",
      "======> epoch: 129/400, Loss:0.019798703491687775\n",
      "======> epoch: 129/400, Loss:0.014778640121221542\n",
      "======> epoch: 129/400, Loss:0.023955075070261955\n",
      "======> epoch: 129/400, Loss:0.018813593313097954\n",
      "Entering Epoch:  130\n",
      "======> epoch: 130/400, Loss:0.014007956720888615\n",
      "======> epoch: 130/400, Loss:0.012096203863620758\n",
      "======> epoch: 130/400, Loss:0.01914568431675434\n",
      "======> epoch: 130/400, Loss:0.018365491181612015\n",
      "======> epoch: 130/400, Loss:0.019548358395695686\n",
      "======> epoch: 130/400, Loss:0.016931094229221344\n",
      "======> epoch: 130/400, Loss:0.01994815282523632\n",
      "======> epoch: 130/400, Loss:0.017401931807398796\n",
      "======> epoch: 130/400, Loss:0.019240370020270348\n",
      "======> epoch: 130/400, Loss:0.020044585689902306\n",
      "Entering Epoch:  131\n",
      "======> epoch: 131/400, Loss:0.02318691648542881\n",
      "======> epoch: 131/400, Loss:0.01606825366616249\n",
      "======> epoch: 131/400, Loss:0.016074327751994133\n",
      "======> epoch: 131/400, Loss:0.01993294432759285\n",
      "======> epoch: 131/400, Loss:0.019630514085292816\n",
      "======> epoch: 131/400, Loss:0.01236413512378931\n",
      "======> epoch: 131/400, Loss:0.01944124512374401\n",
      "======> epoch: 131/400, Loss:0.014333121478557587\n",
      "======> epoch: 131/400, Loss:0.018721114844083786\n",
      "======> epoch: 131/400, Loss:0.016527164727449417\n",
      "Entering Epoch:  132\n",
      "======> epoch: 132/400, Loss:0.015042094513773918\n",
      "======> epoch: 132/400, Loss:0.015002782456576824\n",
      "======> epoch: 132/400, Loss:0.018315566703677177\n",
      "======> epoch: 132/400, Loss:0.01512665580958128\n",
      "======> epoch: 132/400, Loss:0.01762252487242222\n",
      "======> epoch: 132/400, Loss:0.02214154228568077\n",
      "======> epoch: 132/400, Loss:0.01552991010248661\n",
      "======> epoch: 132/400, Loss:0.01994502916932106\n",
      "======> epoch: 132/400, Loss:0.01572263240814209\n",
      "======> epoch: 132/400, Loss:0.017327414825558662\n",
      "Entering Epoch:  133\n",
      "======> epoch: 133/400, Loss:0.020662594586610794\n",
      "======> epoch: 133/400, Loss:0.019806990399956703\n",
      "======> epoch: 133/400, Loss:0.01871723309159279\n",
      "======> epoch: 133/400, Loss:0.014669020660221577\n",
      "======> epoch: 133/400, Loss:0.022289225831627846\n",
      "======> epoch: 133/400, Loss:0.01907223090529442\n",
      "======> epoch: 133/400, Loss:0.012399645522236824\n",
      "======> epoch: 133/400, Loss:0.019332103431224823\n",
      "======> epoch: 133/400, Loss:0.015080831944942474\n",
      "======> epoch: 133/400, Loss:0.018583163619041443\n",
      "Entering Epoch:  134\n",
      "======> epoch: 134/400, Loss:0.014933156780898571\n",
      "======> epoch: 134/400, Loss:0.0205109640955925\n",
      "======> epoch: 134/400, Loss:0.019599907100200653\n",
      "======> epoch: 134/400, Loss:0.02068351022899151\n",
      "======> epoch: 134/400, Loss:0.011784396134316921\n",
      "======> epoch: 134/400, Loss:0.018110264092683792\n",
      "======> epoch: 134/400, Loss:0.014047397300601006\n",
      "======> epoch: 134/400, Loss:0.0185944102704525\n",
      "======> epoch: 134/400, Loss:0.02391187846660614\n",
      "======> epoch: 134/400, Loss:0.022806433960795403\n",
      "Entering Epoch:  135\n",
      "======> epoch: 135/400, Loss:0.015757417306303978\n",
      "======> epoch: 135/400, Loss:0.017783137038350105\n",
      "======> epoch: 135/400, Loss:0.022302890196442604\n",
      "======> epoch: 135/400, Loss:0.01414328534156084\n",
      "======> epoch: 135/400, Loss:0.01470763050019741\n",
      "======> epoch: 135/400, Loss:0.017678983509540558\n",
      "======> epoch: 135/400, Loss:0.0199377853423357\n",
      "======> epoch: 135/400, Loss:0.02536727860569954\n",
      "======> epoch: 135/400, Loss:0.022078439593315125\n",
      "======> epoch: 135/400, Loss:0.02302396669983864\n",
      "Entering Epoch:  136\n",
      "======> epoch: 136/400, Loss:0.021380750462412834\n",
      "======> epoch: 136/400, Loss:0.018117353320121765\n",
      "======> epoch: 136/400, Loss:0.023236270993947983\n",
      "======> epoch: 136/400, Loss:0.017537139356136322\n",
      "======> epoch: 136/400, Loss:0.011811258271336555\n",
      "======> epoch: 136/400, Loss:0.015998942777514458\n",
      "======> epoch: 136/400, Loss:0.016907919198274612\n",
      "======> epoch: 136/400, Loss:0.013603629544377327\n",
      "======> epoch: 136/400, Loss:0.027080688625574112\n",
      "======> epoch: 136/400, Loss:0.017970416694879532\n",
      "Entering Epoch:  137\n",
      "======> epoch: 137/400, Loss:0.018734373152256012\n",
      "======> epoch: 137/400, Loss:0.017872758209705353\n",
      "======> epoch: 137/400, Loss:0.014686135575175285\n",
      "======> epoch: 137/400, Loss:0.01554284431040287\n",
      "======> epoch: 137/400, Loss:0.01999376155436039\n",
      "======> epoch: 137/400, Loss:0.013485677540302277\n",
      "======> epoch: 137/400, Loss:0.018987908959388733\n",
      "======> epoch: 137/400, Loss:0.020789436995983124\n",
      "======> epoch: 137/400, Loss:0.014277872629463673\n",
      "======> epoch: 137/400, Loss:0.018022725358605385\n",
      "Entering Epoch:  138\n",
      "======> epoch: 138/400, Loss:0.013273663818836212\n",
      "======> epoch: 138/400, Loss:0.020243072882294655\n",
      "======> epoch: 138/400, Loss:0.016796721145510674\n",
      "======> epoch: 138/400, Loss:0.018966326490044594\n",
      "======> epoch: 138/400, Loss:0.020532680675387383\n",
      "======> epoch: 138/400, Loss:0.018277570605278015\n",
      "======> epoch: 138/400, Loss:0.02241763286292553\n",
      "======> epoch: 138/400, Loss:0.018310686573386192\n",
      "======> epoch: 138/400, Loss:0.015080233104526997\n",
      "======> epoch: 138/400, Loss:0.017752235755324364\n",
      "Entering Epoch:  139\n",
      "======> epoch: 139/400, Loss:0.01944596879184246\n",
      "======> epoch: 139/400, Loss:0.016727566719055176\n",
      "======> epoch: 139/400, Loss:0.019195852801203728\n",
      "======> epoch: 139/400, Loss:0.02259988524019718\n",
      "======> epoch: 139/400, Loss:0.01839415915310383\n",
      "======> epoch: 139/400, Loss:0.017343156039714813\n",
      "======> epoch: 139/400, Loss:0.019190648570656776\n",
      "======> epoch: 139/400, Loss:0.013953014276921749\n",
      "======> epoch: 139/400, Loss:0.01903495565056801\n",
      "======> epoch: 139/400, Loss:0.014821341261267662\n",
      "Entering Epoch:  140\n",
      "======> epoch: 140/400, Loss:0.011328514665365219\n",
      "======> epoch: 140/400, Loss:0.01708892732858658\n",
      "======> epoch: 140/400, Loss:0.011174174956977367\n",
      "======> epoch: 140/400, Loss:0.014869575388729572\n",
      "======> epoch: 140/400, Loss:0.02110080048441887\n",
      "======> epoch: 140/400, Loss:0.01524672657251358\n",
      "======> epoch: 140/400, Loss:0.02217382937669754\n",
      "======> epoch: 140/400, Loss:0.024540962651371956\n",
      "======> epoch: 140/400, Loss:0.016478894278407097\n",
      "======> epoch: 140/400, Loss:0.015542712062597275\n",
      "Entering Epoch:  141\n",
      "======> epoch: 141/400, Loss:0.01845034584403038\n",
      "======> epoch: 141/400, Loss:0.019841264933347702\n",
      "======> epoch: 141/400, Loss:0.025648269802331924\n",
      "======> epoch: 141/400, Loss:0.015639519318938255\n",
      "======> epoch: 141/400, Loss:0.014588849619030952\n",
      "======> epoch: 141/400, Loss:0.02167559415102005\n",
      "======> epoch: 141/400, Loss:0.013747169636189938\n",
      "======> epoch: 141/400, Loss:0.019705750048160553\n",
      "======> epoch: 141/400, Loss:0.018550770357251167\n",
      "======> epoch: 141/400, Loss:0.019686443731188774\n",
      "Entering Epoch:  142\n",
      "======> epoch: 142/400, Loss:0.01616567373275757\n",
      "======> epoch: 142/400, Loss:0.01756896637380123\n",
      "======> epoch: 142/400, Loss:0.01761801913380623\n",
      "======> epoch: 142/400, Loss:0.012957111932337284\n",
      "======> epoch: 142/400, Loss:0.013268174603581429\n",
      "======> epoch: 142/400, Loss:0.0175323449075222\n",
      "======> epoch: 142/400, Loss:0.016207586973905563\n",
      "======> epoch: 142/400, Loss:0.019503870978951454\n",
      "======> epoch: 142/400, Loss:0.02096078172326088\n",
      "======> epoch: 142/400, Loss:0.0184900164604187\n",
      "Entering Epoch:  143\n",
      "======> epoch: 143/400, Loss:0.01464469451457262\n",
      "======> epoch: 143/400, Loss:0.01604868657886982\n",
      "======> epoch: 143/400, Loss:0.013036418706178665\n",
      "======> epoch: 143/400, Loss:0.021703148260712624\n",
      "======> epoch: 143/400, Loss:0.015712372958660126\n",
      "======> epoch: 143/400, Loss:0.01725335605442524\n",
      "======> epoch: 143/400, Loss:0.016085784882307053\n",
      "======> epoch: 143/400, Loss:0.022524451836943626\n",
      "======> epoch: 143/400, Loss:0.014738996513187885\n",
      "======> epoch: 143/400, Loss:0.013080825097858906\n",
      "Entering Epoch:  144\n",
      "======> epoch: 144/400, Loss:0.01929313689470291\n",
      "======> epoch: 144/400, Loss:0.013538466766476631\n",
      "======> epoch: 144/400, Loss:0.01888812519609928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 144/400, Loss:0.014609549194574356\n",
      "======> epoch: 144/400, Loss:0.018272720277309418\n",
      "======> epoch: 144/400, Loss:0.015483234077692032\n",
      "======> epoch: 144/400, Loss:0.01830294169485569\n",
      "======> epoch: 144/400, Loss:0.0212271586060524\n",
      "======> epoch: 144/400, Loss:0.017267093062400818\n",
      "======> epoch: 144/400, Loss:0.01781364530324936\n",
      "Entering Epoch:  145\n",
      "======> epoch: 145/400, Loss:0.02182939648628235\n",
      "======> epoch: 145/400, Loss:0.02509624883532524\n",
      "======> epoch: 145/400, Loss:0.018447453156113625\n",
      "======> epoch: 145/400, Loss:0.01988799497485161\n",
      "======> epoch: 145/400, Loss:0.01461021788418293\n",
      "======> epoch: 145/400, Loss:0.021875977516174316\n",
      "======> epoch: 145/400, Loss:0.01690049283206463\n",
      "======> epoch: 145/400, Loss:0.023074250668287277\n",
      "======> epoch: 145/400, Loss:0.01302946638315916\n",
      "======> epoch: 145/400, Loss:0.015562397427856922\n",
      "Entering Epoch:  146\n",
      "======> epoch: 146/400, Loss:0.015055586583912373\n",
      "======> epoch: 146/400, Loss:0.015044519677758217\n",
      "======> epoch: 146/400, Loss:0.014869402162730694\n",
      "======> epoch: 146/400, Loss:0.014806557446718216\n",
      "======> epoch: 146/400, Loss:0.019367573782801628\n",
      "======> epoch: 146/400, Loss:0.023029372096061707\n",
      "======> epoch: 146/400, Loss:0.015741493552923203\n",
      "======> epoch: 146/400, Loss:0.018531976267695427\n",
      "======> epoch: 146/400, Loss:0.01782127469778061\n",
      "======> epoch: 146/400, Loss:0.019880386069417\n",
      "Entering Epoch:  147\n",
      "======> epoch: 147/400, Loss:0.02050843834877014\n",
      "======> epoch: 147/400, Loss:0.014911629259586334\n",
      "======> epoch: 147/400, Loss:0.021502021700143814\n",
      "======> epoch: 147/400, Loss:0.01569933257997036\n",
      "======> epoch: 147/400, Loss:0.017536630854010582\n",
      "======> epoch: 147/400, Loss:0.02139725722372532\n",
      "======> epoch: 147/400, Loss:0.017517831176519394\n",
      "======> epoch: 147/400, Loss:0.013774212449789047\n",
      "======> epoch: 147/400, Loss:0.01727122999727726\n",
      "======> epoch: 147/400, Loss:0.020054969936609268\n",
      "Entering Epoch:  148\n",
      "======> epoch: 148/400, Loss:0.016563037410378456\n",
      "======> epoch: 148/400, Loss:0.017505208030343056\n",
      "======> epoch: 148/400, Loss:0.014119341038167477\n",
      "======> epoch: 148/400, Loss:0.014408785849809647\n",
      "======> epoch: 148/400, Loss:0.013982148841023445\n",
      "======> epoch: 148/400, Loss:0.016214318573474884\n",
      "======> epoch: 148/400, Loss:0.018494144082069397\n",
      "======> epoch: 148/400, Loss:0.018500251695513725\n",
      "======> epoch: 148/400, Loss:0.019740216434001923\n",
      "======> epoch: 148/400, Loss:0.022466393187642097\n",
      "Entering Epoch:  149\n",
      "======> epoch: 149/400, Loss:0.014969153329730034\n",
      "======> epoch: 149/400, Loss:0.015847571194171906\n",
      "======> epoch: 149/400, Loss:0.018681654706597328\n",
      "======> epoch: 149/400, Loss:0.01709851436316967\n",
      "======> epoch: 149/400, Loss:0.01606632210314274\n",
      "======> epoch: 149/400, Loss:0.019161460921168327\n",
      "======> epoch: 149/400, Loss:0.018541008234024048\n",
      "======> epoch: 149/400, Loss:0.01469355821609497\n",
      "======> epoch: 149/400, Loss:0.016710534691810608\n",
      "======> epoch: 149/400, Loss:0.02141953818500042\n",
      "Entering Epoch:  150\n",
      "======> epoch: 150/400, Loss:0.01550566777586937\n",
      "======> epoch: 150/400, Loss:0.021734818816184998\n",
      "======> epoch: 150/400, Loss:0.016755037009716034\n",
      "======> epoch: 150/400, Loss:0.018831733614206314\n",
      "======> epoch: 150/400, Loss:0.015886930748820305\n",
      "======> epoch: 150/400, Loss:0.01953916624188423\n",
      "======> epoch: 150/400, Loss:0.021452851593494415\n",
      "======> epoch: 150/400, Loss:0.012716991826891899\n",
      "======> epoch: 150/400, Loss:0.020450502634048462\n",
      "======> epoch: 150/400, Loss:0.02221684344112873\n",
      "Entering Epoch:  151\n",
      "======> epoch: 151/400, Loss:0.012224958278238773\n",
      "======> epoch: 151/400, Loss:0.02122795768082142\n",
      "======> epoch: 151/400, Loss:0.020245317369699478\n",
      "======> epoch: 151/400, Loss:0.014734039083123207\n",
      "======> epoch: 151/400, Loss:0.016786742955446243\n",
      "======> epoch: 151/400, Loss:0.019047044217586517\n",
      "======> epoch: 151/400, Loss:0.023145191371440887\n",
      "======> epoch: 151/400, Loss:0.02058560587465763\n",
      "======> epoch: 151/400, Loss:0.015893233940005302\n",
      "======> epoch: 151/400, Loss:0.018813781440258026\n",
      "Entering Epoch:  152\n",
      "======> epoch: 152/400, Loss:0.017741886898875237\n",
      "======> epoch: 152/400, Loss:0.01783740520477295\n",
      "======> epoch: 152/400, Loss:0.01434574369341135\n",
      "======> epoch: 152/400, Loss:0.014841624535620213\n",
      "======> epoch: 152/400, Loss:0.018263427540659904\n",
      "======> epoch: 152/400, Loss:0.021808193996548653\n",
      "======> epoch: 152/400, Loss:0.017309950664639473\n",
      "======> epoch: 152/400, Loss:0.01540234312415123\n",
      "======> epoch: 152/400, Loss:0.017863959074020386\n",
      "======> epoch: 152/400, Loss:0.011718903668224812\n",
      "Entering Epoch:  153\n",
      "======> epoch: 153/400, Loss:0.018638325855135918\n",
      "======> epoch: 153/400, Loss:0.020566128194332123\n",
      "======> epoch: 153/400, Loss:0.020428692921996117\n",
      "======> epoch: 153/400, Loss:0.02009388990700245\n",
      "======> epoch: 153/400, Loss:0.023637283593416214\n",
      "======> epoch: 153/400, Loss:0.0174979530274868\n",
      "======> epoch: 153/400, Loss:0.016836093738675117\n",
      "======> epoch: 153/400, Loss:0.011714373715221882\n",
      "======> epoch: 153/400, Loss:0.013752427883446217\n",
      "======> epoch: 153/400, Loss:0.02012616954743862\n",
      "Entering Epoch:  154\n",
      "======> epoch: 154/400, Loss:0.014614920131862164\n",
      "======> epoch: 154/400, Loss:0.01579609513282776\n",
      "======> epoch: 154/400, Loss:0.02228243090212345\n",
      "======> epoch: 154/400, Loss:0.0255826897919178\n",
      "======> epoch: 154/400, Loss:0.020052725449204445\n",
      "======> epoch: 154/400, Loss:0.022856319323182106\n",
      "======> epoch: 154/400, Loss:0.018346475437283516\n",
      "======> epoch: 154/400, Loss:0.015820857137441635\n",
      "======> epoch: 154/400, Loss:0.0167356189340353\n",
      "======> epoch: 154/400, Loss:0.018941303715109825\n",
      "Entering Epoch:  155\n",
      "======> epoch: 155/400, Loss:0.01656537503004074\n",
      "======> epoch: 155/400, Loss:0.014592326246201992\n",
      "======> epoch: 155/400, Loss:0.019168514758348465\n",
      "======> epoch: 155/400, Loss:0.01273962389677763\n",
      "======> epoch: 155/400, Loss:0.014278711751103401\n",
      "======> epoch: 155/400, Loss:0.018985949456691742\n",
      "======> epoch: 155/400, Loss:0.017956966534256935\n",
      "======> epoch: 155/400, Loss:0.014825076796114445\n",
      "======> epoch: 155/400, Loss:0.01821255311369896\n",
      "======> epoch: 155/400, Loss:0.018574688583612442\n",
      "Entering Epoch:  156\n",
      "======> epoch: 156/400, Loss:0.019953623414039612\n",
      "======> epoch: 156/400, Loss:0.020153282210230827\n",
      "======> epoch: 156/400, Loss:0.015041244216263294\n",
      "======> epoch: 156/400, Loss:0.014623403549194336\n",
      "======> epoch: 156/400, Loss:0.017364613711833954\n",
      "======> epoch: 156/400, Loss:0.017700443044304848\n",
      "======> epoch: 156/400, Loss:0.01996968314051628\n",
      "======> epoch: 156/400, Loss:0.016148770228028297\n",
      "======> epoch: 156/400, Loss:0.018290890380740166\n",
      "======> epoch: 156/400, Loss:0.01612815074622631\n",
      "Entering Epoch:  157\n",
      "======> epoch: 157/400, Loss:0.018265148624777794\n",
      "======> epoch: 157/400, Loss:0.01703488640487194\n",
      "======> epoch: 157/400, Loss:0.014263066463172436\n",
      "======> epoch: 157/400, Loss:0.013201543129980564\n",
      "======> epoch: 157/400, Loss:0.018582310527563095\n",
      "======> epoch: 157/400, Loss:0.020014792680740356\n",
      "======> epoch: 157/400, Loss:0.016808077692985535\n",
      "======> epoch: 157/400, Loss:0.015398332849144936\n",
      "======> epoch: 157/400, Loss:0.01564348302781582\n",
      "======> epoch: 157/400, Loss:0.022537266835570335\n",
      "Entering Epoch:  158\n",
      "======> epoch: 158/400, Loss:0.01689768023788929\n",
      "======> epoch: 158/400, Loss:0.0137567650526762\n",
      "======> epoch: 158/400, Loss:0.016474571079015732\n",
      "======> epoch: 158/400, Loss:0.026845328509807587\n",
      "======> epoch: 158/400, Loss:0.015905946493148804\n",
      "======> epoch: 158/400, Loss:0.019182845950126648\n",
      "======> epoch: 158/400, Loss:0.013033023104071617\n",
      "======> epoch: 158/400, Loss:0.019054574891924858\n",
      "======> epoch: 158/400, Loss:0.018239106982946396\n",
      "======> epoch: 158/400, Loss:0.015397398732602596\n",
      "Entering Epoch:  159\n",
      "======> epoch: 159/400, Loss:0.011633907444775105\n",
      "======> epoch: 159/400, Loss:0.016684632748365402\n",
      "======> epoch: 159/400, Loss:0.014584728516638279\n",
      "======> epoch: 159/400, Loss:0.017719294875860214\n",
      "======> epoch: 159/400, Loss:0.019074225798249245\n",
      "======> epoch: 159/400, Loss:0.017993900924921036\n",
      "======> epoch: 159/400, Loss:0.018567105755209923\n",
      "======> epoch: 159/400, Loss:0.018326016142964363\n",
      "======> epoch: 159/400, Loss:0.01731237582862377\n",
      "======> epoch: 159/400, Loss:0.017086908221244812\n",
      "Entering Epoch:  160\n",
      "======> epoch: 160/400, Loss:0.01930445060133934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 160/400, Loss:0.017594169825315475\n",
      "======> epoch: 160/400, Loss:0.023464038968086243\n",
      "======> epoch: 160/400, Loss:0.013467315584421158\n",
      "======> epoch: 160/400, Loss:0.017054831609129906\n",
      "======> epoch: 160/400, Loss:0.01706925965845585\n",
      "======> epoch: 160/400, Loss:0.01846933551132679\n",
      "======> epoch: 160/400, Loss:0.015451280400156975\n",
      "======> epoch: 160/400, Loss:0.018298618495464325\n",
      "======> epoch: 160/400, Loss:0.018885942175984383\n",
      "Entering Epoch:  161\n",
      "======> epoch: 161/400, Loss:0.016416043043136597\n",
      "======> epoch: 161/400, Loss:0.019033338874578476\n",
      "======> epoch: 161/400, Loss:0.012486433610320091\n",
      "======> epoch: 161/400, Loss:0.01892702281475067\n",
      "======> epoch: 161/400, Loss:0.01730101741850376\n",
      "======> epoch: 161/400, Loss:0.019448483362793922\n",
      "======> epoch: 161/400, Loss:0.01767205260694027\n",
      "======> epoch: 161/400, Loss:0.01486187893897295\n",
      "======> epoch: 161/400, Loss:0.015836115926504135\n",
      "======> epoch: 161/400, Loss:0.015821240842342377\n",
      "Entering Epoch:  162\n",
      "======> epoch: 162/400, Loss:0.018231084570288658\n",
      "======> epoch: 162/400, Loss:0.01444757916033268\n",
      "======> epoch: 162/400, Loss:0.01611512340605259\n",
      "======> epoch: 162/400, Loss:0.01689099706709385\n",
      "======> epoch: 162/400, Loss:0.014238163828849792\n",
      "======> epoch: 162/400, Loss:0.021806905046105385\n",
      "======> epoch: 162/400, Loss:0.022901447489857674\n",
      "======> epoch: 162/400, Loss:0.019191309809684753\n",
      "======> epoch: 162/400, Loss:0.018215220421552658\n",
      "======> epoch: 162/400, Loss:0.020012425258755684\n",
      "Entering Epoch:  163\n",
      "======> epoch: 163/400, Loss:0.015996484085917473\n",
      "======> epoch: 163/400, Loss:0.014105853624641895\n",
      "======> epoch: 163/400, Loss:0.01761278137564659\n",
      "======> epoch: 163/400, Loss:0.019344903528690338\n",
      "======> epoch: 163/400, Loss:0.02105235680937767\n",
      "======> epoch: 163/400, Loss:0.019607478752732277\n",
      "======> epoch: 163/400, Loss:0.015028298832476139\n",
      "======> epoch: 163/400, Loss:0.015136436559259892\n",
      "======> epoch: 163/400, Loss:0.013163741677999496\n",
      "======> epoch: 163/400, Loss:0.013455444015562534\n",
      "Entering Epoch:  164\n",
      "======> epoch: 164/400, Loss:0.019543906673789024\n",
      "======> epoch: 164/400, Loss:0.018959226086735725\n",
      "======> epoch: 164/400, Loss:0.017466800287365913\n",
      "======> epoch: 164/400, Loss:0.01906702294945717\n",
      "======> epoch: 164/400, Loss:0.019431034103035927\n",
      "======> epoch: 164/400, Loss:0.012392179109156132\n",
      "======> epoch: 164/400, Loss:0.016935361549258232\n",
      "======> epoch: 164/400, Loss:0.02079557627439499\n",
      "======> epoch: 164/400, Loss:0.012848157435655594\n",
      "======> epoch: 164/400, Loss:0.014783959835767746\n",
      "Entering Epoch:  165\n",
      "======> epoch: 165/400, Loss:0.01627722755074501\n",
      "======> epoch: 165/400, Loss:0.017151566222310066\n",
      "======> epoch: 165/400, Loss:0.02364133670926094\n",
      "======> epoch: 165/400, Loss:0.017088228836655617\n",
      "======> epoch: 165/400, Loss:0.013941804878413677\n",
      "======> epoch: 165/400, Loss:0.01854008249938488\n",
      "======> epoch: 165/400, Loss:0.01644793152809143\n",
      "======> epoch: 165/400, Loss:0.01429162360727787\n",
      "======> epoch: 165/400, Loss:0.012707212008535862\n",
      "======> epoch: 165/400, Loss:0.018654724583029747\n",
      "Entering Epoch:  166\n",
      "======> epoch: 166/400, Loss:0.01533995009958744\n",
      "======> epoch: 166/400, Loss:0.020385470241308212\n",
      "======> epoch: 166/400, Loss:0.020708836615085602\n",
      "======> epoch: 166/400, Loss:0.017298847436904907\n",
      "======> epoch: 166/400, Loss:0.018990343436598778\n",
      "======> epoch: 166/400, Loss:0.01764584891498089\n",
      "======> epoch: 166/400, Loss:0.018686911091208458\n",
      "======> epoch: 166/400, Loss:0.01984318718314171\n",
      "======> epoch: 166/400, Loss:0.010567350313067436\n",
      "======> epoch: 166/400, Loss:0.017843114212155342\n",
      "Entering Epoch:  167\n",
      "======> epoch: 167/400, Loss:0.017830865457654\n",
      "======> epoch: 167/400, Loss:0.017281850799918175\n",
      "======> epoch: 167/400, Loss:0.016982145607471466\n",
      "======> epoch: 167/400, Loss:0.01773218624293804\n",
      "======> epoch: 167/400, Loss:0.016488585621118546\n",
      "======> epoch: 167/400, Loss:0.013635436072945595\n",
      "======> epoch: 167/400, Loss:0.013412905856966972\n",
      "======> epoch: 167/400, Loss:0.012741697952151299\n",
      "======> epoch: 167/400, Loss:0.021718736737966537\n",
      "======> epoch: 167/400, Loss:0.016245149075984955\n",
      "Entering Epoch:  168\n",
      "======> epoch: 168/400, Loss:0.02816995047032833\n",
      "======> epoch: 168/400, Loss:0.01960509829223156\n",
      "======> epoch: 168/400, Loss:0.017008397728204727\n",
      "======> epoch: 168/400, Loss:0.014879778027534485\n",
      "======> epoch: 168/400, Loss:0.02049415372312069\n",
      "======> epoch: 168/400, Loss:0.02044474519789219\n",
      "======> epoch: 168/400, Loss:0.02119382843375206\n",
      "======> epoch: 168/400, Loss:0.014815417118370533\n",
      "======> epoch: 168/400, Loss:0.015622392296791077\n",
      "======> epoch: 168/400, Loss:0.01891244761645794\n",
      "Entering Epoch:  169\n",
      "======> epoch: 169/400, Loss:0.015411191619932652\n",
      "======> epoch: 169/400, Loss:0.017335573211312294\n",
      "======> epoch: 169/400, Loss:0.024500731378793716\n",
      "======> epoch: 169/400, Loss:0.018512440845370293\n",
      "======> epoch: 169/400, Loss:0.020624520257115364\n",
      "======> epoch: 169/400, Loss:0.01369782816618681\n",
      "======> epoch: 169/400, Loss:0.01857137680053711\n",
      "======> epoch: 169/400, Loss:0.01532950159162283\n",
      "======> epoch: 169/400, Loss:0.0161701999604702\n",
      "======> epoch: 169/400, Loss:0.013176108710467815\n",
      "Entering Epoch:  170\n",
      "======> epoch: 170/400, Loss:0.02476024068892002\n",
      "======> epoch: 170/400, Loss:0.018143873661756516\n",
      "======> epoch: 170/400, Loss:0.017218338325619698\n",
      "======> epoch: 170/400, Loss:0.013485587202012539\n",
      "======> epoch: 170/400, Loss:0.018797164782881737\n",
      "======> epoch: 170/400, Loss:0.019825609400868416\n",
      "======> epoch: 170/400, Loss:0.018961334601044655\n",
      "======> epoch: 170/400, Loss:0.016582462936639786\n",
      "======> epoch: 170/400, Loss:0.017259320244193077\n",
      "======> epoch: 170/400, Loss:0.013878142461180687\n",
      "Entering Epoch:  171\n",
      "======> epoch: 171/400, Loss:0.02191009558737278\n",
      "======> epoch: 171/400, Loss:0.017056003212928772\n",
      "======> epoch: 171/400, Loss:0.031013356521725655\n",
      "======> epoch: 171/400, Loss:0.016487058252096176\n",
      "======> epoch: 171/400, Loss:0.02380071021616459\n",
      "======> epoch: 171/400, Loss:0.01902152970433235\n",
      "======> epoch: 171/400, Loss:0.014326636679470539\n",
      "======> epoch: 171/400, Loss:0.016989873722195625\n",
      "======> epoch: 171/400, Loss:0.018764827400445938\n",
      "======> epoch: 171/400, Loss:0.015069517306983471\n",
      "Entering Epoch:  172\n",
      "======> epoch: 172/400, Loss:0.023308472707867622\n",
      "======> epoch: 172/400, Loss:0.01966216042637825\n",
      "======> epoch: 172/400, Loss:0.015053654089570045\n",
      "======> epoch: 172/400, Loss:0.013949702493846416\n",
      "======> epoch: 172/400, Loss:0.025440223515033722\n",
      "======> epoch: 172/400, Loss:0.014483368955552578\n",
      "======> epoch: 172/400, Loss:0.017455099150538445\n",
      "======> epoch: 172/400, Loss:0.014403634704649448\n",
      "======> epoch: 172/400, Loss:0.01635207049548626\n",
      "======> epoch: 172/400, Loss:0.011472590267658234\n",
      "Entering Epoch:  173\n",
      "======> epoch: 173/400, Loss:0.018974201753735542\n",
      "======> epoch: 173/400, Loss:0.018085423856973648\n",
      "======> epoch: 173/400, Loss:0.01621992327272892\n",
      "======> epoch: 173/400, Loss:0.014338244684040546\n",
      "======> epoch: 173/400, Loss:0.013360245153307915\n",
      "======> epoch: 173/400, Loss:0.015122728422284126\n",
      "======> epoch: 173/400, Loss:0.020082393661141396\n",
      "======> epoch: 173/400, Loss:0.016873028129339218\n",
      "======> epoch: 173/400, Loss:0.019415726885199547\n",
      "======> epoch: 173/400, Loss:0.026503244414925575\n",
      "Entering Epoch:  174\n",
      "======> epoch: 174/400, Loss:0.019193638116121292\n",
      "======> epoch: 174/400, Loss:0.015074027702212334\n",
      "======> epoch: 174/400, Loss:0.01744353398680687\n",
      "======> epoch: 174/400, Loss:0.022540561854839325\n",
      "======> epoch: 174/400, Loss:0.018082376569509506\n",
      "======> epoch: 174/400, Loss:0.017523974180221558\n",
      "======> epoch: 174/400, Loss:0.013616394251585007\n",
      "======> epoch: 174/400, Loss:0.014446550980210304\n",
      "======> epoch: 174/400, Loss:0.015456930734217167\n",
      "======> epoch: 174/400, Loss:0.020712953060865402\n",
      "Entering Epoch:  175\n",
      "======> epoch: 175/400, Loss:0.02625388652086258\n",
      "======> epoch: 175/400, Loss:0.021612009033560753\n",
      "======> epoch: 175/400, Loss:0.020029477775096893\n",
      "======> epoch: 175/400, Loss:0.017952557653188705\n",
      "======> epoch: 175/400, Loss:0.01966901496052742\n",
      "======> epoch: 175/400, Loss:0.019313661381602287\n",
      "======> epoch: 175/400, Loss:0.016276434063911438\n",
      "======> epoch: 175/400, Loss:0.0150393545627594\n",
      "======> epoch: 175/400, Loss:0.014732496812939644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 175/400, Loss:0.01922120712697506\n",
      "Entering Epoch:  176\n",
      "======> epoch: 176/400, Loss:0.02055048570036888\n",
      "======> epoch: 176/400, Loss:0.016757667064666748\n",
      "======> epoch: 176/400, Loss:0.020227467641234398\n",
      "======> epoch: 176/400, Loss:0.01653166115283966\n",
      "======> epoch: 176/400, Loss:0.015017963014543056\n",
      "======> epoch: 176/400, Loss:0.01562798209488392\n",
      "======> epoch: 176/400, Loss:0.020130639895796776\n",
      "======> epoch: 176/400, Loss:0.019837545230984688\n",
      "======> epoch: 176/400, Loss:0.023763272911310196\n",
      "======> epoch: 176/400, Loss:0.014814069494605064\n",
      "Entering Epoch:  177\n",
      "======> epoch: 177/400, Loss:0.01704726740717888\n",
      "======> epoch: 177/400, Loss:0.020217174664139748\n",
      "======> epoch: 177/400, Loss:0.019396726042032242\n",
      "======> epoch: 177/400, Loss:0.01904858648777008\n",
      "======> epoch: 177/400, Loss:0.016904182732105255\n",
      "======> epoch: 177/400, Loss:0.02484145015478134\n",
      "======> epoch: 177/400, Loss:0.015157217159867287\n",
      "======> epoch: 177/400, Loss:0.01720207929611206\n",
      "======> epoch: 177/400, Loss:0.015381264500319958\n",
      "======> epoch: 177/400, Loss:0.018245890736579895\n",
      "Entering Epoch:  178\n",
      "======> epoch: 178/400, Loss:0.016082139685750008\n",
      "======> epoch: 178/400, Loss:0.013150409795343876\n",
      "======> epoch: 178/400, Loss:0.019534502178430557\n",
      "======> epoch: 178/400, Loss:0.01784694939851761\n",
      "======> epoch: 178/400, Loss:0.014695892110466957\n",
      "======> epoch: 178/400, Loss:0.015442422591149807\n",
      "======> epoch: 178/400, Loss:0.022722050547599792\n",
      "======> epoch: 178/400, Loss:0.01660989224910736\n",
      "======> epoch: 178/400, Loss:0.017135055735707283\n",
      "======> epoch: 178/400, Loss:0.021404651924967766\n",
      "Entering Epoch:  179\n",
      "======> epoch: 179/400, Loss:0.01539467740803957\n",
      "======> epoch: 179/400, Loss:0.022845013067126274\n",
      "======> epoch: 179/400, Loss:0.01304430142045021\n",
      "======> epoch: 179/400, Loss:0.017687493935227394\n",
      "======> epoch: 179/400, Loss:0.016044655814766884\n",
      "======> epoch: 179/400, Loss:0.014103250578045845\n",
      "======> epoch: 179/400, Loss:0.017843807116150856\n",
      "======> epoch: 179/400, Loss:0.016154862940311432\n",
      "======> epoch: 179/400, Loss:0.015262100845575333\n",
      "======> epoch: 179/400, Loss:0.02026318944990635\n",
      "Entering Epoch:  180\n",
      "======> epoch: 180/400, Loss:0.018767256289720535\n",
      "======> epoch: 180/400, Loss:0.01996971108019352\n",
      "======> epoch: 180/400, Loss:0.02214333973824978\n",
      "======> epoch: 180/400, Loss:0.01363451685756445\n",
      "======> epoch: 180/400, Loss:0.02562020905315876\n",
      "======> epoch: 180/400, Loss:0.01589711382985115\n",
      "======> epoch: 180/400, Loss:0.014522281475365162\n",
      "======> epoch: 180/400, Loss:0.016372879967093468\n",
      "======> epoch: 180/400, Loss:0.01690736971795559\n",
      "======> epoch: 180/400, Loss:0.019105849787592888\n",
      "Entering Epoch:  181\n",
      "======> epoch: 181/400, Loss:0.017833007499575615\n",
      "======> epoch: 181/400, Loss:0.0141532514244318\n",
      "======> epoch: 181/400, Loss:0.020574362948536873\n",
      "======> epoch: 181/400, Loss:0.017715254798531532\n",
      "======> epoch: 181/400, Loss:0.01523621380329132\n",
      "======> epoch: 181/400, Loss:0.01808285154402256\n",
      "======> epoch: 181/400, Loss:0.01644960604608059\n",
      "======> epoch: 181/400, Loss:0.024015694856643677\n",
      "======> epoch: 181/400, Loss:0.01587449014186859\n",
      "======> epoch: 181/400, Loss:0.02240937203168869\n",
      "Entering Epoch:  182\n",
      "======> epoch: 182/400, Loss:0.013340314850211143\n",
      "======> epoch: 182/400, Loss:0.018041443079710007\n",
      "======> epoch: 182/400, Loss:0.014076431281864643\n",
      "======> epoch: 182/400, Loss:0.022703610360622406\n",
      "======> epoch: 182/400, Loss:0.015583574771881104\n",
      "======> epoch: 182/400, Loss:0.02006525732576847\n",
      "======> epoch: 182/400, Loss:0.011693138629198074\n",
      "======> epoch: 182/400, Loss:0.017766838893294334\n",
      "======> epoch: 182/400, Loss:0.025223180651664734\n",
      "======> epoch: 182/400, Loss:0.01602337695658207\n",
      "Entering Epoch:  183\n",
      "======> epoch: 183/400, Loss:0.015120217576622963\n",
      "======> epoch: 183/400, Loss:0.016490301117300987\n",
      "======> epoch: 183/400, Loss:0.018560657277703285\n",
      "======> epoch: 183/400, Loss:0.019807998090982437\n",
      "======> epoch: 183/400, Loss:0.020713001489639282\n",
      "======> epoch: 183/400, Loss:0.01703355833888054\n",
      "======> epoch: 183/400, Loss:0.013373165391385555\n",
      "======> epoch: 183/400, Loss:0.014274343848228455\n",
      "======> epoch: 183/400, Loss:0.021120034158229828\n",
      "======> epoch: 183/400, Loss:0.018741048872470856\n",
      "Entering Epoch:  184\n",
      "======> epoch: 184/400, Loss:0.01314590498805046\n",
      "======> epoch: 184/400, Loss:0.024132998660206795\n",
      "======> epoch: 184/400, Loss:0.018894074484705925\n",
      "======> epoch: 184/400, Loss:0.01736234314739704\n",
      "======> epoch: 184/400, Loss:0.016339007765054703\n",
      "======> epoch: 184/400, Loss:0.024188706651329994\n",
      "======> epoch: 184/400, Loss:0.018783172592520714\n",
      "======> epoch: 184/400, Loss:0.020129047334194183\n",
      "======> epoch: 184/400, Loss:0.023154592141509056\n",
      "======> epoch: 184/400, Loss:0.02166382037103176\n",
      "Entering Epoch:  185\n",
      "======> epoch: 185/400, Loss:0.012432409450411797\n",
      "======> epoch: 185/400, Loss:0.022485513240098953\n",
      "======> epoch: 185/400, Loss:0.01868353970348835\n",
      "======> epoch: 185/400, Loss:0.017297664657235146\n",
      "======> epoch: 185/400, Loss:0.01946260593831539\n",
      "======> epoch: 185/400, Loss:0.014115254394710064\n",
      "======> epoch: 185/400, Loss:0.01860949769616127\n",
      "======> epoch: 185/400, Loss:0.012375217862427235\n",
      "======> epoch: 185/400, Loss:0.018615925684571266\n",
      "======> epoch: 185/400, Loss:0.018860528245568275\n",
      "Entering Epoch:  186\n",
      "======> epoch: 186/400, Loss:0.019916502758860588\n",
      "======> epoch: 186/400, Loss:0.023739604279398918\n",
      "======> epoch: 186/400, Loss:0.012550963088870049\n",
      "======> epoch: 186/400, Loss:0.016827477142214775\n",
      "======> epoch: 186/400, Loss:0.022000370547175407\n",
      "======> epoch: 186/400, Loss:0.02059156633913517\n",
      "======> epoch: 186/400, Loss:0.020795688033103943\n",
      "======> epoch: 186/400, Loss:0.020014015957713127\n",
      "======> epoch: 186/400, Loss:0.014356780797243118\n",
      "======> epoch: 186/400, Loss:0.01942899264395237\n",
      "Entering Epoch:  187\n",
      "======> epoch: 187/400, Loss:0.019667714834213257\n",
      "======> epoch: 187/400, Loss:0.023090090602636337\n",
      "======> epoch: 187/400, Loss:0.016550075262784958\n",
      "======> epoch: 187/400, Loss:0.02104812301695347\n",
      "======> epoch: 187/400, Loss:0.01607493683695793\n",
      "======> epoch: 187/400, Loss:0.01644880324602127\n",
      "======> epoch: 187/400, Loss:0.01745028980076313\n",
      "======> epoch: 187/400, Loss:0.017899589613080025\n",
      "======> epoch: 187/400, Loss:0.01481656450778246\n",
      "======> epoch: 187/400, Loss:0.021291514858603477\n",
      "Entering Epoch:  188\n",
      "======> epoch: 188/400, Loss:0.014827460050582886\n",
      "======> epoch: 188/400, Loss:0.02233300916850567\n",
      "======> epoch: 188/400, Loss:0.018108664080500603\n",
      "======> epoch: 188/400, Loss:0.01686687581241131\n",
      "======> epoch: 188/400, Loss:0.019730539992451668\n",
      "======> epoch: 188/400, Loss:0.023817088454961777\n",
      "======> epoch: 188/400, Loss:0.02163328044116497\n",
      "======> epoch: 188/400, Loss:0.013276897370815277\n",
      "======> epoch: 188/400, Loss:0.019017236307263374\n",
      "======> epoch: 188/400, Loss:0.021084189414978027\n",
      "Entering Epoch:  189\n",
      "======> epoch: 189/400, Loss:0.01840192824602127\n",
      "======> epoch: 189/400, Loss:0.015376974828541279\n",
      "======> epoch: 189/400, Loss:0.019282452762126923\n",
      "======> epoch: 189/400, Loss:0.013764290139079094\n",
      "======> epoch: 189/400, Loss:0.017544442787766457\n",
      "======> epoch: 189/400, Loss:0.01877082884311676\n",
      "======> epoch: 189/400, Loss:0.013155811466276646\n",
      "======> epoch: 189/400, Loss:0.016880176961421967\n",
      "======> epoch: 189/400, Loss:0.01793600060045719\n",
      "======> epoch: 189/400, Loss:0.020754443481564522\n",
      "Entering Epoch:  190\n",
      "======> epoch: 190/400, Loss:0.017047202214598656\n",
      "======> epoch: 190/400, Loss:0.019820405170321465\n",
      "======> epoch: 190/400, Loss:0.012887193821370602\n",
      "======> epoch: 190/400, Loss:0.016541775315999985\n",
      "======> epoch: 190/400, Loss:0.024732062593102455\n",
      "======> epoch: 190/400, Loss:0.015430140309035778\n",
      "======> epoch: 190/400, Loss:0.023045741021633148\n",
      "======> epoch: 190/400, Loss:0.02040540985763073\n",
      "======> epoch: 190/400, Loss:0.018626103177666664\n",
      "======> epoch: 190/400, Loss:0.015209757722914219\n",
      "Entering Epoch:  191\n",
      "======> epoch: 191/400, Loss:0.01497334148734808\n",
      "======> epoch: 191/400, Loss:0.01583734340965748\n",
      "======> epoch: 191/400, Loss:0.02103963866829872\n",
      "======> epoch: 191/400, Loss:0.019551798701286316\n",
      "======> epoch: 191/400, Loss:0.020270604640245438\n",
      "======> epoch: 191/400, Loss:0.015439310111105442\n",
      "======> epoch: 191/400, Loss:0.020837845280766487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 191/400, Loss:0.01544458232820034\n",
      "======> epoch: 191/400, Loss:0.020079804584383965\n",
      "======> epoch: 191/400, Loss:0.01818261481821537\n",
      "Entering Epoch:  192\n",
      "======> epoch: 192/400, Loss:0.018828144297003746\n",
      "======> epoch: 192/400, Loss:0.01665521040558815\n",
      "======> epoch: 192/400, Loss:0.01659887097775936\n",
      "======> epoch: 192/400, Loss:0.01769600249826908\n",
      "======> epoch: 192/400, Loss:0.017314983531832695\n",
      "======> epoch: 192/400, Loss:0.015374178998172283\n",
      "======> epoch: 192/400, Loss:0.017724713310599327\n",
      "======> epoch: 192/400, Loss:0.019816264510154724\n",
      "======> epoch: 192/400, Loss:0.01576368510723114\n",
      "======> epoch: 192/400, Loss:0.01731846295297146\n",
      "Entering Epoch:  193\n",
      "======> epoch: 193/400, Loss:0.014911552891135216\n",
      "======> epoch: 193/400, Loss:0.015946727246046066\n",
      "======> epoch: 193/400, Loss:0.01438458077609539\n",
      "======> epoch: 193/400, Loss:0.015567260794341564\n",
      "======> epoch: 193/400, Loss:0.02465209737420082\n",
      "======> epoch: 193/400, Loss:0.01507305633276701\n",
      "======> epoch: 193/400, Loss:0.0158133115619421\n",
      "======> epoch: 193/400, Loss:0.01773526519536972\n",
      "======> epoch: 193/400, Loss:0.021870484575629234\n",
      "======> epoch: 193/400, Loss:0.015532899647951126\n",
      "Entering Epoch:  194\n",
      "======> epoch: 194/400, Loss:0.013690412975847721\n",
      "======> epoch: 194/400, Loss:0.019710598513484\n",
      "======> epoch: 194/400, Loss:0.014025937765836716\n",
      "======> epoch: 194/400, Loss:0.01939336024224758\n",
      "======> epoch: 194/400, Loss:0.02415528893470764\n",
      "======> epoch: 194/400, Loss:0.016878072172403336\n",
      "======> epoch: 194/400, Loss:0.0134348776191473\n",
      "======> epoch: 194/400, Loss:0.015225257724523544\n",
      "======> epoch: 194/400, Loss:0.015408387407660484\n",
      "======> epoch: 194/400, Loss:0.017601462081074715\n",
      "Entering Epoch:  195\n",
      "======> epoch: 195/400, Loss:0.0216375645250082\n",
      "======> epoch: 195/400, Loss:0.014390381053090096\n",
      "======> epoch: 195/400, Loss:0.020145321264863014\n",
      "======> epoch: 195/400, Loss:0.02086317352950573\n",
      "======> epoch: 195/400, Loss:0.019161364063620567\n",
      "======> epoch: 195/400, Loss:0.014126221649348736\n",
      "======> epoch: 195/400, Loss:0.01432577334344387\n",
      "======> epoch: 195/400, Loss:0.01820128597319126\n",
      "======> epoch: 195/400, Loss:0.01808074675500393\n",
      "======> epoch: 195/400, Loss:0.01655740663409233\n",
      "Entering Epoch:  196\n",
      "======> epoch: 196/400, Loss:0.015121331438422203\n",
      "======> epoch: 196/400, Loss:0.01956743746995926\n",
      "======> epoch: 196/400, Loss:0.024162176996469498\n",
      "======> epoch: 196/400, Loss:0.02024879679083824\n",
      "======> epoch: 196/400, Loss:0.01709914021193981\n",
      "======> epoch: 196/400, Loss:0.017217762768268585\n",
      "======> epoch: 196/400, Loss:0.015054691582918167\n",
      "======> epoch: 196/400, Loss:0.01971931755542755\n",
      "======> epoch: 196/400, Loss:0.01787521317601204\n",
      "======> epoch: 196/400, Loss:0.01566123776137829\n",
      "Entering Epoch:  197\n",
      "======> epoch: 197/400, Loss:0.01525701954960823\n",
      "======> epoch: 197/400, Loss:0.011443328112363815\n",
      "======> epoch: 197/400, Loss:0.016476966440677643\n",
      "======> epoch: 197/400, Loss:0.018834194168448448\n",
      "======> epoch: 197/400, Loss:0.01481718197464943\n",
      "======> epoch: 197/400, Loss:0.016216419637203217\n",
      "======> epoch: 197/400, Loss:0.015451008453965187\n",
      "======> epoch: 197/400, Loss:0.018890753388404846\n",
      "======> epoch: 197/400, Loss:0.013799839653074741\n",
      "======> epoch: 197/400, Loss:0.017997736111283302\n",
      "Entering Epoch:  198\n",
      "======> epoch: 198/400, Loss:0.015869131311774254\n",
      "======> epoch: 198/400, Loss:0.019791262224316597\n",
      "======> epoch: 198/400, Loss:0.018664883449673653\n",
      "======> epoch: 198/400, Loss:0.01572733372449875\n",
      "======> epoch: 198/400, Loss:0.015667658299207687\n",
      "======> epoch: 198/400, Loss:0.019408756867051125\n",
      "======> epoch: 198/400, Loss:0.015337926335632801\n",
      "======> epoch: 198/400, Loss:0.014828802086412907\n",
      "======> epoch: 198/400, Loss:0.019373061135411263\n",
      "======> epoch: 198/400, Loss:0.014997247606515884\n",
      "Entering Epoch:  199\n",
      "======> epoch: 199/400, Loss:0.020043745636940002\n",
      "======> epoch: 199/400, Loss:0.014893028885126114\n",
      "======> epoch: 199/400, Loss:0.016961419954895973\n",
      "======> epoch: 199/400, Loss:0.02157939411699772\n",
      "======> epoch: 199/400, Loss:0.019859720021486282\n",
      "======> epoch: 199/400, Loss:0.021316273137927055\n",
      "======> epoch: 199/400, Loss:0.017427336424589157\n",
      "======> epoch: 199/400, Loss:0.014124013483524323\n",
      "======> epoch: 199/400, Loss:0.019125135615468025\n",
      "======> epoch: 199/400, Loss:0.01625688374042511\n",
      "Entering Epoch:  200\n",
      "======> epoch: 200/400, Loss:0.014409933239221573\n",
      "======> epoch: 200/400, Loss:0.017160294577479362\n",
      "======> epoch: 200/400, Loss:0.024479564279317856\n",
      "======> epoch: 200/400, Loss:0.015805179253220558\n",
      "======> epoch: 200/400, Loss:0.021806608885526657\n",
      "======> epoch: 200/400, Loss:0.01895669661462307\n",
      "======> epoch: 200/400, Loss:0.013445386663079262\n",
      "======> epoch: 200/400, Loss:0.01368225459009409\n",
      "======> epoch: 200/400, Loss:0.01897290349006653\n",
      "======> epoch: 200/400, Loss:0.016990043222904205\n",
      "Entering Epoch:  201\n",
      "======> epoch: 201/400, Loss:0.015053902752697468\n",
      "======> epoch: 201/400, Loss:0.01988082192838192\n",
      "======> epoch: 201/400, Loss:0.023468874394893646\n",
      "======> epoch: 201/400, Loss:0.019969386979937553\n",
      "======> epoch: 201/400, Loss:0.02181786485016346\n",
      "======> epoch: 201/400, Loss:0.02033337391912937\n",
      "======> epoch: 201/400, Loss:0.017422936856746674\n",
      "======> epoch: 201/400, Loss:0.019264452159404755\n",
      "======> epoch: 201/400, Loss:0.017344458028674126\n",
      "======> epoch: 201/400, Loss:0.0198067557066679\n",
      "Entering Epoch:  202\n",
      "======> epoch: 202/400, Loss:0.020696595311164856\n",
      "======> epoch: 202/400, Loss:0.01993577554821968\n",
      "======> epoch: 202/400, Loss:0.013624047860503197\n",
      "======> epoch: 202/400, Loss:0.02075432613492012\n",
      "======> epoch: 202/400, Loss:0.017930682748556137\n",
      "======> epoch: 202/400, Loss:0.02130264602601528\n",
      "======> epoch: 202/400, Loss:0.01864142343401909\n",
      "======> epoch: 202/400, Loss:0.014295830391347408\n",
      "======> epoch: 202/400, Loss:0.012552793137729168\n",
      "======> epoch: 202/400, Loss:0.023017995059490204\n",
      "Entering Epoch:  203\n",
      "======> epoch: 203/400, Loss:0.01751682162284851\n",
      "======> epoch: 203/400, Loss:0.01668534427881241\n",
      "======> epoch: 203/400, Loss:0.01753392070531845\n",
      "======> epoch: 203/400, Loss:0.01361082773655653\n",
      "======> epoch: 203/400, Loss:0.01914413832128048\n",
      "======> epoch: 203/400, Loss:0.016345398500561714\n",
      "======> epoch: 203/400, Loss:0.01894577033817768\n",
      "======> epoch: 203/400, Loss:0.02092871256172657\n",
      "======> epoch: 203/400, Loss:0.0137570109218359\n",
      "======> epoch: 203/400, Loss:0.02079368755221367\n",
      "Entering Epoch:  204\n",
      "======> epoch: 204/400, Loss:0.016923142597079277\n",
      "======> epoch: 204/400, Loss:0.020768776535987854\n",
      "======> epoch: 204/400, Loss:0.017714548856019974\n",
      "======> epoch: 204/400, Loss:0.02249988541007042\n",
      "======> epoch: 204/400, Loss:0.019390756264328957\n",
      "======> epoch: 204/400, Loss:0.026485009118914604\n",
      "======> epoch: 204/400, Loss:0.012227128259837627\n",
      "======> epoch: 204/400, Loss:0.01869865693151951\n",
      "======> epoch: 204/400, Loss:0.01615215465426445\n",
      "======> epoch: 204/400, Loss:0.02310950867831707\n",
      "Entering Epoch:  205\n",
      "======> epoch: 205/400, Loss:0.020359454676508904\n",
      "======> epoch: 205/400, Loss:0.021272065117955208\n",
      "======> epoch: 205/400, Loss:0.017585380002856255\n",
      "======> epoch: 205/400, Loss:0.017479507252573967\n",
      "======> epoch: 205/400, Loss:0.01884450763463974\n",
      "======> epoch: 205/400, Loss:0.020425181835889816\n",
      "======> epoch: 205/400, Loss:0.018600482493638992\n",
      "======> epoch: 205/400, Loss:0.016552409157156944\n",
      "======> epoch: 205/400, Loss:0.02091953158378601\n",
      "======> epoch: 205/400, Loss:0.01988951489329338\n",
      "Entering Epoch:  206\n",
      "======> epoch: 206/400, Loss:0.020179955288767815\n",
      "======> epoch: 206/400, Loss:0.01776358112692833\n",
      "======> epoch: 206/400, Loss:0.017876699566841125\n",
      "======> epoch: 206/400, Loss:0.01166108250617981\n",
      "======> epoch: 206/400, Loss:0.022502539679408073\n",
      "======> epoch: 206/400, Loss:0.01882389932870865\n",
      "======> epoch: 206/400, Loss:0.015993749722838402\n",
      "======> epoch: 206/400, Loss:0.01867438293993473\n",
      "======> epoch: 206/400, Loss:0.0210140161216259\n",
      "======> epoch: 206/400, Loss:0.012115808203816414\n",
      "Entering Epoch:  207\n",
      "======> epoch: 207/400, Loss:0.014600586146116257\n",
      "======> epoch: 207/400, Loss:0.020486317574977875\n",
      "======> epoch: 207/400, Loss:0.017398400232195854\n",
      "======> epoch: 207/400, Loss:0.016954580321907997\n",
      "======> epoch: 207/400, Loss:0.015781989321112633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 207/400, Loss:0.01660693995654583\n",
      "======> epoch: 207/400, Loss:0.019329458475112915\n",
      "======> epoch: 207/400, Loss:0.020478323101997375\n",
      "======> epoch: 207/400, Loss:0.020414361730217934\n",
      "======> epoch: 207/400, Loss:0.0263911671936512\n",
      "Entering Epoch:  208\n",
      "======> epoch: 208/400, Loss:0.016764922067523003\n",
      "======> epoch: 208/400, Loss:0.01571449637413025\n",
      "======> epoch: 208/400, Loss:0.014770627953112125\n",
      "======> epoch: 208/400, Loss:0.014626977033913136\n",
      "======> epoch: 208/400, Loss:0.017276428639888763\n",
      "======> epoch: 208/400, Loss:0.020729178562760353\n",
      "======> epoch: 208/400, Loss:0.01901630312204361\n",
      "======> epoch: 208/400, Loss:0.019039640203118324\n",
      "======> epoch: 208/400, Loss:0.015808695927262306\n",
      "======> epoch: 208/400, Loss:0.018474046140909195\n",
      "Entering Epoch:  209\n",
      "======> epoch: 209/400, Loss:0.01991916075348854\n",
      "======> epoch: 209/400, Loss:0.01866711676120758\n",
      "======> epoch: 209/400, Loss:0.017351016402244568\n",
      "======> epoch: 209/400, Loss:0.021283989772200584\n",
      "======> epoch: 209/400, Loss:0.0199284665286541\n",
      "======> epoch: 209/400, Loss:0.012294300831854343\n",
      "======> epoch: 209/400, Loss:0.022204317152500153\n",
      "======> epoch: 209/400, Loss:0.013998054899275303\n",
      "======> epoch: 209/400, Loss:0.017465636134147644\n",
      "======> epoch: 209/400, Loss:0.02434927225112915\n",
      "Entering Epoch:  210\n",
      "======> epoch: 210/400, Loss:0.017459092661738396\n",
      "======> epoch: 210/400, Loss:0.0179311390966177\n",
      "======> epoch: 210/400, Loss:0.022259285673499107\n",
      "======> epoch: 210/400, Loss:0.018848108127713203\n",
      "======> epoch: 210/400, Loss:0.024921394884586334\n",
      "======> epoch: 210/400, Loss:0.019344545900821686\n",
      "======> epoch: 210/400, Loss:0.015158778056502342\n",
      "======> epoch: 210/400, Loss:0.01972389593720436\n",
      "======> epoch: 210/400, Loss:0.016055520623922348\n",
      "======> epoch: 210/400, Loss:0.022583933547139168\n",
      "Entering Epoch:  211\n",
      "======> epoch: 211/400, Loss:0.016335168853402138\n",
      "======> epoch: 211/400, Loss:0.020031355321407318\n",
      "======> epoch: 211/400, Loss:0.015895901247859\n",
      "======> epoch: 211/400, Loss:0.016565999016165733\n",
      "======> epoch: 211/400, Loss:0.02299223467707634\n",
      "======> epoch: 211/400, Loss:0.016641680151224136\n",
      "======> epoch: 211/400, Loss:0.02013685181736946\n",
      "======> epoch: 211/400, Loss:0.01866655796766281\n",
      "======> epoch: 211/400, Loss:0.01942695491015911\n",
      "======> epoch: 211/400, Loss:0.017090346664190292\n",
      "Entering Epoch:  212\n",
      "======> epoch: 212/400, Loss:0.026050902903079987\n",
      "======> epoch: 212/400, Loss:0.015339716337621212\n",
      "======> epoch: 212/400, Loss:0.018102256581187248\n",
      "======> epoch: 212/400, Loss:0.017368817701935768\n",
      "======> epoch: 212/400, Loss:0.020568717271089554\n",
      "======> epoch: 212/400, Loss:0.01984049566090107\n",
      "======> epoch: 212/400, Loss:0.020578445866703987\n",
      "======> epoch: 212/400, Loss:0.01484285295009613\n",
      "======> epoch: 212/400, Loss:0.017042210325598717\n",
      "======> epoch: 212/400, Loss:0.020353110507130623\n",
      "Entering Epoch:  213\n",
      "======> epoch: 213/400, Loss:0.01746848225593567\n",
      "======> epoch: 213/400, Loss:0.014296096749603748\n",
      "======> epoch: 213/400, Loss:0.016408143565058708\n",
      "======> epoch: 213/400, Loss:0.017917335033416748\n",
      "======> epoch: 213/400, Loss:0.01806987263262272\n",
      "======> epoch: 213/400, Loss:0.017021995037794113\n",
      "======> epoch: 213/400, Loss:0.015318991616368294\n",
      "======> epoch: 213/400, Loss:0.018467573449015617\n",
      "======> epoch: 213/400, Loss:0.01619049347937107\n",
      "======> epoch: 213/400, Loss:0.01573857292532921\n",
      "Entering Epoch:  214\n",
      "======> epoch: 214/400, Loss:0.011570277623832226\n",
      "======> epoch: 214/400, Loss:0.016472911462187767\n",
      "======> epoch: 214/400, Loss:0.016830112785100937\n",
      "======> epoch: 214/400, Loss:0.014300785027444363\n",
      "======> epoch: 214/400, Loss:0.016498690471053123\n",
      "======> epoch: 214/400, Loss:0.01771511323750019\n",
      "======> epoch: 214/400, Loss:0.023850949481129646\n",
      "======> epoch: 214/400, Loss:0.015205626375973225\n",
      "======> epoch: 214/400, Loss:0.016512449830770493\n",
      "======> epoch: 214/400, Loss:0.01249116100370884\n",
      "Entering Epoch:  215\n",
      "======> epoch: 215/400, Loss:0.01776193082332611\n",
      "======> epoch: 215/400, Loss:0.01613655686378479\n",
      "======> epoch: 215/400, Loss:0.014604774303734303\n",
      "======> epoch: 215/400, Loss:0.013103242963552475\n",
      "======> epoch: 215/400, Loss:0.01884411834180355\n",
      "======> epoch: 215/400, Loss:0.02011854574084282\n",
      "======> epoch: 215/400, Loss:0.018602708354592323\n",
      "======> epoch: 215/400, Loss:0.012439233250916004\n",
      "======> epoch: 215/400, Loss:0.020080065354704857\n",
      "======> epoch: 215/400, Loss:0.014151409268379211\n",
      "Entering Epoch:  216\n",
      "======> epoch: 216/400, Loss:0.02017342671751976\n",
      "======> epoch: 216/400, Loss:0.011811514385044575\n",
      "======> epoch: 216/400, Loss:0.01640654169023037\n",
      "======> epoch: 216/400, Loss:0.02232654206454754\n",
      "======> epoch: 216/400, Loss:0.02406737580895424\n",
      "======> epoch: 216/400, Loss:0.0165104940533638\n",
      "======> epoch: 216/400, Loss:0.02717946283519268\n",
      "======> epoch: 216/400, Loss:0.015560549683868885\n",
      "======> epoch: 216/400, Loss:0.016299549490213394\n",
      "======> epoch: 216/400, Loss:0.013729335740208626\n",
      "Entering Epoch:  217\n",
      "======> epoch: 217/400, Loss:0.013420761562883854\n",
      "======> epoch: 217/400, Loss:0.020916013047099113\n",
      "======> epoch: 217/400, Loss:0.01199590228497982\n",
      "======> epoch: 217/400, Loss:0.01726147159934044\n",
      "======> epoch: 217/400, Loss:0.01806953176856041\n",
      "======> epoch: 217/400, Loss:0.01824660412967205\n",
      "======> epoch: 217/400, Loss:0.011207803152501583\n",
      "======> epoch: 217/400, Loss:0.014138013124465942\n",
      "======> epoch: 217/400, Loss:0.014400885440409184\n",
      "======> epoch: 217/400, Loss:0.01145253237336874\n",
      "Entering Epoch:  218\n",
      "======> epoch: 218/400, Loss:0.014901558868587017\n",
      "======> epoch: 218/400, Loss:0.017210708931088448\n",
      "======> epoch: 218/400, Loss:0.015116415917873383\n",
      "======> epoch: 218/400, Loss:0.014311774633824825\n",
      "======> epoch: 218/400, Loss:0.02117479406297207\n",
      "======> epoch: 218/400, Loss:0.020780757069587708\n",
      "======> epoch: 218/400, Loss:0.01308367494493723\n",
      "======> epoch: 218/400, Loss:0.01969676837325096\n",
      "======> epoch: 218/400, Loss:0.016828393563628197\n",
      "======> epoch: 218/400, Loss:0.015091781504452229\n",
      "Entering Epoch:  219\n",
      "======> epoch: 219/400, Loss:0.015062592923641205\n",
      "======> epoch: 219/400, Loss:0.017340321093797684\n",
      "======> epoch: 219/400, Loss:0.016343487426638603\n",
      "======> epoch: 219/400, Loss:0.019610244780778885\n",
      "======> epoch: 219/400, Loss:0.014328655786812305\n",
      "======> epoch: 219/400, Loss:0.014902297407388687\n",
      "======> epoch: 219/400, Loss:0.018651209771633148\n",
      "======> epoch: 219/400, Loss:0.01843353360891342\n",
      "======> epoch: 219/400, Loss:0.013967799954116344\n",
      "======> epoch: 219/400, Loss:0.017098410055041313\n",
      "Entering Epoch:  220\n",
      "======> epoch: 220/400, Loss:0.017691222950816154\n",
      "======> epoch: 220/400, Loss:0.013341917656362057\n",
      "======> epoch: 220/400, Loss:0.02570725604891777\n",
      "======> epoch: 220/400, Loss:0.016933048143982887\n",
      "======> epoch: 220/400, Loss:0.01781507208943367\n",
      "======> epoch: 220/400, Loss:0.020717080682516098\n",
      "======> epoch: 220/400, Loss:0.013015199452638626\n",
      "======> epoch: 220/400, Loss:0.01871498115360737\n",
      "======> epoch: 220/400, Loss:0.01776367984712124\n",
      "======> epoch: 220/400, Loss:0.017382552847266197\n",
      "Entering Epoch:  221\n",
      "======> epoch: 221/400, Loss:0.01533022802323103\n"
     ]
    }
   ],
   "source": [
    "losslist = []\n",
    "model.train()\n",
    "def train_den_ae(mat,epochs= 400,steps_per_epoch = 300, _unsqueeze=True):\n",
    "#     epochs = 120\n",
    "    # l = len(trainloader)\n",
    "    # l = 120\n",
    "#     losslist = []\n",
    "    epochloss = 0\n",
    "    running_loss = 0\n",
    "    steps_per_epoch = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(\"Entering Epoch: \", epoch)\n",
    "        for i in range(steps_per_epoch):\n",
    "            orig, masked, idxs = get_random_masked_batch(mat, batch_size=batch_size, p=0.25)\n",
    "\n",
    "            #-----------------Forward Pass----------------------\n",
    "            if _unsqueeze:\n",
    "                masked = masked.unsqueeze(2)\n",
    "            output = model(masked, idxs)\n",
    "#             print((output >1).all())\n",
    "#             print((orig >1).all())\n",
    "#             print(output[0,1:20])\n",
    "#             print(masked[0,1:20])\n",
    "#             print(orig.shape, output.shape)\n",
    "            loss = criterion(output, orig)\n",
    "            #-----------------Backward Pass---------------------\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    #         running_loss += loss.item()\n",
    "    #         epochloss += loss.item()\n",
    "    #         #-----------------Log-------------------------------\n",
    "    #         losslist.append(running_loss/l)\n",
    "    #         running_loss=0\n",
    "    #     print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "            if i%10 == 0:\n",
    "                running_loss += loss.item()\n",
    "                epochloss += loss.item()\n",
    "                #-----------------Log-------------------------------\n",
    "                losslist.append(running_loss/steps_per_epoch)\n",
    "                running_loss=0\n",
    "                print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "                \n",
    "train_den_ae(negative_feedback_mask, _unsqueeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7, device='cuda:0'), tensor(2, device='cuda:0'))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denoising_autoencoder(\n",
       "  (encoder): Linear(in_features=3706, out_features=64, bias=True)\n",
       "  (decoder): Linear(in_features=64, out_features=3706, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output[0] >0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0004, 0.0007, 0.0008, 0.0007, 0.0003, 0.0008, 0.0009, 0.0010, 0.0010,\n",
       "        0.0006, 0.0007, 0.0004, 0.0004, 0.0004, 0.0005, 0.0013, 0.0004, 0.0008,\n",
       "        0.0010, 0.0005, 0.0011, 0.0007, 0.0007, 0.0010, 0.0007, 0.0012, 0.0011,\n",
       "        0.0013, 0.0004, 0.0005, 0.0005, 0.0007, 0.0011, 0.0010, 0.0005, 0.0003,\n",
       "        0.0008, 0.0003, 0.0004, 0.0006, 0.0007, 0.0009, 0.0006, 0.0007, 0.0003,\n",
       "        0.0006, 0.0007, 0.0005, 0.0004, 0.0005, 0.0007, 0.0008, 0.0003, 0.0007,\n",
       "        0.0005, 0.0004, 0.0009, 0.0005, 0.0006, 0.0003, 0.0007, 0.0006, 0.0003,\n",
       "        0.0003, 0.0005, 0.0006, 0.0012, 0.0006, 0.0014, 0.0003, 0.0005, 0.0008,\n",
       "        0.0009, 0.0005, 0.0014, 0.0005, 0.0010, 0.0003, 0.0011, 0.0004, 0.0006,\n",
       "        0.0006, 0.0006, 0.0007, 0.0010, 0.0008, 0.0007, 0.0004, 0.0008, 0.0006,\n",
       "        0.0006, 0.0011, 0.0005, 0.0008, 0.0006, 0.0005, 0.0005, 0.0005, 0.0006,\n",
       "        0.0005, 0.0006, 0.0010, 0.0007, 0.0009, 0.0004, 0.0006, 0.0013, 0.0012,\n",
       "        0.0006, 0.0005, 0.0006, 0.0006, 0.0005, 0.0006, 0.0005, 0.0010, 0.0005,\n",
       "        0.0013, 0.0006, 0.0009, 0.0008, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004,\n",
       "        0.0007, 0.0005, 0.0008, 0.0003, 0.0003, 0.0014, 0.0006, 0.0005, 0.0006,\n",
       "        0.0004, 0.0009, 0.0004, 0.0006, 0.0008, 0.0005, 0.0010, 0.0004, 0.0005,\n",
       "        0.0005, 0.0008, 0.0007, 0.0004, 0.0010, 0.0008, 0.0008, 0.0006, 0.0006,\n",
       "        0.0003, 0.0006, 0.0004, 0.0007, 0.0007, 0.0008, 0.0003, 0.0010, 0.0002,\n",
       "        0.0004, 0.0007, 0.0011, 0.0003, 0.0009, 0.0008, 0.0006, 0.0003, 0.0004,\n",
       "        0.0005, 0.0006, 0.0004, 0.0006, 0.0004, 0.0006, 0.0006, 0.0006, 0.0005,\n",
       "        0.0009, 0.0005, 0.0005, 0.0005, 0.0006, 0.0002, 0.0013, 0.0005, 0.0008,\n",
       "        0.0006, 0.0003, 0.0002, 0.0006, 0.0009, 0.0007, 0.0007, 0.0003, 0.0009,\n",
       "        0.0006, 0.0009, 0.0004, 0.0006, 0.0007, 0.0005, 0.0013, 0.0004, 0.0004,\n",
       "        0.0005, 0.0006, 0.0004, 0.0008, 0.0007, 0.0003, 0.0003, 0.0003, 0.0006,\n",
       "        0.0011, 0.0003, 0.0008, 0.0003, 0.0005, 0.0004, 0.0005, 0.0006, 0.0008,\n",
       "        0.0009, 0.0009, 0.0008, 0.0004, 0.0006, 0.0005, 0.0005, 0.0004, 0.0008,\n",
       "        0.0003, 0.0003, 0.0005, 0.0007, 0.0005, 0.0006, 0.0003, 0.0005, 0.0006,\n",
       "        0.0004, 0.0004, 0.0006, 0.0006, 0.0004, 0.0006, 0.0007, 0.0008, 0.0007,\n",
       "        0.0005, 0.0010, 0.0007, 0.0008, 0.0006, 0.0003, 0.0004, 0.0004, 0.0008,\n",
       "        0.0006, 0.0006, 0.0005, 0.0007, 0.0010, 0.0005, 0.0006, 0.0005, 0.0004,\n",
       "        0.0005, 0.0006, 0.0003, 0.0004, 0.0006, 0.0011, 0.0003, 0.0006, 0.0007,\n",
       "        0.0005, 0.0006, 0.0003, 0.0006, 0.0005, 0.0004, 0.0004, 0.0008, 0.0005,\n",
       "        0.0006, 0.0004, 0.0002, 0.0005, 0.0004, 0.0001, 0.0003, 0.0006, 0.0006,\n",
       "        0.0004, 0.0005, 0.0005, 0.0006, 0.0013, 0.0008, 0.0006, 0.0007, 0.0004,\n",
       "        0.0003, 0.0009, 0.0003, 0.0005, 0.0007, 0.0003, 0.0004, 0.0008, 0.0007,\n",
       "        0.0008, 0.0013, 0.0003, 0.0007, 0.0005, 0.0006, 0.0006, 0.0005, 0.0009,\n",
       "        0.0007, 0.0004, 0.0017, 0.0002, 0.0003, 0.0011, 0.0005, 0.0010, 0.0004,\n",
       "        0.0003, 0.0006, 0.0005, 0.0007, 0.0005, 0.0003, 0.0009, 0.0006, 0.0003,\n",
       "        0.0007, 0.0006, 0.0003, 0.0003, 0.0005, 0.0005, 0.0006, 0.0012, 0.0006,\n",
       "        0.0010, 0.0005, 0.0008, 0.0005, 0.0007, 0.0002, 0.0006, 0.0010, 0.0007,\n",
       "        0.0009, 0.0007, 0.0003, 0.0001, 0.0003, 0.0004, 0.0006, 0.0010, 0.0008,\n",
       "        0.0009, 0.0005, 0.0006, 0.0004, 0.0004, 0.0007, 0.0003, 0.0003, 0.0002,\n",
       "        0.0009, 0.0008, 0.0006, 0.0005, 0.0004, 0.0004, 0.0009, 0.0006, 0.0007,\n",
       "        0.0010, 0.0005, 0.0008, 0.0008, 0.0003, 0.0010, 0.0006, 0.0003, 0.0006,\n",
       "        0.0010, 0.0004, 0.0006, 0.0005, 0.0004, 0.0003, 0.0005, 0.0005, 0.0005,\n",
       "        0.0004, 0.0010, 0.0004, 0.0008, 0.0005, 0.0003, 0.0006, 0.0008, 0.0006,\n",
       "        0.0003, 0.0004, 0.0004, 0.0008, 0.0004, 0.0008, 0.0005, 0.0009, 0.0005,\n",
       "        0.0008, 0.0006, 0.0009, 0.0002, 0.0008, 0.0003, 0.0004, 0.0006, 0.0005,\n",
       "        0.0007, 0.0008, 0.0007, 0.0009, 0.0005, 0.0006, 0.0004, 0.0004, 0.0009,\n",
       "        0.0011, 0.0005, 0.0004, 0.0006, 0.0005, 0.0006, 0.0011, 0.0004, 0.0012,\n",
       "        0.0012, 0.0007, 0.0007, 0.0007, 0.0012, 0.0004, 0.0004, 0.0005, 0.0009,\n",
       "        0.0006, 0.0004, 0.0003, 0.0005, 0.0003, 0.0008, 0.0004, 0.0008, 0.0006,\n",
       "        0.0007, 0.0012, 0.0005, 0.0006, 0.0004, 0.0005, 0.0008, 0.0006, 0.0008,\n",
       "        0.0007, 0.0010, 0.0007, 0.0003, 0.0003, 0.0003, 0.0004, 0.0004, 0.0007,\n",
       "        0.0006, 0.0003, 0.0005, 0.0007, 0.0013, 0.0004, 0.0006, 0.0004, 0.0007,\n",
       "        0.0004, 0.0008, 0.0008, 0.0011, 0.0005, 0.0004, 0.0007, 0.0011, 0.0004,\n",
       "        0.0005, 0.0002, 0.0004, 0.0004, 0.0003, 0.0005, 0.0006, 0.0008, 0.0005,\n",
       "        0.0008, 0.0007, 0.0011, 0.0007, 0.0005, 0.0010, 0.0003, 0.0006, 0.0005,\n",
       "        0.0004, 0.0008, 0.0007, 0.0005, 0.0005, 0.0004, 0.0007, 0.0003, 0.0006,\n",
       "        0.0005, 0.0009, 0.0006, 0.0004, 0.0008, 0.0003, 0.0006, 0.0008, 0.0008,\n",
       "        0.0006, 0.0003, 0.0012, 0.0010, 0.0014, 0.0009, 0.0002, 0.0006, 0.0007,\n",
       "        0.0006, 0.0007, 0.0008, 0.0005, 0.0006, 0.0003, 0.0005, 0.0004, 0.0005,\n",
       "        0.0004, 0.0004, 0.0004, 0.0005, 0.0005, 0.0005, 0.0009, 0.0006, 0.0006,\n",
       "        0.0004, 0.0008, 0.0005, 0.0003, 0.0006, 0.0013, 0.0005, 0.0009, 0.0011,\n",
       "        0.0003, 0.0005, 0.0006, 0.0007, 0.0005, 0.0004, 0.0004, 0.0006, 0.0004,\n",
       "        0.0003, 0.0008, 0.0003, 0.0005, 0.0003, 0.0005, 0.0005, 0.0005, 0.0011,\n",
       "        0.0006, 0.0007, 0.0008, 0.0006, 0.0005, 0.0006, 0.0007, 0.0007, 0.0007,\n",
       "        0.0005, 0.0009, 0.0010, 0.0010, 0.0006, 0.0002, 0.0004, 0.0005, 0.0007,\n",
       "        0.0005, 0.0013, 0.0009, 0.0006, 0.0006, 0.0007, 0.0011, 0.0003, 0.0004,\n",
       "        0.0008, 0.0005, 0.0008, 0.0005, 0.0006, 0.0014, 0.0004, 0.0002, 0.0003,\n",
       "        0.0006, 0.0003, 0.0003, 0.0004, 0.0006, 0.0007, 0.0004, 0.0008, 0.0003,\n",
       "        0.0005, 0.0009, 0.0005, 0.0005, 0.0011, 0.0006, 0.0007, 0.0004, 0.0008,\n",
       "        0.0009, 0.0006, 0.0005, 0.0008, 0.0006, 0.0004, 0.0006, 0.0004, 0.0006,\n",
       "        0.0007, 0.0004, 0.0006, 0.0006, 0.0007, 0.0005, 0.0002, 0.0007, 0.0005,\n",
       "        0.0004, 0.0004, 0.0004, 0.0006, 0.0005, 0.0006, 0.0004, 0.0007, 0.0004,\n",
       "        0.0007, 0.0004, 0.0004, 0.0006, 0.0009, 0.0006, 0.0003, 0.0005, 0.0002,\n",
       "        0.0003, 0.0004, 0.0003, 0.0004, 0.0003, 0.0006, 0.0011, 0.0006, 0.0010,\n",
       "        0.0006, 0.0004, 0.0011, 0.0007, 0.0002, 0.0002, 0.0012, 0.0010, 0.0008,\n",
       "        0.0006, 0.0005, 0.0009, 0.0005, 0.0006, 0.0008, 0.0007, 0.0004, 0.0004,\n",
       "        0.0009, 0.0010, 0.0006, 0.0005, 0.0006, 0.0008, 0.0006, 0.0002, 0.0003,\n",
       "        0.0003, 0.0002, 0.0009, 0.0006, 0.0007, 0.0005, 0.0008, 0.0012, 0.0009,\n",
       "        0.0004, 0.0008, 0.0007, 0.0005, 0.0008, 0.0009, 0.0008, 0.0005, 0.0004,\n",
       "        0.0007, 0.0006, 0.0006, 0.0005, 0.0004, 0.0007, 0.0007, 0.0008, 0.0012,\n",
       "        0.0006, 0.0006, 0.0006, 0.0004, 0.0007, 0.0009, 0.0010, 0.0003, 0.0011,\n",
       "        0.0006, 0.0004, 0.0006, 0.0008, 0.0003, 0.0005, 0.0006, 0.0006, 0.0007,\n",
       "        0.0005, 0.0004, 0.0008, 0.0004, 0.0003, 0.0002, 0.0006, 0.0007, 0.0005,\n",
       "        0.0005, 0.0005, 0.0005, 0.0007, 0.0008, 0.0011, 0.0004, 0.0003, 0.0005,\n",
       "        0.0013, 0.0007, 0.0005, 0.0008, 0.0006, 0.0005, 0.0009, 0.0006, 0.0011,\n",
       "        0.0007, 0.0004, 0.0009, 0.0003, 0.0004, 0.0003, 0.0006, 0.0004, 0.0009,\n",
       "        0.0008, 0.0004, 0.0006, 0.0003, 0.0011, 0.0006, 0.0006, 0.0004, 0.0008,\n",
       "        0.0008, 0.0005, 0.0005, 0.0005, 0.0004, 0.0009, 0.0005, 0.0004, 0.0004,\n",
       "        0.0007, 0.0009, 0.0016, 0.0007, 0.0006, 0.0006, 0.0007, 0.0005, 0.0003,\n",
       "        0.0004, 0.0007, 0.0004, 0.0003, 0.0006, 0.0010, 0.0010, 0.0008, 0.0007,\n",
       "        0.0006, 0.0004, 0.0006, 0.0013, 0.0005, 0.0007, 0.0011, 0.0006, 0.0011,\n",
       "        0.0010, 0.0004, 0.0006, 0.0002, 0.0005, 0.0006, 0.0005, 0.0007, 0.0007,\n",
       "        0.0010, 0.0003, 0.0009, 0.0004, 0.0006, 0.0004, 0.0006, 0.0008, 0.0015,\n",
       "        0.0008, 0.0006, 0.0005, 0.0007, 0.0004, 0.0006, 0.0007, 0.0005, 0.0005,\n",
       "        0.0004, 0.0004, 0.0004, 0.0011, 0.0007, 0.0006, 0.0007, 0.0005, 0.0006,\n",
       "        0.0006, 0.0004, 0.0004, 0.0009, 0.0005, 0.0006, 0.0002, 0.0006, 0.0005,\n",
       "        0.0003, 0.0011, 0.0005, 0.0005, 0.0005, 0.0008, 0.0005, 0.0007, 0.0004,\n",
       "        0.0007, 0.0002, 0.0008, 0.0005, 0.0002, 0.0008, 0.0006, 0.0004, 0.0006,\n",
       "        0.0006, 0.0006, 0.0003, 0.0006, 0.0005, 0.0008, 0.0004, 0.0004, 0.0008,\n",
       "        0.0002, 0.0004, 0.0007, 0.0006, 0.0010, 0.0004, 0.0003, 0.0002, 0.0005,\n",
       "        0.0003, 0.0010, 0.0012, 0.0003, 0.0003, 0.0004, 0.0010, 0.0006, 0.0010,\n",
       "        0.0006, 0.0005, 0.0005, 0.0004, 0.0006, 0.0003, 0.0010, 0.0011, 0.0008,\n",
       "        0.0004, 0.0005, 0.0014, 0.0007, 0.0005, 0.0008, 0.0008, 0.0007, 0.0005,\n",
       "        0.0005, 0.0007, 0.0004, 0.0003, 0.0004, 0.0010, 0.0004, 0.0007, 0.0003,\n",
       "        0.0002, 0.0013, 0.0010, 0.0006, 0.0005, 0.0007, 0.0005, 0.0004, 0.0004,\n",
       "        0.0007, 0.0004, 0.0003, 0.0005, 0.0003, 0.0003, 0.0005, 0.0004, 0.0008,\n",
       "        0.0005, 0.0005, 0.0006, 0.0006, 0.0005, 0.0003, 0.0009, 0.0007, 0.0006,\n",
       "        0.0011, 0.0006, 0.0009, 0.0006, 0.0008, 0.0005, 0.0005, 0.0005, 0.0006,\n",
       "        0.0007], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400](orig.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round()[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_denoising_autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, inSize,fSize = 32, nz=24):  #sigma is the corruption level\n",
    "        super(conv_denoising_autoencoder, self).__init__()\n",
    "        #define layers here\n",
    "\n",
    "        self.inp_size = inSize\n",
    "        self.nz = nz\n",
    "        self.fSize = 32\n",
    "#         self.imSize = imSize\n",
    "#         self.sigma = sigma\n",
    "#         self.multimodalZ = multimodalZ\n",
    "\n",
    "#         inSize = imSize / ( 2 ** 4)\n",
    "#         self.inSize = inSize\n",
    "    \n",
    "        self.enc1 = nn.Conv1d(self.inp_size, fSize, 5, stride=2, padding=2)\n",
    "        self.enc2 = nn.Conv1d(fSize, fSize * 2, 5, stride=2, padding=2)\n",
    "        self.enc3 = nn.Conv1d(fSize * 2, fSize * 4, 5, stride=2, padding=2)\n",
    "        self.enc4 = nn.Conv1d(fSize * 4, fSize * 8, 5, stride=2, padding=2)\n",
    "        self.enc5 = nn.Linear(fSize * 8, self.nz)\n",
    "\n",
    "        self.dec1 = nn.Linear(self.nz, fSize * 8)\n",
    "        self.dec2 = nn.ConvTranspose1d(fSize * 8, fSize * 4, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(fSize * 4, fSize * 2, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(fSize * 2, fSize, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec5 = nn.ConvTranspose1d(fSize, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.last_dec = nn.Linear(32*16, self.inp_size)\n",
    "        \n",
    "        self.useCUDA = torch.cuda.is_available()\n",
    "\n",
    "#     def norm_prior(self, noSamples=25):\n",
    "#         z = torch.randn(noSamples, self.nz)\n",
    "#         return z\n",
    "\n",
    "#     def multi_prior(self, noSamples=25, mode=None):\n",
    "#         #make a 2D sqrt(nz)-by-sqrt(nz) grid of gaussians\n",
    "#         num = np.sqrt(self.nz) #no of modes in x and y\n",
    "#         STD = 1.0\n",
    "#         modes = np.arange(-num,num)\n",
    "#         p = np.random.uniform(0, num,(noSamples*2))\n",
    "\n",
    "#         if mode is None:\n",
    "#             mu = modes[np.floor(2 * p).astype(int)]\n",
    "#         else:\n",
    "#             mu = modes[np.ones((noSamples, 2), dtype=int) * int(mode)]\n",
    "\n",
    "#         z = torch.Tensor(mu).view(-1,2) + STD * torch.randn(noSamples, 2)\n",
    "#         return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.batch_size = x.shape[0]\n",
    "        #define the encoder here return mu(x) and sigma(x)\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#     def corrupt(self, x):\n",
    "#         noise = self.sigma * Variable(torch.randn(x.size())).type_as(x)\n",
    "#         return x + noise\n",
    "\n",
    "#     def sample_z(self, noSamples=25, mode=None):\n",
    "#         if not self.multimodalZ:\n",
    "#             z = self.norm_prior(noSamples=noSamples)\n",
    "#         else:\n",
    "#             z = self.multi_prior(noSamples=noSamples, mode=mode)\n",
    "#         if self.useCUDA:\n",
    "#             return Variable(z.cuda())\n",
    "#         else:\n",
    "#             return Variable(z)\n",
    "\n",
    "    def decode(self, z):\n",
    "        #define the decoder here\n",
    "        z = F.relu(self.dec1(z))\n",
    "        z = z.unsqueeze(2)\n",
    "#         print(z.shape)\n",
    "#         z = z.view(z.size(0), -1, self.inp_size)\n",
    "        z = F.relu(self.dec2(z))\n",
    "        z = F.relu(self.dec3(z))\n",
    "        z = F.relu(self.dec4(z))\n",
    "        z = F.sigmoid(self.dec5(z))\n",
    "#         print(z.shape)\n",
    "#         z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "        z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "    \n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the outputs needed for training\n",
    "#         x_corr = self.corrupt(x)\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_denoising_autoencoder(train[0,:].shape[0]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.01, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "train_den_ae(negative_feedback_mask, steps_per_epoch=steps_per_epoch, _unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output[7]).round() >0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output.shape[0]):\n",
    "    print(((output[i]).round() >0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output).round() >= 1 ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output).round()[0][2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gtshs2/Collaborative-Denoising-Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tr + vr) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1')(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(256, activation='selu', name='LatentSpace')(enc)\n",
    "    lat_space = Dropout(0.8, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1')(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred')(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.35)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = negative_feedback_mask.cpu().numpy()\n",
    "y = (negative_feedback_mask*mask_arr).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "UserScore (InputLayer)       (None, 3706)              0         \n",
      "_________________________________________________________________\n",
      "EncLayer1 (Dense)            (None, 512)               1897984   \n",
      "_________________________________________________________________\n",
      "LatentSpace (Dense)          (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "DecLayer1 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "UserScorePred (Dense)        (None, 3706)              1901178   \n",
      "=================================================================\n",
      "Total params: 4,062,074\n",
      "Trainable params: 4,062,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = autoEncoder(X)\n",
    "\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6040/6040 [==============================] - ETA: 8s - loss: 0.090 - ETA: 5s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.090 - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.089 - ETA: 1s - loss: 0.089 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.084 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - 3s 456us/step - loss: 0.0764\n",
      "Epoch 2/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.056 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.056 - ETA: 2s - loss: 0.056 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.054 - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.056 - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.052 - ETA: 1s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - 3s 434us/step - loss: 0.0500\n",
      "Epoch 3/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.044 - ETA: 2s - loss: 0.042 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 1s - loss: 0.041 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - 3s 430us/step - loss: 0.0362\n",
      "Epoch 4/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.032 - ETA: 2s - loss: 0.031 - ETA: 2s - loss: 0.030 - ETA: 2s - loss: 0.029 - ETA: 2s - loss: 0.029 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - 3s 426us/step - loss: 0.0282\n",
      "Epoch 5/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.023 - ETA: 1s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.022 - 3s 441us/step - loss: 0.0230\n",
      "Epoch 6/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - 3s 455us/step - loss: 0.0196\n",
      "Epoch 7/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 3s 463us/step - loss: 0.0173\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - 3s 440us/step - loss: 0.0157\n",
      "Epoch 9/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 3s 431us/step - loss: 0.0145\n",
      "Epoch 10/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 3s 454us/step - loss: 0.0137\n",
      "Epoch 11/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 3s 418us/step - loss: 0.0130\n",
      "Epoch 12/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 3s 420us/step - loss: 0.0125\n",
      "Epoch 13/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 3s 420us/step - loss: 0.0121\n",
      "Epoch 14/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0118\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0116\n",
      "Epoch 16/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 421us/step - loss: 0.0114\n",
      "Epoch 17/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0112\n",
      "Epoch 18/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0111\n",
      "Epoch 19/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 432us/step - loss: 0.0109\n",
      "Epoch 20/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 425us/step - loss: 0.0109\n",
      "Epoch 21/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 419us/step - loss: 0.0108\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 419us/step - loss: 0.0107\n",
      "Epoch 23/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 420us/step - loss: 0.0106\n",
      "Epoch 24/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 417us/step - loss: 0.0106\n",
      "Epoch 25/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 421us/step - loss: 0.0105\n",
      "Epoch 26/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 463us/step - loss: 0.0105\n",
      "Epoch 27/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 449us/step - loss: 0.0104\n",
      "Epoch 28/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 450us/step - loss: 0.0104\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 445us/step - loss: 0.0104\n",
      "Epoch 30/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 448us/step - loss: 0.0104\n",
      "Epoch 31/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 447us/step - loss: 0.0103\n",
      "Epoch 32/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0103\n",
      "Epoch 33/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0103\n",
      "Epoch 34/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0103\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0103\n",
      "Epoch 36/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0102\n",
      "Epoch 37/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0102\n",
      "Epoch 38/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 455us/step - loss: 0.0102\n",
      "Epoch 39/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0102\n",
      "Epoch 40/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0102\n",
      "Epoch 41/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 459us/step - loss: 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0102\n",
      "Epoch 43/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n",
      "Epoch 44/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0101\n",
      "Epoch 45/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0101\n",
      "Epoch 46/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0101\n",
      "Epoch 47/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 457us/step - loss: 0.0101\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n",
      "Epoch 49/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n",
      "Epoch 50/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=X, y=y,\n",
    "                  epochs=50,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict new Matrix Interactions, set score zero on visualized games\n",
    "# new_matrix = model.predict(X) * (X == 0)\n",
    "new_matrix = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00086715, -0.00470824, -0.01668992, ...,  0.00431625,\n",
       "         0.00356173, -0.01001819],\n",
       "       [ 0.04016824,  0.01442963,  0.01442499, ...,  0.00142624,\n",
       "        -0.00103543,  0.01002616],\n",
       "       [ 0.02150744, -0.00140122,  0.01076992, ..., -0.00505046,\n",
       "        -0.01062662, -0.00433114],\n",
       "       ...,\n",
       "       [-0.00080187,  0.00507312,  0.00350602, ...,  0.00522393,\n",
       "        -0.00791932, -0.00865945],\n",
       "       [ 0.04051276, -0.01438116, -0.02296973, ...,  0.00456508,\n",
       "        -0.01364751,  0.00301851],\n",
       "       [ 0.1159841 ,  0.04672968,  0.03519899, ...,  0.00532098,\n",
       "         0.00525959, -0.02073006]], dtype=float32)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # converting the reconstructed matrix back to a Pandas dataframe\n",
    "# new_users_items_matrix_df  = pd.DataFrame(new_matrix, \n",
    "#                                           columns = users_items_matrix_df.columns, \n",
    "#                                           index   = users_items_matrix_df.index)\n",
    "# new_users_items_matrix_df.head()\n",
    "\n",
    "new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_matrix>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(382567, device='cuda:0')"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(negative_feedback_mask > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
