{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 4.7516655921936035 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Pytorch model doesn't converge - to do - check #################\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.000001))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(512, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.000001))(enc)\n",
    "    lat_space = Dropout(0.5, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.000001))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.000001))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0)\n",
    "positive_feedback_mask = (train > 3)\n",
    "negative_feedback_mask = ((train < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback_mask + negative_feedback_mask != zero_mask).all()\n",
    "assert (positive_feedback_mask + negative_feedback_mask == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback_mask), get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 - get_sparsity(zero_mask), get_sparsity(positive_feedback_mask) + get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.4\n",
    "mask_arr = (np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)\n",
    "y = negative_feedback_mask\n",
    "X = negative_feedback_mask*mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(X), get_sparsity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoEncoder(X)\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tr = np.load('predicted_tr.npy')\n",
    "augmented_train = np.load('augmented_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = model.fit(x=X, y=y,\n",
    "#                   epochs=300,\n",
    "#                   batch_size=128,\n",
    "#                   shuffle=True,\n",
    "#                   validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "predicted_tr = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_tr > 0.4).sum(), (y == 1).sum() # predicted vs real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X>0.5).sum() # trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (predicted_tr>0.4)).sum()/(predicted_tr>0.4).sum() # accuracy on actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (predicted_tr>0.5)).sum()/((predicted_tr>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((predicted_tr>0.5)  * (X<0.5)).sum() # predicted values which were not in the train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y * (((predicted_tr>0.5)  * (X<0.5)))) == 1).sum()/(((predicted_tr>0.5)  * (X<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = model.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(augmented_train>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y* (augmented_train>0.8)).sum()/(((augmented_train>0.8)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((augmented_train > threshold) * (tr==0)).sum() # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_probs = [(tr == 1).sum()/((tr > 0) & (tr < 4)).sum(), (tr == 2).sum()/(((tr > 0) & (tr < 4))).sum(), (tr == 3).sum()/((tr > 0) & (tr < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = tr + (predicted_tr > threshold) * (tr == 0) * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 4.134020185630605)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr), get_sparsity(augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.isin(tr, augmented_train)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25181"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((tr == 0) * (augmented_train > 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predicted_tr', predicted_tr)\n",
    "np.save('augmented_train', augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_iterations = 5\n",
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "nz = 16\n",
    "lamba = 1e-3\n",
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "#         filt = (torch.abs((real > 0.3).float() * fake - real))/real.shape[0]\n",
    "        filt = torch.abs((xr != 0).float().cuda() * xf.cuda() - xr.cuda())/(xf == 0).sum()\n",
    "#         filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())\n",
    "\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "        x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz, 1024), \n",
    "                                torch.nn.ReLU(), \n",
    "                                torch.nn.Linear(1024, 1024), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.5),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.6)\n",
    "                                )\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#                                  nn.Linear(nz,1024),\n",
    "# #                                  nn.Dropout(0.3)\n",
    "#                                  nn.ReLU(),\n",
    "#                                  nn.Linear(1024,2048),\n",
    "#                                  nn.Sigmoid(),\n",
    "#                                  nn.Dropout(0.3),\n",
    "#                                  nn.Linear(2048,features_length),\n",
    "# #                                  nn.Sigmoid()\n",
    "#                                  nn.Dropout(0.5)\n",
    "#                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "#         return x\n",
    "        return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    '''\n",
    "    returns random rows of size batch_size\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))\n",
    "augmented_train = torch.autograd.Variable(torch.Tensor(augmented_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 4.134020185630605)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train.cpu().numpy()), get_sparsity(augmented_train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g): # custom loss -todo\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(5, (5 == train.round()).sum(), (5 == (tr + vr)[:train.shape[0], :].round()).sum())\n",
    "# print(4, (4 == train.round()).sum(), (4 == (tr + vr)[:train.shape[0], :].round()).sum())\n",
    "# print(3, (3 == train.round()).sum(), (3 == (tr + vr)[:train.shape[0], :].round()).sum())\n",
    "# print(2, (2 == train.round()).sum(), (2 == (tr + vr)[:train.shape[0], :].round()).sum())\n",
    "# print(1, (1 == train.round()).sum(), (1 == (tr + vr)[:train.shape[0], :].round()).sum())\n",
    "# print(0, (0 == train.round()).sum(), (0 == (tr + vr)[:train.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(5, (5 == aug_train.round()).sum(), (5 == (tr + vr)[:aug_train.shape[0], :].round()).sum())\n",
    "# print(4, (4 == aug_train.round()).sum(), (4 == (tr + vr)[:aug_train.shape[0], :].round()).sum())\n",
    "# print(3, (3 == aug_train.round()).sum(), (3 == (tr + vr)[:aug_train.shape[0], :].round()).sum())\n",
    "# print(2, (2 == aug_train.round()).sum(), (2 == (tr + vr)[:aug_train.shape[0], :].round()).sum())\n",
    "# print(1, (1 == aug_train.round()).sum(), (1 == (tr + vr)[:aug_train.shape[0], :].round()).sum())\n",
    "# print(0, (0 == aug_train.round()).sum(), (0 == (tr + vr)[:aug_train.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_losses = []\n",
    "def train_GAN(netD, netG, train_mat, steps_per_epoch = 300, epochs = 300):\n",
    "    gen_iterations = 0\n",
    "#     lrD = 5e-4\n",
    "#     lrG = 5e-4\n",
    "#     batch_size = 64\n",
    "#     d_iter = 5\n",
    "#     g_iter = 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#     data_iter = iter(data_loader)\n",
    "        i = 0\n",
    "        while i < steps_per_epoch:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in netD.parameters(): # reset requires_grad\n",
    "                p.requires_grad = True # they are set to False below in netG update\n",
    "#             d_iter = d_iter\n",
    "            j = 0\n",
    "            while j < d_iter*5:\n",
    "                j += 1\n",
    "                # load real data\n",
    "                i += 1\n",
    "    #             X, _ = data_iter.next()\n",
    "                X = get_random_batch(train, batch_size=batch_size)\n",
    "    #             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "    #             print(X >= 0.5)\n",
    "    # #             X = X.view(X.size(0), -1)\n",
    "    #             X = (X >= 0.5).float()\n",
    "    #             if cuda: \n",
    "                X = X.cuda()\n",
    "                real = Variable(X)\n",
    "                # generate fake data\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "    #             if cuda: \n",
    "                noise = noise.cuda()\n",
    "                with torch.no_grad():\n",
    "                    noisev = Variable(noise) # totally freeze netG\n",
    "                fake = Variable(netG(noisev).data)\n",
    "    #             print(real[0,:20], fake[0,:20])\n",
    "                real + fake * (real == 0).float()\n",
    "                fake = fake * Variable(real != 0).float().cuda()\n",
    "    #             real + fake * (real == 0).float()\n",
    "    #             print(real[0,:20], fake[0,:20])\n",
    "                fake.requires_grad = False\n",
    "    #             print(real.shape, fake.shape)\n",
    "\n",
    "                # compute gradient, take step\n",
    "                netD.zero_grad()\n",
    "    #             print('real', real[:10, :20])\n",
    "    #             print('fake', fake[:10, :20])\n",
    "    #             print(real.type(), fake.type())\n",
    "    #             print(fake)\n",
    "                out = netD(real, fake)\n",
    "\n",
    "                outputD = torch.mean(out) + lamba * out.norm()\n",
    "                stdD = torch.std(out)\n",
    "                outputD.backward(mone)\n",
    "                optimizerD.step()\n",
    "    #             print(out.shape)\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "#             g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter*5:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "                netG.zero_grad()\n",
    "\n",
    "                # load real data\n",
    "                i += 1\n",
    "                X = get_random_batch(train, batch_size=batch_size)\n",
    "    #             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "    #             X = X.view(X.size(0), -1)\n",
    "    #             X = (X >= 0.5).float()\n",
    "    #             if cuda: \n",
    "                X = X.cuda()\n",
    "                real = Variable(X)\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "    #             if cuda: \n",
    "                noise = noise.cuda()\n",
    "                noisev = Variable(noise)\n",
    "\n",
    "                fake = netG(noisev)\n",
    "                real + fake * (real == 0).float()\n",
    "                fake = fake * Variable(real != 0).float().cuda()\n",
    "    #             fake = fake * Variable(real != 0).float().cuda()\n",
    "    #             fake.requires_grad = False\n",
    "    #             fake = Variable(netG(noisev)).data\n",
    "    #             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "    #             fake.requires_grad = True\n",
    "\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "\n",
    "                gen_iterations = gen_iterations + 1\n",
    "\n",
    "    #             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "    #             print('output_D', outputD.item(), gen_iterations)\n",
    "    #             print('output_G', outputG.item(), gen_iterations)\n",
    "    #             print('std_D', stdD.item(), gen_iterations)\n",
    "    #             print('std_G', stdG.item(), gen_iterations)\n",
    "\n",
    "                # evaluation\n",
    "                if gen_iterations % 100 == 0: # todo- to change\n",
    "    #                 gen.eval()\n",
    "    #                 z_vector_eval = make_some_noise(128)\n",
    "    #                 fake_rows_eval = gen(z_vector_eval)\n",
    "    #                 real_rows_eval = get_random_batch(train, 128)\n",
    "            #         print(fake_rows[0][:10]) enable to see some results\n",
    "    #                 fake = Variable(netG(noisev).data).round()\n",
    "    #                 fake = ((real != 0) & (fake != 0))\n",
    "    #                 print(fake)\n",
    "                    eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                    eval_losses.append(eval_loss)\n",
    "                    print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                    print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Dropout(p=0.5)\n",
      "    (5): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Dropout(p=0.6)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_tr = NetD().to(device)\n",
    "netG_tr = NetG().to(device)\n",
    "print(netD_tr)\n",
    "print(netG_tr)\n",
    "optimizerG = optim.RMSprop(netG_tr.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_tr.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (-1 * one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1. my distance between random real and fake samples 25827.294921875\n",
      "Epoch number 1. MSE distance between random real and fake samples 0.4181431233882904\n",
      "Epoch number 3. my distance between random real and fake samples 31952.5390625\n",
      "Epoch number 3. MSE distance between random real and fake samples 0.5339747071266174\n",
      "Epoch number 5. my distance between random real and fake samples 34807.57421875\n",
      "Epoch number 5. MSE distance between random real and fake samples 0.6371153593063354\n",
      "Epoch number 7. my distance between random real and fake samples 35503.4296875\n",
      "Epoch number 7. MSE distance between random real and fake samples 0.7381928563117981\n",
      "Epoch number 9. my distance between random real and fake samples 38771.6484375\n",
      "Epoch number 9. MSE distance between random real and fake samples 0.8462054133415222\n",
      "Epoch number 11. my distance between random real and fake samples 46704.84765625\n",
      "Epoch number 11. MSE distance between random real and fake samples 1.0091856718063354\n",
      "Epoch number 13. my distance between random real and fake samples 44666.32421875\n",
      "Epoch number 13. MSE distance between random real and fake samples 0.9332494735717773\n",
      "Epoch number 15. my distance between random real and fake samples 44762.22265625\n",
      "Epoch number 15. MSE distance between random real and fake samples 0.9346705079078674\n",
      "Epoch number 17. my distance between random real and fake samples 55631.9765625\n",
      "Epoch number 17. MSE distance between random real and fake samples 1.1206587553024292\n",
      "Epoch number 19. my distance between random real and fake samples 44692.046875\n",
      "Epoch number 19. MSE distance between random real and fake samples 0.8864185214042664\n",
      "Epoch number 21. my distance between random real and fake samples 36030.375\n",
      "Epoch number 21. MSE distance between random real and fake samples 0.7015178203582764\n",
      "Epoch number 23. my distance between random real and fake samples 38335.5703125\n",
      "Epoch number 23. MSE distance between random real and fake samples 0.728848934173584\n",
      "Epoch number 25. my distance between random real and fake samples 39313.4765625\n",
      "Epoch number 25. MSE distance between random real and fake samples 0.7306464910507202\n",
      "Epoch number 27. my distance between random real and fake samples 31643.359375\n",
      "Epoch number 27. MSE distance between random real and fake samples 0.5922913551330566\n",
      "Epoch number 29. my distance between random real and fake samples 36680.09765625\n",
      "Epoch number 29. MSE distance between random real and fake samples 0.6789612174034119\n",
      "Epoch number 31. my distance between random real and fake samples 33451.0625\n",
      "Epoch number 31. MSE distance between random real and fake samples 0.6057242751121521\n",
      "Epoch number 33. my distance between random real and fake samples 45565.10546875\n",
      "Epoch number 33. MSE distance between random real and fake samples 0.8139366507530212\n",
      "Epoch number 35. my distance between random real and fake samples 36864.89453125\n",
      "Epoch number 35. MSE distance between random real and fake samples 0.644338846206665\n",
      "Epoch number 37. my distance between random real and fake samples 26692.029296875\n",
      "Epoch number 37. MSE distance between random real and fake samples 0.4628809988498688\n",
      "Epoch number 39. my distance between random real and fake samples 32681.662109375\n",
      "Epoch number 39. MSE distance between random real and fake samples 0.5720406770706177\n",
      "Epoch number 41. my distance between random real and fake samples 20960.6640625\n",
      "Epoch number 41. MSE distance between random real and fake samples 0.3614504039287567\n",
      "Epoch number 43. my distance between random real and fake samples 27310.89453125\n",
      "Epoch number 43. MSE distance between random real and fake samples 0.45575040578842163\n",
      "Epoch number 45. my distance between random real and fake samples 32285.677734375\n",
      "Epoch number 45. MSE distance between random real and fake samples 0.5170320868492126\n",
      "Epoch number 47. my distance between random real and fake samples 27785.1875\n",
      "Epoch number 47. MSE distance between random real and fake samples 0.44336801767349243\n",
      "Epoch number 49. my distance between random real and fake samples 25223.96875\n",
      "Epoch number 49. MSE distance between random real and fake samples 0.39727023243904114\n",
      "Epoch number 51. my distance between random real and fake samples 34765.79296875\n",
      "Epoch number 51. MSE distance between random real and fake samples 0.5437857508659363\n",
      "Epoch number 53. my distance between random real and fake samples 21292.48046875\n",
      "Epoch number 53. MSE distance between random real and fake samples 0.3399199843406677\n",
      "Epoch number 55. my distance between random real and fake samples 26223.81640625\n",
      "Epoch number 55. MSE distance between random real and fake samples 0.4179914891719818\n",
      "Epoch number 57. my distance between random real and fake samples 28172.548828125\n",
      "Epoch number 57. MSE distance between random real and fake samples 0.44137296080589294\n",
      "Epoch number 59. my distance between random real and fake samples 21871.650390625\n",
      "Epoch number 59. MSE distance between random real and fake samples 0.3501408100128174\n",
      "Epoch number 61. my distance between random real and fake samples 28690.796875\n",
      "Epoch number 61. MSE distance between random real and fake samples 0.4620695114135742\n",
      "Epoch number 63. my distance between random real and fake samples 29560.2734375\n",
      "Epoch number 63. MSE distance between random real and fake samples 0.4604334831237793\n",
      "Epoch number 65. my distance between random real and fake samples 23420.61328125\n",
      "Epoch number 65. MSE distance between random real and fake samples 0.36354973912239075\n",
      "Epoch number 67. my distance between random real and fake samples 29920.705078125\n",
      "Epoch number 67. MSE distance between random real and fake samples 0.46759679913520813\n",
      "Epoch number 69. my distance between random real and fake samples 21982.4453125\n",
      "Epoch number 69. MSE distance between random real and fake samples 0.33503198623657227\n",
      "Epoch number 71. my distance between random real and fake samples 24652.72265625\n",
      "Epoch number 71. MSE distance between random real and fake samples 0.38228264451026917\n",
      "Epoch number 73. my distance between random real and fake samples 23023.91015625\n",
      "Epoch number 73. MSE distance between random real and fake samples 0.3592173457145691\n",
      "Epoch number 75. my distance between random real and fake samples 23874.61328125\n",
      "Epoch number 75. MSE distance between random real and fake samples 0.3600815236568451\n",
      "Epoch number 77. my distance between random real and fake samples 24152.93359375\n",
      "Epoch number 77. MSE distance between random real and fake samples 0.3581610918045044\n",
      "Epoch number 79. my distance between random real and fake samples 19586.99609375\n",
      "Epoch number 79. MSE distance between random real and fake samples 0.2983446419239044\n",
      "Epoch number 81. my distance between random real and fake samples 27278.54296875\n",
      "Epoch number 81. MSE distance between random real and fake samples 0.42153388261795044\n",
      "Epoch number 83. my distance between random real and fake samples 30060.9140625\n",
      "Epoch number 83. MSE distance between random real and fake samples 0.4589994251728058\n",
      "Epoch number 85. my distance between random real and fake samples 28026.3671875\n",
      "Epoch number 85. MSE distance between random real and fake samples 0.4256570637226105\n",
      "Epoch number 87. my distance between random real and fake samples 17498.5625\n",
      "Epoch number 87. MSE distance between random real and fake samples 0.267253577709198\n",
      "Epoch number 89. my distance between random real and fake samples 26257.76171875\n",
      "Epoch number 89. MSE distance between random real and fake samples 0.40981072187423706\n",
      "Epoch number 91. my distance between random real and fake samples 27720.970703125\n",
      "Epoch number 91. MSE distance between random real and fake samples 0.4249362349510193\n",
      "Epoch number 93. my distance between random real and fake samples 32005.53515625\n",
      "Epoch number 93. MSE distance between random real and fake samples 0.4897707998752594\n",
      "Epoch number 95. my distance between random real and fake samples 27054.208984375\n",
      "Epoch number 95. MSE distance between random real and fake samples 0.40474173426628113\n",
      "Epoch number 97. my distance between random real and fake samples 22350.08203125\n",
      "Epoch number 97. MSE distance between random real and fake samples 0.33768001198768616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 99. my distance between random real and fake samples 27905.234375\n",
      "Epoch number 99. MSE distance between random real and fake samples 0.4230198264122009\n",
      "Epoch number 101. my distance between random real and fake samples 19950.06640625\n",
      "Epoch number 101. MSE distance between random real and fake samples 0.3021640181541443\n",
      "Epoch number 103. my distance between random real and fake samples 22707.375\n",
      "Epoch number 103. MSE distance between random real and fake samples 0.340210497379303\n",
      "Epoch number 105. my distance between random real and fake samples 23508.51953125\n",
      "Epoch number 105. MSE distance between random real and fake samples 0.3507693409919739\n",
      "Epoch number 107. my distance between random real and fake samples 30810.8984375\n",
      "Epoch number 107. MSE distance between random real and fake samples 0.4705582559108734\n",
      "Epoch number 109. my distance between random real and fake samples 24048.271484375\n",
      "Epoch number 109. MSE distance between random real and fake samples 0.36199238896369934\n",
      "Epoch number 111. my distance between random real and fake samples 21940.43359375\n",
      "Epoch number 111. MSE distance between random real and fake samples 0.33350715041160583\n",
      "Epoch number 113. my distance between random real and fake samples 19151.115234375\n",
      "Epoch number 113. MSE distance between random real and fake samples 0.29507842659950256\n",
      "Epoch number 115. my distance between random real and fake samples 26768.181640625\n",
      "Epoch number 115. MSE distance between random real and fake samples 0.3947860896587372\n",
      "Epoch number 117. my distance between random real and fake samples 23367.9453125\n",
      "Epoch number 117. MSE distance between random real and fake samples 0.34886255860328674\n",
      "Epoch number 119. my distance between random real and fake samples 27527.736328125\n",
      "Epoch number 119. MSE distance between random real and fake samples 0.4093315303325653\n",
      "Epoch number 121. my distance between random real and fake samples 23472.65625\n",
      "Epoch number 121. MSE distance between random real and fake samples 0.3597957491874695\n",
      "Epoch number 123. my distance between random real and fake samples 26684.578125\n",
      "Epoch number 123. MSE distance between random real and fake samples 0.40704816579818726\n",
      "Epoch number 125. my distance between random real and fake samples 26961.205078125\n",
      "Epoch number 125. MSE distance between random real and fake samples 0.4061552584171295\n",
      "Epoch number 127. my distance between random real and fake samples 26261.28125\n",
      "Epoch number 127. MSE distance between random real and fake samples 0.3942544460296631\n",
      "Epoch number 129. my distance between random real and fake samples 25140.03515625\n",
      "Epoch number 129. MSE distance between random real and fake samples 0.370373398065567\n",
      "Epoch number 131. my distance between random real and fake samples 25351.8125\n",
      "Epoch number 131. MSE distance between random real and fake samples 0.3836973309516907\n",
      "Epoch number 133. my distance between random real and fake samples 24633.357421875\n",
      "Epoch number 133. MSE distance between random real and fake samples 0.3717942237854004\n",
      "Epoch number 135. my distance between random real and fake samples 21983.10546875\n",
      "Epoch number 135. MSE distance between random real and fake samples 0.3375196158885956\n",
      "Epoch number 137. my distance between random real and fake samples 22241.095703125\n",
      "Epoch number 137. MSE distance between random real and fake samples 0.3318013846874237\n",
      "Epoch number 139. my distance between random real and fake samples 25839.455078125\n",
      "Epoch number 139. MSE distance between random real and fake samples 0.39227378368377686\n",
      "Epoch number 141. my distance between random real and fake samples 20648.2890625\n",
      "Epoch number 141. MSE distance between random real and fake samples 0.3138487637042999\n",
      "Epoch number 143. my distance between random real and fake samples 29580.8515625\n",
      "Epoch number 143. MSE distance between random real and fake samples 0.44038549065589905\n",
      "Epoch number 145. my distance between random real and fake samples 27499.5078125\n",
      "Epoch number 145. MSE distance between random real and fake samples 0.4009976089000702\n",
      "Epoch number 147. my distance between random real and fake samples 26793.42578125\n",
      "Epoch number 147. MSE distance between random real and fake samples 0.40233486890792847\n",
      "Epoch number 149. my distance between random real and fake samples 18877.41015625\n",
      "Epoch number 149. MSE distance between random real and fake samples 0.2824288606643677\n",
      "Epoch number 151. my distance between random real and fake samples 25709.783203125\n",
      "Epoch number 151. MSE distance between random real and fake samples 0.38970422744750977\n",
      "Epoch number 153. my distance between random real and fake samples 18978.603515625\n",
      "Epoch number 153. MSE distance between random real and fake samples 0.2807115614414215\n",
      "Epoch number 155. my distance between random real and fake samples 24349.66015625\n",
      "Epoch number 155. MSE distance between random real and fake samples 0.3737546503543854\n",
      "Epoch number 157. my distance between random real and fake samples 24820.310546875\n",
      "Epoch number 157. MSE distance between random real and fake samples 0.3727370500564575\n",
      "Epoch number 159. my distance between random real and fake samples 23644.1015625\n",
      "Epoch number 159. MSE distance between random real and fake samples 0.3590831756591797\n",
      "Epoch number 161. my distance between random real and fake samples 30553.765625\n",
      "Epoch number 161. MSE distance between random real and fake samples 0.45098310708999634\n",
      "Epoch number 163. my distance between random real and fake samples 17579.00390625\n",
      "Epoch number 163. MSE distance between random real and fake samples 0.2706213593482971\n",
      "Epoch number 165. my distance between random real and fake samples 22003.908203125\n",
      "Epoch number 165. MSE distance between random real and fake samples 0.33020249009132385\n",
      "Epoch number 167. my distance between random real and fake samples 13500.50390625\n",
      "Epoch number 167. MSE distance between random real and fake samples 0.20720627903938293\n",
      "Epoch number 169. my distance between random real and fake samples 33164.30078125\n",
      "Epoch number 169. MSE distance between random real and fake samples 0.4859285056591034\n",
      "Epoch number 171. my distance between random real and fake samples 26840.546875\n",
      "Epoch number 171. MSE distance between random real and fake samples 0.4035957455635071\n",
      "Epoch number 173. my distance between random real and fake samples 25201.62109375\n",
      "Epoch number 173. MSE distance between random real and fake samples 0.38144993782043457\n",
      "Epoch number 175. my distance between random real and fake samples 21935.759765625\n",
      "Epoch number 175. MSE distance between random real and fake samples 0.3235863149166107\n",
      "Epoch number 177. my distance between random real and fake samples 26073.22265625\n",
      "Epoch number 177. MSE distance between random real and fake samples 0.39708656072616577\n",
      "Epoch number 179. my distance between random real and fake samples 23497.85546875\n",
      "Epoch number 179. MSE distance between random real and fake samples 0.3506498634815216\n",
      "Epoch number 181. my distance between random real and fake samples 24556.982421875\n",
      "Epoch number 181. MSE distance between random real and fake samples 0.366782546043396\n",
      "Epoch number 183. my distance between random real and fake samples 25383.751953125\n",
      "Epoch number 183. MSE distance between random real and fake samples 0.3791465759277344\n",
      "Epoch number 185. my distance between random real and fake samples 28644.037109375\n",
      "Epoch number 185. MSE distance between random real and fake samples 0.42547571659088135\n",
      "Epoch number 187. my distance between random real and fake samples 24190.7578125\n",
      "Epoch number 187. MSE distance between random real and fake samples 0.37048545479774475\n",
      "Epoch number 189. my distance between random real and fake samples 26177.81640625\n",
      "Epoch number 189. MSE distance between random real and fake samples 0.4019065201282501\n",
      "Epoch number 191. my distance between random real and fake samples 26447.4296875\n",
      "Epoch number 191. MSE distance between random real and fake samples 0.3907475769519806\n",
      "Epoch number 193. my distance between random real and fake samples 23875.703125\n",
      "Epoch number 193. MSE distance between random real and fake samples 0.3596324622631073\n",
      "Epoch number 195. my distance between random real and fake samples 23736.841796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 195. MSE distance between random real and fake samples 0.3466697335243225\n",
      "Epoch number 197. my distance between random real and fake samples 22303.69140625\n",
      "Epoch number 197. MSE distance between random real and fake samples 0.33095240592956543\n",
      "Epoch number 199. my distance between random real and fake samples 21787.4375\n",
      "Epoch number 199. MSE distance between random real and fake samples 0.32528647780418396\n",
      "Epoch number 201. my distance between random real and fake samples 23542.91796875\n",
      "Epoch number 201. MSE distance between random real and fake samples 0.35351333022117615\n",
      "Epoch number 203. my distance between random real and fake samples 27382.1171875\n",
      "Epoch number 203. MSE distance between random real and fake samples 0.4032948613166809\n",
      "Epoch number 205. my distance between random real and fake samples 25190.00390625\n",
      "Epoch number 205. MSE distance between random real and fake samples 0.3831724524497986\n",
      "Epoch number 207. my distance between random real and fake samples 26000.986328125\n",
      "Epoch number 207. MSE distance between random real and fake samples 0.3911970555782318\n",
      "Epoch number 209. my distance between random real and fake samples 24528.162109375\n",
      "Epoch number 209. MSE distance between random real and fake samples 0.36612391471862793\n",
      "Epoch number 211. my distance between random real and fake samples 26602.291015625\n",
      "Epoch number 211. MSE distance between random real and fake samples 0.4000563323497772\n",
      "Epoch number 213. my distance between random real and fake samples 22709.140625\n",
      "Epoch number 213. MSE distance between random real and fake samples 0.3429209291934967\n",
      "Epoch number 215. my distance between random real and fake samples 19576.0625\n",
      "Epoch number 215. MSE distance between random real and fake samples 0.28602755069732666\n",
      "Epoch number 217. my distance between random real and fake samples 20668.01171875\n",
      "Epoch number 217. MSE distance between random real and fake samples 0.3161199390888214\n",
      "Epoch number 219. my distance between random real and fake samples 22198.6640625\n",
      "Epoch number 219. MSE distance between random real and fake samples 0.33348509669303894\n",
      "Epoch number 221. my distance between random real and fake samples 26551.732421875\n",
      "Epoch number 221. MSE distance between random real and fake samples 0.3942108452320099\n",
      "Epoch number 223. my distance between random real and fake samples 24775.07421875\n",
      "Epoch number 223. MSE distance between random real and fake samples 0.368406742811203\n",
      "Epoch number 225. my distance between random real and fake samples 20271.171875\n",
      "Epoch number 225. MSE distance between random real and fake samples 0.3043900728225708\n",
      "Epoch number 227. my distance between random real and fake samples 21776.365234375\n",
      "Epoch number 227. MSE distance between random real and fake samples 0.32589611411094666\n",
      "Epoch number 229. my distance between random real and fake samples 22822.97265625\n",
      "Epoch number 229. MSE distance between random real and fake samples 0.3436530828475952\n",
      "Epoch number 231. my distance between random real and fake samples 20439.802734375\n",
      "Epoch number 231. MSE distance between random real and fake samples 0.31110695004463196\n",
      "Epoch number 233. my distance between random real and fake samples 23407.13671875\n",
      "Epoch number 233. MSE distance between random real and fake samples 0.35842233896255493\n",
      "Epoch number 235. my distance between random real and fake samples 21212.744140625\n",
      "Epoch number 235. MSE distance between random real and fake samples 0.3139123022556305\n",
      "Epoch number 237. my distance between random real and fake samples 25760.2890625\n",
      "Epoch number 237. MSE distance between random real and fake samples 0.3881763517856598\n",
      "Epoch number 239. my distance between random real and fake samples 22331.63671875\n",
      "Epoch number 239. MSE distance between random real and fake samples 0.34046390652656555\n",
      "Epoch number 241. my distance between random real and fake samples 26726.75390625\n",
      "Epoch number 241. MSE distance between random real and fake samples 0.41471460461616516\n",
      "Epoch number 243. my distance between random real and fake samples 19039.265625\n",
      "Epoch number 243. MSE distance between random real and fake samples 0.2876189947128296\n",
      "Epoch number 245. my distance between random real and fake samples 19556.587890625\n",
      "Epoch number 245. MSE distance between random real and fake samples 0.2925719618797302\n",
      "Epoch number 247. my distance between random real and fake samples 21701.58203125\n",
      "Epoch number 247. MSE distance between random real and fake samples 0.3301924765110016\n",
      "Epoch number 249. my distance between random real and fake samples 21716.263671875\n",
      "Epoch number 249. MSE distance between random real and fake samples 0.33435487747192383\n",
      "Epoch number 251. my distance between random real and fake samples 25754.0546875\n",
      "Epoch number 251. MSE distance between random real and fake samples 0.3815211057662964\n",
      "Epoch number 253. my distance between random real and fake samples 31471.25390625\n",
      "Epoch number 253. MSE distance between random real and fake samples 0.46480271220207214\n",
      "Epoch number 255. my distance between random real and fake samples 21127.44921875\n",
      "Epoch number 255. MSE distance between random real and fake samples 0.3250141739845276\n",
      "Epoch number 257. my distance between random real and fake samples 29673.4296875\n",
      "Epoch number 257. MSE distance between random real and fake samples 0.46735140681266785\n",
      "Epoch number 259. my distance between random real and fake samples 20232.255859375\n",
      "Epoch number 259. MSE distance between random real and fake samples 0.3107626438140869\n",
      "Epoch number 261. my distance between random real and fake samples 24542.2734375\n",
      "Epoch number 261. MSE distance between random real and fake samples 0.3793381452560425\n",
      "Epoch number 263. my distance between random real and fake samples 23756.1015625\n",
      "Epoch number 263. MSE distance between random real and fake samples 0.35684940218925476\n",
      "Epoch number 265. my distance between random real and fake samples 18722.3125\n",
      "Epoch number 265. MSE distance between random real and fake samples 0.29309844970703125\n",
      "Epoch number 267. my distance between random real and fake samples 25753.24609375\n",
      "Epoch number 267. MSE distance between random real and fake samples 0.3727610409259796\n",
      "Epoch number 269. my distance between random real and fake samples 24684.78515625\n",
      "Epoch number 269. MSE distance between random real and fake samples 0.3712192475795746\n",
      "Epoch number 271. my distance between random real and fake samples 23475.5859375\n",
      "Epoch number 271. MSE distance between random real and fake samples 0.3492732048034668\n",
      "Epoch number 273. my distance between random real and fake samples 18344.51953125\n",
      "Epoch number 273. MSE distance between random real and fake samples 0.27319660782814026\n",
      "Epoch number 275. my distance between random real and fake samples 22847.580078125\n",
      "Epoch number 275. MSE distance between random real and fake samples 0.3503156304359436\n",
      "Epoch number 277. my distance between random real and fake samples 29259.578125\n",
      "Epoch number 277. MSE distance between random real and fake samples 0.43517303466796875\n",
      "Epoch number 279. my distance between random real and fake samples 21390.615234375\n",
      "Epoch number 279. MSE distance between random real and fake samples 0.316153883934021\n",
      "Epoch number 281. my distance between random real and fake samples 24845.29296875\n",
      "Epoch number 281. MSE distance between random real and fake samples 0.3759043514728546\n",
      "Epoch number 283. my distance between random real and fake samples 29704.84375\n",
      "Epoch number 283. MSE distance between random real and fake samples 0.44872942566871643\n",
      "Epoch number 285. my distance between random real and fake samples 21895.09765625\n",
      "Epoch number 285. MSE distance between random real and fake samples 0.3327038586139679\n",
      "Epoch number 287. my distance between random real and fake samples 24963.828125\n",
      "Epoch number 287. MSE distance between random real and fake samples 0.36751288175582886\n",
      "Epoch number 289. my distance between random real and fake samples 22046.37890625\n",
      "Epoch number 289. MSE distance between random real and fake samples 0.3299909830093384\n",
      "Epoch number 291. my distance between random real and fake samples 25855.119140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 291. MSE distance between random real and fake samples 0.38453003764152527\n",
      "Epoch number 293. my distance between random real and fake samples 18331.5234375\n",
      "Epoch number 293. MSE distance between random real and fake samples 0.27359598875045776\n",
      "Epoch number 295. my distance between random real and fake samples 27791.1484375\n",
      "Epoch number 295. MSE distance between random real and fake samples 0.4210446774959564\n",
      "Epoch number 297. my distance between random real and fake samples 18481.234375\n",
      "Epoch number 297. MSE distance between random real and fake samples 0.2809845209121704\n",
      "Epoch number 299. my distance between random real and fake samples 18310.984375\n",
      "Epoch number 299. MSE distance between random real and fake samples 0.28803738951683044\n"
     ]
    }
   ],
   "source": [
    "train_GAN(netD_tr, netG_tr, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(netG.state_dict(), './netG-1m')\n",
    "# torch.save(netD.state_dict(), './netD-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_tr(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1465021 226310\n",
      "4 2819271 348971\n",
      "3 2622931 261197\n",
      "2 836925 107557\n",
      "1 44763 56174\n",
      "0 14595329 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see there is a significant bias towards higher ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Dropout(p=0.5)\n",
      "    (5): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Dropout(p=0.6)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_augm = NetD().to(device)\n",
    "netG_augm = NetG().to(device)\n",
    "print(netD_augm)\n",
    "print(netG_augm)\n",
    "optimizerG = optim.RMSprop(netG_augm.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_augm.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (-1 * one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor(1752921, device='cuda:0') 226310\n",
      "4 tensor(624216, device='cuda:0') 348971\n",
      "3 tensor(93823, device='cuda:0') 261197\n",
      "2 tensor(3141, device='cuda:0') 107557\n",
      "1 tensor(1, device='cuda:0') 56174\n",
      "0 tensor(13426588, device='cuda:0') 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1. my distance between random real and fake samples 27796.94921875\n",
      "Epoch number 1. MSE distance between random real and fake samples 0.46380364894866943\n",
      "Epoch number 3. my distance between random real and fake samples 33339.7890625\n",
      "Epoch number 3. MSE distance between random real and fake samples 0.5964004993438721\n",
      "Epoch number 5. my distance between random real and fake samples 35039.26953125\n",
      "Epoch number 5. MSE distance between random real and fake samples 0.7198690176010132\n",
      "Epoch number 7. my distance between random real and fake samples 54702.88671875\n",
      "Epoch number 7. MSE distance between random real and fake samples 1.1715189218521118\n",
      "Epoch number 9. my distance between random real and fake samples 32133.794921875\n",
      "Epoch number 9. MSE distance between random real and fake samples 0.6946011185646057\n",
      "Epoch number 11. my distance between random real and fake samples 48702.703125\n",
      "Epoch number 11. MSE distance between random real and fake samples 1.0291062593460083\n",
      "Epoch number 13. my distance between random real and fake samples 41387.3671875\n",
      "Epoch number 13. MSE distance between random real and fake samples 0.8770630955696106\n",
      "Epoch number 15. my distance between random real and fake samples 29704.044921875\n",
      "Epoch number 15. MSE distance between random real and fake samples 0.6123911738395691\n",
      "Epoch number 17. my distance between random real and fake samples 37680.2734375\n",
      "Epoch number 17. MSE distance between random real and fake samples 0.7545572519302368\n",
      "Epoch number 19. my distance between random real and fake samples 36673.1171875\n",
      "Epoch number 19. MSE distance between random real and fake samples 0.7066500782966614\n",
      "Epoch number 21. my distance between random real and fake samples 35134.53125\n",
      "Epoch number 21. MSE distance between random real and fake samples 0.6665993332862854\n",
      "Epoch number 23. my distance between random real and fake samples 40787.90234375\n",
      "Epoch number 23. MSE distance between random real and fake samples 0.7655326128005981\n",
      "Epoch number 25. my distance between random real and fake samples 34656.453125\n",
      "Epoch number 25. MSE distance between random real and fake samples 0.6418845653533936\n",
      "Epoch number 27. my distance between random real and fake samples 35562.60546875\n",
      "Epoch number 27. MSE distance between random real and fake samples 0.6630224585533142\n",
      "Epoch number 29. my distance between random real and fake samples 37682.578125\n",
      "Epoch number 29. MSE distance between random real and fake samples 0.6872434616088867\n",
      "Epoch number 31. my distance between random real and fake samples 29182.587890625\n",
      "Epoch number 31. MSE distance between random real and fake samples 0.519768238067627\n",
      "Epoch number 33. my distance between random real and fake samples 30803.41015625\n",
      "Epoch number 33. MSE distance between random real and fake samples 0.5462448000907898\n",
      "Epoch number 35. my distance between random real and fake samples 34720.45703125\n",
      "Epoch number 35. MSE distance between random real and fake samples 0.6050499081611633\n",
      "Epoch number 37. my distance between random real and fake samples 34061.265625\n",
      "Epoch number 37. MSE distance between random real and fake samples 0.5882612466812134\n",
      "Epoch number 39. my distance between random real and fake samples 41212.5390625\n",
      "Epoch number 39. MSE distance between random real and fake samples 0.6970362663269043\n",
      "Epoch number 41. my distance between random real and fake samples 29908.98046875\n",
      "Epoch number 41. MSE distance between random real and fake samples 0.4925168752670288\n",
      "Epoch number 43. my distance between random real and fake samples 27095.94921875\n",
      "Epoch number 43. MSE distance between random real and fake samples 0.4476775527000427\n",
      "Epoch number 45. my distance between random real and fake samples 29787.751953125\n",
      "Epoch number 45. MSE distance between random real and fake samples 0.4729015827178955\n",
      "Epoch number 47. my distance between random real and fake samples 22658.359375\n",
      "Epoch number 47. MSE distance between random real and fake samples 0.36480382084846497\n",
      "Epoch number 49. my distance between random real and fake samples 22645.353515625\n",
      "Epoch number 49. MSE distance between random real and fake samples 0.36178648471832275\n",
      "Epoch number 51. my distance between random real and fake samples 35511.2109375\n",
      "Epoch number 51. MSE distance between random real and fake samples 0.5694667100906372\n",
      "Epoch number 53. my distance between random real and fake samples 30966.6953125\n",
      "Epoch number 53. MSE distance between random real and fake samples 0.48972073197364807\n",
      "Epoch number 55. my distance between random real and fake samples 28497.46484375\n",
      "Epoch number 55. MSE distance between random real and fake samples 0.44955679774284363\n",
      "Epoch number 57. my distance between random real and fake samples 34830.06640625\n",
      "Epoch number 57. MSE distance between random real and fake samples 0.5256803035736084\n",
      "Epoch number 59. my distance between random real and fake samples 23378.638671875\n",
      "Epoch number 59. MSE distance between random real and fake samples 0.36688441038131714\n",
      "Epoch number 61. my distance between random real and fake samples 25438.578125\n",
      "Epoch number 61. MSE distance between random real and fake samples 0.395323783159256\n",
      "Epoch number 63. my distance between random real and fake samples 34736.6875\n",
      "Epoch number 63. MSE distance between random real and fake samples 0.5348813533782959\n",
      "Epoch number 65. my distance between random real and fake samples 20146.69140625\n",
      "Epoch number 65. MSE distance between random real and fake samples 0.31708329916000366\n",
      "Epoch number 67. my distance between random real and fake samples 25768.23828125\n",
      "Epoch number 67. MSE distance between random real and fake samples 0.40697017312049866\n",
      "Epoch number 69. my distance between random real and fake samples 24490.63671875\n",
      "Epoch number 69. MSE distance between random real and fake samples 0.3905000388622284\n",
      "Epoch number 71. my distance between random real and fake samples 27898.123046875\n",
      "Epoch number 71. MSE distance between random real and fake samples 0.4318619966506958\n",
      "Epoch number 73. my distance between random real and fake samples 22113.015625\n",
      "Epoch number 73. MSE distance between random real and fake samples 0.333609014749527\n",
      "Epoch number 75. my distance between random real and fake samples 23581.8515625\n",
      "Epoch number 75. MSE distance between random real and fake samples 0.35412225127220154\n",
      "Epoch number 77. my distance between random real and fake samples 24181.44921875\n",
      "Epoch number 77. MSE distance between random real and fake samples 0.3642881214618683\n",
      "Epoch number 79. my distance between random real and fake samples 24641.708984375\n",
      "Epoch number 79. MSE distance between random real and fake samples 0.3805884122848511\n",
      "Epoch number 81. my distance between random real and fake samples 22691.21484375\n",
      "Epoch number 81. MSE distance between random real and fake samples 0.3533160090446472\n",
      "Epoch number 83. my distance between random real and fake samples 24790.375\n",
      "Epoch number 83. MSE distance between random real and fake samples 0.3844313621520996\n",
      "Epoch number 85. my distance between random real and fake samples 28309.6953125\n",
      "Epoch number 85. MSE distance between random real and fake samples 0.4342455267906189\n",
      "Epoch number 87. my distance between random real and fake samples 28715.4453125\n",
      "Epoch number 87. MSE distance between random real and fake samples 0.44099530577659607\n",
      "Epoch number 89. my distance between random real and fake samples 24503.3828125\n",
      "Epoch number 89. MSE distance between random real and fake samples 0.3814889192581177\n",
      "Epoch number 91. my distance between random real and fake samples 26557.40234375\n",
      "Epoch number 91. MSE distance between random real and fake samples 0.415202260017395\n",
      "Epoch number 93. my distance between random real and fake samples 24921.25390625\n",
      "Epoch number 93. MSE distance between random real and fake samples 0.3814629018306732\n",
      "Epoch number 95. my distance between random real and fake samples 21503.96484375\n",
      "Epoch number 95. MSE distance between random real and fake samples 0.33789393305778503\n",
      "Epoch number 97. my distance between random real and fake samples 28733.87890625\n",
      "Epoch number 97. MSE distance between random real and fake samples 0.4340388774871826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 99. my distance between random real and fake samples 22288.869140625\n",
      "Epoch number 99. MSE distance between random real and fake samples 0.3363337516784668\n",
      "Epoch number 101. my distance between random real and fake samples 22992.205078125\n",
      "Epoch number 101. MSE distance between random real and fake samples 0.3525981307029724\n",
      "Epoch number 103. my distance between random real and fake samples 23430.234375\n",
      "Epoch number 103. MSE distance between random real and fake samples 0.3511309027671814\n",
      "Epoch number 105. my distance between random real and fake samples 25422.236328125\n",
      "Epoch number 105. MSE distance between random real and fake samples 0.3868910074234009\n",
      "Epoch number 107. my distance between random real and fake samples 24883.759765625\n",
      "Epoch number 107. MSE distance between random real and fake samples 0.37296029925346375\n",
      "Epoch number 109. my distance between random real and fake samples 27345.15234375\n",
      "Epoch number 109. MSE distance between random real and fake samples 0.41453438997268677\n",
      "Epoch number 111. my distance between random real and fake samples 30971.1875\n",
      "Epoch number 111. MSE distance between random real and fake samples 0.4649317264556885\n",
      "Epoch number 113. my distance between random real and fake samples 23108.94140625\n",
      "Epoch number 113. MSE distance between random real and fake samples 0.35289546847343445\n",
      "Epoch number 115. my distance between random real and fake samples 25590.60546875\n",
      "Epoch number 115. MSE distance between random real and fake samples 0.3842889368534088\n",
      "Epoch number 117. my distance between random real and fake samples 20423.56640625\n",
      "Epoch number 117. MSE distance between random real and fake samples 0.3174407482147217\n",
      "Epoch number 119. my distance between random real and fake samples 24830.09375\n",
      "Epoch number 119. MSE distance between random real and fake samples 0.36088377237319946\n",
      "Epoch number 121. my distance between random real and fake samples 21900.876953125\n",
      "Epoch number 121. MSE distance between random real and fake samples 0.33149421215057373\n",
      "Epoch number 123. my distance between random real and fake samples 23916.779296875\n",
      "Epoch number 123. MSE distance between random real and fake samples 0.36219948530197144\n",
      "Epoch number 125. my distance between random real and fake samples 26204.033203125\n",
      "Epoch number 125. MSE distance between random real and fake samples 0.39627113938331604\n",
      "Epoch number 127. my distance between random real and fake samples 21313.5234375\n",
      "Epoch number 127. MSE distance between random real and fake samples 0.31507793068885803\n",
      "Epoch number 129. my distance between random real and fake samples 24582.1328125\n",
      "Epoch number 129. MSE distance between random real and fake samples 0.37330323457717896\n",
      "Epoch number 131. my distance between random real and fake samples 26209.396484375\n",
      "Epoch number 131. MSE distance between random real and fake samples 0.3972661793231964\n",
      "Epoch number 133. my distance between random real and fake samples 19016.470703125\n",
      "Epoch number 133. MSE distance between random real and fake samples 0.2843768000602722\n",
      "Epoch number 135. my distance between random real and fake samples 30056.498046875\n",
      "Epoch number 135. MSE distance between random real and fake samples 0.43100807070732117\n",
      "Epoch number 137. my distance between random real and fake samples 27867.845703125\n",
      "Epoch number 137. MSE distance between random real and fake samples 0.4307629466056824\n",
      "Epoch number 139. my distance between random real and fake samples 27503.337890625\n",
      "Epoch number 139. MSE distance between random real and fake samples 0.410795658826828\n",
      "Epoch number 141. my distance between random real and fake samples 19749.841796875\n",
      "Epoch number 141. MSE distance between random real and fake samples 0.2941274642944336\n",
      "Epoch number 143. my distance between random real and fake samples 28477.900390625\n",
      "Epoch number 143. MSE distance between random real and fake samples 0.4225744307041168\n",
      "Epoch number 145. my distance between random real and fake samples 15669.1220703125\n",
      "Epoch number 145. MSE distance between random real and fake samples 0.24124661087989807\n",
      "Epoch number 147. my distance between random real and fake samples 22068.47265625\n",
      "Epoch number 147. MSE distance between random real and fake samples 0.32555389404296875\n",
      "Epoch number 149. my distance between random real and fake samples 19159.78515625\n",
      "Epoch number 149. MSE distance between random real and fake samples 0.2916724979877472\n",
      "Epoch number 151. my distance between random real and fake samples 25441.212890625\n",
      "Epoch number 151. MSE distance between random real and fake samples 0.3758847415447235\n",
      "Epoch number 153. my distance between random real and fake samples 23358.05859375\n",
      "Epoch number 153. MSE distance between random real and fake samples 0.36079350113868713\n",
      "Epoch number 155. my distance between random real and fake samples 21468.666015625\n",
      "Epoch number 155. MSE distance between random real and fake samples 0.3302270472049713\n",
      "Epoch number 157. my distance between random real and fake samples 23865.5546875\n",
      "Epoch number 157. MSE distance between random real and fake samples 0.36348772048950195\n",
      "Epoch number 159. my distance between random real and fake samples 23714.705078125\n",
      "Epoch number 159. MSE distance between random real and fake samples 0.351004421710968\n",
      "Epoch number 161. my distance between random real and fake samples 20731.576171875\n",
      "Epoch number 161. MSE distance between random real and fake samples 0.31758183240890503\n",
      "Epoch number 163. my distance between random real and fake samples 29343.74609375\n",
      "Epoch number 163. MSE distance between random real and fake samples 0.44646772742271423\n",
      "Epoch number 165. my distance between random real and fake samples 25541.27734375\n",
      "Epoch number 165. MSE distance between random real and fake samples 0.39054107666015625\n",
      "Epoch number 167. my distance between random real and fake samples 25617.640625\n",
      "Epoch number 167. MSE distance between random real and fake samples 0.40050119161605835\n",
      "Epoch number 169. my distance between random real and fake samples 21851.22265625\n",
      "Epoch number 169. MSE distance between random real and fake samples 0.3268153667449951\n",
      "Epoch number 171. my distance between random real and fake samples 23901.9296875\n",
      "Epoch number 171. MSE distance between random real and fake samples 0.35960060358047485\n",
      "Epoch number 173. my distance between random real and fake samples 25553.611328125\n",
      "Epoch number 173. MSE distance between random real and fake samples 0.3878312408924103\n",
      "Epoch number 175. my distance between random real and fake samples 26054.00390625\n",
      "Epoch number 175. MSE distance between random real and fake samples 0.3817468583583832\n",
      "Epoch number 177. my distance between random real and fake samples 19837.97265625\n",
      "Epoch number 177. MSE distance between random real and fake samples 0.2974499464035034\n",
      "Epoch number 179. my distance between random real and fake samples 29129.58203125\n",
      "Epoch number 179. MSE distance between random real and fake samples 0.44324979186058044\n",
      "Epoch number 181. my distance between random real and fake samples 23079.16015625\n",
      "Epoch number 181. MSE distance between random real and fake samples 0.3529480993747711\n",
      "Epoch number 183. my distance between random real and fake samples 23652.16015625\n",
      "Epoch number 183. MSE distance between random real and fake samples 0.36579036712646484\n",
      "Epoch number 185. my distance between random real and fake samples 29956.701171875\n",
      "Epoch number 185. MSE distance between random real and fake samples 0.4633232057094574\n",
      "Epoch number 187. my distance between random real and fake samples 27752.5546875\n",
      "Epoch number 187. MSE distance between random real and fake samples 0.42294952273368835\n",
      "Epoch number 189. my distance between random real and fake samples 20812.77734375\n",
      "Epoch number 189. MSE distance between random real and fake samples 0.3208451271057129\n",
      "Epoch number 191. my distance between random real and fake samples 29629.40625\n",
      "Epoch number 191. MSE distance between random real and fake samples 0.44579043984413147\n",
      "Epoch number 193. my distance between random real and fake samples 27837.27734375\n",
      "Epoch number 193. MSE distance between random real and fake samples 0.4258987009525299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 195. my distance between random real and fake samples 18432.5390625\n",
      "Epoch number 195. MSE distance between random real and fake samples 0.2825202941894531\n",
      "Epoch number 197. my distance between random real and fake samples 20937.763671875\n",
      "Epoch number 197. MSE distance between random real and fake samples 0.315379798412323\n",
      "Epoch number 199. my distance between random real and fake samples 26144.24609375\n",
      "Epoch number 199. MSE distance between random real and fake samples 0.386090487241745\n",
      "Epoch number 201. my distance between random real and fake samples 19612.2265625\n",
      "Epoch number 201. MSE distance between random real and fake samples 0.2961622178554535\n",
      "Epoch number 203. my distance between random real and fake samples 27350.72265625\n",
      "Epoch number 203. MSE distance between random real and fake samples 0.4189007878303528\n",
      "Epoch number 205. my distance between random real and fake samples 36302.46484375\n",
      "Epoch number 205. MSE distance between random real and fake samples 0.53180330991745\n",
      "Epoch number 207. my distance between random real and fake samples 26996.0703125\n",
      "Epoch number 207. MSE distance between random real and fake samples 0.4079376459121704\n",
      "Epoch number 209. my distance between random real and fake samples 23576.58203125\n",
      "Epoch number 209. MSE distance between random real and fake samples 0.3498147130012512\n",
      "Epoch number 211. my distance between random real and fake samples 25972.771484375\n",
      "Epoch number 211. MSE distance between random real and fake samples 0.38212230801582336\n",
      "Epoch number 213. my distance between random real and fake samples 19177.125\n",
      "Epoch number 213. MSE distance between random real and fake samples 0.2954029440879822\n",
      "Epoch number 215. my distance between random real and fake samples 24040.35546875\n",
      "Epoch number 215. MSE distance between random real and fake samples 0.3603193759918213\n",
      "Epoch number 217. my distance between random real and fake samples 22144.814453125\n",
      "Epoch number 217. MSE distance between random real and fake samples 0.33425915241241455\n",
      "Epoch number 219. my distance between random real and fake samples 23266.453125\n",
      "Epoch number 219. MSE distance between random real and fake samples 0.35078251361846924\n",
      "Epoch number 221. my distance between random real and fake samples 23247.349609375\n",
      "Epoch number 221. MSE distance between random real and fake samples 0.3515464663505554\n",
      "Epoch number 223. my distance between random real and fake samples 25840.125\n",
      "Epoch number 223. MSE distance between random real and fake samples 0.37684178352355957\n",
      "Epoch number 225. my distance between random real and fake samples 22682.2265625\n",
      "Epoch number 225. MSE distance between random real and fake samples 0.3473796546459198\n",
      "Epoch number 227. my distance between random real and fake samples 26861.931640625\n",
      "Epoch number 227. MSE distance between random real and fake samples 0.3996785283088684\n",
      "Epoch number 229. my distance between random real and fake samples 21925.5859375\n",
      "Epoch number 229. MSE distance between random real and fake samples 0.32956787943840027\n",
      "Epoch number 231. my distance between random real and fake samples 22354.798828125\n",
      "Epoch number 231. MSE distance between random real and fake samples 0.33739617466926575\n",
      "Epoch number 233. my distance between random real and fake samples 20699.07421875\n",
      "Epoch number 233. MSE distance between random real and fake samples 0.31537488102912903\n",
      "Epoch number 235. my distance between random real and fake samples 31876.0078125\n",
      "Epoch number 235. MSE distance between random real and fake samples 0.48751723766326904\n",
      "Epoch number 237. my distance between random real and fake samples 34180.25\n",
      "Epoch number 237. MSE distance between random real and fake samples 0.5060368776321411\n",
      "Epoch number 239. my distance between random real and fake samples 29730.060546875\n",
      "Epoch number 239. MSE distance between random real and fake samples 0.44098663330078125\n",
      "Epoch number 241. my distance between random real and fake samples 20040.064453125\n",
      "Epoch number 241. MSE distance between random real and fake samples 0.29315185546875\n",
      "Epoch number 243. my distance between random real and fake samples 22523.28515625\n",
      "Epoch number 243. MSE distance between random real and fake samples 0.34038621187210083\n",
      "Epoch number 245. my distance between random real and fake samples 22206.357421875\n",
      "Epoch number 245. MSE distance between random real and fake samples 0.32946157455444336\n",
      "Epoch number 247. my distance between random real and fake samples 32165.34765625\n",
      "Epoch number 247. MSE distance between random real and fake samples 0.4892423450946808\n",
      "Epoch number 249. my distance between random real and fake samples 25204.390625\n",
      "Epoch number 249. MSE distance between random real and fake samples 0.3786742389202118\n",
      "Epoch number 251. my distance between random real and fake samples 26280.71875\n",
      "Epoch number 251. MSE distance between random real and fake samples 0.39143460988998413\n",
      "Epoch number 253. my distance between random real and fake samples 18625.5234375\n",
      "Epoch number 253. MSE distance between random real and fake samples 0.28168079257011414\n",
      "Epoch number 255. my distance between random real and fake samples 29764.4453125\n",
      "Epoch number 255. MSE distance between random real and fake samples 0.4492188096046448\n",
      "Epoch number 257. my distance between random real and fake samples 22547.19140625\n",
      "Epoch number 257. MSE distance between random real and fake samples 0.33748945593833923\n",
      "Epoch number 259. my distance between random real and fake samples 23930.99609375\n",
      "Epoch number 259. MSE distance between random real and fake samples 0.354338139295578\n",
      "Epoch number 261. my distance between random real and fake samples 19146.34375\n",
      "Epoch number 261. MSE distance between random real and fake samples 0.2877127230167389\n",
      "Epoch number 263. my distance between random real and fake samples 24508.11328125\n",
      "Epoch number 263. MSE distance between random real and fake samples 0.3764212727546692\n",
      "Epoch number 265. my distance between random real and fake samples 24097.572265625\n",
      "Epoch number 265. MSE distance between random real and fake samples 0.3693888783454895\n",
      "Epoch number 267. my distance between random real and fake samples 21298.11328125\n",
      "Epoch number 267. MSE distance between random real and fake samples 0.32668012380599976\n",
      "Epoch number 269. my distance between random real and fake samples 28760.1796875\n",
      "Epoch number 269. MSE distance between random real and fake samples 0.4383470416069031\n",
      "Epoch number 271. my distance between random real and fake samples 25559.30078125\n",
      "Epoch number 271. MSE distance between random real and fake samples 0.3852567970752716\n",
      "Epoch number 273. my distance between random real and fake samples 22587.65234375\n",
      "Epoch number 273. MSE distance between random real and fake samples 0.3324242830276489\n",
      "Epoch number 275. my distance between random real and fake samples 16946.169921875\n",
      "Epoch number 275. MSE distance between random real and fake samples 0.2615453898906708\n",
      "Epoch number 277. my distance between random real and fake samples 23382.23828125\n",
      "Epoch number 277. MSE distance between random real and fake samples 0.35157662630081177\n",
      "Epoch number 279. my distance between random real and fake samples 31369.775390625\n",
      "Epoch number 279. MSE distance between random real and fake samples 0.4470950663089752\n",
      "Epoch number 281. my distance between random real and fake samples 24497.3203125\n",
      "Epoch number 281. MSE distance between random real and fake samples 0.3670576512813568\n",
      "Epoch number 283. my distance between random real and fake samples 25347.845703125\n",
      "Epoch number 283. MSE distance between random real and fake samples 0.37431013584136963\n",
      "Epoch number 285. my distance between random real and fake samples 22842.34765625\n",
      "Epoch number 285. MSE distance between random real and fake samples 0.33066844940185547\n",
      "Epoch number 287. my distance between random real and fake samples 21428.78125\n",
      "Epoch number 287. MSE distance between random real and fake samples 0.32805904746055603\n",
      "Epoch number 289. my distance between random real and fake samples 21759.1875\n",
      "Epoch number 289. MSE distance between random real and fake samples 0.3254375159740448\n",
      "Epoch number 291. my distance between random real and fake samples 17776.677734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 291. MSE distance between random real and fake samples 0.2762313187122345\n",
      "Epoch number 293. my distance between random real and fake samples 22676.5703125\n",
      "Epoch number 293. MSE distance between random real and fake samples 0.33846592903137207\n",
      "Epoch number 295. my distance between random real and fake samples 27362.423828125\n",
      "Epoch number 295. MSE distance between random real and fake samples 0.411761999130249\n",
      "Epoch number 297. my distance between random real and fake samples 24244.328125\n",
      "Epoch number 297. MSE distance between random real and fake samples 0.3624785244464874\n",
      "Epoch number 299. my distance between random real and fake samples 25390.75\n",
      "Epoch number 299. MSE distance between random real and fake samples 0.36495715379714966\n"
     ]
    }
   ],
   "source": [
    "train_GAN(netD_augm, netG_augm, augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1436913 226310\n",
      "4 2788828 348971\n",
      "3 2627194 261197\n",
      "2 849886 107557\n",
      "1 45757 56174\n",
      "0 14635662 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "\n",
    "fake_tr = netG_tr(noisev)\n",
    "fake_aug = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor(1462331, device='cuda:0') 226310\n",
      "4 tensor(2819034, device='cuda:0') 348971\n",
      "3 tensor(2621625, device='cuda:0') 261197\n",
      "2 tensor(837891, device='cuda:0') 107557\n",
      "1 tensor(44331, device='cuda:0') 56174\n",
      "0 tensor(13432479, device='cuda:0') 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake_tr.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake_tr.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake_tr.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake_tr.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake_tr.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake_tr.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor(1436910, device='cuda:0') 226310\n",
      "4 tensor(2791855, device='cuda:0') 348971\n",
      "3 tensor(2625343, device='cuda:0') 261197\n",
      "2 tensor(848793, device='cuda:0') 107557\n",
      "1 tensor(45754, device='cuda:0') 56174\n",
      "0 tensor(13430978, device='cuda:0') 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake_aug.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake_aug.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake_aug.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake_aug.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake_aug.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake_aug.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_orig, vr_1 = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed,  transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter \n",
    "import matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(train.cpu().numpy(), 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(aug_train, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ix = np.random.randint(0, fake.shape[0], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake = fake[rand_ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = np.append(aug_train.cpu().numpy(), adding_fake, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = torch.Tensor(aug_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 300\n",
    "gen_iterations = 0\n",
    "eval_losses = []\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < steps_per_epoch:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter*5:\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "#             X, _ = data_iter.next()\n",
    "            X = get_random_batch(aug_train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             print(X >= 0.5)\n",
    "# #             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             real + fake * (real == 0).float()\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            fake.requires_grad = False\n",
    "#             print(real.shape, fake.shape)\n",
    "    \n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "#             print('real', real[:10, :20])\n",
    "#             print('fake', fake[:10, :20])\n",
    "#             print(real.type(), fake.type())\n",
    "#             print(fake)\n",
    "            out = netD(real, fake)\n",
    "            \n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "#             print(out.shape)\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        g_iter = g_iter\n",
    "        j = 0\n",
    "        while j < g_iter*5:\n",
    "            j += 1\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = get_random_batch(aug_train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            \n",
    "            # update generator\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            noisev = Variable(noise)\n",
    "            \n",
    "            fake = netG(noisev)\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake.requires_grad = False\n",
    "#             fake = Variable(netG(noisev)).data\n",
    "#             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "#             fake.requires_grad = True\n",
    "            \n",
    "            out = netD(real, fake)\n",
    "            outputG = torch.mean(out) + lamba * out.norm()\n",
    "            stdG = torch.std(out)\n",
    "            outputG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "#             print('output_D', outputD.item(), gen_iterations)\n",
    "#             print('output_G', outputG.item(), gen_iterations)\n",
    "#             print('std_D', stdD.item(), gen_iterations)\n",
    "#             print('std_G', stdG.item(), gen_iterations)\n",
    "            torch.save(netG.state_dict(), './netG-1m')\n",
    "            torch.save(netD.state_dict(), './netD-1m')\n",
    "            # evaluation\n",
    "            if gen_iterations % 100 == 0: # todo- to change\n",
    "#                 gen.eval()\n",
    "#                 z_vector_eval = make_some_noise(128)\n",
    "#                 fake_rows_eval = gen(z_vector_eval)\n",
    "#                 real_rows_eval = get_random_batch(train, 128)\n",
    "        #         print(fake_rows[0][:10]) enable to see some results\n",
    "#                 fake = Variable(netG(noisev).data).round()\n",
    "#                 fake = ((real != 0) & (fake != 0))\n",
    "#                 print(fake)\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                eval_losses.append(eval_loss)\n",
    "                print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = np.append(aug, adding_fake, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train > 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(train > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[0,:] > 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.Tensor(tr.copy()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback_mask = (train > 3).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_feedback_mask = ((train < 4).to(device).float() * (1 - zero_mask)).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((positive_feedback_mask + negative_feedback_mask) != zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(negative_feedback_mask.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class denoising_autoencoder(nn.Module):\n",
    "#     def __init__(self, n_users, input_size, z=256):\n",
    "#         ''' \n",
    "#         mimic the network architecture from the paper\n",
    "#         '''\n",
    "#         super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "#         self.V = torch.FloatTensor(n_users, z).to(device)\n",
    "#         self.b = torch.FloatTensor(n_users, 1).to(device)\n",
    "#         self.b_shtrix = torch.FloatTensor(n_users, 1).to(device)\n",
    "    \n",
    "#         self.encoder = nn.Linear(input_size, z)\n",
    "#         self.decoder = nn.Linear(z, input_size)\n",
    "        \n",
    "#         torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    "#         torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    " \n",
    "#     def forward(self, x, i):\n",
    "#         z = self.encoder(x)\n",
    "#         z = z + self.V[i, :] + self.b[i, :] \n",
    "#         z = torch.nn.functional.relu(z)\n",
    "#         x = self.decoder(z)\n",
    "#         x = x + self.b_shtrix[i, :]\n",
    "#         return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# # class denoising_autoencoder(nn.Module):\n",
    "# #     def __init__(self, n_users, input_size, z=256):\n",
    "# #         super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "# # #         torch.nn.init.xavier_uniform(self.V)\n",
    "# # #         torch.nn.init.xavier_uniform(self.b.weight)\n",
    "# # #         torch.nn.init.xavier_uniform(self.b_shtrix.weight)\n",
    "\n",
    "# #         self.encoder=nn.Sequential(\n",
    "# #                       nn.Linear(input_size, 1024),\n",
    "# #                       nn.Dropout(0.4),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(1024,512),\n",
    "# #                       nn.Dropout(0.6),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(512, z),\n",
    "# # #                       nn.Sigmoid()\n",
    "# #                       )\n",
    "\n",
    "# #         self.decoder=nn.Sequential(\n",
    "# #                       nn.Linear(z, 512),\n",
    "# #                       nn.Dropout(0.4),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(512, 1024),\n",
    "# #                       nn.Dropout(0.6),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(1024, input_size),\n",
    "# # #                       nn.Sigmoid(),\n",
    "# #                       )\n",
    "        \n",
    "# #         self.init_weights()\n",
    "    \n",
    "# #     def init_weights(self):\n",
    "\n",
    "# #         for l, ll in zip(self.encoder, self.decoder):\n",
    "# #             if type(l) == torch.nn.Linear:\n",
    "# #                 torch.nn.init.xavier_uniform_(l.weight)\n",
    "# #             if type(ll) == torch.nn.Linear:\n",
    "# #                 torch.nn.init.xavier_uniform_(ll.weight)\n",
    "\n",
    "# #     def forward(self, x, i):\n",
    "# #         z = self.encoder(x)\n",
    "# #         x = self.decoder(z)\n",
    "    \n",
    "# #         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = denoising_autoencoder(input_size=train[0,:].shape[0], n_users=train.shape[0]).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(orig_mat, corrupted_mat, batch_size = 64):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(orig_mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = orig_mat[rand_rows].clone()\n",
    "    corrupted = corrupted_mat[rand_rows].clone()\n",
    "\n",
    "    return orig, corrupted, rand_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_masked_batch(mat, batch_size = 32, p=0.5):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = mat[rand_rows].clone()\n",
    "    corrupted = mat[rand_rows].clone()\n",
    "    mask_arr = torch.FloatTensor((np.random.rand(orig.shape[0], orig.shape[1]) > p)).to(device)\n",
    "\n",
    "    return orig, corrupted*mask_arr, rand_rows\n",
    "\n",
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(orig.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(masked.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.4\n",
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)).to(device)\n",
    "y = negative_feedback_mask\n",
    "X = negative_feedback_mask*mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(X.cpu().numpy()), get_sparsity(y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losslist = []\n",
    "model.train()\n",
    "def train_den_ae(mat,epochs= 400,steps_per_epoch = 300, _unsqueeze=True):\n",
    "    epochloss = 0\n",
    "    running_loss = 0\n",
    "    steps_per_epoch = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i in range(steps_per_epoch):\n",
    "            orig, masked, idxs = get_random_batch(y, X, batch_size=batch_size)\n",
    "\n",
    "            if _unsqueeze:\n",
    "                masked = masked.unsqueeze(2)\n",
    "            output = model(masked, idxs)\n",
    "            loss = criterion(output, orig)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i%20 == 0:\n",
    "                running_loss += loss.item()\n",
    "                epochloss += loss.item()\n",
    "                losslist.append(loss.item())\n",
    "                running_loss=0\n",
    "                print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "                \n",
    "train_den_ae(negative_feedback_mask, _unsqueeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked[0] >0.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output[0] >0.2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400](orig.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round()[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class conv_denoising_autoencoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, inSize,fSize = 32, nz=24):  #sigma is the corruption level\n",
    "#         super(conv_denoising_autoencoder, self).__init__()\n",
    "#         #define layers here\n",
    "\n",
    "#         self.inp_size = inSize\n",
    "#         self.nz = nz\n",
    "#         self.fSize = 32\n",
    "# #         self.imSize = imSize\n",
    "# #         self.sigma = sigma\n",
    "# #         self.multimodalZ = multimodalZ\n",
    "\n",
    "# #         inSize = imSize / ( 2 ** 4)\n",
    "# #         self.inSize = inSize\n",
    "    \n",
    "#         self.enc1 = nn.Conv1d(self.inp_size, fSize, 5, stride=2, padding=2)\n",
    "#         self.enc2 = nn.Conv1d(fSize, fSize * 2, 5, stride=2, padding=2)\n",
    "#         self.enc3 = nn.Conv1d(fSize * 2, fSize * 4, 5, stride=2, padding=2)\n",
    "#         self.enc4 = nn.Conv1d(fSize * 4, fSize * 8, 5, stride=2, padding=2)\n",
    "#         self.enc5 = nn.Linear(fSize * 8, self.nz)\n",
    "\n",
    "#         self.dec1 = nn.Linear(self.nz, fSize * 8)\n",
    "#         self.dec2 = nn.ConvTranspose1d(fSize * 8, fSize * 4, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.dec3 = nn.ConvTranspose1d(fSize * 4, fSize * 2, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.dec4 = nn.ConvTranspose1d(fSize * 2, fSize, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.dec5 = nn.ConvTranspose1d(fSize, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.last_dec = nn.Linear(32*16, self.inp_size)\n",
    "        \n",
    "#         self.useCUDA = torch.cuda.is_available()\n",
    "\n",
    "# #     def norm_prior(self, noSamples=25):\n",
    "# #         z = torch.randn(noSamples, self.nz)\n",
    "# #         return z\n",
    "\n",
    "# #     def multi_prior(self, noSamples=25, mode=None):\n",
    "# #         #make a 2D sqrt(nz)-by-sqrt(nz) grid of gaussians\n",
    "# #         num = np.sqrt(self.nz) #no of modes in x and y\n",
    "# #         STD = 1.0\n",
    "# #         modes = np.arange(-num,num)\n",
    "# #         p = np.random.uniform(0, num,(noSamples*2))\n",
    "\n",
    "# #         if mode is None:\n",
    "# #             mu = modes[np.floor(2 * p).astype(int)]\n",
    "# #         else:\n",
    "# #             mu = modes[np.ones((noSamples, 2), dtype=int) * int(mode)]\n",
    "\n",
    "# #         z = torch.Tensor(mu).view(-1,2) + STD * torch.randn(noSamples, 2)\n",
    "# #         return z\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         self.batch_size = x.shape[0]\n",
    "#         #define the encoder here return mu(x) and sigma(x)\n",
    "#         x = F.relu(self.enc1(x))\n",
    "#         x = F.relu(self.enc2(x))\n",
    "#         x = F.relu(self.enc3(x))\n",
    "#         x = F.relu(self.enc4(x))\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.enc5(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# #     def corrupt(self, x):\n",
    "# #         noise = self.sigma * Variable(torch.randn(x.size())).type_as(x)\n",
    "# #         return x + noise\n",
    "\n",
    "# #     def sample_z(self, noSamples=25, mode=None):\n",
    "# #         if not self.multimodalZ:\n",
    "# #             z = self.norm_prior(noSamples=noSamples)\n",
    "# #         else:\n",
    "# #             z = self.multi_prior(noSamples=noSamples, mode=mode)\n",
    "# #         if self.useCUDA:\n",
    "# #             return Variable(z.cuda())\n",
    "# #         else:\n",
    "# #             return Variable(z)\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         #define the decoder here\n",
    "#         z = F.relu(self.dec1(z))\n",
    "#         z = z.unsqueeze(2)\n",
    "# #         print(z.shape)\n",
    "# #         z = z.view(z.size(0), -1, self.inp_size)\n",
    "#         z = F.relu(self.dec2(z))\n",
    "#         z = F.relu(self.dec3(z))\n",
    "#         z = F.relu(self.dec4(z))\n",
    "#         z = F.sigmoid(self.dec5(z))\n",
    "# #         print(z.shape)\n",
    "# #         z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "#         z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "    \n",
    "#         return z\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # the outputs needed for training\n",
    "# #         x_corr = self.corrupt(x)\n",
    "#         z = self.encode(x)\n",
    "#         return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_denoising_autoencoder(train[0,:].shape[0]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.01, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "train_den_ae(negative_feedback_mask, steps_per_epoch=steps_per_epoch, _unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output[7]).round() >0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output.shape[0]):\n",
    "    print(((output[i]).round() >0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output).round() >= 1 ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output).round()[0][2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gtshs2/Collaborative-Denoising-Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tr + vr) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.000001))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(512, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.000001))(enc)\n",
    "    lat_space = Dropout(0.5, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.000001))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.000001))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.4)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.4)).to(device)\n",
    "y = negative_feedback_mask.cpu().numpy()\n",
    "X = (negative_feedback_mask*mask_arr).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(y), get_sparsity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoEncoder(X)\n",
    "\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x=X, y=y,\n",
    "                  epochs=300,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict new Matrix Interactions, set score zero on visualized games\n",
    "# new_matrix = model.predict(X) * (X == 0)\n",
    "new_matrix = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting the reconstructed matrix back to a Pandas dataframe\n",
    "# new_users_items_matrix_df  = pd.DataFrame(new_matrix, \n",
    "#                                           columns = users_items_matrix_df.columns, \n",
    "#                                           index   = users_items_matrix_df.index)\n",
    "# new_users_items_matrix_df.head()\n",
    "\n",
    "new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix>0.4).sum() # predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X>0.5).sum() # trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y > 0.99).sum() # actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (new_matrix>0.4) == 1).sum()/(new_matrix>0.4).sum() # accuracy on actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (new_matrix>0.5)).sum()/((new_matrix>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (new_matrix>0.5)).sum()/((new_matrix>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y * (((new_matrix>0.8)  * (X<0.5)))) == 1).sum()/(((new_matrix>0.8)  * (X<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix_2 = model.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix_2>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y* (((new_matrix_2>0.5)) == 1))).sum()/(((new_matrix_2>0.5)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=1)\n",
    "new_matrix[:20, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:20, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix_2>0.3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix_2 > 0.3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((new_matrix_2 > threshold).astype(float) * (train > 0).cpu().numpy().astype(float)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)).sum() # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 3 # delta could be 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((train > 0).cpu().numpy().astype(float) > 0) *(new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)).sum() # checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = train.cpu().numpy().astype(float) + delta * (new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_probs = [(train == 1).sum().item()/((train > 0) & (train < 4)).sum().item(), (train == 2).sum().item()/(((train > 0) & (train < 4))).sum().item(), (train == 3).sum().item()/((train > 0) & (train < 4)).sum().item()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train == 1).sum().item()/((train > 0) & (train < 4)).sum().item() + (train == 2).sum().item()/(((train > 0) & (train < 4))).sum().item() + (train == 3).sum().item()/((train > 0) & (train < 4)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(1,4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del augmented_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(np.arange(1, 4), train.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train > 0).cpu().numpy().astype(float)  * (new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)*np.random.choice(np.arange(1, 4), train.shape, p=p_probs)).sum() # should be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = train.cpu().numpy().astype(float) + (new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)*np.random.choice(np.arange(1, 4), train.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(aug_train > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(aug_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
