{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 4.924822568893433 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Pytorch model doesn't converge - to do - check #################\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.000001))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(512, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.000001))(enc)\n",
    "    lat_space = Dropout(0.5, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.000001))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.000001))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0)\n",
    "positive_feedback_mask = (train > 3)\n",
    "negative_feedback_mask = ((train < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback_mask + negative_feedback_mask != zero_mask).all()\n",
    "assert (positive_feedback_mask + negative_feedback_mask == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback_mask), get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 - get_sparsity(zero_mask), get_sparsity(positive_feedback_mask) + get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.4\n",
    "mask_arr = (np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)\n",
    "y = negative_feedback_mask\n",
    "X = negative_feedback_mask*mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(X), get_sparsity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoEncoder(X)\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tr = np.load('predicted_tr.npy')\n",
    "augmented_train = np.load('augmented_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = model.fit(x=X, y=y,\n",
    "#                   epochs=300,\n",
    "#                   batch_size=128,\n",
    "#                   shuffle=True,\n",
    "#                   validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "predicted_tr = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_tr > 0.4).sum(), (y == 1).sum() # predicted vs real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X>0.5).sum() # trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (predicted_tr>0.4)).sum()/(predicted_tr>0.4).sum() # accuracy on actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (predicted_tr>0.5)).sum()/((predicted_tr>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((predicted_tr>0.5)  * (X<0.5)).sum() # predicted values which were not in the train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y * (((predicted_tr>0.5)  * (X<0.5)))) == 1).sum()/(((predicted_tr>0.5)  * (X<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = model.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(augmented_train>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y* (augmented_train>0.8)).sum()/(((augmented_train>0.8)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((augmented_train > threshold) * (tr==0)).sum() # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_probs = [(tr == 1).sum()/((tr > 0) & (tr < 4)).sum(), (tr == 2).sum()/(((tr > 0) & (tr < 4))).sum(), (tr == 3).sum()/((tr > 0) & (tr < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = tr + (predicted_tr > threshold) * (tr == 0) * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr), get_sparsity(augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.isin(tr, augmented_train)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tr == 0) * (augmented_train > 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predicted_tr', predicted_tr)\n",
    "np.save('augmented_train', augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "#         filt = (torch.abs((real > 0.3).float() * fake - real))/real.shape[0]\n",
    "#         filt = torch.abs((xr != 0).float().cuda() * xf.cuda() - xr.cuda())/xr.shape[0]\n",
    "#         filt = torch.abs(xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "#         filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())\n",
    "\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "#         x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "#         print(x.shape, xr.shape, xf.shape, d_my(xr, xf))\n",
    "#         print(x)\n",
    "#         print(xr)\n",
    "#         print(xf)\n",
    "#         print (x)\n",
    "#         print(d_my(xr, xf))\n",
    "#         print(x * (x <= d_my(xr, xf)).float() + d_my(xr, xf) * (x > d_my(xr, xf)).float())\n",
    "#         if x > d_my(xr, xf):\n",
    "#             x = d_my(xr, xf)\n",
    "        x = x * (x <= d_my(xr, xf)).float() + d_my(xr, xf) * (x > d_my(xr, xf)).float()\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz, 1024), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.5),\n",
    "                                torch.nn.Linear(1024, 1024),\n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "                                torch.nn.ReLU(), \n",
    "                                nn.Dropout(0.6),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#                                  nn.Linear(nz,1024),\n",
    "# #                                  nn.Dropout(0.3)\n",
    "#                                  nn.ReLU(),\n",
    "#                                  nn.Linear(1024,2048),\n",
    "#                                  nn.Sigmoid(),\n",
    "#                                  nn.Dropout(0.3),\n",
    "#                                  nn.Linear(2048,features_length),\n",
    "# #                                  nn.Sigmoid()\n",
    "#                                  nn.Dropout(0.5)\n",
    "#                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.netGen(x)\n",
    "        return F.dropout(x, 0.7)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=64):\n",
    "    '''\n",
    "    returns random rows of size batch_size\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))\n",
    "augmented_train = torch.autograd.Variable(torch.Tensor(augmented_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 4.134020185630605)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train.cpu().numpy()), get_sparsity(augmented_train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = get_random_batch(train)\n",
    "# xy = get_random_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_my(xx, xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(torch.abs(torch.abs(xx != 0).float()*xy - xy), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx > xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g): # custom loss -todo\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r), 1)/x_r.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN(netD, netG, train_mat, steps_per_epoch = 300, epochs = 300):\n",
    "    gen_iterations = 0\n",
    "    eval_losses = []\n",
    "    d_iter = 5\n",
    "    g_iter = 1\n",
    "    for epoch in range(epochs):\n",
    "        i = 0\n",
    "        while i < steps_per_epoch:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in netD.parameters(): # reset requires_grad\n",
    "                p.requires_grad = True # they are set to False below in netG update\n",
    "            d_iter = d_iter\n",
    "            j = 0\n",
    "            while j < d_iter*5:\n",
    "                j += 1\n",
    "                # load real data\n",
    "                i += 1\n",
    "    #             X, _ = data_iter.next()\n",
    "                X = get_random_batch(train, batch_size=batch_size).to(device)\n",
    "    #             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "    #             print(X >= 0.5)\n",
    "    # #             X = X.view(X.size(0), -1)\n",
    "    #             X = (X >= 0.5).float()\n",
    "#                 if cuda: \n",
    "#                     X = X.cuda()\n",
    "#                 real = Variable(X)\n",
    "#                 real = X.clone()\n",
    "#                 real = X + fake * (X == 0).float()\n",
    "                # generate fake data\n",
    "                noise = torch.randn(batch_size, nz).to(device)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "                with torch.no_grad():\n",
    "                    noisev = Variable(noise) # totally freeze netG\n",
    "                    \n",
    "#                 fake = netG(noisev).to(device)\n",
    "                fake = Variable(netG(noisev).data).to(device)\n",
    "#                 fake.requires_grad = False\n",
    "    #             print(real[0,:20], fake[0,:20])\n",
    "#                 real + fake * (real == 0).float()\n",
    "#                 fake = fake * Variable(real != 0).float().cuda()\n",
    "\n",
    "    #             real + fake * (real == 0).float()\n",
    "    #             print(real[0,:20], fake[0,:20])\n",
    "#                 fake.requires_grad = False\n",
    "    #             print(real.shape, fake.shape)\n",
    "\n",
    "                # compute gradient, take step\n",
    "                netD.zero_grad()\n",
    "    #             print('real', real[:10, :20])\n",
    "    #             print('fake', fake[:10, :20])\n",
    "    #             print(real.type(), fake.type())\n",
    "    #             print(fake)\n",
    "#                 print(real)\n",
    "#                 real = X + fake * (X == 0).float()\n",
    "#                 print(real[0])\n",
    "#                 print(fake[0])\n",
    "                real = Variable(X)\n",
    "                out = netD(real, fake)\n",
    "\n",
    "                outputD = torch.mean(out) + lamba * out.norm()\n",
    "                stdD = torch.std(out)\n",
    "                outputD.backward(mone)\n",
    "                optimizerD.step()\n",
    "    #             print(out.shape)\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "            g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "#                 for p in netG.parameters():\n",
    "#                     p.requires_grad = True # to avoid computation\n",
    "                    \n",
    "                netG.zero_grad()\n",
    "\n",
    "                # load real data\n",
    "                i += 1\n",
    "                X = get_random_batch(train, batch_size=batch_size).to(device)\n",
    "    #             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "    #             X = X.view(X.size(0), -1)\n",
    "    #             X = (X >= 0.5).float()\n",
    "#                 if cuda: \n",
    "#                     X = X.cuda()\n",
    "#                 real = Variable(X)\n",
    "#                 real = X.clone()\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz).to(device)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "                fake = netG(noisev).to(device)\n",
    "#                 noisev = Variable(noise)\n",
    "\n",
    "#                 fake = Variable(netG(noisev).data)\n",
    "\n",
    "#                 real = real + fake * (real == 0).float()\n",
    "#                 fake = fake * Variable(real != 0).float().cuda()\n",
    "                \n",
    "    #             fake = fake * Variable(real != 0).float().cuda()\n",
    "#                 fake.requires_grad = False\n",
    "    #             fake = Variable(netG(noisev)).data\n",
    "    #             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "    \n",
    "#                 fake.requires_grad = True\n",
    "#                 real = X + fake * (X == 0).float()\n",
    "                real = Variable(X)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "#                 print(real.requires_grad, fake.requires_grad)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "\n",
    "                gen_iterations += 1\n",
    "\n",
    "    #             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "    #             print('output_D', outputD.item(), gen_iterations)\n",
    "    #             print('output_G', outputG.item(), gen_iterations)\n",
    "    #             print('std_D', stdD.item(), gen_iterations)\n",
    "    #             print('std_G', stdG.item(), gen_iterations)\n",
    "\n",
    "                # evaluation\n",
    "                if gen_iterations % 100 == 0: # todo- to change\n",
    "    #                 gen.eval()\n",
    "    #                 z_vector_eval = make_some_noise(128)\n",
    "    #                 fake_rows_eval = gen(z_vector_eval)\n",
    "    #                 real_rows_eval = get_random_batch(train, 128)\n",
    "            #         print(fake_rows[0][:10]) enable to see some results\n",
    "    #                 fake = Variable(netG(noisev).data).round()\n",
    "    #                 fake = ((real != 0) & (fake != 0))\n",
    "    #                 print(fake)\n",
    "                    eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                    eval_losses.append(eval_loss)\n",
    "                    print(\"######\"*40)\n",
    "                    print('Epoch number {}. my distance between random real and fake samples sum {}'.format(epoch, d_my(real, fake).sum()))\n",
    "                    print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake).mean()))\n",
    "                    print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))\n",
    "                    print(\"######\"*40)\n",
    "                    \n",
    "    return eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 128\n",
    "cuda = True\n",
    "epochs = 500 #change\n",
    "seed = 1\n",
    "nz = 16\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.6)\n",
      "    (6): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_tr = NetD().to(device)\n",
    "netG_tr = NetG().to(device)\n",
    "print(netD_tr)\n",
    "print(netG_tr)\n",
    "optimizerG = optim.RMSprop(netG_tr.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_tr.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (-1 * one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-cf13460d51f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_losses_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_GAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetD_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetG_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-560594c0a357>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[1;34m(netD, netG, train_mat, steps_per_epoch, epochs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0moutputD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlamba\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mstdD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0moutputD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                 \u001b[0moptimizerD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m#             print(out.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_losses_tr = train_GAN(netD_tr, netG_tr, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(netG.state_dict(), './netG-1m')\n",
    "# torch.save(netD.state_dict(), './netD-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=1024, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.6)\n",
       "    (6): Linear(in_features=1024, out_features=3706, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_tr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_tr(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.924827467897057"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 566145 226310\n",
      "4 1576161 348971\n",
      "3 1537743 261197\n",
      "2 1191324 107557\n",
      "1 812316 56174\n",
      "0 16357322 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see there is a significant bias towards higher ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.6)\n",
      "    (6): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_augm = NetD().to(device)\n",
    "netG_augm = NetG().to(device)\n",
    "print(netD_augm)\n",
    "print(netG_augm)\n",
    "optimizerG = optim.RMSprop(netG_augm.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_augm.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (-1 * one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())\n",
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0 226310\n",
      "4 338 348971\n",
      "3 17629 261197\n",
      "2 295849 107557\n",
      "1 1623798 56174\n",
      "0 18513349 21384031\n"
     ]
    }
   ],
   "source": [
    "# without train\n",
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 8. my distance between random real and fake samples sum 14.237488746643066\n",
      "Epoch number 8. my distance between random real and fake samples 0.11123038083314896\n",
      "Epoch number 8. MSE distance between random real and fake samples 3.736663818359375\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 16. my distance between random real and fake samples sum 14.89666748046875\n",
      "Epoch number 16. my distance between random real and fake samples 0.11638021469116211\n",
      "Epoch number 16. MSE distance between random real and fake samples 3.6654000282287598\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 24. my distance between random real and fake samples sum 14.159454345703125\n",
      "Epoch number 24. my distance between random real and fake samples 0.11062073707580566\n",
      "Epoch number 24. MSE distance between random real and fake samples 3.3332951068878174\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 33. my distance between random real and fake samples sum 13.974613189697266\n",
      "Epoch number 33. my distance between random real and fake samples 0.10917666554450989\n",
      "Epoch number 33. MSE distance between random real and fake samples 3.745903491973877\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 41. my distance between random real and fake samples sum 16.14279556274414\n",
      "Epoch number 41. my distance between random real and fake samples 0.1261155903339386\n",
      "Epoch number 41. MSE distance between random real and fake samples 2.7270102500915527\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 49. my distance between random real and fake samples sum 15.711910247802734\n",
      "Epoch number 49. my distance between random real and fake samples 0.12274929881095886\n",
      "Epoch number 49. MSE distance between random real and fake samples 3.579230785369873\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 58. my distance between random real and fake samples sum 12.779783248901367\n",
      "Epoch number 58. my distance between random real and fake samples 0.09984205663204193\n",
      "Epoch number 58. MSE distance between random real and fake samples 2.5602691173553467\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 66. my distance between random real and fake samples sum 12.12394905090332\n",
      "Epoch number 66. my distance between random real and fake samples 0.09471835196018219\n",
      "Epoch number 66. MSE distance between random real and fake samples 3.4821908473968506\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 74. my distance between random real and fake samples sum 14.1106595993042\n",
      "Epoch number 74. my distance between random real and fake samples 0.11023952811956406\n",
      "Epoch number 74. MSE distance between random real and fake samples 4.124244689941406\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 83. my distance between random real and fake samples sum 15.000091552734375\n",
      "Epoch number 83. my distance between random real and fake samples 0.1171882152557373\n",
      "Epoch number 83. MSE distance between random real and fake samples 4.922124862670898\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 91. my distance between random real and fake samples sum 16.042797088623047\n",
      "Epoch number 91. my distance between random real and fake samples 0.12533435225486755\n",
      "Epoch number 91. MSE distance between random real and fake samples 4.103261470794678\n",
      "################################################################################################################################################################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 99. my distance between random real and fake samples sum 15.171722412109375\n",
      "Epoch number 99. my distance between random real and fake samples 0.11852908134460449\n",
      "Epoch number 99. MSE distance between random real and fake samples 2.4651362895965576\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 108. my distance between random real and fake samples sum 17.04256248474121\n",
      "Epoch number 108. my distance between random real and fake samples 0.1331450194120407\n",
      "Epoch number 108. MSE distance between random real and fake samples 3.0425870418548584\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 116. my distance between random real and fake samples sum 16.20321273803711\n",
      "Epoch number 116. my distance between random real and fake samples 0.12658759951591492\n",
      "Epoch number 116. MSE distance between random real and fake samples 3.872493028640747\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 124. my distance between random real and fake samples sum 18.511228561401367\n",
      "Epoch number 124. my distance between random real and fake samples 0.14461897313594818\n",
      "Epoch number 124. MSE distance between random real and fake samples 2.9818789958953857\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 133. my distance between random real and fake samples sum 12.942033767700195\n",
      "Epoch number 133. my distance between random real and fake samples 0.10110963881015778\n",
      "Epoch number 133. MSE distance between random real and fake samples 3.4675731658935547\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 141. my distance between random real and fake samples sum 15.58129596710205\n",
      "Epoch number 141. my distance between random real and fake samples 0.12172887474298477\n",
      "Epoch number 141. MSE distance between random real and fake samples 3.124551296234131\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 149. my distance between random real and fake samples sum 14.751016616821289\n",
      "Epoch number 149. my distance between random real and fake samples 0.11524231731891632\n",
      "Epoch number 149. MSE distance between random real and fake samples 3.197676420211792\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 158. my distance between random real and fake samples sum 15.254528045654297\n",
      "Epoch number 158. my distance between random real and fake samples 0.1191760003566742\n",
      "Epoch number 158. MSE distance between random real and fake samples 4.37696647644043\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 166. my distance between random real and fake samples sum 17.126163482666016\n",
      "Epoch number 166. my distance between random real and fake samples 0.13379815220832825\n",
      "Epoch number 166. MSE distance between random real and fake samples 4.19257116317749\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 174. my distance between random real and fake samples sum 13.76868724822998\n",
      "Epoch number 174. my distance between random real and fake samples 0.10756786912679672\n",
      "Epoch number 174. MSE distance between random real and fake samples 3.8103513717651367\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 183. my distance between random real and fake samples sum 11.92171573638916\n",
      "Epoch number 183. my distance between random real and fake samples 0.09313840419054031\n",
      "Epoch number 183. MSE distance between random real and fake samples 3.758768320083618\n",
      "################################################################################################################################################################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 191. my distance between random real and fake samples sum 15.955431938171387\n",
      "Epoch number 191. my distance between random real and fake samples 0.12465181201696396\n",
      "Epoch number 191. MSE distance between random real and fake samples 4.444432258605957\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 199. my distance between random real and fake samples sum 15.857946395874023\n",
      "Epoch number 199. my distance between random real and fake samples 0.12389020621776581\n",
      "Epoch number 199. MSE distance between random real and fake samples 3.8216843605041504\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 208. my distance between random real and fake samples sum 14.243042945861816\n",
      "Epoch number 208. my distance between random real and fake samples 0.11127377301454544\n",
      "Epoch number 208. MSE distance between random real and fake samples 3.0289154052734375\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 216. my distance between random real and fake samples sum 15.609427452087402\n",
      "Epoch number 216. my distance between random real and fake samples 0.12194865196943283\n",
      "Epoch number 216. MSE distance between random real and fake samples 3.5233049392700195\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 224. my distance between random real and fake samples sum 10.407804489135742\n",
      "Epoch number 224. my distance between random real and fake samples 0.08131097257137299\n",
      "Epoch number 224. MSE distance between random real and fake samples 4.083154678344727\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 233. my distance between random real and fake samples sum 16.308320999145508\n",
      "Epoch number 233. my distance between random real and fake samples 0.12740875780582428\n",
      "Epoch number 233. MSE distance between random real and fake samples 3.700423002243042\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 241. my distance between random real and fake samples sum 14.818551063537598\n",
      "Epoch number 241. my distance between random real and fake samples 0.11576993018388748\n",
      "Epoch number 241. MSE distance between random real and fake samples 3.627958297729492\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 249. my distance between random real and fake samples sum 12.351320266723633\n",
      "Epoch number 249. my distance between random real and fake samples 0.09649468958377838\n",
      "Epoch number 249. MSE distance between random real and fake samples 3.484828472137451\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 258. my distance between random real and fake samples sum 12.747453689575195\n",
      "Epoch number 258. my distance between random real and fake samples 0.09958948194980621\n",
      "Epoch number 258. MSE distance between random real and fake samples 3.5795605182647705\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 266. my distance between random real and fake samples sum 13.451933860778809\n",
      "Epoch number 266. my distance between random real and fake samples 0.10509323328733444\n",
      "Epoch number 266. MSE distance between random real and fake samples 3.0834996700286865\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 274. my distance between random real and fake samples sum 14.704833030700684\n",
      "Epoch number 274. my distance between random real and fake samples 0.11488150805234909\n",
      "Epoch number 274. MSE distance between random real and fake samples 3.5369393825531006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 283. my distance between random real and fake samples sum 13.72924518585205\n",
      "Epoch number 283. my distance between random real and fake samples 0.10725972801446915\n",
      "Epoch number 283. MSE distance between random real and fake samples 3.363431930541992\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 291. my distance between random real and fake samples sum 14.697626113891602\n",
      "Epoch number 291. my distance between random real and fake samples 0.11482520401477814\n",
      "Epoch number 291. MSE distance between random real and fake samples 3.3600406646728516\n",
      "################################################################################################################################################################################################################################################\n",
      "################################################################################################################################################################################################################################################\n",
      "Epoch number 299. my distance between random real and fake samples sum 13.517598152160645\n",
      "Epoch number 299. my distance between random real and fake samples 0.10560623556375504\n",
      "Epoch number 299. MSE distance between random real and fake samples 2.5789642333984375\n",
      "################################################################################################################################################################################################################################################\n"
     ]
    }
   ],
   "source": [
    "eval_losses_aug = train_GAN(netD_augm, netG_augm, augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses_aug)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,\n",
       "        9., 10., 11., 12., 13.], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 871909 226310\n",
      "4 2270321 348971\n",
      "3 2134716 261197\n",
      "2 942572 107557\n",
      "1 251716 56174\n",
      "0 15891479 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=1024, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.6)\n",
       "    (6): Linear(in_features=1024, out_features=3706, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_tr.eval()\n",
    "netG_augm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "\n",
    "fake_tr = netG_tr(noisev)\n",
    "fake_aug = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-3., -2., -1., -0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "        device='cuda:0', grad_fn=<NotImplemented>),\n",
       " tensor([-1., -0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.], device='cuda:0',\n",
       "        grad_fn=<NotImplemented>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(fake_tr.round()), torch.unique(fake_aug.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tr = fake_tr.clamp(0,5).detach().cpu().numpy().round()\n",
    "fake_aug = fake_aug.clamp(0,5).detach().cpu().numpy().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 634896 226310\n",
      "4 1577801 348971\n",
      "3 1537091 261197\n",
      "2 1192444 107557\n",
      "1 812699 56174\n",
      "0 16629309 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake_tr).sum(), (5 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(4, (4 == fake_tr).sum(), (4 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(3, (3 == fake_tr).sum(), (3 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(2, (2 == fake_tr).sum(), (2 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(1, (1 == fake_tr).sum(), (1 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(0, (0 == fake_tr).sum(), (0 == (tr + vr)[:fake.shape[0], :]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 506360 226310\n",
      "4 2963175 348971\n",
      "3 2273712 261197\n",
      "2 769288 107557\n",
      "1 156109 56174\n",
      "0 15715596 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake_aug).sum(), (5 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(4, (4 == fake_aug).sum(), (4 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(3, (3 == fake_aug).sum(), (3 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(2, (2 == fake_aug).sum(), (2 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(1, (1 == fake_aug).sum(), (1 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(0, (0 == fake_aug).sum(), (0 == (tr + vr)[:fake.shape[0], :]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 25.70974489194183, 29.791692726668405)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr), get_sparsity(fake_tr), get_sparsity(fake_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_orig, vr_1 = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed,  transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from matrix_factorization.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter \n",
    "import matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ix = np.random.randint(0, fake_tr.shape[0], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake_autoenc = fake_tr[rand_ix,:]\n",
    "adding_fake_autoenc_lus_gan = fake_aug[rand_ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(adding_fake_autoenc_lus_gan[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40476"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adding_fake_autoenc == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7862"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(adding_fake_autoenc_lus_gan == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(adding_fake_autoenc_lus_gan[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adding_fake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-dff2a6caf8b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0madding_fake\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'adding_fake' is not defined"
     ]
    }
   ],
   "source": [
    "adding_fake[0,0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 3., 0., 4., 0., 3., 0., 0., 0., 2., 0., 4., 0., 4., 0.,\n",
       "       0., 0., 3., 0., 0., 3., 0., 0., 3., 0., 5., 0., 3., 0., 0., 4., 0.,\n",
       "       0., 4., 0., 0., 0., 0., 0., 0., 3., 3., 4., 0., 0., 0., 4., 0., 4.,\n",
       "       5., 0., 0., 1., 0., 4., 2., 3., 3., 4., 3., 0., 3., 0., 0., 3., 0.,\n",
       "       3., 2., 3., 0., 0., 2., 0., 4., 0., 2., 0., 0., 0., 4., 0., 4., 0.,\n",
       "       0., 0., 0., 0., 3., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4.,\n",
       "       3., 0., 3., 0., 0., 4., 0., 3., 0., 0., 0., 0., 0., 3., 5., 3., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 3., 0., 0., 4., 0., 0., 3., 0., 0., 4., 3., 0., 0.,\n",
       "       0., 0., 0., 0., 4., 0., 0., 3., 0., 3., 0., 0., 0., 0., 2., 0., 0.,\n",
       "       4., 0., 4., 2., 0., 0., 0., 4., 3., 0., 3., 4., 4., 3., 0., 0., 0.,\n",
       "       0., 4., 0., 3., 3., 0., 0., 0., 0., 0., 3., 3., 0.], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adding_fake_autoenc_lus_gan[0,0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_auto_enc = np.append(tr, adding_fake_autoenc, axis=0)\n",
    "tr_auto_enc_plus_gan = np.append(tr, adding_fake_autoenc_lus_gan, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 60\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "Train mse: 0.7187739765016317\n",
      "Test mse: 0.7761838306708981\n"
     ]
    }
   ],
   "source": [
    "iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(augmented_train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 60\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "Train mse: 0.717847971729219\n",
      "Test mse: 0.772331136931587\n"
     ]
    }
   ],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_train.cpu().numpy(), 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([50], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_train.cpu().numpy(), 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([50], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.shape, augmented_train.cpu().numpy().shape, tr_auto_enc.shape, tr_auto_enc_plus_gan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr_auto_enc), get_sparsity(tr_auto_enc_plus_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25, 40]\n",
    "\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr_auto_enc, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 60\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "Train mse: 0.5350130473722049\n",
      "Test mse: 0.7831244021517524\n"
     ]
    }
   ],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25, 40]\n",
    "\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr_auto_enc_plus_gan, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
