{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 4.7700066566467285 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Pytorch model doesn't converge - to do - check #################\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.000001))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(512, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.000001))(enc)\n",
    "    lat_space = Dropout(0.5, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.000001))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.000001))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0)\n",
    "positive_feedback_mask = (train > 3)\n",
    "negative_feedback_mask = ((train < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback_mask + negative_feedback_mask != zero_mask).all()\n",
    "assert (positive_feedback_mask + negative_feedback_mask == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback_mask), get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 - get_sparsity(zero_mask), get_sparsity(positive_feedback_mask) + get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.4\n",
    "mask_arr = (np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)\n",
    "y = negative_feedback_mask\n",
    "X = negative_feedback_mask*mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(X), get_sparsity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoEncoder(X)\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tr = np.load('predicted_tr.npy')\n",
    "augmented_train = np.load('augmented_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = model.fit(x=X, y=y,\n",
    "#                   epochs=300,\n",
    "#                   batch_size=128,\n",
    "#                   shuffle=True,\n",
    "#                   validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "predicted_tr = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_tr > 0.4).sum(), (y == 1).sum() # predicted vs real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X>0.5).sum() # trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (predicted_tr>0.4)).sum()/(predicted_tr>0.4).sum() # accuracy on actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (predicted_tr>0.5)).sum()/((predicted_tr>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((predicted_tr>0.5)  * (X<0.5)).sum() # predicted values which were not in the train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y * (((predicted_tr>0.5)  * (X<0.5)))) == 1).sum()/(((predicted_tr>0.5)  * (X<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = model.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(augmented_train>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y* (augmented_train>0.8)).sum()/(((augmented_train>0.8)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((augmented_train > threshold) * (tr==0)).sum() # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_probs = [(tr == 1).sum()/((tr > 0) & (tr < 4)).sum(), (tr == 2).sum()/(((tr > 0) & (tr < 4))).sum(), (tr == 3).sum()/((tr > 0) & (tr < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = tr + (predicted_tr > threshold) * (tr == 0) * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr), get_sparsity(augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.isin(tr, augmented_train)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tr == 0) * (augmented_train > 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('predicted_tr', predicted_tr)\n",
    "np.save('augmented_train', augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "#         filt = (torch.abs((real > 0.3).float() * fake - real))/real.shape[0]\n",
    "#         filt = torch.abs((xr != 0).float().cuda() * xf.cuda() - xr.cuda())/xr.shape[0]\n",
    "#         filt = torch.abs(xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "#         filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())\n",
    "\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "#         x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "#         print(x.shape, xr.shape, xf.shape, d_my(xr, xf))\n",
    "#         print(x)\n",
    "#         print(xr)\n",
    "#         print(xf)\n",
    "#         print (x)\n",
    "#         print(d_my(xr, xf))\n",
    "#         print(x * (x <= d_my(xr, xf)).float() + d_my(xr, xf) * (x > d_my(xr, xf)).float())\n",
    "#         if x > d_my(xr, xf):\n",
    "#             x = d_my(xr, xf)\n",
    "        x = x * (x <= d_my(xr, xf)).float() + d_my(xr, xf) * (x > d_my(xr, xf)).float()\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz, 1024), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.4),\n",
    "                                torch.nn.Linear(1024, 1024),\n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "                                torch.nn.ReLU(), \n",
    "                                nn.Dropout(0.5),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#                                  nn.Linear(nz,1024),\n",
    "# #                                  nn.Dropout(0.3)\n",
    "#                                  nn.ReLU(),\n",
    "#                                  nn.Linear(1024,2048),\n",
    "#                                  nn.Sigmoid(),\n",
    "#                                  nn.Dropout(0.3),\n",
    "#                                  nn.Linear(2048,features_length),\n",
    "# #                                  nn.Sigmoid()\n",
    "#                                  nn.Dropout(0.5)\n",
    "#                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.netGen(x)\n",
    "        return F.dropout(x, 0.6)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=64):\n",
    "    '''\n",
    "    returns random rows of size batch_size\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))\n",
    "augmented_train = torch.autograd.Variable(torch.Tensor(augmented_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 4.134020185630605)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train.cpu().numpy()), get_sparsity(augmented_train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = get_random_batch(train)\n",
    "xy = get_random_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_my' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8ef96df456d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md_my\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd_my' is not defined"
     ]
    }
   ],
   "source": [
    "d_my(xx, xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 706.,  759., 1693., 1535.,  667.,   46.,  132., 1016.,  200.,   82.,\n",
       "         378., 2417., 2221.,  266.,  191.,  331.,  145.,  533.,  264.,  441.,\n",
       "         211.,   84.,  246.,  122.,   86.,  137.,   65.,  206.,  154.,  736.,\n",
       "        1454.,  266.,  247.,   83.,   58.,   58.,  359.,   88.,   80.,  243.,\n",
       "          71.,  103.,  233.,  243.,  213.,  265., 2156.,  132., 1127., 1646.,\n",
       "         271.,  587.,  175.,  175.,  163.,  168., 2165.,  165.,  583.,  223.,\n",
       "         564.,  194.,  417.,  474.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(torch.abs(xx != 0).float()*xy - xy), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx > xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g): # custom loss -todo\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r), 1)/x_r.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN(netD, netG, train_mat, steps_per_epoch = 300, epochs = 300):\n",
    "    gen_iterations = 0\n",
    "    eval_losses = []\n",
    "    d_iter = 5\n",
    "    g_iter = 1\n",
    "    for epoch in range(epochs):\n",
    "        i = 0\n",
    "        while i < steps_per_epoch:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in netD.parameters(): # reset requires_grad\n",
    "                p.requires_grad = True # they are set to False below in netG update\n",
    "            d_iter = d_iter\n",
    "            j = 0\n",
    "            while j < d_iter*5:\n",
    "                j += 1\n",
    "                # load real data\n",
    "                i += 1\n",
    "    #             X, _ = data_iter.next()\n",
    "                X = get_random_batch(train, batch_size=batch_size).to(device)\n",
    "    #             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "    #             print(X >= 0.5)\n",
    "    # #             X = X.view(X.size(0), -1)\n",
    "    #             X = (X >= 0.5).float()\n",
    "#                 if cuda: \n",
    "#                     X = X.cuda()\n",
    "#                 real = Variable(X)\n",
    "#                 real = X.clone()\n",
    "#                 real = X + fake * (X == 0).float()\n",
    "                # generate fake data\n",
    "                noise = torch.randn(batch_size, nz).to(device)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "                with torch.no_grad():\n",
    "                    noisev = Variable(noise) # totally freeze netG\n",
    "                    \n",
    "#                 fake = netG(noisev).to(device)\n",
    "                fake = Variable(netG(noisev).data).to(device)\n",
    "#                 fake.requires_grad = False\n",
    "    #             print(real[0,:20], fake[0,:20])\n",
    "#                 real + fake * (real == 0).float()\n",
    "#                 fake = fake * Variable(real != 0).float().cuda()\n",
    "\n",
    "    #             real + fake * (real == 0).float()\n",
    "    #             print(real[0,:20], fake[0,:20])\n",
    "#                 fake.requires_grad = False\n",
    "    #             print(real.shape, fake.shape)\n",
    "\n",
    "                # compute gradient, take step\n",
    "                netD.zero_grad()\n",
    "    #             print('real', real[:10, :20])\n",
    "    #             print('fake', fake[:10, :20])\n",
    "    #             print(real.type(), fake.type())\n",
    "    #             print(fake)\n",
    "#                 print(real)\n",
    "#                 real = X + fake * (X == 0).float()\n",
    "#                 print(real[0])\n",
    "#                 print(fake[0])\n",
    "                real = Variable(X)\n",
    "                out = netD(real, fake)\n",
    "\n",
    "                outputD = torch.mean(out) + lamba * out.norm()\n",
    "                stdD = torch.std(out)\n",
    "                outputD.backward(mone)\n",
    "                optimizerD.step()\n",
    "    #             print(out.shape)\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "            g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "#                 for p in netG.parameters():\n",
    "#                     p.requires_grad = True # to avoid computation\n",
    "                    \n",
    "                netG.zero_grad()\n",
    "\n",
    "                # load real data\n",
    "                i += 1\n",
    "                X = get_random_batch(train, batch_size=batch_size).to(device)\n",
    "    #             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "    #             X = X.view(X.size(0), -1)\n",
    "    #             X = (X >= 0.5).float()\n",
    "#                 if cuda: \n",
    "#                     X = X.cuda()\n",
    "#                 real = Variable(X)\n",
    "#                 real = X.clone()\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz).to(device)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "                fake = netG(noisev).to(device)\n",
    "#                 noisev = Variable(noise)\n",
    "\n",
    "#                 fake = Variable(netG(noisev).data)\n",
    "\n",
    "#                 real = real + fake * (real == 0).float()\n",
    "#                 fake = fake * Variable(real != 0).float().cuda()\n",
    "                \n",
    "    #             fake = fake * Variable(real != 0).float().cuda()\n",
    "#                 fake.requires_grad = False\n",
    "    #             fake = Variable(netG(noisev)).data\n",
    "    #             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "    \n",
    "#                 fake.requires_grad = True\n",
    "#                 real = X + fake * (X == 0).float()\n",
    "                real = Variable(X)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "#                 print(real.requires_grad, fake.requires_grad)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "\n",
    "                gen_iterations += 1\n",
    "\n",
    "    #             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "    #             print('output_D', outputD.item(), gen_iterations)\n",
    "    #             print('output_G', outputG.item(), gen_iterations)\n",
    "    #             print('std_D', stdD.item(), gen_iterations)\n",
    "    #             print('std_G', stdG.item(), gen_iterations)\n",
    "\n",
    "                # evaluation\n",
    "                if gen_iterations % 100 == 0: # todo- to change\n",
    "    #                 gen.eval()\n",
    "    #                 z_vector_eval = make_some_noise(128)\n",
    "    #                 fake_rows_eval = gen(z_vector_eval)\n",
    "    #                 real_rows_eval = get_random_batch(train, 128)\n",
    "            #         print(fake_rows[0][:10]) enable to see some results\n",
    "    #                 fake = Variable(netG(noisev).data).round()\n",
    "    #                 fake = ((real != 0) & (fake != 0))\n",
    "    #                 print(fake)\n",
    "                    eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                    eval_losses.append(eval_loss)\n",
    "                    print('Epoch number {}. my distance between random real and fake samples sum {}'.format(epoch, d_my(real, fake).sum()))\n",
    "                    print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake).mean()))\n",
    "                    print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))\n",
    "                    \n",
    "    return eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 300 #change\n",
    "seed = 1\n",
    "nz = 20\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=1024, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Dropout(p=0.4)\n",
      "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_tr = NetD().to(device)\n",
    "netG_tr = NetG().to(device)\n",
    "print(netD_tr)\n",
    "print(netG_tr)\n",
    "optimizerG = optim.RMSprop(netG_tr.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_tr.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (-1 * one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 8. my distance between random real and fake samples sum 4.805522918701172\n",
      "Epoch number 8. my distance between random real and fake samples 0.07508629560470581\n",
      "Epoch number 8. MSE distance between random real and fake samples 3.38498592376709\n",
      "Epoch number 16. my distance between random real and fake samples sum 5.077851295471191\n",
      "Epoch number 16. my distance between random real and fake samples 0.07934142649173737\n",
      "Epoch number 16. MSE distance between random real and fake samples 4.093999862670898\n",
      "Epoch number 24. my distance between random real and fake samples sum 5.500062942504883\n",
      "Epoch number 24. my distance between random real and fake samples 0.0859384834766388\n",
      "Epoch number 24. MSE distance between random real and fake samples 3.1052849292755127\n",
      "Epoch number 33. my distance between random real and fake samples sum 7.96456241607666\n",
      "Epoch number 33. my distance between random real and fake samples 0.12444628775119781\n",
      "Epoch number 33. MSE distance between random real and fake samples 5.259739398956299\n",
      "Epoch number 41. my distance between random real and fake samples sum 6.681240081787109\n",
      "Epoch number 41. my distance between random real and fake samples 0.10439437627792358\n",
      "Epoch number 41. MSE distance between random real and fake samples 4.203575611114502\n",
      "Epoch number 49. my distance between random real and fake samples sum 7.46129035949707\n",
      "Epoch number 49. my distance between random real and fake samples 0.11658266186714172\n",
      "Epoch number 49. MSE distance between random real and fake samples 3.6787779331207275\n",
      "Epoch number 58. my distance between random real and fake samples sum 4.979170799255371\n",
      "Epoch number 58. my distance between random real and fake samples 0.07779954373836517\n",
      "Epoch number 58. MSE distance between random real and fake samples 4.416417598724365\n",
      "Epoch number 66. my distance between random real and fake samples sum 5.600380897521973\n",
      "Epoch number 66. my distance between random real and fake samples 0.08750595152378082\n",
      "Epoch number 66. MSE distance between random real and fake samples 4.657054901123047\n",
      "Epoch number 74. my distance between random real and fake samples sum 6.950413227081299\n",
      "Epoch number 74. my distance between random real and fake samples 0.1086002066731453\n",
      "Epoch number 74. MSE distance between random real and fake samples 5.041593074798584\n",
      "Epoch number 83. my distance between random real and fake samples sum 6.217818260192871\n",
      "Epoch number 83. my distance between random real and fake samples 0.09715341031551361\n",
      "Epoch number 83. MSE distance between random real and fake samples 5.324183940887451\n",
      "Epoch number 91. my distance between random real and fake samples sum 5.077198028564453\n",
      "Epoch number 91. my distance between random real and fake samples 0.07933121919631958\n",
      "Epoch number 91. MSE distance between random real and fake samples 3.989603042602539\n",
      "Epoch number 99. my distance between random real and fake samples sum 6.959934234619141\n",
      "Epoch number 99. my distance between random real and fake samples 0.10874897241592407\n",
      "Epoch number 99. MSE distance between random real and fake samples 4.449607849121094\n",
      "Epoch number 108. my distance between random real and fake samples sum 6.690485000610352\n",
      "Epoch number 108. my distance between random real and fake samples 0.10453882813453674\n",
      "Epoch number 108. MSE distance between random real and fake samples 3.5610158443450928\n",
      "Epoch number 116. my distance between random real and fake samples sum 6.770645618438721\n",
      "Epoch number 116. my distance between random real and fake samples 0.10579133778810501\n",
      "Epoch number 116. MSE distance between random real and fake samples 4.829815864562988\n",
      "Epoch number 124. my distance between random real and fake samples sum 8.664762496948242\n",
      "Epoch number 124. my distance between random real and fake samples 0.13538691401481628\n",
      "Epoch number 124. MSE distance between random real and fake samples 5.091718673706055\n",
      "Epoch number 133. my distance between random real and fake samples sum 7.436239242553711\n",
      "Epoch number 133. my distance between random real and fake samples 0.11619123816490173\n",
      "Epoch number 133. MSE distance between random real and fake samples 4.814407825469971\n",
      "Epoch number 141. my distance between random real and fake samples sum 6.966625690460205\n",
      "Epoch number 141. my distance between random real and fake samples 0.1088535264134407\n",
      "Epoch number 141. MSE distance between random real and fake samples 5.837355613708496\n",
      "Epoch number 149. my distance between random real and fake samples sum 6.872114658355713\n",
      "Epoch number 149. my distance between random real and fake samples 0.10737679153680801\n",
      "Epoch number 149. MSE distance between random real and fake samples 3.5708086490631104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-cf13460d51f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_losses_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_GAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetD_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetG_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-6ba9d98d5cac>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[1;34m(netD, netG, train_mat, steps_per_epoch, epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#             X, _ = data_iter.next()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_random_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#             print(X >= 0.5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-d029a01f2015>\u001b[0m in \u001b[0;36mget_random_batch\u001b[1;34m(mat, batch_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     print(mat.shape, rand_rows)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print(mat[rand_rows].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_rows\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_losses_tr = train_GAN(netD_tr, netG_tr, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(netG.state_dict(), './netG-1m')\n",
    "# torch.save(netD.state_dict(), './netD-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=1024, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Dropout(p=0.4)\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=1024, out_features=3706, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_tr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_tr(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  0.,  1.,  2.,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.70998792007234"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 565406 226310\n",
      "4 3778968 348971\n",
      "3 3132649 261197\n",
      "2 1153866 107557\n",
      "1 239981 56174\n",
      "0 13495461 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see there is a significant bias towards higher ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# networks\n",
    "netD_augm = NetD().to(device)\n",
    "netG_augm = NetG().to(device)\n",
    "print(netD_augm)\n",
    "print(netG_augm)\n",
    "optimizerG = optim.RMSprop(netG_augm.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_augm.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (-1 * one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without train\n",
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_losses_aug = train_GAN(netD_augm, netG_augm, augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses_aug)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "\n",
    "fake_tr = netG_tr(noisev)\n",
    "fake_aug = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake_tr.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake_tr.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake_tr.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake_tr.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake_tr.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake_tr.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake_aug.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake_aug.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake_aug.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake_aug.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake_aug.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake_aug.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_orig, vr_1 = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed,  transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter \n",
    "import matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(train.cpu().numpy(), 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(aug_train, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ix = np.random.randint(0, fake.shape[0], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake = fake[rand_ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = np.append(aug_train.cpu().numpy(), adding_fake, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = torch.Tensor(aug_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 300\n",
    "gen_iterations = 0\n",
    "eval_losses = []\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < steps_per_epoch:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter*5:\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "#             X, _ = data_iter.next()\n",
    "            X = get_random_batch(aug_train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             print(X >= 0.5)\n",
    "# #             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             real + fake * (real == 0).float()\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            fake.requires_grad = False\n",
    "#             print(real.shape, fake.shape)\n",
    "    \n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "#             print('real', real[:10, :20])\n",
    "#             print('fake', fake[:10, :20])\n",
    "#             print(real.type(), fake.type())\n",
    "#             print(fake)\n",
    "            out = netD(real, fake)\n",
    "            \n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "#             print(out.shape)\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        g_iter = g_iter\n",
    "        j = 0\n",
    "        while j < g_iter*5:\n",
    "            j += 1\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = get_random_batch(aug_train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            \n",
    "            # update generator\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            noisev = Variable(noise)\n",
    "            \n",
    "            fake = netG(noisev)\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake.requires_grad = False\n",
    "#             fake = Variable(netG(noisev)).data\n",
    "#             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "#             fake.requires_grad = True\n",
    "            \n",
    "            out = netD(real, fake)\n",
    "            outputG = torch.mean(out) + lamba * out.norm()\n",
    "            stdG = torch.std(out)\n",
    "            outputG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "#             print('output_D', outputD.item(), gen_iterations)\n",
    "#             print('output_G', outputG.item(), gen_iterations)\n",
    "#             print('std_D', stdD.item(), gen_iterations)\n",
    "#             print('std_G', stdG.item(), gen_iterations)\n",
    "            torch.save(netG.state_dict(), './netG-1m')\n",
    "            torch.save(netD.state_dict(), './netD-1m')\n",
    "            # evaluation\n",
    "            if gen_iterations % 100 == 0: # todo- to change\n",
    "#                 gen.eval()\n",
    "#                 z_vector_eval = make_some_noise(128)\n",
    "#                 fake_rows_eval = gen(z_vector_eval)\n",
    "#                 real_rows_eval = get_random_batch(train, 128)\n",
    "        #         print(fake_rows[0][:10]) enable to see some results\n",
    "#                 fake = Variable(netG(noisev).data).round()\n",
    "#                 fake = ((real != 0) & (fake != 0))\n",
    "#                 print(fake)\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                eval_losses.append(eval_loss)\n",
    "                print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = np.append(aug, adding_fake, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25, 40]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train > 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(train > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train[0,:] > 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.Tensor(tr.copy()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback_mask = (train > 3).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_feedback_mask = ((train < 4).to(device).float() * (1 - zero_mask)).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((positive_feedback_mask + negative_feedback_mask) != zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(negative_feedback_mask.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class denoising_autoencoder(nn.Module):\n",
    "#     def __init__(self, n_users, input_size, z=256):\n",
    "#         ''' \n",
    "#         mimic the network architecture from the paper\n",
    "#         '''\n",
    "#         super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "#         self.V = torch.FloatTensor(n_users, z).to(device)\n",
    "#         self.b = torch.FloatTensor(n_users, 1).to(device)\n",
    "#         self.b_shtrix = torch.FloatTensor(n_users, 1).to(device)\n",
    "    \n",
    "#         self.encoder = nn.Linear(input_size, z)\n",
    "#         self.decoder = nn.Linear(z, input_size)\n",
    "        \n",
    "#         torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    "#         torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    " \n",
    "#     def forward(self, x, i):\n",
    "#         z = self.encoder(x)\n",
    "#         z = z + self.V[i, :] + self.b[i, :] \n",
    "#         z = torch.nn.functional.relu(z)\n",
    "#         x = self.decoder(z)\n",
    "#         x = x + self.b_shtrix[i, :]\n",
    "#         return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# # class denoising_autoencoder(nn.Module):\n",
    "# #     def __init__(self, n_users, input_size, z=256):\n",
    "# #         super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "# # #         torch.nn.init.xavier_uniform(self.V)\n",
    "# # #         torch.nn.init.xavier_uniform(self.b.weight)\n",
    "# # #         torch.nn.init.xavier_uniform(self.b_shtrix.weight)\n",
    "\n",
    "# #         self.encoder=nn.Sequential(\n",
    "# #                       nn.Linear(input_size, 1024),\n",
    "# #                       nn.Dropout(0.4),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(1024,512),\n",
    "# #                       nn.Dropout(0.6),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(512, z),\n",
    "# # #                       nn.Sigmoid()\n",
    "# #                       )\n",
    "\n",
    "# #         self.decoder=nn.Sequential(\n",
    "# #                       nn.Linear(z, 512),\n",
    "# #                       nn.Dropout(0.4),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(512, 1024),\n",
    "# #                       nn.Dropout(0.6),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(1024, input_size),\n",
    "# # #                       nn.Sigmoid(),\n",
    "# #                       )\n",
    "        \n",
    "# #         self.init_weights()\n",
    "    \n",
    "# #     def init_weights(self):\n",
    "\n",
    "# #         for l, ll in zip(self.encoder, self.decoder):\n",
    "# #             if type(l) == torch.nn.Linear:\n",
    "# #                 torch.nn.init.xavier_uniform_(l.weight)\n",
    "# #             if type(ll) == torch.nn.Linear:\n",
    "# #                 torch.nn.init.xavier_uniform_(ll.weight)\n",
    "\n",
    "# #     def forward(self, x, i):\n",
    "# #         z = self.encoder(x)\n",
    "# #         x = self.decoder(z)\n",
    "    \n",
    "# #         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = denoising_autoencoder(input_size=train[0,:].shape[0], n_users=train.shape[0]).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(orig_mat, corrupted_mat, batch_size = 64):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(orig_mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = orig_mat[rand_rows].clone()\n",
    "    corrupted = corrupted_mat[rand_rows].clone()\n",
    "\n",
    "    return orig, corrupted, rand_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_masked_batch(mat, batch_size = 32, p=0.5):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = mat[rand_rows].clone()\n",
    "    corrupted = mat[rand_rows].clone()\n",
    "    mask_arr = torch.FloatTensor((np.random.rand(orig.shape[0], orig.shape[1]) > p)).to(device)\n",
    "\n",
    "    return orig, corrupted*mask_arr, rand_rows\n",
    "\n",
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(orig.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(masked.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.4\n",
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)).to(device)\n",
    "y = negative_feedback_mask\n",
    "X = negative_feedback_mask*mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(X.cpu().numpy()), get_sparsity(y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losslist = []\n",
    "model.train()\n",
    "def train_den_ae(mat,epochs= 400,steps_per_epoch = 300, _unsqueeze=True):\n",
    "    epochloss = 0\n",
    "    running_loss = 0\n",
    "    steps_per_epoch = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i in range(steps_per_epoch):\n",
    "            orig, masked, idxs = get_random_batch(y, X, batch_size=batch_size)\n",
    "\n",
    "            if _unsqueeze:\n",
    "                masked = masked.unsqueeze(2)\n",
    "            output = model(masked, idxs)\n",
    "            loss = criterion(output, orig)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i%20 == 0:\n",
    "                running_loss += loss.item()\n",
    "                epochloss += loss.item()\n",
    "                losslist.append(loss.item())\n",
    "                running_loss=0\n",
    "                print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "                \n",
    "train_den_ae(negative_feedback_mask, _unsqueeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked[0] >0.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output[0] >0.2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400](orig.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round()[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class conv_denoising_autoencoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, inSize,fSize = 32, nz=24):  #sigma is the corruption level\n",
    "#         super(conv_denoising_autoencoder, self).__init__()\n",
    "#         #define layers here\n",
    "\n",
    "#         self.inp_size = inSize\n",
    "#         self.nz = nz\n",
    "#         self.fSize = 32\n",
    "# #         self.imSize = imSize\n",
    "# #         self.sigma = sigma\n",
    "# #         self.multimodalZ = multimodalZ\n",
    "\n",
    "# #         inSize = imSize / ( 2 ** 4)\n",
    "# #         self.inSize = inSize\n",
    "    \n",
    "#         self.enc1 = nn.Conv1d(self.inp_size, fSize, 5, stride=2, padding=2)\n",
    "#         self.enc2 = nn.Conv1d(fSize, fSize * 2, 5, stride=2, padding=2)\n",
    "#         self.enc3 = nn.Conv1d(fSize * 2, fSize * 4, 5, stride=2, padding=2)\n",
    "#         self.enc4 = nn.Conv1d(fSize * 4, fSize * 8, 5, stride=2, padding=2)\n",
    "#         self.enc5 = nn.Linear(fSize * 8, self.nz)\n",
    "\n",
    "#         self.dec1 = nn.Linear(self.nz, fSize * 8)\n",
    "#         self.dec2 = nn.ConvTranspose1d(fSize * 8, fSize * 4, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.dec3 = nn.ConvTranspose1d(fSize * 4, fSize * 2, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.dec4 = nn.ConvTranspose1d(fSize * 2, fSize, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.dec5 = nn.ConvTranspose1d(fSize, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "#         self.last_dec = nn.Linear(32*16, self.inp_size)\n",
    "        \n",
    "#         self.useCUDA = torch.cuda.is_available()\n",
    "\n",
    "# #     def norm_prior(self, noSamples=25):\n",
    "# #         z = torch.randn(noSamples, self.nz)\n",
    "# #         return z\n",
    "\n",
    "# #     def multi_prior(self, noSamples=25, mode=None):\n",
    "# #         #make a 2D sqrt(nz)-by-sqrt(nz) grid of gaussians\n",
    "# #         num = np.sqrt(self.nz) #no of modes in x and y\n",
    "# #         STD = 1.0\n",
    "# #         modes = np.arange(-num,num)\n",
    "# #         p = np.random.uniform(0, num,(noSamples*2))\n",
    "\n",
    "# #         if mode is None:\n",
    "# #             mu = modes[np.floor(2 * p).astype(int)]\n",
    "# #         else:\n",
    "# #             mu = modes[np.ones((noSamples, 2), dtype=int) * int(mode)]\n",
    "\n",
    "# #         z = torch.Tensor(mu).view(-1,2) + STD * torch.randn(noSamples, 2)\n",
    "# #         return z\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         self.batch_size = x.shape[0]\n",
    "#         #define the encoder here return mu(x) and sigma(x)\n",
    "#         x = F.relu(self.enc1(x))\n",
    "#         x = F.relu(self.enc2(x))\n",
    "#         x = F.relu(self.enc3(x))\n",
    "#         x = F.relu(self.enc4(x))\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.enc5(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# #     def corrupt(self, x):\n",
    "# #         noise = self.sigma * Variable(torch.randn(x.size())).type_as(x)\n",
    "# #         return x + noise\n",
    "\n",
    "# #     def sample_z(self, noSamples=25, mode=None):\n",
    "# #         if not self.multimodalZ:\n",
    "# #             z = self.norm_prior(noSamples=noSamples)\n",
    "# #         else:\n",
    "# #             z = self.multi_prior(noSamples=noSamples, mode=mode)\n",
    "# #         if self.useCUDA:\n",
    "# #             return Variable(z.cuda())\n",
    "# #         else:\n",
    "# #             return Variable(z)\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         #define the decoder here\n",
    "#         z = F.relu(self.dec1(z))\n",
    "#         z = z.unsqueeze(2)\n",
    "# #         print(z.shape)\n",
    "# #         z = z.view(z.size(0), -1, self.inp_size)\n",
    "#         z = F.relu(self.dec2(z))\n",
    "#         z = F.relu(self.dec3(z))\n",
    "#         z = F.relu(self.dec4(z))\n",
    "#         z = F.sigmoid(self.dec5(z))\n",
    "# #         print(z.shape)\n",
    "# #         z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "#         z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "    \n",
    "#         return z\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # the outputs needed for training\n",
    "# #         x_corr = self.corrupt(x)\n",
    "#         z = self.encode(x)\n",
    "#         return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_denoising_autoencoder(train[0,:].shape[0]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.01, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "train_den_ae(negative_feedback_mask, steps_per_epoch=steps_per_epoch, _unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output[7]).round() >0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output.shape[0]):\n",
    "    print(((output[i]).round() >0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output).round() >= 1 ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output).round()[0][2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gtshs2/Collaborative-Denoising-Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tr + vr) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.000001))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(512, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.000001))(enc)\n",
    "    lat_space = Dropout(0.5, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.000001))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.000001))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.4)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.4)).to(device)\n",
    "y = negative_feedback_mask.cpu().numpy()\n",
    "X = (negative_feedback_mask*mask_arr).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(y), get_sparsity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoEncoder(X)\n",
    "\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x=X, y=y,\n",
    "                  epochs=300,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict new Matrix Interactions, set score zero on visualized games\n",
    "# new_matrix = model.predict(X) * (X == 0)\n",
    "new_matrix = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # converting the reconstructed matrix back to a Pandas dataframe\n",
    "# new_users_items_matrix_df  = pd.DataFrame(new_matrix, \n",
    "#                                           columns = users_items_matrix_df.columns, \n",
    "#                                           index   = users_items_matrix_df.index)\n",
    "# new_users_items_matrix_df.head()\n",
    "\n",
    "new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix>0.4).sum() # predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X>0.5).sum() # trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y > 0.99).sum() # actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (new_matrix>0.4) == 1).sum()/(new_matrix>0.4).sum() # accuracy on actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (new_matrix>0.5)).sum()/((new_matrix>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y * (new_matrix>0.5)).sum()/((new_matrix>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y * (((new_matrix>0.8)  * (X<0.5)))) == 1).sum()/(((new_matrix>0.8)  * (X<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix_2 = model.predict(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix_2>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((y* (((new_matrix_2>0.5)) == 1))).sum()/(((new_matrix_2>0.5)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=1)\n",
    "new_matrix[:20, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:20, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix_2>0.3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_matrix_2 > 0.3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((new_matrix_2 > threshold).astype(float) * (train > 0).cpu().numpy().astype(float)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)).sum() # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 3 # delta could be 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((train > 0).cpu().numpy().astype(float) > 0) *(new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)).sum() # checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = train.cpu().numpy().astype(float) + delta * (new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_probs = [(train == 1).sum().item()/((train > 0) & (train < 4)).sum().item(), (train == 2).sum().item()/(((train > 0) & (train < 4))).sum().item(), (train == 3).sum().item()/((train > 0) & (train < 4)).sum().item()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train == 1).sum().item()/((train > 0) & (train < 4)).sum().item() + (train == 2).sum().item()/(((train > 0) & (train < 4))).sum().item() + (train == 3).sum().item()/((train > 0) & (train < 4)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(1,4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del augmented_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(np.arange(1, 4), train.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train > 0).cpu().numpy().astype(float)  * (new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)*np.random.choice(np.arange(1, 4), train.shape, p=p_probs)).sum() # should be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train = train.cpu().numpy().astype(float) + (new_matrix_2 > threshold).astype(float) * (train == 0).cpu().numpy().astype(float)*np.random.choice(np.arange(1, 4), train.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(aug_train > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(aug_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
