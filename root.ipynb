{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import sys\n",
    "from dataLoader import loadData\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 5.784438371658325 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from os.path import isfile, isdir, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 0 #change\n",
    "nz = 10\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 1e-3 # constant for L2 penalty (diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available()==True:\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device =\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Dropout(p=0.5)\n",
      "    (5): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Dropout(p=0.6)\n",
      "  )\n",
      ")\n",
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "#         filt = (torch.abs((real > 0.3).float() * fake - real))/real.shape[0]\n",
    "        filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())/(fake == 0).sum()\n",
    "#         filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())\n",
    "\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "        x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz, 1024), \n",
    "                                torch.nn.ReLU(), \n",
    "                                torch.nn.Linear(1024, 1024), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.5),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.6)\n",
    "                                )\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#                                  nn.Linear(nz,1024),\n",
    "# #                                  nn.Dropout(0.3)\n",
    "#                                  nn.ReLU(),\n",
    "#                                  nn.Linear(1024,2048),\n",
    "#                                  nn.Sigmoid(),\n",
    "#                                  nn.Dropout(0.3),\n",
    "#                                  nn.Linear(2048,features_length),\n",
    "# #                                  nn.Sigmoid()\n",
    "#                                  nn.Dropout(0.5)\n",
    "#                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "#         return x\n",
    "        return x*5 # to get values in range [0,5]\n",
    "    \n",
    "# networks\n",
    "netD = NetD().to(device)\n",
    "netG = NetG().to(device)\n",
    "print(netG)\n",
    "print(netD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (one * -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in netD.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #\n",
    "    \n",
    "for p in netG.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3706)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_batch(train, batch_size=batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g): # custom loss -todo\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 300\n",
    "gen_iterations = 0\n",
    "eval_losses = []\n",
    "for epoch in range(0):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < steps_per_epoch:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter*5:\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "#             X, _ = data_iter.next()\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             print(X >= 0.5)\n",
    "# #             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             real + fake * (real == 0).float()\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            fake.requires_grad = False\n",
    "#             print(real.shape, fake.shape)\n",
    "    \n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "#             print('real', real[:10, :20])\n",
    "#             print('fake', fake[:10, :20])\n",
    "#             print(real.type(), fake.type())\n",
    "#             print(fake)\n",
    "            out = netD(real, fake)\n",
    "            \n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "#             print(out.shape)\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        g_iter = g_iter\n",
    "        j = 0\n",
    "        while j < g_iter*5:\n",
    "            j += 1\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            \n",
    "            # update generator\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            noisev = Variable(noise)\n",
    "            \n",
    "            fake = netG(noisev)\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake.requires_grad = False\n",
    "#             fake = Variable(netG(noisev)).data\n",
    "#             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "#             fake.requires_grad = True\n",
    "            \n",
    "            out = netD(real, fake)\n",
    "            outputG = torch.mean(out) + lamba * out.norm()\n",
    "            stdG = torch.std(out)\n",
    "            outputG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "#             print('output_D', outputD.item(), gen_iterations)\n",
    "#             print('output_G', outputG.item(), gen_iterations)\n",
    "#             print('std_D', stdD.item(), gen_iterations)\n",
    "#             print('std_G', stdG.item(), gen_iterations)\n",
    "            torch.save(netG.state_dict(), './netG-1m')\n",
    "            torch.save(netD.state_dict(), './netD-1m')\n",
    "            # evaluation\n",
    "            if gen_iterations % 100 == 0: # todo- to change\n",
    "#                 gen.eval()\n",
    "#                 z_vector_eval = make_some_noise(128)\n",
    "#                 fake_rows_eval = gen(z_vector_eval)\n",
    "#                 real_rows_eval = get_random_batch(train, 128)\n",
    "        #         print(fake_rows[0][:10]) enable to see some results\n",
    "#                 fake = Variable(netG(noisev).data).round()\n",
    "#                 fake = ((real != 0) & (fake != 0))\n",
    "#                 print(fake)\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                eval_losses.append(eval_loss)\n",
    "                print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(netG.state_dict(), './netG-1m')\n",
    "# torch.save(netD.state_dict(), './netD-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1772354 226310\n",
      "4 631569 348971\n",
      "3 95589 261197\n",
      "2 3395 107557\n",
      "1 2 56174\n",
      "0 19881331 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train > 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.934857739195076e-06"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[0,:] > 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [3., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.Tensor(tr.copy()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback_mask = (train > 3).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_feedback_mask = ((train < 4).to(device).float() * (1 - zero_mask)).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((positive_feedback_mask + negative_feedback_mask) != zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7090908603553212"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(negative_feedback_mask.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_autoencoder(nn.Module):\n",
    "    def __init__(self, n_users, input_size, z=256):\n",
    "        ''' \n",
    "        mimic the network architecture from the paper\n",
    "        '''\n",
    "        super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "        self.V = torch.FloatTensor(n_users, z).to(device)\n",
    "        self.b = torch.FloatTensor(n_users, 1).to(device)\n",
    "        self.b_shtrix = torch.FloatTensor(n_users, 1).to(device)\n",
    "    \n",
    "        self.encoder = nn.Linear(input_size, z)\n",
    "        self.decoder = nn.Linear(z, input_size)\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    " \n",
    "    def forward(self, x, i):\n",
    "        z = self.encoder(x)\n",
    "        z = z + self.V[i, :] + self.b[i, :] \n",
    "        z = torch.nn.functional.relu(z)\n",
    "        x = self.decoder(z)\n",
    "        x = x + self.b_shtrix[i, :]\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# class denoising_autoencoder(nn.Module):\n",
    "#     def __init__(self, n_users, input_size, z=256):\n",
    "#         super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "# #         torch.nn.init.xavier_uniform(self.V)\n",
    "# #         torch.nn.init.xavier_uniform(self.b.weight)\n",
    "# #         torch.nn.init.xavier_uniform(self.b_shtrix.weight)\n",
    "\n",
    "#         self.encoder=nn.Sequential(\n",
    "#                       nn.Linear(input_size, 1024),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(1024,512),\n",
    "#                       nn.Dropout(0.6),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(512, z),\n",
    "# #                       nn.Sigmoid()\n",
    "#                       )\n",
    "\n",
    "#         self.decoder=nn.Sequential(\n",
    "#                       nn.Linear(z, 512),\n",
    "#                       nn.Dropout(0.4),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(512, 1024),\n",
    "#                       nn.Dropout(0.6),\n",
    "#                       nn.ReLU(),\n",
    "#                       nn.Linear(1024, input_size),\n",
    "# #                       nn.Sigmoid(),\n",
    "#                       )\n",
    "        \n",
    "#         self.init_weights()\n",
    "    \n",
    "#     def init_weights(self):\n",
    "\n",
    "#         for l, ll in zip(self.encoder, self.decoder):\n",
    "#             if type(l) == torch.nn.Linear:\n",
    "#                 torch.nn.init.xavier_uniform_(l.weight)\n",
    "#             if type(ll) == torch.nn.Linear:\n",
    "#                 torch.nn.init.xavier_uniform_(ll.weight)\n",
    "\n",
    "#     def forward(self, x, i):\n",
    "#         z = self.encoder(x)\n",
    "#         x = self.decoder(z)\n",
    "    \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = denoising_autoencoder(input_size=train[0,:].shape[0], n_users=train.shape[0]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(orig_mat, corrupted_mat, batch_size = 64):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(orig_mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = orig_mat[rand_rows].clone()\n",
    "    corrupted = corrupted_mat[rand_rows].clone()\n",
    "\n",
    "    return orig, corrupted, rand_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_masked_batch(mat, batch_size = 32, p=0.5):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = mat[rand_rows].clone()\n",
    "    corrupted = mat[rand_rows].clone()\n",
    "    mask_arr = torch.FloatTensor((np.random.rand(orig.shape[0], orig.shape[1]) > p)).to(device)\n",
    "\n",
    "    return orig, corrupted*mask_arr, rand_rows\n",
    "\n",
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7294589854290339"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(orig.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8685240151106314"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(masked.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.4\n",
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)).to(device)\n",
    "y = negative_feedback_mask\n",
    "X = negative_feedback_mask*mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0229920694202708, 1.7090908603553212)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(X.cpu().numpy()), get_sparsity(y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n",
      "======> epoch: 0/400, Loss:nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-3e75682fb7ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"======> epoch: {}/{}, Loss:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtrain_den_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_feedback_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_unsqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-3e75682fb7ea>\u001b[0m in \u001b[0;36mtrain_den_ae\u001b[1;34m(mat, epochs, steps_per_epoch, _unsqueeze)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[1;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losslist = []\n",
    "model.train()\n",
    "def train_den_ae(mat,epochs= 400,steps_per_epoch = 300, _unsqueeze=True):\n",
    "    epochloss = 0\n",
    "    running_loss = 0\n",
    "    steps_per_epoch = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i in range(steps_per_epoch):\n",
    "            orig, masked, idxs = get_random_batch(y, X, batch_size=batch_size)\n",
    "\n",
    "            if _unsqueeze:\n",
    "                masked = masked.unsqueeze(2)\n",
    "            output = model(masked, idxs)\n",
    "            loss = criterion(output, orig)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i%20 == 0:\n",
    "                running_loss += loss.item()\n",
    "                epochloss += loss.item()\n",
    "                losslist.append(loss.item())\n",
    "                running_loss=0\n",
    "                print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "                \n",
    "train_den_ae(negative_feedback_mask, _unsqueeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked[0] >0.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output[0] >0.2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400](orig.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round()[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_denoising_autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, inSize,fSize = 32, nz=24):  #sigma is the corruption level\n",
    "        super(conv_denoising_autoencoder, self).__init__()\n",
    "        #define layers here\n",
    "\n",
    "        self.inp_size = inSize\n",
    "        self.nz = nz\n",
    "        self.fSize = 32\n",
    "#         self.imSize = imSize\n",
    "#         self.sigma = sigma\n",
    "#         self.multimodalZ = multimodalZ\n",
    "\n",
    "#         inSize = imSize / ( 2 ** 4)\n",
    "#         self.inSize = inSize\n",
    "    \n",
    "        self.enc1 = nn.Conv1d(self.inp_size, fSize, 5, stride=2, padding=2)\n",
    "        self.enc2 = nn.Conv1d(fSize, fSize * 2, 5, stride=2, padding=2)\n",
    "        self.enc3 = nn.Conv1d(fSize * 2, fSize * 4, 5, stride=2, padding=2)\n",
    "        self.enc4 = nn.Conv1d(fSize * 4, fSize * 8, 5, stride=2, padding=2)\n",
    "        self.enc5 = nn.Linear(fSize * 8, self.nz)\n",
    "\n",
    "        self.dec1 = nn.Linear(self.nz, fSize * 8)\n",
    "        self.dec2 = nn.ConvTranspose1d(fSize * 8, fSize * 4, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(fSize * 4, fSize * 2, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(fSize * 2, fSize, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec5 = nn.ConvTranspose1d(fSize, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.last_dec = nn.Linear(32*16, self.inp_size)\n",
    "        \n",
    "        self.useCUDA = torch.cuda.is_available()\n",
    "\n",
    "#     def norm_prior(self, noSamples=25):\n",
    "#         z = torch.randn(noSamples, self.nz)\n",
    "#         return z\n",
    "\n",
    "#     def multi_prior(self, noSamples=25, mode=None):\n",
    "#         #make a 2D sqrt(nz)-by-sqrt(nz) grid of gaussians\n",
    "#         num = np.sqrt(self.nz) #no of modes in x and y\n",
    "#         STD = 1.0\n",
    "#         modes = np.arange(-num,num)\n",
    "#         p = np.random.uniform(0, num,(noSamples*2))\n",
    "\n",
    "#         if mode is None:\n",
    "#             mu = modes[np.floor(2 * p).astype(int)]\n",
    "#         else:\n",
    "#             mu = modes[np.ones((noSamples, 2), dtype=int) * int(mode)]\n",
    "\n",
    "#         z = torch.Tensor(mu).view(-1,2) + STD * torch.randn(noSamples, 2)\n",
    "#         return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.batch_size = x.shape[0]\n",
    "        #define the encoder here return mu(x) and sigma(x)\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#     def corrupt(self, x):\n",
    "#         noise = self.sigma * Variable(torch.randn(x.size())).type_as(x)\n",
    "#         return x + noise\n",
    "\n",
    "#     def sample_z(self, noSamples=25, mode=None):\n",
    "#         if not self.multimodalZ:\n",
    "#             z = self.norm_prior(noSamples=noSamples)\n",
    "#         else:\n",
    "#             z = self.multi_prior(noSamples=noSamples, mode=mode)\n",
    "#         if self.useCUDA:\n",
    "#             return Variable(z.cuda())\n",
    "#         else:\n",
    "#             return Variable(z)\n",
    "\n",
    "    def decode(self, z):\n",
    "        #define the decoder here\n",
    "        z = F.relu(self.dec1(z))\n",
    "        z = z.unsqueeze(2)\n",
    "#         print(z.shape)\n",
    "#         z = z.view(z.size(0), -1, self.inp_size)\n",
    "        z = F.relu(self.dec2(z))\n",
    "        z = F.relu(self.dec3(z))\n",
    "        z = F.relu(self.dec4(z))\n",
    "        z = F.sigmoid(self.dec5(z))\n",
    "#         print(z.shape)\n",
    "#         z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "        z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "    \n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the outputs needed for training\n",
    "#         x_corr = self.corrupt(x)\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_denoising_autoencoder(train[0,:].shape[0]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.01, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "train_den_ae(negative_feedback_mask, steps_per_epoch=steps_per_epoch, _unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output[7]).round() >0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output.shape[0]):\n",
    "    print(((output[i]).round() >0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output).round() >= 1 ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output).round()[0][2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gtshs2/Collaborative-Denoising-Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tr + vr) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.01))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(256, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.01))(enc)\n",
    "    lat_space = Dropout(0.8, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.01))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.01))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.35)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.35)).to(device)\n",
    "y = negative_feedback_mask.cpu().numpy()\n",
    "X = (negative_feedback_mask*mask_arr).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "UserScore (InputLayer)       (None, 3706)              0         \n",
      "_________________________________________________________________\n",
      "EncLayer1 (Dense)            (None, 512)               1897984   \n",
      "_________________________________________________________________\n",
      "LatentSpace (Dense)          (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "DecLayer1 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "UserScorePred (Dense)        (None, 3706)              1901178   \n",
      "=================================================================\n",
      "Total params: 4,062,074\n",
      "Trainable params: 4,062,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = autoEncoder(X)\n",
    "\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6040/6040 [==============================] - ETA: 20s - loss: 24.892 - ETA: 11s - loss: 24.816 - ETA: 8s - loss: 24.736 - ETA: 6s - loss: 24.65 - ETA: 5s - loss: 24.57 - ETA: 5s - loss: 24.49 - ETA: 4s - loss: 24.41 - ETA: 4s - loss: 24.33 - ETA: 3s - loss: 24.25 - ETA: 3s - loss: 24.16 - ETA: 3s - loss: 24.08 - ETA: 3s - loss: 24.00 - ETA: 3s - loss: 23.92 - ETA: 2s - loss: 23.84 - ETA: 2s - loss: 23.77 - ETA: 2s - loss: 23.69 - ETA: 2s - loss: 23.61 - ETA: 2s - loss: 23.53 - ETA: 2s - loss: 23.45 - ETA: 2s - loss: 23.38 - ETA: 2s - loss: 23.30 - ETA: 1s - loss: 23.22 - ETA: 1s - loss: 23.15 - ETA: 1s - loss: 23.07 - ETA: 1s - loss: 22.99 - ETA: 1s - loss: 22.92 - ETA: 1s - loss: 22.84 - ETA: 1s - loss: 22.77 - ETA: 1s - loss: 22.69 - ETA: 1s - loss: 22.62 - ETA: 1s - loss: 22.54 - ETA: 1s - loss: 22.47 - ETA: 1s - loss: 22.40 - ETA: 0s - loss: 22.32 - ETA: 0s - loss: 22.25 - ETA: 0s - loss: 22.18 - ETA: 0s - loss: 22.10 - ETA: 0s - loss: 22.03 - ETA: 0s - loss: 21.96 - ETA: 0s - loss: 21.89 - ETA: 0s - loss: 21.82 - ETA: 0s - loss: 21.75 - ETA: 0s - loss: 21.68 - ETA: 0s - loss: 21.61 - ETA: 0s - loss: 21.54 - ETA: 0s - loss: 21.47 - ETA: 0s - loss: 21.40 - 3s 535us/step - loss: 21.3883\n",
      "Epoch 2/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 17.96 - ETA: 2s - loss: 17.90 - ETA: 2s - loss: 17.84 - ETA: 2s - loss: 17.78 - ETA: 2s - loss: 17.72 - ETA: 2s - loss: 17.66 - ETA: 2s - loss: 17.60 - ETA: 2s - loss: 17.54 - ETA: 2s - loss: 17.48 - ETA: 2s - loss: 17.42 - ETA: 2s - loss: 17.36 - ETA: 2s - loss: 17.30 - ETA: 2s - loss: 17.24 - ETA: 2s - loss: 17.19 - ETA: 1s - loss: 17.13 - ETA: 1s - loss: 17.07 - ETA: 1s - loss: 17.02 - ETA: 1s - loss: 16.96 - ETA: 1s - loss: 16.90 - ETA: 1s - loss: 16.85 - ETA: 1s - loss: 16.79 - ETA: 1s - loss: 16.73 - ETA: 1s - loss: 16.68 - ETA: 1s - loss: 16.62 - ETA: 1s - loss: 16.57 - ETA: 1s - loss: 16.51 - ETA: 1s - loss: 16.46 - ETA: 1s - loss: 16.41 - ETA: 1s - loss: 16.35 - ETA: 1s - loss: 16.30 - ETA: 0s - loss: 16.25 - ETA: 0s - loss: 16.19 - ETA: 0s - loss: 16.14 - ETA: 0s - loss: 16.09 - ETA: 0s - loss: 16.03 - ETA: 0s - loss: 15.98 - ETA: 0s - loss: 15.93 - ETA: 0s - loss: 15.88 - ETA: 0s - loss: 15.83 - ETA: 0s - loss: 15.78 - ETA: 0s - loss: 15.72 - ETA: 0s - loss: 15.67 - ETA: 0s - loss: 15.62 - ETA: 0s - loss: 15.57 - ETA: 0s - loss: 15.52 - ETA: 0s - loss: 15.47 - ETA: 0s - loss: 15.42 - 3s 479us/step - loss: 15.4188\n",
      "Epoch 3/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 12.97 - ETA: 2s - loss: 12.92 - ETA: 2s - loss: 12.88 - ETA: 2s - loss: 12.83 - ETA: 2s - loss: 12.79 - ETA: 2s - loss: 12.75 - ETA: 2s - loss: 12.71 - ETA: 2s - loss: 12.66 - ETA: 2s - loss: 12.62 - ETA: 2s - loss: 12.58 - ETA: 2s - loss: 12.54 - ETA: 2s - loss: 12.50 - ETA: 2s - loss: 12.45 - ETA: 2s - loss: 12.41 - ETA: 2s - loss: 12.37 - ETA: 2s - loss: 12.33 - ETA: 1s - loss: 12.29 - ETA: 1s - loss: 12.25 - ETA: 1s - loss: 12.21 - ETA: 1s - loss: 12.17 - ETA: 1s - loss: 12.13 - ETA: 1s - loss: 12.09 - ETA: 1s - loss: 12.05 - ETA: 1s - loss: 12.01 - ETA: 1s - loss: 11.97 - ETA: 1s - loss: 11.93 - ETA: 1s - loss: 11.90 - ETA: 1s - loss: 11.86 - ETA: 1s - loss: 11.82 - ETA: 1s - loss: 11.78 - ETA: 1s - loss: 11.74 - ETA: 0s - loss: 11.70 - ETA: 0s - loss: 11.67 - ETA: 0s - loss: 11.63 - ETA: 0s - loss: 11.59 - ETA: 0s - loss: 11.55 - ETA: 0s - loss: 11.52 - ETA: 0s - loss: 11.48 - ETA: 0s - loss: 11.44 - ETA: 0s - loss: 11.41 - ETA: 0s - loss: 11.37 - ETA: 0s - loss: 11.33 - ETA: 0s - loss: 11.30 - ETA: 0s - loss: 11.26 - ETA: 0s - loss: 11.23 - ETA: 0s - loss: 11.19 - ETA: 0s - loss: 11.15 - 3s 521us/step - loss: 11.1530\n",
      "Epoch 4/50\n",
      "6040/6040 [==============================] - ETA: 3s - loss: 9.403 - ETA: 3s - loss: 9.373 - ETA: 3s - loss: 9.342 - ETA: 3s - loss: 9.310 - ETA: 3s - loss: 9.279 - ETA: 2s - loss: 9.248 - ETA: 2s - loss: 9.217 - ETA: 2s - loss: 9.187 - ETA: 2s - loss: 9.156 - ETA: 2s - loss: 9.126 - ETA: 2s - loss: 9.096 - ETA: 2s - loss: 9.067 - ETA: 2s - loss: 9.037 - ETA: 2s - loss: 9.008 - ETA: 2s - loss: 8.979 - ETA: 2s - loss: 8.950 - ETA: 2s - loss: 8.921 - ETA: 2s - loss: 8.892 - ETA: 2s - loss: 8.864 - ETA: 1s - loss: 8.835 - ETA: 1s - loss: 8.806 - ETA: 1s - loss: 8.778 - ETA: 1s - loss: 8.750 - ETA: 1s - loss: 8.722 - ETA: 1s - loss: 8.694 - ETA: 1s - loss: 8.666 - ETA: 1s - loss: 8.638 - ETA: 1s - loss: 8.611 - ETA: 1s - loss: 8.583 - ETA: 1s - loss: 8.556 - ETA: 1s - loss: 8.529 - ETA: 1s - loss: 8.502 - ETA: 0s - loss: 8.475 - ETA: 0s - loss: 8.448 - ETA: 0s - loss: 8.421 - ETA: 0s - loss: 8.395 - ETA: 0s - loss: 8.368 - ETA: 0s - loss: 8.342 - ETA: 0s - loss: 8.315 - ETA: 0s - loss: 8.289 - ETA: 0s - loss: 8.263 - ETA: 0s - loss: 8.238 - ETA: 0s - loss: 8.212 - ETA: 0s - loss: 8.186 - ETA: 0s - loss: 8.161 - ETA: 0s - loss: 8.135 - ETA: 0s - loss: 8.110 - 3s 554us/step - loss: 8.1057\n",
      "Epoch 5/50\n",
      "6040/6040 [==============================] - ETA: 3s - loss: 6.859 - ETA: 3s - loss: 6.836 - ETA: 3s - loss: 6.813 - ETA: 2s - loss: 6.790 - ETA: 2s - loss: 6.769 - ETA: 2s - loss: 6.748 - ETA: 2s - loss: 6.727 - ETA: 2s - loss: 6.706 - ETA: 2s - loss: 6.684 - ETA: 2s - loss: 6.662 - ETA: 2s - loss: 6.641 - ETA: 2s - loss: 6.620 - ETA: 2s - loss: 6.599 - ETA: 2s - loss: 6.578 - ETA: 2s - loss: 6.557 - ETA: 2s - loss: 6.536 - ETA: 2s - loss: 6.515 - ETA: 2s - loss: 6.495 - ETA: 1s - loss: 6.474 - ETA: 1s - loss: 6.454 - ETA: 1s - loss: 6.434 - ETA: 1s - loss: 6.414 - ETA: 1s - loss: 6.394 - ETA: 1s - loss: 6.374 - ETA: 1s - loss: 6.354 - ETA: 1s - loss: 6.334 - ETA: 1s - loss: 6.315 - ETA: 1s - loss: 6.295 - ETA: 1s - loss: 6.276 - ETA: 1s - loss: 6.256 - ETA: 1s - loss: 6.237 - ETA: 1s - loss: 6.218 - ETA: 0s - loss: 6.198 - ETA: 0s - loss: 6.179 - ETA: 0s - loss: 6.160 - ETA: 0s - loss: 6.141 - ETA: 0s - loss: 6.122 - ETA: 0s - loss: 6.104 - ETA: 0s - loss: 6.085 - ETA: 0s - loss: 6.067 - ETA: 0s - loss: 6.048 - ETA: 0s - loss: 6.030 - ETA: 0s - loss: 6.011 - ETA: 0s - loss: 5.993 - ETA: 0s - loss: 5.975 - ETA: 0s - loss: 5.957 - ETA: 0s - loss: 5.939 - 3s 555us/step - loss: 5.9359\n",
      "Epoch 6/50\n",
      "6040/6040 [==============================] - ETA: 3s - loss: 5.051 - ETA: 3s - loss: 5.033 - ETA: 2s - loss: 5.017 - ETA: 2s - loss: 5.001 - ETA: 2s - loss: 4.986 - ETA: 2s - loss: 4.969 - ETA: 2s - loss: 4.954 - ETA: 2s - loss: 4.939 - ETA: 2s - loss: 4.923 - ETA: 2s - loss: 4.908 - ETA: 2s - loss: 4.893 - ETA: 2s - loss: 4.878 - ETA: 2s - loss: 4.863 - ETA: 2s - loss: 4.848 - ETA: 2s - loss: 4.833 - ETA: 1s - loss: 4.818 - ETA: 1s - loss: 4.804 - ETA: 1s - loss: 4.789 - ETA: 1s - loss: 4.774 - ETA: 1s - loss: 4.760 - ETA: 1s - loss: 4.745 - ETA: 1s - loss: 4.731 - ETA: 1s - loss: 4.717 - ETA: 1s - loss: 4.703 - ETA: 1s - loss: 4.689 - ETA: 1s - loss: 4.675 - ETA: 1s - loss: 4.661 - ETA: 1s - loss: 4.647 - ETA: 1s - loss: 4.633 - ETA: 1s - loss: 4.619 - ETA: 0s - loss: 4.605 - ETA: 0s - loss: 4.592 - ETA: 0s - loss: 4.578 - ETA: 0s - loss: 4.564 - ETA: 0s - loss: 4.550 - ETA: 0s - loss: 4.537 - ETA: 0s - loss: 4.523 - ETA: 0s - loss: 4.510 - ETA: 0s - loss: 4.497 - ETA: 0s - loss: 4.483 - ETA: 0s - loss: 4.470 - ETA: 0s - loss: 4.457 - ETA: 0s - loss: 4.444 - ETA: 0s - loss: 4.431 - ETA: 0s - loss: 4.418 - ETA: 0s - loss: 4.405 - ETA: 0s - loss: 4.393 - 3s 483us/step - loss: 4.3906\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 3.755 - ETA: 2s - loss: 3.742 - ETA: 2s - loss: 3.733 - ETA: 2s - loss: 3.722 - ETA: 2s - loss: 3.712 - ETA: 2s - loss: 3.700 - ETA: 2s - loss: 3.690 - ETA: 2s - loss: 3.679 - ETA: 2s - loss: 3.668 - ETA: 2s - loss: 3.657 - ETA: 2s - loss: 3.646 - ETA: 2s - loss: 3.636 - ETA: 2s - loss: 3.625 - ETA: 1s - loss: 3.614 - ETA: 1s - loss: 3.603 - ETA: 1s - loss: 3.593 - ETA: 1s - loss: 3.583 - ETA: 1s - loss: 3.572 - ETA: 1s - loss: 3.562 - ETA: 1s - loss: 3.551 - ETA: 1s - loss: 3.541 - ETA: 1s - loss: 3.531 - ETA: 1s - loss: 3.520 - ETA: 1s - loss: 3.510 - ETA: 1s - loss: 3.500 - ETA: 1s - loss: 3.490 - ETA: 1s - loss: 3.480 - ETA: 1s - loss: 3.470 - ETA: 1s - loss: 3.460 - ETA: 1s - loss: 3.450 - ETA: 0s - loss: 3.440 - ETA: 0s - loss: 3.431 - ETA: 0s - loss: 3.421 - ETA: 0s - loss: 3.411 - ETA: 0s - loss: 3.402 - ETA: 0s - loss: 3.392 - ETA: 0s - loss: 3.382 - ETA: 0s - loss: 3.373 - ETA: 0s - loss: 3.363 - ETA: 0s - loss: 3.354 - ETA: 0s - loss: 3.344 - ETA: 0s - loss: 3.335 - ETA: 0s - loss: 3.325 - ETA: 0s - loss: 3.316 - ETA: 0s - loss: 3.307 - ETA: 0s - loss: 3.298 - ETA: 0s - loss: 3.289 - 3s 472us/step - loss: 3.2874\n",
      "Epoch 8/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 2.833 - ETA: 2s - loss: 2.824 - ETA: 2s - loss: 2.818 - ETA: 2s - loss: 2.809 - ETA: 2s - loss: 2.801 - ETA: 2s - loss: 2.793 - ETA: 2s - loss: 2.786 - ETA: 2s - loss: 2.778 - ETA: 2s - loss: 2.770 - ETA: 2s - loss: 2.762 - ETA: 2s - loss: 2.754 - ETA: 2s - loss: 2.747 - ETA: 2s - loss: 2.739 - ETA: 1s - loss: 2.732 - ETA: 1s - loss: 2.724 - ETA: 1s - loss: 2.716 - ETA: 1s - loss: 2.709 - ETA: 1s - loss: 2.701 - ETA: 1s - loss: 2.693 - ETA: 1s - loss: 2.686 - ETA: 1s - loss: 2.678 - ETA: 1s - loss: 2.671 - ETA: 1s - loss: 2.664 - ETA: 1s - loss: 2.656 - ETA: 1s - loss: 2.649 - ETA: 1s - loss: 2.642 - ETA: 1s - loss: 2.634 - ETA: 1s - loss: 2.627 - ETA: 1s - loss: 2.620 - ETA: 1s - loss: 2.613 - ETA: 0s - loss: 2.606 - ETA: 0s - loss: 2.599 - ETA: 0s - loss: 2.592 - ETA: 0s - loss: 2.585 - ETA: 0s - loss: 2.578 - ETA: 0s - loss: 2.571 - ETA: 0s - loss: 2.564 - ETA: 0s - loss: 2.557 - ETA: 0s - loss: 2.550 - ETA: 0s - loss: 2.543 - ETA: 0s - loss: 2.536 - ETA: 0s - loss: 2.529 - ETA: 0s - loss: 2.523 - ETA: 0s - loss: 2.516 - ETA: 0s - loss: 2.509 - ETA: 0s - loss: 2.503 - ETA: 0s - loss: 2.496 - 3s 470us/step - loss: 2.4951\n",
      "Epoch 9/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 2.166 - ETA: 2s - loss: 2.160 - ETA: 2s - loss: 2.154 - ETA: 2s - loss: 2.149 - ETA: 2s - loss: 2.143 - ETA: 2s - loss: 2.137 - ETA: 2s - loss: 2.132 - ETA: 2s - loss: 2.126 - ETA: 2s - loss: 2.120 - ETA: 2s - loss: 2.115 - ETA: 2s - loss: 2.109 - ETA: 2s - loss: 2.103 - ETA: 2s - loss: 2.098 - ETA: 1s - loss: 2.092 - ETA: 1s - loss: 2.086 - ETA: 1s - loss: 2.081 - ETA: 1s - loss: 2.075 - ETA: 1s - loss: 2.070 - ETA: 1s - loss: 2.064 - ETA: 1s - loss: 2.059 - ETA: 1s - loss: 2.054 - ETA: 1s - loss: 2.048 - ETA: 1s - loss: 2.043 - ETA: 1s - loss: 2.038 - ETA: 1s - loss: 2.032 - ETA: 1s - loss: 2.027 - ETA: 1s - loss: 2.022 - ETA: 1s - loss: 2.017 - ETA: 1s - loss: 2.011 - ETA: 1s - loss: 2.006 - ETA: 0s - loss: 2.001 - ETA: 0s - loss: 1.996 - ETA: 0s - loss: 1.991 - ETA: 0s - loss: 1.986 - ETA: 0s - loss: 1.980 - ETA: 0s - loss: 1.975 - ETA: 0s - loss: 1.970 - ETA: 0s - loss: 1.965 - ETA: 0s - loss: 1.960 - ETA: 0s - loss: 1.955 - ETA: 0s - loss: 1.950 - ETA: 0s - loss: 1.945 - ETA: 0s - loss: 1.940 - ETA: 0s - loss: 1.935 - ETA: 0s - loss: 1.931 - ETA: 0s - loss: 1.926 - ETA: 0s - loss: 1.921 - 3s 469us/step - loss: 1.9203\n",
      "Epoch 10/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 1.679 - ETA: 2s - loss: 1.674 - ETA: 2s - loss: 1.671 - ETA: 2s - loss: 1.667 - ETA: 2s - loss: 1.663 - ETA: 2s - loss: 1.659 - ETA: 2s - loss: 1.654 - ETA: 2s - loss: 1.650 - ETA: 2s - loss: 1.646 - ETA: 2s - loss: 1.641 - ETA: 2s - loss: 1.637 - ETA: 2s - loss: 1.633 - ETA: 2s - loss: 1.629 - ETA: 1s - loss: 1.625 - ETA: 1s - loss: 1.621 - ETA: 1s - loss: 1.617 - ETA: 1s - loss: 1.612 - ETA: 1s - loss: 1.608 - ETA: 1s - loss: 1.604 - ETA: 1s - loss: 1.600 - ETA: 1s - loss: 1.596 - ETA: 1s - loss: 1.592 - ETA: 1s - loss: 1.588 - ETA: 1s - loss: 1.584 - ETA: 1s - loss: 1.580 - ETA: 1s - loss: 1.577 - ETA: 1s - loss: 1.573 - ETA: 1s - loss: 1.569 - ETA: 1s - loss: 1.565 - ETA: 1s - loss: 1.561 - ETA: 0s - loss: 1.557 - ETA: 0s - loss: 1.553 - ETA: 0s - loss: 1.549 - ETA: 0s - loss: 1.546 - ETA: 0s - loss: 1.542 - ETA: 0s - loss: 1.538 - ETA: 0s - loss: 1.534 - ETA: 0s - loss: 1.531 - ETA: 0s - loss: 1.527 - ETA: 0s - loss: 1.523 - ETA: 0s - loss: 1.520 - ETA: 0s - loss: 1.516 - ETA: 0s - loss: 1.512 - ETA: 0s - loss: 1.509 - ETA: 0s - loss: 1.505 - ETA: 0s - loss: 1.501 - ETA: 0s - loss: 1.498 - 3s 475us/step - loss: 1.4975\n",
      "Epoch 11/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 1.321 - ETA: 2s - loss: 1.316 - ETA: 2s - loss: 1.312 - ETA: 2s - loss: 1.309 - ETA: 2s - loss: 1.306 - ETA: 2s - loss: 1.303 - ETA: 2s - loss: 1.299 - ETA: 2s - loss: 1.296 - ETA: 2s - loss: 1.293 - ETA: 2s - loss: 1.290 - ETA: 2s - loss: 1.286 - ETA: 2s - loss: 1.284 - ETA: 2s - loss: 1.280 - ETA: 1s - loss: 1.277 - ETA: 1s - loss: 1.274 - ETA: 1s - loss: 1.271 - ETA: 1s - loss: 1.268 - ETA: 1s - loss: 1.265 - ETA: 1s - loss: 1.262 - ETA: 1s - loss: 1.259 - ETA: 1s - loss: 1.256 - ETA: 1s - loss: 1.253 - ETA: 1s - loss: 1.250 - ETA: 1s - loss: 1.246 - ETA: 1s - loss: 1.243 - ETA: 1s - loss: 1.241 - ETA: 1s - loss: 1.238 - ETA: 1s - loss: 1.235 - ETA: 1s - loss: 1.232 - ETA: 1s - loss: 1.229 - ETA: 0s - loss: 1.226 - ETA: 0s - loss: 1.223 - ETA: 0s - loss: 1.220 - ETA: 0s - loss: 1.217 - ETA: 0s - loss: 1.214 - ETA: 0s - loss: 1.211 - ETA: 0s - loss: 1.209 - ETA: 0s - loss: 1.206 - ETA: 0s - loss: 1.203 - ETA: 0s - loss: 1.200 - ETA: 0s - loss: 1.197 - ETA: 0s - loss: 1.195 - ETA: 0s - loss: 1.192 - ETA: 0s - loss: 1.189 - ETA: 0s - loss: 1.186 - ETA: 0s - loss: 1.184 - ETA: 0s - loss: 1.181 - 3s 476us/step - loss: 1.1809\n",
      "Epoch 12/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 1.045 - ETA: 2s - loss: 1.043 - ETA: 2s - loss: 1.041 - ETA: 2s - loss: 1.038 - ETA: 2s - loss: 1.035 - ETA: 2s - loss: 1.033 - ETA: 2s - loss: 1.030 - ETA: 2s - loss: 1.028 - ETA: 2s - loss: 1.025 - ETA: 2s - loss: 1.023 - ETA: 2s - loss: 1.021 - ETA: 2s - loss: 1.018 - ETA: 2s - loss: 1.016 - ETA: 1s - loss: 1.013 - ETA: 1s - loss: 1.011 - ETA: 1s - loss: 1.009 - ETA: 1s - loss: 1.007 - ETA: 1s - loss: 1.004 - ETA: 1s - loss: 1.002 - ETA: 1s - loss: 0.999 - ETA: 1s - loss: 0.997 - ETA: 1s - loss: 0.995 - ETA: 1s - loss: 0.993 - ETA: 1s - loss: 0.990 - ETA: 1s - loss: 0.988 - ETA: 1s - loss: 0.986 - ETA: 1s - loss: 0.983 - ETA: 1s - loss: 0.981 - ETA: 1s - loss: 0.979 - ETA: 1s - loss: 0.977 - ETA: 0s - loss: 0.974 - ETA: 0s - loss: 0.972 - ETA: 0s - loss: 0.970 - ETA: 0s - loss: 0.968 - ETA: 0s - loss: 0.965 - ETA: 0s - loss: 0.963 - ETA: 0s - loss: 0.961 - ETA: 0s - loss: 0.959 - ETA: 0s - loss: 0.956 - ETA: 0s - loss: 0.954 - ETA: 0s - loss: 0.952 - ETA: 0s - loss: 0.950 - ETA: 0s - loss: 0.948 - ETA: 0s - loss: 0.946 - ETA: 0s - loss: 0.944 - ETA: 0s - loss: 0.941 - ETA: 0s - loss: 0.939 - 3s 474us/step - loss: 0.9394\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.834 - ETA: 2s - loss: 0.832 - ETA: 2s - loss: 0.829 - ETA: 2s - loss: 0.827 - ETA: 2s - loss: 0.826 - ETA: 2s - loss: 0.824 - ETA: 2s - loss: 0.822 - ETA: 2s - loss: 0.821 - ETA: 2s - loss: 0.819 - ETA: 2s - loss: 0.817 - ETA: 2s - loss: 0.815 - ETA: 2s - loss: 0.813 - ETA: 2s - loss: 0.811 - ETA: 1s - loss: 0.809 - ETA: 1s - loss: 0.807 - ETA: 1s - loss: 0.806 - ETA: 1s - loss: 0.804 - ETA: 1s - loss: 0.802 - ETA: 1s - loss: 0.800 - ETA: 1s - loss: 0.798 - ETA: 1s - loss: 0.796 - ETA: 1s - loss: 0.795 - ETA: 1s - loss: 0.793 - ETA: 1s - loss: 0.791 - ETA: 1s - loss: 0.789 - ETA: 1s - loss: 0.788 - ETA: 1s - loss: 0.786 - ETA: 1s - loss: 0.784 - ETA: 1s - loss: 0.782 - ETA: 1s - loss: 0.781 - ETA: 0s - loss: 0.779 - ETA: 0s - loss: 0.777 - ETA: 0s - loss: 0.775 - ETA: 0s - loss: 0.774 - ETA: 0s - loss: 0.772 - ETA: 0s - loss: 0.770 - ETA: 0s - loss: 0.769 - ETA: 0s - loss: 0.767 - ETA: 0s - loss: 0.765 - ETA: 0s - loss: 0.764 - ETA: 0s - loss: 0.762 - ETA: 0s - loss: 0.760 - ETA: 0s - loss: 0.758 - ETA: 0s - loss: 0.757 - ETA: 0s - loss: 0.755 - ETA: 0s - loss: 0.753 - ETA: 0s - loss: 0.752 - 3s 478us/step - loss: 0.7519\n",
      "Epoch 14/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.667 - ETA: 2s - loss: 0.667 - ETA: 2s - loss: 0.666 - ETA: 2s - loss: 0.665 - ETA: 2s - loss: 0.663 - ETA: 2s - loss: 0.661 - ETA: 2s - loss: 0.660 - ETA: 2s - loss: 0.659 - ETA: 2s - loss: 0.657 - ETA: 2s - loss: 0.656 - ETA: 2s - loss: 0.654 - ETA: 2s - loss: 0.653 - ETA: 2s - loss: 0.651 - ETA: 2s - loss: 0.650 - ETA: 1s - loss: 0.648 - ETA: 1s - loss: 0.647 - ETA: 1s - loss: 0.646 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.643 - ETA: 1s - loss: 0.641 - ETA: 1s - loss: 0.640 - ETA: 1s - loss: 0.639 - ETA: 1s - loss: 0.637 - ETA: 1s - loss: 0.636 - ETA: 1s - loss: 0.634 - ETA: 1s - loss: 0.633 - ETA: 1s - loss: 0.631 - ETA: 1s - loss: 0.630 - ETA: 1s - loss: 0.629 - ETA: 1s - loss: 0.627 - ETA: 0s - loss: 0.626 - ETA: 0s - loss: 0.624 - ETA: 0s - loss: 0.623 - ETA: 0s - loss: 0.621 - ETA: 0s - loss: 0.620 - ETA: 0s - loss: 0.619 - ETA: 0s - loss: 0.617 - ETA: 0s - loss: 0.616 - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.613 - ETA: 0s - loss: 0.612 - ETA: 0s - loss: 0.611 - ETA: 0s - loss: 0.609 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.607 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.604 - 3s 478us/step - loss: 0.6039\n",
      "Epoch 15/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.535 - ETA: 2s - loss: 0.536 - ETA: 2s - loss: 0.535 - ETA: 2s - loss: 0.534 - ETA: 2s - loss: 0.533 - ETA: 2s - loss: 0.532 - ETA: 2s - loss: 0.531 - ETA: 2s - loss: 0.529 - ETA: 2s - loss: 0.528 - ETA: 2s - loss: 0.527 - ETA: 2s - loss: 0.526 - ETA: 2s - loss: 0.525 - ETA: 2s - loss: 0.524 - ETA: 2s - loss: 0.523 - ETA: 1s - loss: 0.522 - ETA: 1s - loss: 0.521 - ETA: 1s - loss: 0.519 - ETA: 1s - loss: 0.518 - ETA: 1s - loss: 0.517 - ETA: 1s - loss: 0.516 - ETA: 1s - loss: 0.515 - ETA: 1s - loss: 0.514 - ETA: 1s - loss: 0.512 - ETA: 1s - loss: 0.511 - ETA: 1s - loss: 0.510 - ETA: 1s - loss: 0.509 - ETA: 1s - loss: 0.508 - ETA: 1s - loss: 0.507 - ETA: 1s - loss: 0.506 - ETA: 1s - loss: 0.504 - ETA: 0s - loss: 0.503 - ETA: 0s - loss: 0.502 - ETA: 0s - loss: 0.501 - ETA: 0s - loss: 0.500 - ETA: 0s - loss: 0.499 - ETA: 0s - loss: 0.497 - ETA: 0s - loss: 0.496 - ETA: 0s - loss: 0.495 - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.493 - ETA: 0s - loss: 0.492 - ETA: 0s - loss: 0.491 - ETA: 0s - loss: 0.490 - ETA: 0s - loss: 0.489 - ETA: 0s - loss: 0.488 - ETA: 0s - loss: 0.487 - ETA: 0s - loss: 0.486 - 3s 477us/step - loss: 0.4859\n",
      "Epoch 16/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.435 - ETA: 2s - loss: 0.434 - ETA: 2s - loss: 0.432 - ETA: 2s - loss: 0.431 - ETA: 2s - loss: 0.430 - ETA: 2s - loss: 0.430 - ETA: 2s - loss: 0.428 - ETA: 2s - loss: 0.427 - ETA: 2s - loss: 0.426 - ETA: 2s - loss: 0.425 - ETA: 2s - loss: 0.424 - ETA: 2s - loss: 0.423 - ETA: 2s - loss: 0.422 - ETA: 2s - loss: 0.421 - ETA: 1s - loss: 0.420 - ETA: 1s - loss: 0.419 - ETA: 1s - loss: 0.418 - ETA: 1s - loss: 0.417 - ETA: 1s - loss: 0.416 - ETA: 1s - loss: 0.415 - ETA: 1s - loss: 0.414 - ETA: 1s - loss: 0.413 - ETA: 1s - loss: 0.412 - ETA: 1s - loss: 0.411 - ETA: 1s - loss: 0.410 - ETA: 1s - loss: 0.409 - ETA: 1s - loss: 0.409 - ETA: 1s - loss: 0.408 - ETA: 1s - loss: 0.407 - ETA: 1s - loss: 0.406 - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.403 - ETA: 0s - loss: 0.402 - ETA: 0s - loss: 0.401 - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.398 - ETA: 0s - loss: 0.397 - ETA: 0s - loss: 0.396 - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.394 - ETA: 0s - loss: 0.393 - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.391 - 3s 476us/step - loss: 0.3910\n",
      "Epoch 17/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.351 - ETA: 2s - loss: 0.349 - ETA: 2s - loss: 0.348 - ETA: 2s - loss: 0.347 - ETA: 2s - loss: 0.346 - ETA: 2s - loss: 0.345 - ETA: 2s - loss: 0.344 - ETA: 2s - loss: 0.343 - ETA: 2s - loss: 0.342 - ETA: 2s - loss: 0.342 - ETA: 2s - loss: 0.341 - ETA: 2s - loss: 0.340 - ETA: 2s - loss: 0.339 - ETA: 2s - loss: 0.338 - ETA: 1s - loss: 0.337 - ETA: 1s - loss: 0.336 - ETA: 1s - loss: 0.335 - ETA: 1s - loss: 0.335 - ETA: 1s - loss: 0.334 - ETA: 1s - loss: 0.333 - ETA: 1s - loss: 0.332 - ETA: 1s - loss: 0.332 - ETA: 1s - loss: 0.331 - ETA: 1s - loss: 0.330 - ETA: 1s - loss: 0.330 - ETA: 1s - loss: 0.329 - ETA: 1s - loss: 0.328 - ETA: 1s - loss: 0.327 - ETA: 1s - loss: 0.327 - ETA: 1s - loss: 0.326 - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.325 - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.320 - ETA: 0s - loss: 0.319 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.316 - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.314 - 3s 478us/step - loss: 0.3143\n",
      "Epoch 18/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.279 - ETA: 2s - loss: 0.278 - ETA: 2s - loss: 0.278 - ETA: 2s - loss: 0.278 - ETA: 2s - loss: 0.277 - ETA: 2s - loss: 0.277 - ETA: 2s - loss: 0.276 - ETA: 2s - loss: 0.275 - ETA: 2s - loss: 0.274 - ETA: 2s - loss: 0.274 - ETA: 2s - loss: 0.273 - ETA: 2s - loss: 0.273 - ETA: 2s - loss: 0.272 - ETA: 1s - loss: 0.271 - ETA: 1s - loss: 0.271 - ETA: 1s - loss: 0.270 - ETA: 1s - loss: 0.270 - ETA: 1s - loss: 0.269 - ETA: 1s - loss: 0.268 - ETA: 1s - loss: 0.268 - ETA: 1s - loss: 0.267 - ETA: 1s - loss: 0.266 - ETA: 1s - loss: 0.266 - ETA: 1s - loss: 0.265 - ETA: 1s - loss: 0.265 - ETA: 1s - loss: 0.264 - ETA: 1s - loss: 0.264 - ETA: 1s - loss: 0.263 - ETA: 1s - loss: 0.262 - ETA: 1s - loss: 0.262 - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.257 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.255 - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.252 - 3s 478us/step - loss: 0.2524\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.222 - ETA: 2s - loss: 0.223 - ETA: 2s - loss: 0.222 - ETA: 2s - loss: 0.222 - ETA: 2s - loss: 0.222 - ETA: 2s - loss: 0.221 - ETA: 2s - loss: 0.221 - ETA: 2s - loss: 0.220 - ETA: 2s - loss: 0.220 - ETA: 2s - loss: 0.219 - ETA: 2s - loss: 0.219 - ETA: 2s - loss: 0.218 - ETA: 2s - loss: 0.218 - ETA: 2s - loss: 0.217 - ETA: 1s - loss: 0.217 - ETA: 1s - loss: 0.217 - ETA: 1s - loss: 0.216 - ETA: 1s - loss: 0.216 - ETA: 1s - loss: 0.215 - ETA: 1s - loss: 0.215 - ETA: 1s - loss: 0.214 - ETA: 1s - loss: 0.214 - ETA: 1s - loss: 0.213 - ETA: 1s - loss: 0.213 - ETA: 1s - loss: 0.213 - ETA: 1s - loss: 0.212 - ETA: 1s - loss: 0.212 - ETA: 1s - loss: 0.211 - ETA: 1s - loss: 0.211 - ETA: 1s - loss: 0.210 - ETA: 0s - loss: 0.210 - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.202 - 3s 480us/step - loss: 0.2024\n",
      "Epoch 20/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.184 - ETA: 2s - loss: 0.183 - ETA: 2s - loss: 0.181 - ETA: 2s - loss: 0.181 - ETA: 2s - loss: 0.180 - ETA: 2s - loss: 0.180 - ETA: 2s - loss: 0.179 - ETA: 2s - loss: 0.178 - ETA: 2s - loss: 0.177 - ETA: 2s - loss: 0.176 - ETA: 2s - loss: 0.176 - ETA: 2s - loss: 0.176 - ETA: 2s - loss: 0.175 - ETA: 2s - loss: 0.175 - ETA: 1s - loss: 0.174 - ETA: 1s - loss: 0.174 - ETA: 1s - loss: 0.173 - ETA: 1s - loss: 0.173 - ETA: 1s - loss: 0.172 - ETA: 1s - loss: 0.172 - ETA: 1s - loss: 0.171 - ETA: 1s - loss: 0.171 - ETA: 1s - loss: 0.171 - ETA: 1s - loss: 0.170 - ETA: 1s - loss: 0.170 - ETA: 1s - loss: 0.169 - ETA: 1s - loss: 0.169 - ETA: 1s - loss: 0.169 - ETA: 1s - loss: 0.168 - ETA: 1s - loss: 0.168 - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.165 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.162 - 3s 478us/step - loss: 0.1622\n",
      "Epoch 21/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.147 - ETA: 2s - loss: 0.145 - ETA: 2s - loss: 0.144 - ETA: 2s - loss: 0.143 - ETA: 2s - loss: 0.143 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.142 - ETA: 2s - loss: 0.141 - ETA: 2s - loss: 0.141 - ETA: 2s - loss: 0.141 - ETA: 2s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.140 - ETA: 1s - loss: 0.139 - ETA: 1s - loss: 0.139 - ETA: 1s - loss: 0.139 - ETA: 1s - loss: 0.138 - ETA: 1s - loss: 0.138 - ETA: 1s - loss: 0.138 - ETA: 1s - loss: 0.137 - ETA: 1s - loss: 0.137 - ETA: 1s - loss: 0.137 - ETA: 1s - loss: 0.136 - ETA: 1s - loss: 0.136 - ETA: 1s - loss: 0.136 - ETA: 1s - loss: 0.135 - ETA: 1s - loss: 0.135 - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.132 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.130 - 3s 479us/step - loss: 0.1300\n",
      "Epoch 22/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.116 - ETA: 2s - loss: 0.114 - ETA: 2s - loss: 0.115 - ETA: 2s - loss: 0.115 - ETA: 2s - loss: 0.114 - ETA: 2s - loss: 0.114 - ETA: 2s - loss: 0.113 - ETA: 2s - loss: 0.113 - ETA: 2s - loss: 0.113 - ETA: 2s - loss: 0.113 - ETA: 2s - loss: 0.113 - ETA: 2s - loss: 0.113 - ETA: 2s - loss: 0.112 - ETA: 2s - loss: 0.112 - ETA: 1s - loss: 0.112 - ETA: 1s - loss: 0.111 - ETA: 1s - loss: 0.111 - ETA: 1s - loss: 0.111 - ETA: 1s - loss: 0.110 - ETA: 1s - loss: 0.110 - ETA: 1s - loss: 0.110 - ETA: 1s - loss: 0.110 - ETA: 1s - loss: 0.110 - ETA: 1s - loss: 0.109 - ETA: 1s - loss: 0.109 - ETA: 1s - loss: 0.109 - ETA: 1s - loss: 0.109 - ETA: 1s - loss: 0.109 - ETA: 1s - loss: 0.108 - ETA: 1s - loss: 0.108 - ETA: 0s - loss: 0.108 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.107 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.104 - 3s 478us/step - loss: 0.1043\n",
      "Epoch 23/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.090 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.090 - ETA: 2s - loss: 0.090 - ETA: 2s - loss: 0.089 - ETA: 2s - loss: 0.089 - ETA: 2s - loss: 0.089 - ETA: 2s - loss: 0.089 - ETA: 1s - loss: 0.089 - ETA: 1s - loss: 0.089 - ETA: 1s - loss: 0.089 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.087 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.084 - 3s 476us/step - loss: 0.0840\n",
      "Epoch 24/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.072 - ETA: 2s - loss: 0.072 - ETA: 2s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.072 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.070 - ETA: 1s - loss: 0.070 - ETA: 1s - loss: 0.070 - ETA: 1s - loss: 0.070 - ETA: 1s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 3s 476us/step - loss: 0.0681\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.059 - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.061 - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.059 - ETA: 2s - loss: 0.059 - ETA: 2s - loss: 0.059 - ETA: 2s - loss: 0.059 - ETA: 1s - loss: 0.059 - ETA: 1s - loss: 0.059 - ETA: 1s - loss: 0.059 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.058 - ETA: 1s - loss: 0.057 - ETA: 1s - loss: 0.057 - ETA: 1s - loss: 0.057 - ETA: 1s - loss: 0.057 - ETA: 1s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.057 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.056 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - ETA: 0s - loss: 0.055 - 3s 512us/step - loss: 0.0556\n",
      "Epoch 26/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.052 - ETA: 2s - loss: 0.050 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.048 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.049 - ETA: 2s - loss: 0.048 - ETA: 2s - loss: 0.048 - ETA: 2s - loss: 0.048 - ETA: 2s - loss: 0.048 - ETA: 2s - loss: 0.048 - ETA: 2s - loss: 0.048 - ETA: 1s - loss: 0.048 - ETA: 1s - loss: 0.048 - ETA: 1s - loss: 0.048 - ETA: 1s - loss: 0.048 - ETA: 1s - loss: 0.048 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 1s - loss: 0.047 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.046 - ETA: 0s - loss: 0.045 - ETA: 0s - loss: 0.045 - 3s 511us/step - loss: 0.0459\n",
      "Epoch 27/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.042 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 2s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.041 - ETA: 1s - loss: 0.041 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.039 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.038 - 3s 477us/step - loss: 0.0384\n",
      "Epoch 28/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.037 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.036 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.035 - ETA: 2s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 2s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.034 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 1s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.033 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - ETA: 0s - loss: 0.032 - 3s 480us/step - loss: 0.0327\n",
      "Epoch 29/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.030 - ETA: 2s - loss: 0.029 - ETA: 2s - loss: 0.029 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.028 - ETA: 2s - loss: 0.029 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 1s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - 3s 481us/step - loss: 0.0284\n",
      "Epoch 30/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.026 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.026 - ETA: 2s - loss: 0.026 - ETA: 2s - loss: 0.026 - ETA: 2s - loss: 0.026 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.026 - ETA: 2s - loss: 0.026 - ETA: 1s - loss: 0.026 - ETA: 1s - loss: 0.026 - ETA: 1s - loss: 0.026 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.026 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - ETA: 0s - loss: 0.025 - 3s 480us/step - loss: 0.0251\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 1s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - 3s 478us/step - loss: 0.0227\n",
      "Epoch 32/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.022 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 1s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.021 - ETA: 0s - loss: 0.020 - 3s 496us/step - loss: 0.0209\n",
      "Epoch 33/50\n",
      "6040/6040 [==============================] - ETA: 3s - loss: 0.018 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.021 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - 3s 541us/step - loss: 0.0196\n",
      "Epoch 34/50\n",
      "6040/6040 [==============================] - ETA: 3s - loss: 0.022 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.021 - ETA: 3s - loss: 0.020 - ETA: 3s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - 3s 520us/step - loss: 0.0187\n",
      "Epoch 35/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - 3s 515us/step - loss: 0.0180\n",
      "Epoch 36/50\n",
      "2688/6040 [============>.................] - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 2s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.0175"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=X, y=y,\n",
    "                  epochs=50,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict new Matrix Interactions, set score zero on visualized games\n",
    "# new_matrix = model.predict(X) * (X == 0)\n",
    "new_matrix = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0. , -0. , -0. , ...,  0. ,  0. ,  0. ],\n",
       "       [ 0.1,  0. ,  0. , ...,  0. , -0. ,  0. ],\n",
       "       [ 0. , -0. ,  0. , ...,  0. ,  0. , -0. ],\n",
       "       ...,\n",
       "       [ 0. , -0. , -0. , ...,  0. ,  0. ,  0. ],\n",
       "       [ 0.1,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
       "       [ 0.1, -0. ,  0. , ..., -0. ,  0. ,  0. ]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # converting the reconstructed matrix back to a Pandas dataframe\n",
    "# new_users_items_matrix_df  = pd.DataFrame(new_matrix, \n",
    "#                                           columns = users_items_matrix_df.columns, \n",
    "#                                           index   = users_items_matrix_df.index)\n",
    "# new_users_items_matrix_df.head()\n",
    "\n",
    "new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148533"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_matrix>0.3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(382567, device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(negative_feedback_mask.round() > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0. , -0. , -0. , -0. , -0. , -0. , -0. , -0. , -0. , -0. ,  0. ,\n",
       "         0. ,  0. , -0. ,  0. , -0. ,  0. ,  0. , -0. , -0. ],\n",
       "       [ 0.1,  0. ,  0. ,  0. , -0. ,  0.1,  0. , -0. ,  0. ,  0.1,  0. ,\n",
       "        -0. ,  0. ,  0. ,  0.1,  0.1,  0. ,  0. , -0. ,  0.1],\n",
       "       [ 0. , -0. ,  0. , -0. , -0. ,  0. , -0. ,  0. , -0. ,  0. ,  0. ,\n",
       "         0. , -0. , -0. ,  0. , -0. , -0. , -0. , -0. ,  0. ],\n",
       "       [ 0. ,  0. , -0. ,  0. ,  0. , -0. , -0. , -0. , -0. , -0. , -0. ,\n",
       "         0. , -0. , -0. ,  0. , -0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0.1, -0. , -0. ,  0. ,  0. ,  0.1,  0. , -0. ,  0. ,  0.1,  0.1,\n",
       "         0. ,  0. ,  0. ,  0. ,  0.2,  0.1,  0. , -0. , -0. ],\n",
       "       [ 0. , -0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ,  0. ,  0. , -0. ,  0. ,  0. , -0. , -0. , -0. ],\n",
       "       [ 0. , -0. , -0. , -0. ,  0. ,  0. , -0. , -0. ,  0. , -0. , -0. ,\n",
       "         0. , -0. , -0. ,  0. ,  0. , -0. ,  0. , -0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0.1,  0. ,  0. ,  0. , -0. ,  0. ,  0.1,  0.1,\n",
       "         0. , -0. ,  0. ,  0. ,  0.1,  0.1,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0. ,  0. ,  0.1,  0. ,\n",
       "        -0. , -0. ,  0. ,  0. ,  0.1,  0. , -0. ,  0. ,  0. ],\n",
       "       [ 0.1,  0. ,  0.1,  0. ,  0.1,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. , -0. ,  0. ,  0. , -0. ,  0. , -0. ,  0.1,  0. ],\n",
       "       [ 0.1, -0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0. ,  0. , -0. ,  0.1,\n",
       "         0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0.1, -0. ],\n",
       "       [ 0. ,  0. , -0. , -0. , -0. ,  0. , -0. ,  0. ,  0. , -0. , -0. ,\n",
       "         0. ,  0. , -0. ,  0. , -0. , -0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0.2,  0. , -0. ,  0. ,  0. , -0. ,  0. ,  0. ,  0.2,  0. ,\n",
       "         0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0. ,  0. ,  0. ],\n",
       "       [ 0. , -0. , -0. , -0. ,  0. ,  0. , -0. , -0. , -0. , -0. , -0. ,\n",
       "         0. ,  0. , -0. ,  0. , -0. , -0. ,  0. , -0. ,  0. ],\n",
       "       [ 0.1,  0.1,  0. ,  0. ,  0. ,  0.1,  0.1,  0. ,  0. ,  0.1,  0. ,\n",
       "        -0. , -0. ,  0. , -0. ,  0.1,  0. ,  0. ,  0. ,  0. ],\n",
       "       [-0. ,  0. , -0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0. , -0. ,\n",
       "         0. ,  0. ,  0. ,  0. , -0. ,  0. , -0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0. , -0. , -0. , -0. ,\n",
       "        -0. ,  0. , -0. ,  0. ,  0. ,  0. , -0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0.2,  0. ,  0. ,  0. ,  0.1,  0.1,  0. ,  0. ,  0.2,  0.1,\n",
       "         0. ,  0. ,  0. ,  0.1, -0. , -0. ,  0. ,  0.1,  0.1],\n",
       "       [ 0.1,  0.2,  0. , -0. ,  0. ,  0.1, -0.1,  0. ,  0. ,  0.1, -0. ,\n",
       "         0.1,  0. ,  0. , -0. ,  0. , -0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. , -0. , -0. ,  0. , -0. , -0. , -0. , -0. , -0. , -0. ,\n",
       "         0. , -0. ,  0. ,  0. , -0. , -0. ,  0. ,  0. ,  0. ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=1)\n",
    "new_matrix[:20, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_feedback_mask.round()[:20, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matrix_2 = model.predict(negative_feedback_mask.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304668"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_matrix_2>0.3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35029"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_matrix>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7287675925661594"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((negative_feedback_mask.cpu().numpy() * (new_matrix>0.5)) == 1).sum()/(new_matrix>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
