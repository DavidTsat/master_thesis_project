{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 5.6060099601745605 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "############## Pytorch model doesn't converge - to do - check #################\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.000001))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(512, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.000001))(enc)\n",
    "    lat_space = Dropout(0.5, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.000001))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.000001))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0)\n",
    "positive_feedback_mask = (train > 3)\n",
    "negative_feedback_mask = ((train < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback_mask + negative_feedback_mask != zero_mask).all()\n",
    "assert (positive_feedback_mask + negative_feedback_mask == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95.97847414073473, 2.3124349989099473, 1.7090908603553212)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback_mask), get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265267, 2.3124349989099473, 1.7090908603553212, 4.021525859265268)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - get_sparsity(zero_mask), get_sparsity(positive_feedback_mask), get_sparsity(negative_feedback_mask), get_sparsity(positive_feedback_mask) + get_sparsity(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.4\n",
    "mask_arr_neg = (np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)\n",
    "y_neg = negative_feedback_mask\n",
    "X_neg = negative_feedback_mask*mask_arr_neg # corrupting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr_pos = (np.random.rand(positive_feedback_mask.shape[0], positive_feedback_mask.shape[1]) > P)\n",
    "y_pos = positive_feedback_mask\n",
    "X_pos = positive_feedback_mask*mask_arr_pos # corrupting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.026195215919772, 1.7090908603553212, 1.3870741200058612, 2.3124349989099473)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(X_neg), get_sparsity(y_neg), get_sparsity(X_pos), get_sparsity(y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg = autoEncoder(X_neg)\n",
    "model_neg.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "\n",
    "model_pos = autoEncoder(X_pos)\n",
    "model_pos.compile(optimizer = Adam(lr=0.0001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg = keras.models.load_model('./model_neg')\n",
    "model_pos = keras.models.load_model('./model_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "UserScore (InputLayer)       [(None, 3706)]            0         \n",
      "_________________________________________________________________\n",
      "EncLayer1 (Dense)            (None, 512)               1897984   \n",
      "_________________________________________________________________\n",
      "LatentSpace (Dense)          (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "DecLayer1 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "UserScorePred (Dense)        (None, 3706)              1901178   \n",
      "=================================================================\n",
      "Total params: 4,324,474\n",
      "Trainable params: 4,324,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_neg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_tr = np.load('predicted_tr.npy')\n",
    "# augmented_train = np.load('augmented_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6040 samples\n",
      "Epoch 1/300\n",
      "6040/6040 [==============================] - ETA: 31s - loss: 0.01 - ETA: 11s - loss: 0.01 - ETA: 7s - loss: 0.0117 - ETA: 5s - loss: 0.011 - ETA: 4s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 3s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 437us/sample - loss: 0.0114\n",
      "Epoch 2/300\n",
      "6040/6040 [==============================] - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 2s 326us/sample - loss: 0.0114\n",
      "Epoch 3/300\n",
      " 640/6040 [==>...........................] - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.0109"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-aa88de048e02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# augmented_train = np.load('augmented_train.npy')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                   validation_split=0.0)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    492\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist_neg = model_neg.fit(x=X_neg, y=y_neg,\n",
    "                  epochs=300,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "# augmented_train = np.load('augmented_train.npy')\n",
    "                  validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6040 samples\n",
      "Epoch 1/300\n",
      "6040/6040 [==============================] - ETA: 31s - loss: 0.01 - ETA: 11s - loss: 0.01 - ETA: 7s - loss: 0.0124 - ETA: 5s - loss: 0.012 - ETA: 4s - loss: 0.013 - ETA: 3s - loss: 0.012 - ETA: 3s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 3s 423us/sample - loss: 0.0134\n",
      "Epoch 2/300\n",
      "6016/6040 [============================>.] - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.0134"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8f3e73194fbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                   validation_split=0.0)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    492\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist_pos = model_pos.fit(x=X_pos, y=y_pos,\n",
    "                  epochs=300,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_neg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-5451e604f110>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_hist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_neg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'hist_neg' is not defined"
     ]
    }
   ],
   "source": [
    "plot_hist(hist_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(hist_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg.save('./model_neg')\n",
    "model_pos.save('./model_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "predicted_neg = model_neg.predict(X_neg)\n",
    "predicted_pos = model_pos.predict(X_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170800, 382567)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_neg > 0.4).sum(), (y_neg == 1).sum() # predicted vs real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116139, 216900)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_neg>0.5).sum(), (predicted_pos>0.5).sum() # trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9506147540983606\n",
      "0.9162219462901074\n"
     ]
    }
   ],
   "source": [
    "print((y_neg * (predicted_neg>0.4)).sum()/(predicted_neg>0.4).sum()) # accuracy on actual \n",
    "print((y_pos * (predicted_pos>0.4)).sum()/(predicted_pos>0.4).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004997865233443002\n",
      "0.017272791432611803\n"
     ]
    }
   ],
   "source": [
    "print((y_pos * (predicted_neg>0.4)).sum()/(y_pos>0.4).sum()) # just to see that it's a low number\n",
    "print((y_neg * (predicted_pos>0.4)).sum()/(y_neg>0.4).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846046547671324"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_neg * (predicted_neg>0.5)).sum()/((predicted_neg>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9687551867219917"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pos * (predicted_pos>0.5)).sum()/((predicted_pos>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39945"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((predicted_neg>0.5)  * (X_neg<0.5)).sum() # predicted values which were not in the train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71113"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((predicted_pos>0.5)  * (X_pos<0.5)).sum() # predicted values which were not in the train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9552384528726999"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_neg * (((predicted_neg>0.5)  * (X_neg<0.5)))) == 1).sum()/(((predicted_neg>0.5)  * (X_neg<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9047009688805141"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_pos * (((predicted_pos>0.5)  * (X_pos<0.5)))) == 1).sum()/(((predicted_pos>0.5)  * (X_pos<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add_negative = model_neg.predict(y_neg)\n",
    "to_add_positive = model_neg.predict(y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208239, 223363)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(to_add_negative>0.5).sum(), (to_add_positive>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9859022292187896, 0.9160156677389002)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_neg* (to_add_negative>0.8)).sum()/(((to_add_negative>0.8)).sum()), (y_pos* (to_add_positive>0.8)).sum()/(((to_add_positive>0.8)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep the balance\n",
    "threshold_neg = 0.2\n",
    "threshold_pos = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348459, 224127)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((to_add_negative > threshold_neg) * (tr==0)).sum(), ((to_add_positive > threshold_pos) * (tr==0)).sum() # new values # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13207882540835983, 0.25313474502505445, 0.6147864295665857]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_probs_neg = [(tr == 1).sum()/((tr > 0) & (tr < 4)).sum(), (tr == 2).sum()/(((tr > 0) & (tr < 4))).sum(), (tr == 3).sum()/((tr > 0) & (tr < 4)).sum()]\n",
    "p_probs_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6064147320143503, 0.39358526798564974]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_probs_pos = [(tr == 4).sum()/((tr > 3) & (tr <= 5)).sum(), (tr == 5).sum()/((tr > 3) & (tr <= 5)).sum()]\n",
    "p_probs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = tr + (to_add_negative > threshold_neg) * (tr == 0) * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs_neg) + (to_add_positive > threshold_pos) * (tr == 0) * np.random.choice(np.arange(4, 6), tr.shape, p=p_probs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 6.3707367326297435)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr), get_sparsity(augmented_train) # reduced sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.isin(tr, augmented_train)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 203728\n",
      "4 313893\n",
      "3 235197\n",
      "2 96841\n",
      "1 50529\n",
      "0 21484052\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == tr).sum())\n",
    "print(4, (4 == tr).sum())\n",
    "print(3, (3 == tr).sum())\n",
    "print(2, (2 == tr).sum())\n",
    "print(1, (1 == tr).sum())\n",
    "print(0, (0 == tr).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 277468\n",
      "4 421283\n",
      "3 420578\n",
      "2 173280\n",
      "1 90435\n",
      "0 20958199\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == augmented_train).sum())\n",
    "print(4, (4 == augmented_train).sum())\n",
    "print(3, (3 == augmented_train).sum())\n",
    "print(2, (2 == augmented_train).sum())\n",
    "print(1, (1 == augmented_train).sum())\n",
    "print(0, (0 == augmented_train).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525853"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((tr == 0) * (augmented_train > 0)).sum() # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('predicted_tr', predicted_tr)\n",
    "np.save('augmented_train', augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, feat_size):\n",
    "        super(NetD, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "#         self.use_cuda = True\n",
    "#         self.feat_size = feat_size\n",
    "        # top\n",
    "#         print(self.feat_size*2)\n",
    "        self.t1 = torch.nn.Linear(self.feat_size, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(self.feat_size, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, self.feat_size)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        \n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "#         if self.use_cuda: \n",
    "        idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = F.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "        x = filt * x\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_size):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz + self.feat_size, 1024), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "                                torch.nn.ReLU(), \n",
    "#                                 nn.Dropout(0.5),\n",
    "#                                 torch.nn.Linear(1024, 1024),\n",
    "# #                                 torch.nn.BatchNorm1d(1024),\n",
    "#                                 torch.nn.ReLU(), \n",
    "#                                 nn.Dropout(0.6),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "                                torch.nn.Sigmoid()\n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "        \n",
    "    def forward(self, e_mask, x):\n",
    "        x = self.netGen(x)\n",
    "        x = x * e_mask\n",
    "        return x\n",
    "#         return F.dropout(x, 0.7)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=64):\n",
    "    '''\n",
    "    returns random rows of size batch_size\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))\n",
    "augmented_train = torch.autograd.Variable(torch.Tensor(augmented_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 6.3707367326297435)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train.cpu().numpy()), get_sparsity(augmented_train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = get_random_batch(train)\n",
    "# xy = get_random_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_my(xx, xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(torch.abs(torch.abs(xx != 0).float()*xy - xy), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx > xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def d_my(x_r, x_g): # custom loss -todo\n",
    "# #     return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r), 1)/x_r.shape[1]\n",
    "\n",
    "# def d_my(x_r, x_g): # custom loss -todo\n",
    "#     return torch.sum(torch.abs(x_g - x_r), 1)/x_r.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(corrupted, original, batch_size=64):\n",
    "    rand_rows = np.random.randint(corrupted.shape[0], size=batch_size)\n",
    "    return torch.Tensor(corrupted[rand_rows]).cuda().float(), torch.Tensor(original[rand_rows]).cuda().float()\n",
    "#     return torch.from_numpy(corrupted[rand_rows]).float(), torch.from_numpy(original[rand_rows]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.026195215919772, 1.7090908603553212)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(X_neg), get_sparsity(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-d2c2b398b600>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_neg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mget_sparsity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_sparsity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "a,b = batch_generator(X_neg, y_neg)\n",
    "\n",
    "get_sparsity(a.numpy()), get_sparsity(b.numpy()), a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GAN(netD, netG, negative, steps_per_epoch = 1000, epochs = 1000):\n",
    "    d_iter = 5\n",
    "    g_iter = 1\n",
    "    gen_iterations = 0\n",
    "    eval_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for c in range(steps_per_epoch):\n",
    "            data_iter = 100\n",
    "            i = 0\n",
    "            while i < 100:\n",
    "                ############################\n",
    "                # (1) Update D network\n",
    "                ###########################\n",
    "                for p in netD.parameters(): # reset requires_grad\n",
    "                    p.requires_grad = True # they are set to False below in netG update\n",
    "    #             d_iter = d_iter\n",
    "                j = 0\n",
    "                while j < d_iter*5:\n",
    "                    j += 1\n",
    "                    # load real data\n",
    "                    i += 1\n",
    "                    if negative:\n",
    "                        condition, X = batch_generator(X_neg, y_neg)\n",
    "    #                 X, _ = data_iter.next()\n",
    "    #                 X = X.view(X.size(0), -1)\n",
    "    #                 X = (X >= 0.5).float()\n",
    "                    if cuda: \n",
    "                        X = X.cuda()\n",
    "                        condition = condition.cuda()\n",
    "    #                 print(condition.shape, X_neg.shape, y_neg.shape)\n",
    "                    real = Variable(X)\n",
    "\n",
    "                    # generate fake data\n",
    "                    noise = torch.randn(batch_size, nz)\n",
    "                    if cuda: \n",
    "                        noise = noise.cuda()\n",
    "                    noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "                    concated = torch.cat((noisev, condition), 1)\n",
    "    #                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\n",
    "                    e_mask = (real == 1).float()\n",
    "                    fake = Variable(netG(e_mask, concated).data)\n",
    "\n",
    "                    # compute gradient, take step\n",
    "                    netD.zero_grad()\n",
    "    #                 concated_real = torch.cat((real, condition), 1)\n",
    "    #                 print(concated_real)\n",
    "                    out = netD(real, fake)\n",
    "                    outputD = torch.mean(out) + lamba * out.norm()\n",
    "                    stdD = torch.std(out)\n",
    "                    outputD.backward(mone)\n",
    "                    optimizerD.step()\n",
    "#                     print('AAAAAAAAA mse:=WWWWWWWWWWWWWWWWWWWWWW')\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "\n",
    "    #         g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter*5:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "                netG.zero_grad()\n",
    "                # load real data\n",
    "\n",
    "                i += 1\n",
    "        #         X, _ = data_iter.next()\n",
    "        #         X = X.view(X.size(0), -1)\n",
    "        #         X = (X >= 0.5).float()\n",
    "                if negative:\n",
    "                    condition, X = batch_generator(X_neg, y_neg)\n",
    "    #             if cuda: \n",
    "                X = X.cuda()\n",
    "                condition = condition.cuda()\n",
    "                real = Variable(X)\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "    #             if args.cuda: \n",
    "                noise = noise.cuda()\n",
    "                noisev = Variable(noise)\n",
    "    #             print(condition.shape, X_neg.shape, y_neg.shape, noisev.shape)\n",
    "                concated_ = torch.cat((noisev, condition), 1)\n",
    "    #                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\n",
    "    #             e_mask = Variable((real == 1)).float()\n",
    "    #             e_mask.requires_grad = True\n",
    "    #             fake = Variable(netG(concated, X).data)\n",
    "                e_mask_ = real.clone()\n",
    "    #             concated_.requires_grad = True\n",
    "    #             e_mask_.requires_grad = True\n",
    "                real.requires_grad = True\n",
    "                fake.requires_grad = True\n",
    "                fake = Variable(netG(e_mask_, concated_).data)\n",
    "    #             print((fake > 0).float().sum() == real.float().sum())\n",
    "    #             print(fake)\n",
    "    #             print(real)\n",
    "    #             fake = netG(concated)\n",
    "    #             concated_real = torch.cat((real, condition), 1)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "                gen_iterations += 1\n",
    "#             print('AAAAAA')\n",
    "            eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "            eval_losses.append(eval_loss)\n",
    "            print('mse:', eval_loss)\n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, 100, gen_iterations, outputD.item(), outputG.item()))\n",
    "    return eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.026195215919772, 1.7090908603553212)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(X_neg), get_sparsity(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrD = 5e-4\n",
    "# lrG = 5e-4\n",
    "# batch_size = 128\n",
    "# cuda = True\n",
    "# epochs = 1000 #change\n",
    "# seed = 1\n",
    "# nz = 16\n",
    "# d_iter = 5\n",
    "# g_iter = 1\n",
    "# lamba = 2e-4\n",
    "\n",
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 1000\n",
    "device = 5\n",
    "seed = 1\n",
    "nz = 10\n",
    "lamba = 1e-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3706"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_feedback_mask.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=3716, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_neg = NetD(negative_feedback_mask.shape[1]).cuda()\n",
    "netG_neg = NetG(negative_feedback_mask.shape[1]).cuda()\n",
    "print(netD_neg)\n",
    "print(netG_neg)\n",
    "optimizerG = optim.RMSprop(netG_neg.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_neg.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = (-1 * one).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][5] Loss_D: 0.007600 Loss_G: 0.007457 \n",
      "mse: tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][10] Loss_D: 0.012600 Loss_G: 0.009962 \n",
      "mse: tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][15] Loss_D: 0.006205 Loss_G: 0.007759 \n",
      "mse: tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][20] Loss_D: 0.007272 Loss_G: 0.008353 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][25] Loss_D: 0.009498 Loss_G: 0.009186 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][30] Loss_D: 0.009597 Loss_G: 0.009201 \n",
      "mse: tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][35] Loss_D: 0.010787 Loss_G: 0.010193 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][40] Loss_D: 0.008516 Loss_G: 0.009059 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][45] Loss_D: 0.006578 Loss_G: 0.008903 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][50] Loss_D: 0.008544 Loss_G: 0.008437 \n",
      "mse: tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][55] Loss_D: 0.007247 Loss_G: 0.010193 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][60] Loss_D: 0.007982 Loss_G: 0.008406 \n",
      "mse: tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][65] Loss_D: 0.008458 Loss_G: 0.009185 \n",
      "mse: tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][70] Loss_D: 0.008329 Loss_G: 0.011152 \n",
      "mse: tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][75] Loss_D: 0.007834 Loss_G: 0.007979 \n",
      "mse: tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][80] Loss_D: 0.010994 Loss_G: 0.010085 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][85] Loss_D: 0.007310 Loss_G: 0.008323 \n",
      "mse: tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][90] Loss_D: 0.005749 Loss_G: 0.009741 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][95] Loss_D: 0.011396 Loss_G: 0.009788 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][100] Loss_D: 0.011690 Loss_G: 0.008500 \n",
      "mse: tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][105] Loss_D: 0.010160 Loss_G: 0.007104 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][110] Loss_D: 0.012459 Loss_G: 0.009110 \n",
      "mse: tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][115] Loss_D: 0.009766 Loss_G: 0.010020 \n",
      "mse: tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][120] Loss_D: 0.007948 Loss_G: 0.008669 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][125] Loss_D: 0.010320 Loss_G: 0.008244 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][130] Loss_D: 0.008380 Loss_G: 0.009292 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][135] Loss_D: 0.011055 Loss_G: 0.008524 \n",
      "mse: tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][140] Loss_D: 0.011546 Loss_G: 0.010276 \n",
      "mse: tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][145] Loss_D: 0.010818 Loss_G: 0.009229 \n",
      "mse: tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][150] Loss_D: 0.008805 Loss_G: 0.009274 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][155] Loss_D: 0.007112 Loss_G: 0.008942 \n",
      "mse: tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][160] Loss_D: 0.009326 Loss_G: 0.012051 \n",
      "mse: tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][165] Loss_D: 0.008707 Loss_G: 0.006508 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][170] Loss_D: 0.007690 Loss_G: 0.009559 \n",
      "mse: tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][175] Loss_D: 0.008264 Loss_G: 0.010301 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][180] Loss_D: 0.009729 Loss_G: 0.008730 \n",
      "mse: tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][185] Loss_D: 0.008054 Loss_G: 0.010669 \n",
      "mse: tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][190] Loss_D: 0.010482 Loss_G: 0.009329 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][195] Loss_D: 0.008808 Loss_G: 0.008919 \n",
      "mse: tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][200] Loss_D: 0.010989 Loss_G: 0.007463 \n",
      "mse: tensor(0.0057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][205] Loss_D: 0.010442 Loss_G: 0.012050 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][210] Loss_D: 0.009840 Loss_G: 0.008942 \n",
      "mse: tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][215] Loss_D: 0.009206 Loss_G: 0.009836 \n",
      "mse: tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][220] Loss_D: 0.011018 Loss_G: 0.008538 \n",
      "mse: tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][225] Loss_D: 0.007752 Loss_G: 0.009284 \n",
      "mse: tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][230] Loss_D: 0.008875 Loss_G: 0.008206 \n",
      "mse: tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][235] Loss_D: 0.010569 Loss_G: 0.009277 \n",
      "mse: tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][240] Loss_D: 0.006498 Loss_G: 0.006563 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][245] Loss_D: 0.009790 Loss_G: 0.008360 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][250] Loss_D: 0.010755 Loss_G: 0.008587 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][255] Loss_D: 0.006343 Loss_G: 0.009217 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][260] Loss_D: 0.010815 Loss_G: 0.008926 \n",
      "mse: tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][265] Loss_D: 0.008724 Loss_G: 0.006462 \n",
      "mse: tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][270] Loss_D: 0.009025 Loss_G: 0.011078 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][275] Loss_D: 0.007010 Loss_G: 0.008770 \n",
      "mse: tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][280] Loss_D: 0.009916 Loss_G: 0.010026 \n",
      "mse: tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][285] Loss_D: 0.007395 Loss_G: 0.011143 \n",
      "mse: tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][290] Loss_D: 0.009515 Loss_G: 0.006073 \n",
      "mse: tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][295] Loss_D: 0.008136 Loss_G: 0.009777 \n",
      "mse: tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][300] Loss_D: 0.005719 Loss_G: 0.013916 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][305] Loss_D: 0.011387 Loss_G: 0.008663 \n",
      "mse: tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][310] Loss_D: 0.008802 Loss_G: 0.007924 \n",
      "mse: tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][315] Loss_D: 0.008230 Loss_G: 0.007689 \n",
      "mse: tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][320] Loss_D: 0.009810 Loss_G: 0.009534 \n",
      "mse: tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][325] Loss_D: 0.011695 Loss_G: 0.010741 \n",
      "mse: tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][330] Loss_D: 0.011373 Loss_G: 0.007156 \n",
      "mse: tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][335] Loss_D: 0.014037 Loss_G: 0.007102 \n",
      "mse: tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][340] Loss_D: 0.010258 Loss_G: 0.011388 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][345] Loss_D: 0.009768 Loss_G: 0.012047 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][350] Loss_D: 0.008149 Loss_G: 0.009802 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][355] Loss_D: 0.011110 Loss_G: 0.008427 \n",
      "mse: tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][360] Loss_D: 0.010924 Loss_G: 0.011408 \n",
      "mse: tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][365] Loss_D: 0.008433 Loss_G: 0.011672 \n",
      "mse: tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][370] Loss_D: 0.007623 Loss_G: 0.007092 \n",
      "mse: tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][375] Loss_D: 0.012545 Loss_G: 0.006490 \n",
      "mse: tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][380] Loss_D: 0.011646 Loss_G: 0.012982 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][385] Loss_D: 0.006811 Loss_G: 0.009732 \n",
      "mse: tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][390] Loss_D: 0.010668 Loss_G: 0.008044 \n",
      "mse: tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][395] Loss_D: 0.009389 Loss_G: 0.011026 \n",
      "mse: tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][400] Loss_D: 0.011357 Loss_G: 0.008028 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][405] Loss_D: 0.009941 Loss_G: 0.009495 \n",
      "mse: tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][410] Loss_D: 0.009991 Loss_G: 0.006763 \n",
      "mse: tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][415] Loss_D: 0.011665 Loss_G: 0.006408 \n",
      "mse: tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][420] Loss_D: 0.009972 Loss_G: 0.011135 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][425] Loss_D: 0.008394 Loss_G: 0.009880 \n",
      "mse: tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][430] Loss_D: 0.009581 Loss_G: 0.007566 \n",
      "mse: tensor(0.0048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][435] Loss_D: 0.010463 Loss_G: 0.010848 \n",
      "mse: tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][440] Loss_D: 0.008069 Loss_G: 0.009674 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][445] Loss_D: 0.009377 Loss_G: 0.008539 \n",
      "mse: tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][450] Loss_D: 0.006169 Loss_G: 0.008006 \n",
      "mse: tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][455] Loss_D: 0.008047 Loss_G: 0.013463 \n",
      "mse: tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][460] Loss_D: 0.008610 Loss_G: 0.011434 \n",
      "mse: tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][465] Loss_D: 0.008515 Loss_G: 0.010183 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][470] Loss_D: 0.012929 Loss_G: 0.009516 \n",
      "mse: tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][475] Loss_D: 0.008592 Loss_G: 0.007999 \n",
      "mse: tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][480] Loss_D: 0.006995 Loss_G: 0.011881 \n",
      "mse: tensor(0.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][485] Loss_D: 0.009821 Loss_G: 0.006864 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][490] Loss_D: 0.006758 Loss_G: 0.008607 \n",
      "mse: tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][495] Loss_D: 0.007600 Loss_G: 0.010521 \n",
      "mse: tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][500] Loss_D: 0.007593 Loss_G: 0.010089 \n",
      "mse: tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][505] Loss_D: 0.007637 Loss_G: 0.010994 \n",
      "mse: tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][510] Loss_D: 0.010882 Loss_G: 0.011127 \n",
      "mse: tensor(0.0066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][515] Loss_D: 0.008381 Loss_G: 0.014935 \n",
      "mse: tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][520] Loss_D: 0.011304 Loss_G: 0.011325 \n",
      "mse: tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][525] Loss_D: 0.006833 Loss_G: 0.011503 \n",
      "mse: tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][530] Loss_D: 0.008826 Loss_G: 0.008033 \n",
      "mse: tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][535] Loss_D: 0.012489 Loss_G: 0.009579 \n",
      "mse: tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][540] Loss_D: 0.007925 Loss_G: 0.010746 \n",
      "mse: tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][545] Loss_D: 0.008545 Loss_G: 0.007816 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][550] Loss_D: 0.008613 Loss_G: 0.009266 \n",
      "mse: tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][555] Loss_D: 0.008120 Loss_G: 0.008686 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][560] Loss_D: 0.011639 Loss_G: 0.009225 \n",
      "mse: tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][565] Loss_D: 0.009556 Loss_G: 0.006332 \n",
      "mse: tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][570] Loss_D: 0.008357 Loss_G: 0.009271 \n",
      "mse: tensor(0.0032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][575] Loss_D: 0.007506 Loss_G: 0.007190 \n",
      "mse: tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][580] Loss_D: 0.008609 Loss_G: 0.007445 \n",
      "mse: tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][585] Loss_D: 0.011143 Loss_G: 0.012328 \n",
      "mse: tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][590] Loss_D: 0.008239 Loss_G: 0.008067 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][595] Loss_D: 0.008809 Loss_G: 0.010064 \n",
      "mse: tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][600] Loss_D: 0.010680 Loss_G: 0.008766 \n",
      "mse: tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][605] Loss_D: 0.009529 Loss_G: 0.007327 \n",
      "mse: tensor(0.0039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][610] Loss_D: 0.006518 Loss_G: 0.009130 \n",
      "mse: tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][615] Loss_D: 0.009531 Loss_G: 0.009854 \n",
      "mse: tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][620] Loss_D: 0.008455 Loss_G: 0.011568 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][625] Loss_D: 0.011768 Loss_G: 0.008891 \n",
      "mse: tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][630] Loss_D: 0.007844 Loss_G: 0.007273 \n",
      "mse: tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][635] Loss_D: 0.007703 Loss_G: 0.009095 \n",
      "mse: tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "[0/1000][105/100][640] Loss_D: 0.007694 Loss_G: 0.009631 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-254-14507e4a16fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnetD_neg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnetG_neg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0meval_losses_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_GAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetD_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetG_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-249-f1ec3e1b01bb>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[1;34m(netD, netG, negative, steps_per_epoch, epochs)\u001b[0m\n\u001b[0;32m     36\u001b[0m                         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0mnoisev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvolatile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# totally freeze netG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                     \u001b[0mconcated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoisev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m#                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                     \u001b[0me_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "netD_neg.train()\n",
    "netG_neg.train()\n",
    "eval_losses_neg = train_GAN(netD_neg, netG_neg, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD_neg.state_dict(), './netD_neg-1m')\n",
    "torch.save(netG_neg.state_dict(), './netG_neg-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetD(\n",
       "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
       "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
       "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition, X = batch_generator(X_neg, y_neg, batch_size=1)\n",
    "\n",
    "X = X.cuda()\n",
    "condition = condition.cuda()\n",
    "# real = Variable(X)\n",
    "\n",
    "noise = torch.randn(1, nz).cuda()\n",
    "# noise = noise.cuda()\n",
    "# noisev = Variable(noise)\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake = netG_neg(X, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21, device='cuda:0'),\n",
       " tensor(8, device='cuda:0'),\n",
       " tensor(13., device='cuda:0'),\n",
       " tensor(21., device='cuda:0'))"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake >= 0.4).sum(), ((fake >= 0.4) * (condition==0)).sum(), condition.sum(), X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12, device='cuda:0'),\n",
       " tensor(4, device='cuda:0'),\n",
       " tensor(13., device='cuda:0'),\n",
       " tensor(21., device='cuda:0'))"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake >= 0.5).sum(), ((fake >= 0.5) * (condition==0)).sum(), condition.sum(), X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29, device='cuda:0')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake > 0.51).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3706])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(160, device='cuda:0'),\n",
       " tensor(55, device='cuda:0'),\n",
       " tensor(107., device='cuda:0'))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake > 0.48).sum(), ((fake > 0.48) * (condition==0)).sum(), condition.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin(condition.cpu().numpy(), X.cpu().numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6708, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((fake > 0.48).float() * (condition==0).float() * X).sum()/(X * (1 - fake)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3354, device='cuda:0')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((fake > 0.48).float() * (condition==0).float()*X).sum()/(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6708, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((fake > 0.48).float() * (condition==0).float() * X).sum()/(X * (1 - fake)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn(train.shape[0], nz).to(device)\n",
    "# noisev = Variable(noise)\n",
    "# fake = netG_tr(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see there is a significant bias towards higher ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# networks\n",
    "netD_augm = NetD().to(device)\n",
    "netG_augm = NetG().to(device)\n",
    "print(netD_augm)\n",
    "print(netG_augm)\n",
    "optimizerG = optim.RMSprop(netG_augm.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD_augm.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (-1 * one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())\n",
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without train\n",
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netD_augm.load_state_dict(torch.load('./netG_augm-1m'))\n",
    "# netD_augm.load_state_dict(torch.load('./netD_augm-1m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD_augm.train()\n",
    "netG_augm.train()\n",
    "eval_losses_aug = train_GAN(netD_augm, netG_augm, augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses_aug)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG_tr.eval()\n",
    "netG_augm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake = fake * (fake <= 5).astype(int)\n",
    "\n",
    "fake = fake.clip(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "\n",
    "fake_tr = netG_tr(noisev)\n",
    "fake_aug = netG_augm(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(fake_tr.round()), torch.unique(fake_aug.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tr = fake_tr.clamp(0,5).detach().cpu().numpy().round()\n",
    "fake_aug = fake_aug.clamp(0,5).detach().cpu().numpy().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake_tr).sum(), (5 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(4, (4 == fake_tr).sum(), (4 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(3, (3 == fake_tr).sum(), (3 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(2, (2 == fake_tr).sum(), (2 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(1, (1 == fake_tr).sum(), (1 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(0, (0 == fake_tr).sum(), (0 == (tr + vr)[:fake.shape[0], :]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake_aug).sum(), (5 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(4, (4 == fake_aug).sum(), (4 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(3, (3 == fake_aug).sum(), (3 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(2, (2 == fake_aug).sum(), (2 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(1, (1 == fake_aug).sum(), (1 == (tr + vr)[:fake.shape[0], :]).sum())\n",
    "print(0, (0 == fake_aug).sum(), (0 == (tr + vr)[:fake.shape[0], :]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr), get_sparsity(fake_tr), get_sparsity(fake_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_orig, vr_1 = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed,  transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter \n",
    "import matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ix = np.random.randint(0, fake.shape[0], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding_fake_autoenc = fake_tr[rand_ix,:]\n",
    "adding_fake_autoenc_lus_gan = fake_aug[rand_ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(adding_fake_autoenc_lus_gan[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(adding_fake_autoenc == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(adding_fake_autoenc_lus_gan == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(adding_fake_autoenc_lus_gan[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake[0,0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake_autoenc_lus_gan[0,0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_auto_enc = np.append(tr, adding_fake_autoenc, axis=0)\n",
    "tr_auto_enc_plus_gan = np.append(tr, adding_fake_autoenc_lus_gan, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(augmented_train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_train.cpu().numpy(), 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([50], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_train.cpu().numpy(), 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([50], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.shape, augmented_train.cpu().numpy().shape, tr_auto_enc.shape, tr_auto_enc_plus_gan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr_auto_enc), get_sparsity(tr_auto_enc_plus_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25, 40]\n",
    "\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr_auto_enc, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_array = [1, 2, 5, 10, 25, 40]\n",
    "\n",
    "MF_SGD = matrix_factorization.ExplicitMF(tr_auto_enc_plus_gan, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr_auto_enc_plus_gan), tr_auto_enc_plus_gan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr_auto_enc_plus_gan, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr_auto_enc_plus_gan, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([60], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(fake_aug, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_tr = augmented_train.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_tr, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='als', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_tr, 40, learning='als', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
