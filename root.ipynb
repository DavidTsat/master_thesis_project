{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import sys\n",
    "from dataLoader import loadData\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 5.011397123336792 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from os.path import isfile, isdir, join\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 0 #change\n",
    "nz = 10\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 1e-3 # constant for L2 penalty (diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available()==True:\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device =\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Dropout(p=0.5)\n",
      "    (5): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Dropout(p=0.6)\n",
      "  )\n",
      ")\n",
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "#         filt = (torch.abs((real > 0.3).float() * fake - real))/real.shape[0]\n",
    "        filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())/(fake == 0).sum()\n",
    "#         filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())\n",
    "\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "        x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz, 1024), \n",
    "                                torch.nn.ReLU(), \n",
    "                                torch.nn.Linear(1024, 1024), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.5),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.6)\n",
    "                                )\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#                                  nn.Linear(nz,1024),\n",
    "# #                                  nn.Dropout(0.3)\n",
    "#                                  nn.ReLU(),\n",
    "#                                  nn.Linear(1024,2048),\n",
    "#                                  nn.Sigmoid(),\n",
    "#                                  nn.Dropout(0.3),\n",
    "#                                  nn.Linear(2048,features_length),\n",
    "# #                                  nn.Sigmoid()\n",
    "#                                  nn.Dropout(0.5)\n",
    "#                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "#         return x\n",
    "        return x*5 # to get values in range [0,5]\n",
    "    \n",
    "# networks\n",
    "netD = NetD().to(device)\n",
    "netG = NetG().to(device)\n",
    "print(netG)\n",
    "print(netD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1]).to(device)\n",
    "mone = (one * -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in netD.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #\n",
    "    \n",
    "for p in netG.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3706)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_batch(train, batch_size=batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g): # custom loss -todo\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 300\n",
    "gen_iterations = 0\n",
    "eval_losses = []\n",
    "for epoch in range(0):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < steps_per_epoch:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter*5:\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "#             X, _ = data_iter.next()\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             print(X >= 0.5)\n",
    "# #             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             real + fake * (real == 0).float()\n",
    "#             print(real[0,:20], fake[0,:20])\n",
    "            fake.requires_grad = False\n",
    "#             print(real.shape, fake.shape)\n",
    "    \n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "#             print('real', real[:10, :20])\n",
    "#             print('fake', fake[:10, :20])\n",
    "#             print(real.type(), fake.type())\n",
    "#             print(fake)\n",
    "            out = netD(real, fake)\n",
    "            \n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "#             print(out.shape)\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        g_iter = g_iter\n",
    "        j = 0\n",
    "        while j < g_iter*5:\n",
    "            j += 1\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            \n",
    "            # update generator\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            noisev = Variable(noise)\n",
    "            \n",
    "            fake = netG(noisev)\n",
    "            real + fake * (real == 0).float()\n",
    "            fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake = fake * Variable(real != 0).float().cuda()\n",
    "#             fake.requires_grad = False\n",
    "#             fake = Variable(netG(noisev)).data\n",
    "#             fake = fake * Variable(((real != 0) & (fake > 0.8))).float().cuda()\n",
    "#             fake.requires_grad = True\n",
    "            \n",
    "            out = netD(real, fake)\n",
    "            outputG = torch.mean(out) + lamba * out.norm()\n",
    "            stdG = torch.std(out)\n",
    "            outputG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "#             print('output_D', outputD.item(), gen_iterations)\n",
    "#             print('output_G', outputG.item(), gen_iterations)\n",
    "#             print('std_D', stdD.item(), gen_iterations)\n",
    "#             print('std_G', stdG.item(), gen_iterations)\n",
    "            torch.save(netG.state_dict(), './netG-1m')\n",
    "            torch.save(netD.state_dict(), './netD-1m')\n",
    "            # evaluation\n",
    "            if gen_iterations % 100 == 0: # todo- to change\n",
    "#                 gen.eval()\n",
    "#                 z_vector_eval = make_some_noise(128)\n",
    "#                 fake_rows_eval = gen(z_vector_eval)\n",
    "#                 real_rows_eval = get_random_batch(train, 128)\n",
    "        #         print(fake_rows[0][:10]) enable to see some results\n",
    "#                 fake = Variable(netG(noisev).data).round()\n",
    "#                 fake = ((real != 0) & (fake != 0))\n",
    "#                 print(fake)\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                eval_losses.append(eval_loss)\n",
    "                print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(netG.state_dict(), './netG-1m')\n",
    "# torch.save(netD.state_dict(), './netD-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(train.shape[0], nz).to(device)\n",
    "noisev = Variable(noise)\n",
    "fake = netG(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = np.around(fake.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake * (fake <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1787729 226310\n",
      "4 634312 348971\n",
      "3 94038 261197\n",
      "2 2991 107557\n",
      "1 5 56174\n",
      "0 19865165 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr)[:fake.shape[0], :].round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr)[:fake.shape[0], :].round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train > 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.934857739195076e-06"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[0,:] > 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [3., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.Tensor(tr.copy()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback_mask = (train > 3).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_feedback_mask = ((train < 4).to(device).float() * (1 - zero_mask)).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((positive_feedback_mask + negative_feedback_mask) != zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7090908603553212"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(negative_feedback_mask.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "UserScore (InputLayer)       (None, 3706)              0         \n",
      "_________________________________________________________________\n",
      "EncLayer1 (Dense)            (None, 512)               1897984   \n",
      "_________________________________________________________________\n",
      "LatentSpace (Dense)          (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "DecLayer1 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "UserScorePred (Dense)        (None, 3706)              1901178   \n",
      "=================================================================\n",
      "Total params: 4,062,074\n",
      "Trainable params: 4,062,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denoising_autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=3706, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (3): Dropout(p=0.8)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (3): Dropout(p=0.8)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=1024, out_features=3706, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class denoising_autoencoder(nn.Module):\n",
    "#     def __init__(self, n_users, input_size, z=256):\n",
    "#         super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "# #         self.V = torch.FloatTensor(z).to(device)\n",
    "#         self.V = torch.FloatTensor(n_users, z).to(device)\n",
    "#         self.b = torch.FloatTensor(n_users, 1).to(device)\n",
    "#         self.b_shtrix = torch.FloatTensor(n_users, 1).to(device)\n",
    "    \n",
    "#         self.encoder = nn.Linear(input_size, z)\n",
    "#         self.decoder = nn.Linear(z, input_size)\n",
    "        \n",
    "#         torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    "#         torch.nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        \n",
    "# #         torch.nn.init.xavier_uniform(self.V)\n",
    "# #         torch.nn.init.xavier_uniform(self.b.weight)\n",
    "# #         torch.nn.init.xavier_uniform(self.b_shtrix.weight)\n",
    "\n",
    "# #         self.encoder=nn.Sequential(\n",
    "# #                       nn.Linear(input_size, 1024),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(1024,512),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(512, z),\n",
    "# # #                       nn.Sigmoid()\n",
    "# #                       )\n",
    "\n",
    "# #         self.decoder=nn.Sequential(\n",
    "# #                       nn.Linear(z, 512),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(512, 1024),\n",
    "# #                       nn.ReLU(),\n",
    "# #                       nn.Linear(1024, input_size),\n",
    "# # #                       nn.Sigmoid(),\n",
    "# #                       )\n",
    " \n",
    "#     def forward(self, x, i):\n",
    "#         z = self.encoder(x)\n",
    "# #         print(z.t().shape, x.shape, self.V.shape, self.V[i, :].shape)\n",
    "# #         torch.Size([32, 64]) torch.Size([64, 3706]) torch.Size([6040, 32]) torch.Size([64, 32])\n",
    "# # #         torch.Size([64, 32]) torch.Size([64, 3706]) torch.Size([3706])\n",
    "# #         print(self.V)\n",
    "# #         print(z.t + self.V)\n",
    "# #         print(self.b[i, :].shape, z.shape, self.V[i, :].shape)\n",
    "#         z = z + self.V[i, :] + self.b[i, :] \n",
    "# #         z = z + self.V[i, :]\n",
    "#         z = torch.nn.functional.relu(z)\n",
    "# #         print(z)\n",
    "#         x = self.decoder(z)\n",
    "# #     torch.Size([64, 3706]) torch.Size([64, 32]) torch.Size([64, 32])\n",
    "# #         print(x.shape, z.shape, self.b_shtrix[i, :].shape)\n",
    "# #         print(x.t().shape, self.b_shtrix[i, :].shape)\n",
    "#         x = x + self.b_shtrix[i, :]\n",
    "    \n",
    "# #         return x\n",
    "    \n",
    "#         return torch.nn.functional.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class denoising_autoencoder(nn.Module):\n",
    "    def __init__(self, n_users, input_size, z=256):\n",
    "        super(denoising_autoencoder, self).__init__()\n",
    "        \n",
    "#         torch.nn.init.xavier_uniform(self.V)\n",
    "#         torch.nn.init.xavier_uniform(self.b.weight)\n",
    "#         torch.nn.init.xavier_uniform(self.b_shtrix.weight)\n",
    "\n",
    "        self.encoder=nn.Sequential(\n",
    "                      nn.Linear(input_size, 1024),\n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(1024,512),\n",
    "                      nn.Dropout(0.6),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512, z),\n",
    "#                       nn.Sigmoid()\n",
    "                      )\n",
    "\n",
    "        self.decoder=nn.Sequential(\n",
    "                      nn.Linear(z, 512),\n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512, 1024),\n",
    "                      nn.Dropout(0.6),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(1024, input_size),\n",
    "#                       nn.Sigmoid(),\n",
    "                      )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "\n",
    "        for l, ll in zip(self.encoder, self.decoder):\n",
    "            if type(l) == torch.nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(l.weight)\n",
    "            if type(ll) == torch.nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(ll.weight)\n",
    "\n",
    "    def forward(self, x, i):\n",
    "        z = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(10, 32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = denoising_autoencoder(input_size=train[0,:].shape[0], n_users=train.shape[0]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(orig_mat, corrupted_mat, batch_size = 64):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(orig_mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = orig_mat[rand_rows].clone()\n",
    "    corrupted = corrupted_mat[rand_rows].clone()\n",
    "\n",
    "    return orig, corrupted, rand_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_masked_batch(mat, batch_size = 32, p=0.5):\n",
    "    '''\n",
    "    This works as a trainloader for denoising autoencoder.\n",
    "    Randomly masks observed entries (replaces 1s with 0s) to add a noise\n",
    "    '''\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(rand_rows)\n",
    "#     return mat[rand_rows], mat[rand_rows]\n",
    "    orig = mat[rand_rows].clone()\n",
    "    corrupted = mat[rand_rows].clone()\n",
    "    mask_arr = torch.FloatTensor((np.random.rand(orig.shape[0], orig.shape[1]) > p)).to(device)\n",
    "\n",
    "    return orig, corrupted*mask_arr, rand_rows\n",
    "\n",
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2572517539125743"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(orig.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6130261737722612"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(masked.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6130261737722612"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(torch.nn.functional.dropout(masked, training=False).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.functional.dropout(orig, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.35\n",
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)).to(device)\n",
    "X = negative_feedback_mask\n",
    "y = negative_feedback_mask*mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering Epoch:  0\n",
      "======> epoch: 0/400, Loss:0.026156367734074593\n",
      "======> epoch: 0/400, Loss:0.017574528232216835\n",
      "======> epoch: 0/400, Loss:0.021116357296705246\n",
      "======> epoch: 0/400, Loss:0.01715056784451008\n",
      "======> epoch: 0/400, Loss:0.015665775164961815\n",
      "======> epoch: 0/400, Loss:0.017570868134498596\n",
      "======> epoch: 0/400, Loss:0.019236689433455467\n",
      "======> epoch: 0/400, Loss:0.020895211026072502\n",
      "======> epoch: 0/400, Loss:0.015288612805306911\n",
      "======> epoch: 0/400, Loss:0.021682923659682274\n",
      "======> epoch: 0/400, Loss:0.01437020767480135\n",
      "======> epoch: 0/400, Loss:0.023246705532073975\n",
      "======> epoch: 0/400, Loss:0.01614203304052353\n",
      "======> epoch: 0/400, Loss:0.015197454951703548\n",
      "======> epoch: 0/400, Loss:0.01441205758601427\n",
      "======> epoch: 0/400, Loss:0.018286844715476036\n",
      "======> epoch: 0/400, Loss:0.015333020128309727\n",
      "======> epoch: 0/400, Loss:0.021358145400881767\n",
      "======> epoch: 0/400, Loss:0.011841966770589352\n",
      "======> epoch: 0/400, Loss:0.018867826089262962\n",
      "======> epoch: 0/400, Loss:0.01846533827483654\n",
      "======> epoch: 0/400, Loss:0.014918225817382336\n",
      "======> epoch: 0/400, Loss:0.016810277476906776\n",
      "======> epoch: 0/400, Loss:0.015199413523077965\n",
      "======> epoch: 0/400, Loss:0.01414275262504816\n",
      "======> epoch: 0/400, Loss:0.012164349667727947\n",
      "======> epoch: 0/400, Loss:0.01399993896484375\n",
      "======> epoch: 0/400, Loss:0.017059577628970146\n",
      "======> epoch: 0/400, Loss:0.01606917753815651\n",
      "======> epoch: 0/400, Loss:0.012567468918859959\n",
      "======> epoch: 0/400, Loss:0.016345638781785965\n",
      "======> epoch: 0/400, Loss:0.013782316818833351\n",
      "======> epoch: 0/400, Loss:0.013866031542420387\n",
      "======> epoch: 0/400, Loss:0.014461230486631393\n",
      "======> epoch: 0/400, Loss:0.015863599255681038\n",
      "======> epoch: 0/400, Loss:0.01406210009008646\n",
      "======> epoch: 0/400, Loss:0.016544893383979797\n",
      "======> epoch: 0/400, Loss:0.01846250891685486\n",
      "======> epoch: 0/400, Loss:0.01789052039384842\n",
      "======> epoch: 0/400, Loss:0.01967407390475273\n",
      "======> epoch: 0/400, Loss:0.02470162697136402\n",
      "======> epoch: 0/400, Loss:0.019152622669935226\n",
      "======> epoch: 0/400, Loss:0.015403459779918194\n",
      "======> epoch: 0/400, Loss:0.015468096360564232\n",
      "======> epoch: 0/400, Loss:0.013779195956885815\n",
      "======> epoch: 0/400, Loss:0.01165337860584259\n",
      "======> epoch: 0/400, Loss:0.013950014486908913\n",
      "======> epoch: 0/400, Loss:0.014574795961380005\n",
      "======> epoch: 0/400, Loss:0.014161186292767525\n",
      "======> epoch: 0/400, Loss:0.012820717878639698\n",
      "======> epoch: 0/400, Loss:0.015900947153568268\n",
      "======> epoch: 0/400, Loss:0.022120587527751923\n",
      "======> epoch: 0/400, Loss:0.014481832273304462\n",
      "======> epoch: 0/400, Loss:0.012830525636672974\n",
      "======> epoch: 0/400, Loss:0.015495456755161285\n",
      "======> epoch: 0/400, Loss:0.014240401796996593\n",
      "======> epoch: 0/400, Loss:0.012812711298465729\n",
      "======> epoch: 0/400, Loss:0.015478400513529778\n",
      "======> epoch: 0/400, Loss:0.01350200455635786\n",
      "======> epoch: 0/400, Loss:0.011938944458961487\n",
      "======> epoch: 0/400, Loss:0.017181148752570152\n",
      "======> epoch: 0/400, Loss:0.015140240080654621\n",
      "======> epoch: 0/400, Loss:0.011346480809152126\n",
      "======> epoch: 0/400, Loss:0.01586945913732052\n",
      "======> epoch: 0/400, Loss:0.015343246050179005\n",
      "======> epoch: 0/400, Loss:0.012162761762738228\n",
      "======> epoch: 0/400, Loss:0.01044656615704298\n",
      "======> epoch: 0/400, Loss:0.02203286811709404\n",
      "======> epoch: 0/400, Loss:0.012607274577021599\n",
      "======> epoch: 0/400, Loss:0.015927957370877266\n",
      "======> epoch: 0/400, Loss:0.019505389034748077\n",
      "======> epoch: 0/400, Loss:0.014535779133439064\n",
      "======> epoch: 0/400, Loss:0.014789209701120853\n",
      "======> epoch: 0/400, Loss:0.01587633602321148\n",
      "======> epoch: 0/400, Loss:0.02171163447201252\n",
      "======> epoch: 0/400, Loss:0.0165278110653162\n",
      "======> epoch: 0/400, Loss:0.015178571455180645\n",
      "======> epoch: 0/400, Loss:0.015636205673217773\n",
      "======> epoch: 0/400, Loss:0.01705084554851055\n",
      "======> epoch: 0/400, Loss:0.017484068870544434\n",
      "======> epoch: 0/400, Loss:0.015049072913825512\n",
      "======> epoch: 0/400, Loss:0.016208339482545853\n",
      "======> epoch: 0/400, Loss:0.014104624278843403\n",
      "======> epoch: 0/400, Loss:0.011499082669615746\n",
      "======> epoch: 0/400, Loss:0.01503409631550312\n",
      "======> epoch: 0/400, Loss:0.017526933923363686\n",
      "======> epoch: 0/400, Loss:0.014768990688025951\n",
      "======> epoch: 0/400, Loss:0.013149214908480644\n",
      "======> epoch: 0/400, Loss:0.01437856163829565\n",
      "======> epoch: 0/400, Loss:0.011041278950870037\n",
      "======> epoch: 0/400, Loss:0.01569567434489727\n",
      "======> epoch: 0/400, Loss:0.011950240470468998\n",
      "======> epoch: 0/400, Loss:0.01817408949136734\n",
      "======> epoch: 0/400, Loss:0.01649082824587822\n",
      "======> epoch: 0/400, Loss:0.014823614619672298\n",
      "======> epoch: 0/400, Loss:0.016545377671718597\n",
      "======> epoch: 0/400, Loss:0.017267826944589615\n",
      "======> epoch: 0/400, Loss:0.011534266173839569\n",
      "======> epoch: 0/400, Loss:0.015339563600718975\n",
      "======> epoch: 0/400, Loss:0.016563910990953445\n",
      "Entering Epoch:  1\n",
      "======> epoch: 1/400, Loss:0.018466703593730927\n",
      "======> epoch: 1/400, Loss:0.01803172379732132\n",
      "======> epoch: 1/400, Loss:0.01374645996838808\n",
      "======> epoch: 1/400, Loss:0.020643621683120728\n",
      "======> epoch: 1/400, Loss:0.018042394891381264\n",
      "======> epoch: 1/400, Loss:0.017659973353147507\n",
      "======> epoch: 1/400, Loss:0.012743126600980759\n",
      "======> epoch: 1/400, Loss:0.016799326986074448\n",
      "======> epoch: 1/400, Loss:0.014850583858788013\n",
      "======> epoch: 1/400, Loss:0.016867103055119514\n",
      "======> epoch: 1/400, Loss:0.012426442466676235\n",
      "======> epoch: 1/400, Loss:0.015717335045337677\n",
      "======> epoch: 1/400, Loss:0.015239635482430458\n",
      "======> epoch: 1/400, Loss:0.014544903300702572\n",
      "======> epoch: 1/400, Loss:0.013651536777615547\n",
      "======> epoch: 1/400, Loss:0.012118947692215443\n",
      "======> epoch: 1/400, Loss:0.015918880701065063\n",
      "======> epoch: 1/400, Loss:0.0169750414788723\n",
      "======> epoch: 1/400, Loss:0.015993447974324226\n",
      "======> epoch: 1/400, Loss:0.016560982912778854\n",
      "======> epoch: 1/400, Loss:0.011142236180603504\n",
      "======> epoch: 1/400, Loss:0.01807180419564247\n",
      "======> epoch: 1/400, Loss:0.014335554093122482\n",
      "======> epoch: 1/400, Loss:0.018614651635289192\n",
      "======> epoch: 1/400, Loss:0.01802186854183674\n",
      "======> epoch: 1/400, Loss:0.013907065615057945\n",
      "======> epoch: 1/400, Loss:0.016629941761493683\n",
      "======> epoch: 1/400, Loss:0.014122453518211842\n",
      "======> epoch: 1/400, Loss:0.0153734739869833\n",
      "======> epoch: 1/400, Loss:0.017785917967557907\n",
      "======> epoch: 1/400, Loss:0.015211353078484535\n",
      "======> epoch: 1/400, Loss:0.014670159667730331\n",
      "======> epoch: 1/400, Loss:0.014792611822485924\n",
      "======> epoch: 1/400, Loss:0.01659625582396984\n",
      "======> epoch: 1/400, Loss:0.01618257351219654\n",
      "======> epoch: 1/400, Loss:0.021184096112847328\n",
      "======> epoch: 1/400, Loss:0.014651209115982056\n",
      "======> epoch: 1/400, Loss:0.01752869412302971\n",
      "======> epoch: 1/400, Loss:0.014203855767846107\n",
      "======> epoch: 1/400, Loss:0.012315179221332073\n",
      "======> epoch: 1/400, Loss:0.013000263832509518\n",
      "======> epoch: 1/400, Loss:0.020746374502778053\n",
      "======> epoch: 1/400, Loss:0.012610831297934055\n",
      "======> epoch: 1/400, Loss:0.017651837319135666\n",
      "======> epoch: 1/400, Loss:0.01429932750761509\n",
      "======> epoch: 1/400, Loss:0.016508502885699272\n",
      "======> epoch: 1/400, Loss:0.01060331892222166\n",
      "======> epoch: 1/400, Loss:0.01711144857108593\n",
      "======> epoch: 1/400, Loss:0.019279299303889275\n",
      "======> epoch: 1/400, Loss:0.015606408007442951\n",
      "======> epoch: 1/400, Loss:0.01591583341360092\n",
      "======> epoch: 1/400, Loss:0.014980842359364033\n",
      "======> epoch: 1/400, Loss:0.01442780252546072\n",
      "======> epoch: 1/400, Loss:0.013135824352502823\n",
      "======> epoch: 1/400, Loss:0.01787295565009117\n",
      "======> epoch: 1/400, Loss:0.014414479956030846\n",
      "======> epoch: 1/400, Loss:0.012375351041555405\n",
      "======> epoch: 1/400, Loss:0.017463410273194313\n",
      "======> epoch: 1/400, Loss:0.014522004872560501\n",
      "======> epoch: 1/400, Loss:0.01264253444969654\n",
      "======> epoch: 1/400, Loss:0.014814462512731552\n",
      "======> epoch: 1/400, Loss:0.01599930226802826\n",
      "======> epoch: 1/400, Loss:0.013245606794953346\n",
      "======> epoch: 1/400, Loss:0.011534790508449078\n",
      "======> epoch: 1/400, Loss:0.015428605489432812\n",
      "======> epoch: 1/400, Loss:0.01502147689461708\n",
      "======> epoch: 1/400, Loss:0.014118175953626633\n",
      "======> epoch: 1/400, Loss:0.013081409968435764\n",
      "======> epoch: 1/400, Loss:0.014016900211572647\n",
      "======> epoch: 1/400, Loss:0.019944580271840096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 1/400, Loss:0.010194015689194202\n",
      "======> epoch: 1/400, Loss:0.01345776580274105\n",
      "======> epoch: 1/400, Loss:0.012599621899425983\n",
      "======> epoch: 1/400, Loss:0.014895838685333729\n",
      "======> epoch: 1/400, Loss:0.016376802697777748\n",
      "======> epoch: 1/400, Loss:0.012716944329440594\n",
      "======> epoch: 1/400, Loss:0.014698184095323086\n",
      "======> epoch: 1/400, Loss:0.01722283847630024\n",
      "======> epoch: 1/400, Loss:0.01552625186741352\n",
      "======> epoch: 1/400, Loss:0.01740938052535057\n",
      "======> epoch: 1/400, Loss:0.015001046471297741\n",
      "======> epoch: 1/400, Loss:0.018153546378016472\n",
      "======> epoch: 1/400, Loss:0.014320814050734043\n",
      "======> epoch: 1/400, Loss:0.013914057053625584\n",
      "======> epoch: 1/400, Loss:0.01540749054402113\n",
      "======> epoch: 1/400, Loss:0.017692606896162033\n",
      "======> epoch: 1/400, Loss:0.012928291223943233\n",
      "======> epoch: 1/400, Loss:0.018159907311201096\n",
      "======> epoch: 1/400, Loss:0.010937845334410667\n",
      "======> epoch: 1/400, Loss:0.021077994257211685\n",
      "======> epoch: 1/400, Loss:0.012116163037717342\n",
      "======> epoch: 1/400, Loss:0.014453881420195103\n",
      "======> epoch: 1/400, Loss:0.012315519154071808\n",
      "======> epoch: 1/400, Loss:0.016836484894156456\n",
      "======> epoch: 1/400, Loss:0.014569122344255447\n",
      "======> epoch: 1/400, Loss:0.013985732570290565\n",
      "======> epoch: 1/400, Loss:0.015377700328826904\n",
      "======> epoch: 1/400, Loss:0.014822904951870441\n",
      "======> epoch: 1/400, Loss:0.012230017222464085\n",
      "======> epoch: 1/400, Loss:0.014566540718078613\n",
      "Entering Epoch:  2\n",
      "======> epoch: 2/400, Loss:0.018093932420015335\n",
      "======> epoch: 2/400, Loss:0.015822265297174454\n",
      "======> epoch: 2/400, Loss:0.011571832932531834\n",
      "======> epoch: 2/400, Loss:0.01400107704102993\n",
      "======> epoch: 2/400, Loss:0.01763029955327511\n",
      "======> epoch: 2/400, Loss:0.013971658423542976\n",
      "======> epoch: 2/400, Loss:0.013801372610032558\n",
      "======> epoch: 2/400, Loss:0.014336854219436646\n",
      "======> epoch: 2/400, Loss:0.013889341615140438\n",
      "======> epoch: 2/400, Loss:0.016933860257267952\n",
      "======> epoch: 2/400, Loss:0.01330595463514328\n",
      "======> epoch: 2/400, Loss:0.015152732841670513\n",
      "======> epoch: 2/400, Loss:0.014996808022260666\n",
      "======> epoch: 2/400, Loss:0.016654212027788162\n",
      "======> epoch: 2/400, Loss:0.013441957533359528\n",
      "======> epoch: 2/400, Loss:0.011153565719723701\n",
      "======> epoch: 2/400, Loss:0.012789309024810791\n",
      "======> epoch: 2/400, Loss:0.020036175847053528\n",
      "======> epoch: 2/400, Loss:0.01351459976285696\n",
      "======> epoch: 2/400, Loss:0.025101106613874435\n",
      "======> epoch: 2/400, Loss:0.018139760941267014\n",
      "======> epoch: 2/400, Loss:0.016422348096966743\n",
      "======> epoch: 2/400, Loss:0.012349209748208523\n",
      "======> epoch: 2/400, Loss:0.013546675443649292\n",
      "======> epoch: 2/400, Loss:0.01571025513112545\n",
      "======> epoch: 2/400, Loss:0.014242525212466717\n",
      "======> epoch: 2/400, Loss:0.015124661847949028\n",
      "======> epoch: 2/400, Loss:0.01476011797785759\n",
      "======> epoch: 2/400, Loss:0.009056641720235348\n",
      "======> epoch: 2/400, Loss:0.012968303635716438\n",
      "======> epoch: 2/400, Loss:0.01503080502152443\n",
      "======> epoch: 2/400, Loss:0.012849800288677216\n",
      "======> epoch: 2/400, Loss:0.008699965663254261\n",
      "======> epoch: 2/400, Loss:0.01053632516413927\n",
      "======> epoch: 2/400, Loss:0.016698410734534264\n",
      "======> epoch: 2/400, Loss:0.011803828179836273\n",
      "======> epoch: 2/400, Loss:0.010851440951228142\n",
      "======> epoch: 2/400, Loss:0.014929993078112602\n",
      "======> epoch: 2/400, Loss:0.018487531691789627\n",
      "======> epoch: 2/400, Loss:0.014354156330227852\n",
      "======> epoch: 2/400, Loss:0.013770249672234058\n",
      "======> epoch: 2/400, Loss:0.012968714348971844\n",
      "======> epoch: 2/400, Loss:0.011874132789671421\n",
      "======> epoch: 2/400, Loss:0.012934956699609756\n",
      "======> epoch: 2/400, Loss:0.01064699050039053\n",
      "======> epoch: 2/400, Loss:0.014837754890322685\n",
      "======> epoch: 2/400, Loss:0.014990625903010368\n",
      "======> epoch: 2/400, Loss:0.018101194873452187\n",
      "======> epoch: 2/400, Loss:0.013695920817553997\n",
      "======> epoch: 2/400, Loss:0.016633016988635063\n",
      "======> epoch: 2/400, Loss:0.012614917941391468\n",
      "======> epoch: 2/400, Loss:0.01540406420826912\n",
      "======> epoch: 2/400, Loss:0.01841866411268711\n",
      "======> epoch: 2/400, Loss:0.01555552240461111\n",
      "======> epoch: 2/400, Loss:0.013768194243311882\n",
      "======> epoch: 2/400, Loss:0.014355041086673737\n",
      "======> epoch: 2/400, Loss:0.012116538360714912\n",
      "======> epoch: 2/400, Loss:0.012109813280403614\n",
      "======> epoch: 2/400, Loss:0.01467307098209858\n",
      "======> epoch: 2/400, Loss:0.016774343326687813\n",
      "======> epoch: 2/400, Loss:0.012572265230119228\n",
      "======> epoch: 2/400, Loss:0.013755732215940952\n",
      "======> epoch: 2/400, Loss:0.014176568947732449\n",
      "======> epoch: 2/400, Loss:0.01320724468678236\n",
      "======> epoch: 2/400, Loss:0.017033781856298447\n",
      "======> epoch: 2/400, Loss:0.012509969063103199\n",
      "======> epoch: 2/400, Loss:0.014420113526284695\n",
      "======> epoch: 2/400, Loss:0.014594553038477898\n",
      "======> epoch: 2/400, Loss:0.012748503126204014\n",
      "======> epoch: 2/400, Loss:0.01914008893072605\n",
      "======> epoch: 2/400, Loss:0.010991668328642845\n",
      "======> epoch: 2/400, Loss:0.012002350762486458\n",
      "======> epoch: 2/400, Loss:0.014950955286622047\n",
      "======> epoch: 2/400, Loss:0.012315926142036915\n",
      "======> epoch: 2/400, Loss:0.012829475104808807\n",
      "======> epoch: 2/400, Loss:0.01228933222591877\n",
      "======> epoch: 2/400, Loss:0.01454669889062643\n",
      "======> epoch: 2/400, Loss:0.014255233108997345\n",
      "======> epoch: 2/400, Loss:0.011863000690937042\n",
      "======> epoch: 2/400, Loss:0.013659574091434479\n",
      "======> epoch: 2/400, Loss:0.017792975530028343\n",
      "======> epoch: 2/400, Loss:0.0147362956777215\n",
      "======> epoch: 2/400, Loss:0.0125340037047863\n",
      "======> epoch: 2/400, Loss:0.012243564240634441\n",
      "======> epoch: 2/400, Loss:0.017480960115790367\n",
      "======> epoch: 2/400, Loss:0.014607503078877926\n",
      "======> epoch: 2/400, Loss:0.017155721783638\n",
      "======> epoch: 2/400, Loss:0.01904558390378952\n",
      "======> epoch: 2/400, Loss:0.01300681009888649\n",
      "======> epoch: 2/400, Loss:0.014660272747278214\n",
      "======> epoch: 2/400, Loss:0.012416264973580837\n",
      "======> epoch: 2/400, Loss:0.011675222776830196\n",
      "======> epoch: 2/400, Loss:0.01409160252660513\n",
      "======> epoch: 2/400, Loss:0.012774710543453693\n",
      "======> epoch: 2/400, Loss:0.016389643773436546\n",
      "======> epoch: 2/400, Loss:0.010840114206075668\n",
      "======> epoch: 2/400, Loss:0.01453269924968481\n",
      "======> epoch: 2/400, Loss:0.016601039096713066\n",
      "======> epoch: 2/400, Loss:0.010629559867084026\n",
      "======> epoch: 2/400, Loss:0.013056280091404915\n",
      "Entering Epoch:  3\n",
      "======> epoch: 3/400, Loss:0.013017919845879078\n",
      "======> epoch: 3/400, Loss:0.016372326761484146\n",
      "======> epoch: 3/400, Loss:0.013552935793995857\n",
      "======> epoch: 3/400, Loss:0.016364526003599167\n",
      "======> epoch: 3/400, Loss:0.015239950269460678\n",
      "======> epoch: 3/400, Loss:0.012534025125205517\n",
      "======> epoch: 3/400, Loss:0.013309433124959469\n",
      "======> epoch: 3/400, Loss:0.014226661063730717\n",
      "======> epoch: 3/400, Loss:0.014660501852631569\n",
      "======> epoch: 3/400, Loss:0.019952652975916862\n",
      "======> epoch: 3/400, Loss:0.011760308407247066\n",
      "======> epoch: 3/400, Loss:0.013327390886843204\n",
      "======> epoch: 3/400, Loss:0.014832327142357826\n",
      "======> epoch: 3/400, Loss:0.014386664144694805\n",
      "======> epoch: 3/400, Loss:0.014160077087581158\n",
      "======> epoch: 3/400, Loss:0.012857438065111637\n",
      "======> epoch: 3/400, Loss:0.013282133266329765\n",
      "======> epoch: 3/400, Loss:0.016386890783905983\n",
      "======> epoch: 3/400, Loss:0.011570170521736145\n",
      "======> epoch: 3/400, Loss:0.015108958818018436\n",
      "======> epoch: 3/400, Loss:0.011840044520795345\n",
      "======> epoch: 3/400, Loss:0.016826802864670753\n",
      "======> epoch: 3/400, Loss:0.015039435587823391\n",
      "======> epoch: 3/400, Loss:0.01591215282678604\n",
      "======> epoch: 3/400, Loss:0.01410849578678608\n",
      "======> epoch: 3/400, Loss:0.014286397024989128\n",
      "======> epoch: 3/400, Loss:0.010945278219878674\n",
      "======> epoch: 3/400, Loss:0.019308412447571754\n",
      "======> epoch: 3/400, Loss:0.012932331301271915\n",
      "======> epoch: 3/400, Loss:0.0143778370693326\n",
      "======> epoch: 3/400, Loss:0.012126725167036057\n",
      "======> epoch: 3/400, Loss:0.015215283259749413\n",
      "======> epoch: 3/400, Loss:0.015197096392512321\n",
      "======> epoch: 3/400, Loss:0.015605182386934757\n",
      "======> epoch: 3/400, Loss:0.014113068580627441\n",
      "======> epoch: 3/400, Loss:0.011661636643111706\n",
      "======> epoch: 3/400, Loss:0.014031504280865192\n",
      "======> epoch: 3/400, Loss:0.013250986114144325\n",
      "======> epoch: 3/400, Loss:0.011864777654409409\n",
      "======> epoch: 3/400, Loss:0.01096799410879612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 3/400, Loss:0.010475147515535355\n",
      "======> epoch: 3/400, Loss:0.017184045165777206\n",
      "======> epoch: 3/400, Loss:0.01307817455381155\n",
      "======> epoch: 3/400, Loss:0.016792694106698036\n",
      "======> epoch: 3/400, Loss:0.015380402095615864\n",
      "======> epoch: 3/400, Loss:0.012249557301402092\n",
      "======> epoch: 3/400, Loss:0.010661368258297443\n",
      "======> epoch: 3/400, Loss:0.012692081741988659\n",
      "======> epoch: 3/400, Loss:0.016094358637928963\n",
      "======> epoch: 3/400, Loss:0.011059512384235859\n",
      "======> epoch: 3/400, Loss:0.01339571364223957\n",
      "======> epoch: 3/400, Loss:0.014689366333186626\n",
      "======> epoch: 3/400, Loss:0.015741070732474327\n",
      "======> epoch: 3/400, Loss:0.017325326800346375\n",
      "======> epoch: 3/400, Loss:0.014304331503808498\n",
      "======> epoch: 3/400, Loss:0.015138753689825535\n",
      "======> epoch: 3/400, Loss:0.018014781177043915\n",
      "======> epoch: 3/400, Loss:0.01516762375831604\n",
      "======> epoch: 3/400, Loss:0.013104275800287724\n",
      "======> epoch: 3/400, Loss:0.01713680848479271\n",
      "======> epoch: 3/400, Loss:0.013587510213255882\n",
      "======> epoch: 3/400, Loss:0.012917302548885345\n",
      "======> epoch: 3/400, Loss:0.012465420179069042\n",
      "======> epoch: 3/400, Loss:0.01370023749768734\n",
      "======> epoch: 3/400, Loss:0.014140434563159943\n",
      "======> epoch: 3/400, Loss:0.014917907305061817\n",
      "======> epoch: 3/400, Loss:0.015013223513960838\n",
      "======> epoch: 3/400, Loss:0.014223354868590832\n",
      "======> epoch: 3/400, Loss:0.011844570748507977\n",
      "======> epoch: 3/400, Loss:0.012566782534122467\n",
      "======> epoch: 3/400, Loss:0.011765946634113789\n",
      "======> epoch: 3/400, Loss:0.01289089024066925\n",
      "======> epoch: 3/400, Loss:0.010497898794710636\n",
      "======> epoch: 3/400, Loss:0.013067396357655525\n",
      "======> epoch: 3/400, Loss:0.012677565217018127\n",
      "======> epoch: 3/400, Loss:0.01448562927544117\n",
      "======> epoch: 3/400, Loss:0.01569022797048092\n",
      "======> epoch: 3/400, Loss:0.01605270802974701\n",
      "======> epoch: 3/400, Loss:0.013164907693862915\n",
      "======> epoch: 3/400, Loss:0.014654910191893578\n",
      "======> epoch: 3/400, Loss:0.011855153366923332\n",
      "======> epoch: 3/400, Loss:0.013680336065590382\n",
      "======> epoch: 3/400, Loss:0.018374985083937645\n",
      "======> epoch: 3/400, Loss:0.01599550060927868\n",
      "======> epoch: 3/400, Loss:0.013545012101531029\n",
      "======> epoch: 3/400, Loss:0.015247766859829426\n",
      "======> epoch: 3/400, Loss:0.012974392622709274\n",
      "======> epoch: 3/400, Loss:0.016010846942663193\n",
      "======> epoch: 3/400, Loss:0.011888628825545311\n",
      "======> epoch: 3/400, Loss:0.015119476243853569\n",
      "======> epoch: 3/400, Loss:0.010653764009475708\n",
      "======> epoch: 3/400, Loss:0.015388338826596737\n",
      "======> epoch: 3/400, Loss:0.01633189059793949\n",
      "======> epoch: 3/400, Loss:0.013734680600464344\n",
      "======> epoch: 3/400, Loss:0.013837832026183605\n",
      "======> epoch: 3/400, Loss:0.01182658039033413\n",
      "======> epoch: 3/400, Loss:0.010708639398217201\n",
      "======> epoch: 3/400, Loss:0.01663576066493988\n",
      "======> epoch: 3/400, Loss:0.014088078401982784\n",
      "======> epoch: 3/400, Loss:0.014925562776625156\n",
      "Entering Epoch:  4\n",
      "======> epoch: 4/400, Loss:0.008788899518549442\n",
      "======> epoch: 4/400, Loss:0.01776130497455597\n",
      "======> epoch: 4/400, Loss:0.012985059060156345\n",
      "======> epoch: 4/400, Loss:0.0112234465777874\n",
      "======> epoch: 4/400, Loss:0.01563050039112568\n",
      "======> epoch: 4/400, Loss:0.012839548289775848\n",
      "======> epoch: 4/400, Loss:0.013597632758319378\n",
      "======> epoch: 4/400, Loss:0.017447255551815033\n",
      "======> epoch: 4/400, Loss:0.015261329710483551\n",
      "======> epoch: 4/400, Loss:0.013683020137250423\n",
      "======> epoch: 4/400, Loss:0.013553300872445107\n",
      "======> epoch: 4/400, Loss:0.018802626058459282\n",
      "======> epoch: 4/400, Loss:0.01380715798586607\n",
      "======> epoch: 4/400, Loss:0.01244212407618761\n",
      "======> epoch: 4/400, Loss:0.01707412861287594\n",
      "======> epoch: 4/400, Loss:0.010952310636639595\n",
      "======> epoch: 4/400, Loss:0.0167120061814785\n",
      "======> epoch: 4/400, Loss:0.0170943234115839\n",
      "======> epoch: 4/400, Loss:0.017946191132068634\n",
      "======> epoch: 4/400, Loss:0.011037804186344147\n",
      "======> epoch: 4/400, Loss:0.015918906778097153\n",
      "======> epoch: 4/400, Loss:0.016485102474689484\n",
      "======> epoch: 4/400, Loss:0.012089474126696587\n",
      "======> epoch: 4/400, Loss:0.013845584355294704\n",
      "======> epoch: 4/400, Loss:0.015268547460436821\n",
      "======> epoch: 4/400, Loss:0.014319264329969883\n",
      "======> epoch: 4/400, Loss:0.015384243801236153\n",
      "======> epoch: 4/400, Loss:0.012353151105344296\n",
      "======> epoch: 4/400, Loss:0.013739376328885555\n",
      "======> epoch: 4/400, Loss:0.014445124194025993\n",
      "======> epoch: 4/400, Loss:0.017497677356004715\n",
      "======> epoch: 4/400, Loss:0.016876457259058952\n",
      "======> epoch: 4/400, Loss:0.011744018644094467\n",
      "======> epoch: 4/400, Loss:0.015234692022204399\n",
      "======> epoch: 4/400, Loss:0.014151773415505886\n",
      "======> epoch: 4/400, Loss:0.01285676471889019\n",
      "======> epoch: 4/400, Loss:0.013443955220282078\n",
      "======> epoch: 4/400, Loss:0.015214509330689907\n",
      "======> epoch: 4/400, Loss:0.019772198051214218\n",
      "======> epoch: 4/400, Loss:0.013030389323830605\n",
      "======> epoch: 4/400, Loss:0.015037703327834606\n",
      "======> epoch: 4/400, Loss:0.013397951610386372\n",
      "======> epoch: 4/400, Loss:0.01335206814110279\n",
      "======> epoch: 4/400, Loss:0.012784253805875778\n",
      "======> epoch: 4/400, Loss:0.014785894192755222\n",
      "======> epoch: 4/400, Loss:0.010444612242281437\n",
      "======> epoch: 4/400, Loss:0.014658733271062374\n",
      "======> epoch: 4/400, Loss:0.014581813476979733\n",
      "======> epoch: 4/400, Loss:0.015515916049480438\n",
      "======> epoch: 4/400, Loss:0.015905385836958885\n",
      "======> epoch: 4/400, Loss:0.012337174266576767\n",
      "======> epoch: 4/400, Loss:0.016717400401830673\n",
      "======> epoch: 4/400, Loss:0.012272825464606285\n",
      "======> epoch: 4/400, Loss:0.012481738813221455\n",
      "======> epoch: 4/400, Loss:0.011654820293188095\n",
      "======> epoch: 4/400, Loss:0.015189138241112232\n",
      "======> epoch: 4/400, Loss:0.015094944275915623\n",
      "======> epoch: 4/400, Loss:0.01032867282629013\n",
      "======> epoch: 4/400, Loss:0.01583944819867611\n",
      "======> epoch: 4/400, Loss:0.01737714186310768\n",
      "======> epoch: 4/400, Loss:0.011870301328599453\n",
      "======> epoch: 4/400, Loss:0.014440846629440784\n",
      "======> epoch: 4/400, Loss:0.013409368693828583\n",
      "======> epoch: 4/400, Loss:0.012861223891377449\n",
      "======> epoch: 4/400, Loss:0.011656118556857109\n",
      "======> epoch: 4/400, Loss:0.01559228915721178\n",
      "======> epoch: 4/400, Loss:0.01379813440144062\n",
      "======> epoch: 4/400, Loss:0.011014891788363457\n",
      "======> epoch: 4/400, Loss:0.015420395880937576\n",
      "======> epoch: 4/400, Loss:0.01554964855313301\n",
      "======> epoch: 4/400, Loss:0.01053257193416357\n",
      "======> epoch: 4/400, Loss:0.015234122052788734\n",
      "======> epoch: 4/400, Loss:0.012961294502019882\n",
      "======> epoch: 4/400, Loss:0.015126112848520279\n",
      "======> epoch: 4/400, Loss:0.01707277074456215\n",
      "======> epoch: 4/400, Loss:0.016275528818368912\n",
      "======> epoch: 4/400, Loss:0.013364323414862156\n",
      "======> epoch: 4/400, Loss:0.008572092279791832\n",
      "======> epoch: 4/400, Loss:0.011521607637405396\n",
      "======> epoch: 4/400, Loss:0.014932911843061447\n",
      "======> epoch: 4/400, Loss:0.012759502977132797\n",
      "======> epoch: 4/400, Loss:0.008746741339564323\n",
      "======> epoch: 4/400, Loss:0.01284430269151926\n",
      "======> epoch: 4/400, Loss:0.013791785575449467\n",
      "======> epoch: 4/400, Loss:0.014505541883409023\n",
      "======> epoch: 4/400, Loss:0.014191953465342522\n",
      "======> epoch: 4/400, Loss:0.014176174998283386\n",
      "======> epoch: 4/400, Loss:0.013183954171836376\n",
      "======> epoch: 4/400, Loss:0.020277827978134155\n",
      "======> epoch: 4/400, Loss:0.015059995464980602\n",
      "======> epoch: 4/400, Loss:0.013315797783434391\n",
      "======> epoch: 4/400, Loss:0.012612717226147652\n",
      "======> epoch: 4/400, Loss:0.014241833239793777\n",
      "======> epoch: 4/400, Loss:0.012278526090085506\n",
      "======> epoch: 4/400, Loss:0.016047414392232895\n",
      "======> epoch: 4/400, Loss:0.01473415270447731\n",
      "======> epoch: 4/400, Loss:0.015539403073489666\n",
      "======> epoch: 4/400, Loss:0.013094224967062473\n",
      "======> epoch: 4/400, Loss:0.01585894264280796\n",
      "======> epoch: 4/400, Loss:0.012552744708955288\n",
      "Entering Epoch:  5\n",
      "======> epoch: 5/400, Loss:0.01660604402422905\n",
      "======> epoch: 5/400, Loss:0.012635298073291779\n",
      "======> epoch: 5/400, Loss:0.013060428202152252\n",
      "======> epoch: 5/400, Loss:0.01573721505701542\n",
      "======> epoch: 5/400, Loss:0.01631009206175804\n",
      "======> epoch: 5/400, Loss:0.012724598869681358\n",
      "======> epoch: 5/400, Loss:0.009754952043294907\n",
      "======> epoch: 5/400, Loss:0.014592040330171585\n",
      "======> epoch: 5/400, Loss:0.019185297191143036\n",
      "======> epoch: 5/400, Loss:0.01551908254623413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======> epoch: 5/400, Loss:0.015087977983057499\n",
      "======> epoch: 5/400, Loss:0.017277313396334648\n",
      "======> epoch: 5/400, Loss:0.01778082549571991\n",
      "======> epoch: 5/400, Loss:0.015725431963801384\n",
      "======> epoch: 5/400, Loss:0.010808336548507214\n",
      "======> epoch: 5/400, Loss:0.01134143490344286\n",
      "======> epoch: 5/400, Loss:0.014161908067762852\n",
      "======> epoch: 5/400, Loss:0.012777709402143955\n",
      "======> epoch: 5/400, Loss:0.012892469763755798\n",
      "======> epoch: 5/400, Loss:0.0164274200797081\n",
      "======> epoch: 5/400, Loss:0.01278205867856741\n",
      "======> epoch: 5/400, Loss:0.011358330957591534\n",
      "======> epoch: 5/400, Loss:0.012279507704079151\n",
      "======> epoch: 5/400, Loss:0.01549795363098383\n",
      "======> epoch: 5/400, Loss:0.016011903062462807\n",
      "======> epoch: 5/400, Loss:0.012543214485049248\n",
      "======> epoch: 5/400, Loss:0.018343450501561165\n",
      "======> epoch: 5/400, Loss:0.015873238444328308\n",
      "======> epoch: 5/400, Loss:0.013280529528856277\n",
      "======> epoch: 5/400, Loss:0.011825397610664368\n",
      "======> epoch: 5/400, Loss:0.014226004481315613\n",
      "======> epoch: 5/400, Loss:0.01564454473555088\n",
      "======> epoch: 5/400, Loss:0.013008564710617065\n",
      "======> epoch: 5/400, Loss:0.01638745702803135\n",
      "======> epoch: 5/400, Loss:0.015179462730884552\n",
      "======> epoch: 5/400, Loss:0.011853691190481186\n",
      "======> epoch: 5/400, Loss:0.013556774705648422\n",
      "======> epoch: 5/400, Loss:0.013835492543876171\n",
      "======> epoch: 5/400, Loss:0.014843501150608063\n",
      "======> epoch: 5/400, Loss:0.013776055537164211\n",
      "======> epoch: 5/400, Loss:0.016500461846590042\n",
      "======> epoch: 5/400, Loss:0.01770935207605362\n",
      "======> epoch: 5/400, Loss:0.013554804027080536\n",
      "======> epoch: 5/400, Loss:0.012482069432735443\n",
      "======> epoch: 5/400, Loss:0.011922631412744522\n",
      "======> epoch: 5/400, Loss:0.010843660682439804\n",
      "======> epoch: 5/400, Loss:0.01186524610966444\n",
      "======> epoch: 5/400, Loss:0.01340466272085905\n",
      "======> epoch: 5/400, Loss:0.016644923016428947\n",
      "======> epoch: 5/400, Loss:0.011304032988846302\n",
      "======> epoch: 5/400, Loss:0.01774449273943901\n",
      "======> epoch: 5/400, Loss:0.012397744692862034\n",
      "======> epoch: 5/400, Loss:0.016459064558148384\n",
      "======> epoch: 5/400, Loss:0.01659289561212063\n",
      "======> epoch: 5/400, Loss:0.0150329964235425\n",
      "======> epoch: 5/400, Loss:0.009516130201518536\n",
      "======> epoch: 5/400, Loss:0.015314701944589615\n",
      "======> epoch: 5/400, Loss:0.015489468351006508\n",
      "======> epoch: 5/400, Loss:0.011700913310050964\n",
      "======> epoch: 5/400, Loss:0.012724388390779495\n",
      "======> epoch: 5/400, Loss:0.016599133610725403\n",
      "======> epoch: 5/400, Loss:0.014127332717180252\n",
      "======> epoch: 5/400, Loss:0.014689753763377666\n",
      "======> epoch: 5/400, Loss:0.017384754493832588\n",
      "======> epoch: 5/400, Loss:0.015647999942302704\n",
      "======> epoch: 5/400, Loss:0.01744587905704975\n",
      "======> epoch: 5/400, Loss:0.01139130163937807\n",
      "======> epoch: 5/400, Loss:0.016048133373260498\n",
      "======> epoch: 5/400, Loss:0.013419002294540405\n",
      "======> epoch: 5/400, Loss:0.014640690758824348\n",
      "======> epoch: 5/400, Loss:0.014052819460630417\n",
      "======> epoch: 5/400, Loss:0.01523943617939949\n",
      "======> epoch: 5/400, Loss:0.012933870777487755\n",
      "======> epoch: 5/400, Loss:0.012667477130889893\n",
      "======> epoch: 5/400, Loss:0.01622435823082924\n",
      "======> epoch: 5/400, Loss:0.014603044837713242\n",
      "======> epoch: 5/400, Loss:0.017737435176968575\n",
      "======> epoch: 5/400, Loss:0.015975382179021835\n",
      "======> epoch: 5/400, Loss:0.01228500995784998\n",
      "======> epoch: 5/400, Loss:0.013035206124186516\n",
      "======> epoch: 5/400, Loss:0.011128072626888752\n",
      "======> epoch: 5/400, Loss:0.015125269070267677\n",
      "======> epoch: 5/400, Loss:0.017362138256430626\n",
      "======> epoch: 5/400, Loss:0.014877361245453358\n",
      "======> epoch: 5/400, Loss:0.011030617170035839\n",
      "======> epoch: 5/400, Loss:0.013654458336532116\n",
      "======> epoch: 5/400, Loss:0.012539023533463478\n",
      "======> epoch: 5/400, Loss:0.013000579550862312\n",
      "======> epoch: 5/400, Loss:0.015850020572543144\n",
      "======> epoch: 5/400, Loss:0.017763743177056313\n",
      "======> epoch: 5/400, Loss:0.018698178231716156\n",
      "======> epoch: 5/400, Loss:0.015154208056628704\n",
      "======> epoch: 5/400, Loss:0.014827940613031387\n",
      "======> epoch: 5/400, Loss:0.01721188612282276\n",
      "======> epoch: 5/400, Loss:0.014380470849573612\n",
      "======> epoch: 5/400, Loss:0.014599191956222057\n",
      "======> epoch: 5/400, Loss:0.012463690713047981\n",
      "======> epoch: 5/400, Loss:0.015007656998932362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-540-667749118d4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"======> epoch: {}/{}, Loss:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mtrain_den_ae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_feedback_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_unsqueeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-540-667749118d4d>\u001b[0m in \u001b[0;36mtrain_den_ae\u001b[1;34m(mat, epochs, steps_per_epoch, _unsqueeze)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m#-----------------Backward Pass---------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losslist = []\n",
    "model.train()\n",
    "def train_den_ae(mat,epochs= 400,steps_per_epoch = 300, _unsqueeze=True):\n",
    "#     epochs = 120\n",
    "    # l = len(trainloader)\n",
    "    # l = 120\n",
    "#     losslist = []\n",
    "    epochloss = 0\n",
    "    running_loss = 0\n",
    "    steps_per_epoch = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(\"Entering Epoch: \", epoch)\n",
    "        for i in range(steps_per_epoch):\n",
    "            orig, masked, idxs = get_random_batch(X, y, batch_size=batch_size)\n",
    "\n",
    "            #-----------------Forward Pass----------------------\n",
    "            if _unsqueeze:\n",
    "                masked = masked.unsqueeze(2)\n",
    "            output = model(masked, idxs)\n",
    "#             print((output >1).all())\n",
    "#             print((orig >1).all())\n",
    "#             print(output[0,1:20])\n",
    "#             print(masked[0,1:20])\n",
    "#             print(orig.shape, output.shape)\n",
    "            loss = criterion(output, orig)\n",
    "            #-----------------Backward Pass---------------------\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    #         running_loss += loss.item()\n",
    "    #         epochloss += loss.item()\n",
    "    #         #-----------------Log-------------------------------\n",
    "    #         losslist.append(running_loss/l)\n",
    "    #         running_loss=0\n",
    "    #     print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "            if i%10 == 0:\n",
    "                running_loss += loss.item()\n",
    "                epochloss += loss.item()\n",
    "                #-----------------Log-------------------------------\n",
    "                losslist.append(loss.item())\n",
    "                running_loss=0\n",
    "                print(\"======> epoch: {}/{}, Loss:{}\".format(epoch,epochs,loss.item()))\n",
    "                \n",
    "train_den_ae(negative_feedback_mask, _unsqueeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14, device='cuda:0'), tensor(6, device='cuda:0'))"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denoising_autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=3706, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.4)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): Dropout(p=0.6)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.4)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (4): Dropout(p=0.6)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=3706, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked, _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(67, device='cuda:0')"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(masked[0] >0.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4, device='cuda:0')"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output[0] >0.2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1380e-01,  3.8650e-02,  4.6790e-02,  2.2474e-02,  2.4358e-02,\n",
       "         4.2643e-02,  5.7862e-02, -6.0688e-03, -3.6485e-03,  5.4624e-02,\n",
       "         8.3541e-02, -2.6001e-03,  6.8723e-03,  1.2548e-02, -1.3274e-02,\n",
       "         2.8933e-02,  5.9324e-02,  1.3882e-02,  3.2916e-02, -9.5463e-03,\n",
       "         1.0550e-01,  3.2322e-04,  2.4655e-03,  1.0243e-02,  8.3865e-02,\n",
       "         8.0733e-03,  8.6615e-03,  1.2767e-02,  2.6963e-02,  8.2864e-03,\n",
       "        -3.8595e-03,  7.4488e-02,  1.2473e-04,  1.3132e-01,  1.1364e-02,\n",
       "         5.6776e-02,  3.2312e-04,  3.9459e-03,  1.2311e-01,  2.3514e-03,\n",
       "         1.8861e-03,  7.6935e-03,  1.7633e-02, -1.3301e-02,  4.5213e-02,\n",
       "         1.3644e-02,  5.8095e-02,  4.2983e-02,  2.5736e-03,  4.1560e-02,\n",
       "         6.4531e-02, -1.9457e-15,  1.5898e-03,  1.1939e-02, -2.0128e-03,\n",
       "         1.8887e-03,  3.8369e-02, -5.1881e-04,  3.4139e-02, -6.0152e-03,\n",
       "         3.7497e-02,  8.5049e-03,  2.1491e-03, -3.8508e-03, -7.8575e-04,\n",
       "         1.5950e-03,  6.5138e-03,  1.1259e-02,  2.4226e-02, -2.6867e-03,\n",
       "         1.1034e-02,  2.4772e-02,  1.1748e-04, -1.0780e-03, -1.0336e-02,\n",
       "         8.7505e-03,  1.3066e-03, -2.2558e-03,  7.0896e-03,  2.3801e-03,\n",
       "         1.2567e-02,  3.6734e-03, -2.7582e-03,  4.2024e-02, -1.1978e-02,\n",
       "         8.7472e-03, -4.7314e-03, -7.1648e-03, -4.5578e-03, -1.0795e-03,\n",
       "        -1.9503e-03,  1.1471e-02,  1.9247e-02, -1.1425e-03, -8.9776e-04,\n",
       "         5.0158e-03,  2.6625e-03, -8.7783e-03,  9.2172e-03,  3.7275e-03],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400](orig.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output.round() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.round()[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0,100:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_denoising_autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, inSize,fSize = 32, nz=24):  #sigma is the corruption level\n",
    "        super(conv_denoising_autoencoder, self).__init__()\n",
    "        #define layers here\n",
    "\n",
    "        self.inp_size = inSize\n",
    "        self.nz = nz\n",
    "        self.fSize = 32\n",
    "#         self.imSize = imSize\n",
    "#         self.sigma = sigma\n",
    "#         self.multimodalZ = multimodalZ\n",
    "\n",
    "#         inSize = imSize / ( 2 ** 4)\n",
    "#         self.inSize = inSize\n",
    "    \n",
    "        self.enc1 = nn.Conv1d(self.inp_size, fSize, 5, stride=2, padding=2)\n",
    "        self.enc2 = nn.Conv1d(fSize, fSize * 2, 5, stride=2, padding=2)\n",
    "        self.enc3 = nn.Conv1d(fSize * 2, fSize * 4, 5, stride=2, padding=2)\n",
    "        self.enc4 = nn.Conv1d(fSize * 4, fSize * 8, 5, stride=2, padding=2)\n",
    "        self.enc5 = nn.Linear(fSize * 8, self.nz)\n",
    "\n",
    "        self.dec1 = nn.Linear(self.nz, fSize * 8)\n",
    "        self.dec2 = nn.ConvTranspose1d(fSize * 8, fSize * 4, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec3 = nn.ConvTranspose1d(fSize * 4, fSize * 2, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec4 = nn.ConvTranspose1d(fSize * 2, fSize, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.dec5 = nn.ConvTranspose1d(fSize, 32, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.last_dec = nn.Linear(32*16, self.inp_size)\n",
    "        \n",
    "        self.useCUDA = torch.cuda.is_available()\n",
    "\n",
    "#     def norm_prior(self, noSamples=25):\n",
    "#         z = torch.randn(noSamples, self.nz)\n",
    "#         return z\n",
    "\n",
    "#     def multi_prior(self, noSamples=25, mode=None):\n",
    "#         #make a 2D sqrt(nz)-by-sqrt(nz) grid of gaussians\n",
    "#         num = np.sqrt(self.nz) #no of modes in x and y\n",
    "#         STD = 1.0\n",
    "#         modes = np.arange(-num,num)\n",
    "#         p = np.random.uniform(0, num,(noSamples*2))\n",
    "\n",
    "#         if mode is None:\n",
    "#             mu = modes[np.floor(2 * p).astype(int)]\n",
    "#         else:\n",
    "#             mu = modes[np.ones((noSamples, 2), dtype=int) * int(mode)]\n",
    "\n",
    "#         z = torch.Tensor(mu).view(-1,2) + STD * torch.randn(noSamples, 2)\n",
    "#         return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        self.batch_size = x.shape[0]\n",
    "        #define the encoder here return mu(x) and sigma(x)\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#     def corrupt(self, x):\n",
    "#         noise = self.sigma * Variable(torch.randn(x.size())).type_as(x)\n",
    "#         return x + noise\n",
    "\n",
    "#     def sample_z(self, noSamples=25, mode=None):\n",
    "#         if not self.multimodalZ:\n",
    "#             z = self.norm_prior(noSamples=noSamples)\n",
    "#         else:\n",
    "#             z = self.multi_prior(noSamples=noSamples, mode=mode)\n",
    "#         if self.useCUDA:\n",
    "#             return Variable(z.cuda())\n",
    "#         else:\n",
    "#             return Variable(z)\n",
    "\n",
    "    def decode(self, z):\n",
    "        #define the decoder here\n",
    "        z = F.relu(self.dec1(z))\n",
    "        z = z.unsqueeze(2)\n",
    "#         print(z.shape)\n",
    "#         z = z.view(z.size(0), -1, self.inp_size)\n",
    "        z = F.relu(self.dec2(z))\n",
    "        z = F.relu(self.dec3(z))\n",
    "        z = F.relu(self.dec4(z))\n",
    "        z = F.sigmoid(self.dec5(z))\n",
    "#         print(z.shape)\n",
    "#         z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "        z = F.sigmoid(self.last_dec(z.view(self.batch_size, -1)))\n",
    "    \n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the outputs needed for training\n",
    "#         x_corr = self.corrupt(x)\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_denoising_autoencoder(train[0,:].shape[0]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),lr=0.01, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "train_den_ae(negative_feedback_mask, steps_per_epoch=steps_per_epoch, _unsqueeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losslist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig, masked, _ = get_random_masked_batch(negative_feedback_mask, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(orig[0][:400] > 0).sum(), (masked[0][:400] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked[0][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------Forward Pass----------------------\n",
    "output = model(masked.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output[7]).round() >0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output.shape[0]):\n",
    "    print(((output[i]).round() >0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((output).round() >= 1 ).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output).round()[0][2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/gtshs2/Collaborative-Denoising-Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tr + vr) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "def autoEncoder(X):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(X.shape[1],), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1')(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(256, activation='selu', name='LatentSpace')(enc)\n",
    "    lat_space = Dropout(0.8, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1')(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(X.shape[1], activation='linear', name='UserScorePred')(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.35)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = torch.FloatTensor((np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > 0.35)).to(device)\n",
    "X = negative_feedback_mask.cpu().numpy()\n",
    "y = (negative_feedback_mask*mask_arr).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "UserScore (InputLayer)       (None, 3706)              0         \n",
      "_________________________________________________________________\n",
      "EncLayer1 (Dense)            (None, 512)               1897984   \n",
      "_________________________________________________________________\n",
      "LatentSpace (Dense)          (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "DecLayer1 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "UserScorePred (Dense)        (None, 3706)              1901178   \n",
      "=================================================================\n",
      "Total params: 4,062,074\n",
      "Trainable params: 4,062,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = autoEncoder(X)\n",
    "\n",
    "model.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mmdnn\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/20/1fb6420b806c546392c045f98ff3d0ede51011db2b56f9552a18a1b31506/mmdnn-0.2.5-py2.py3-none-any.whl (317kB)\n",
      "Requirement already satisfied: pillow>=3.1.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from mmdnn) (5.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from mmdnn) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from mmdnn) (1.16.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from mmdnn) (3.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\david\\anaconda3\\lib\\site-packages (from protobuf>=3.6.0->mmdnn) (41.6.0)\n",
      "Installing collected packages: mmdnn\n",
      "Successfully installed mmdnn-0.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install mmdnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6040/6040 [==============================] - ETA: 8s - loss: 0.090 - ETA: 5s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 3s - loss: 0.093 - ETA: 3s - loss: 0.094 - ETA: 2s - loss: 0.095 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.094 - ETA: 2s - loss: 0.093 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.092 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.091 - ETA: 2s - loss: 0.090 - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.089 - ETA: 1s - loss: 0.089 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.084 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - 3s 456us/step - loss: 0.0764\n",
      "Epoch 2/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.056 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.056 - ETA: 2s - loss: 0.056 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.055 - ETA: 2s - loss: 0.054 - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.056 - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.054 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.053 - ETA: 1s - loss: 0.052 - ETA: 1s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.051 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.050 - 3s 434us/step - loss: 0.0500\n",
      "Epoch 3/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.043 - ETA: 2s - loss: 0.044 - ETA: 2s - loss: 0.042 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 2s - loss: 0.041 - ETA: 1s - loss: 0.041 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.040 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 1s - loss: 0.039 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.038 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.037 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - ETA: 0s - loss: 0.036 - 3s 430us/step - loss: 0.0362\n",
      "Epoch 4/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.032 - ETA: 2s - loss: 0.031 - ETA: 2s - loss: 0.030 - ETA: 2s - loss: 0.029 - ETA: 2s - loss: 0.029 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.030 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 1s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.029 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - ETA: 0s - loss: 0.028 - 3s 426us/step - loss: 0.0282\n",
      "Epoch 5/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.023 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.025 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 2s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.025 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.024 - ETA: 1s - loss: 0.023 - ETA: 1s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.022 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.023 - ETA: 0s - loss: 0.022 - 3s 441us/step - loss: 0.0230\n",
      "Epoch 6/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.018 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.019 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 1s - loss: 0.020 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - ETA: 0s - loss: 0.019 - 3s 455us/step - loss: 0.0196\n",
      "Epoch 7/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.020 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.019 - ETA: 2s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.018 - ETA: 1s - loss: 0.017 - ETA: 1s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.018 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - ETA: 0s - loss: 0.017 - 3s 463us/step - loss: 0.0173\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.016 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - ETA: 0s - loss: 0.015 - 3s 440us/step - loss: 0.0157\n",
      "Epoch 9/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.016 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.015 - ETA: 2s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.015 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 1s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - ETA: 0s - loss: 0.014 - 3s 431us/step - loss: 0.0145\n",
      "Epoch 10/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 3s 454us/step - loss: 0.0137\n",
      "Epoch 11/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.014 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 1s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - ETA: 0s - loss: 0.013 - 3s 418us/step - loss: 0.0130\n",
      "Epoch 12/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 3s 420us/step - loss: 0.0125\n",
      "Epoch 13/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - 3s 420us/step - loss: 0.0121\n",
      "Epoch 14/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 1s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.012 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0118\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0116\n",
      "Epoch 16/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 421us/step - loss: 0.0114\n",
      "Epoch 17/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0112\n",
      "Epoch 18/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.013 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 420us/step - loss: 0.0111\n",
      "Epoch 19/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - ETA: 0s - loss: 0.011 - 3s 432us/step - loss: 0.0109\n",
      "Epoch 20/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 425us/step - loss: 0.0109\n",
      "Epoch 21/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 419us/step - loss: 0.0108\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 419us/step - loss: 0.0107\n",
      "Epoch 23/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 420us/step - loss: 0.0106\n",
      "Epoch 24/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 417us/step - loss: 0.0106\n",
      "Epoch 25/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 421us/step - loss: 0.0105\n",
      "Epoch 26/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 463us/step - loss: 0.0105\n",
      "Epoch 27/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 449us/step - loss: 0.0104\n",
      "Epoch 28/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 450us/step - loss: 0.0104\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 445us/step - loss: 0.0104\n",
      "Epoch 30/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 448us/step - loss: 0.0104\n",
      "Epoch 31/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 447us/step - loss: 0.0103\n",
      "Epoch 32/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0103\n",
      "Epoch 33/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.011 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0103\n",
      "Epoch 34/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0103\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.012 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0103\n",
      "Epoch 36/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0102\n",
      "Epoch 37/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0102\n",
      "Epoch 38/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 455us/step - loss: 0.0102\n",
      "Epoch 39/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0102\n",
      "Epoch 40/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0102\n",
      "Epoch 41/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 459us/step - loss: 0.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0102\n",
      "Epoch 43/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n",
      "Epoch 44/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0101\n",
      "Epoch 45/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.011 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 451us/step - loss: 0.0101\n",
      "Epoch 46/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 453us/step - loss: 0.0101\n",
      "Epoch 47/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.009 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 457us/step - loss: 0.0101\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040/6040 [==============================] - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 2s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n",
      "Epoch 49/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n",
      "Epoch 50/50\n",
      "6040/6040 [==============================] - ETA: 2s - loss: 0.007 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.008 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 2s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.009 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 1s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - ETA: 0s - loss: 0.010 - 3s 454us/step - loss: 0.0101\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=X, y=y,\n",
    "                  epochs=50,\n",
    "                  batch_size=128,\n",
    "                  shuffle=True,\n",
    "                  validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hist(hist):\n",
    "    # summarize history for loss\n",
    "    fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    #plt.plot(hist.history['val_loss'])\n",
    "\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict new Matrix Interactions, set score zero on visualized games\n",
    "# new_matrix = model.predict(X) * (X == 0)\n",
    "new_matrix = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00086715, -0.00470824, -0.01668992, ...,  0.00431625,\n",
       "         0.00356173, -0.01001819],\n",
       "       [ 0.04016824,  0.01442963,  0.01442499, ...,  0.00142624,\n",
       "        -0.00103543,  0.01002616],\n",
       "       [ 0.02150744, -0.00140122,  0.01076992, ..., -0.00505046,\n",
       "        -0.01062662, -0.00433114],\n",
       "       ...,\n",
       "       [-0.00080187,  0.00507312,  0.00350602, ...,  0.00522393,\n",
       "        -0.00791932, -0.00865945],\n",
       "       [ 0.04051276, -0.01438116, -0.02296973, ...,  0.00456508,\n",
       "        -0.01364751,  0.00301851],\n",
       "       [ 0.1159841 ,  0.04672968,  0.03519899, ...,  0.00532098,\n",
       "         0.00525959, -0.02073006]], dtype=float32)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # converting the reconstructed matrix back to a Pandas dataframe\n",
    "# new_users_items_matrix_df  = pd.DataFrame(new_matrix, \n",
    "#                                           columns = users_items_matrix_df.columns, \n",
    "#                                           index   = users_items_matrix_df.index)\n",
    "# new_users_items_matrix_df.head()\n",
    "\n",
    "new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_matrix>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(382567, device='cuda:0')"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(negative_feedback_mask > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
