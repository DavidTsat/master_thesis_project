{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 4.516679763793945 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tr_movielens_1m', tr)\n",
    "np.save('./vr_movielens_1m', vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "############## Pytorch model doesn't converge - to do - check #################\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def autoEncoder(input_size):\n",
    "    '''\n",
    "    Autoencoder for Collaborative Filter Model\n",
    "    '''\n",
    "\n",
    "    # Input\n",
    "    input_layer = Input(shape=(input_size,), name='UserScore')\n",
    "    \n",
    "    # Encoder\n",
    "    # -----------------------------\n",
    "    enc = Dense(512, activation='selu', name='EncLayer1', kernel_regularizer=regularizers.l2(0.000001))(input_layer)\n",
    "\n",
    "    # Latent Space\n",
    "    # -----------------------------\n",
    "    lat_space = Dense(512, activation='selu', name='LatentSpace', kernel_regularizer=regularizers.l2(0.000001))(enc)\n",
    "    lat_space = Dropout(0.5, name='Dropout')(lat_space) # Dropout\n",
    "\n",
    "    # Decoder\n",
    "    # -----------------------------\n",
    "    dec = Dense(512, activation='selu', name='DecLayer1', kernel_regularizer=regularizers.l2(0.000001))(lat_space)\n",
    "\n",
    "    # Output\n",
    "    output_layer = Dense(input_size, activation='linear', name='UserScorePred', kernel_regularizer=regularizers.l2(0.000001))(dec)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    model = Model(input_layer, output_layer)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train == 0)\n",
    "positive_feedback = (train > 3)\n",
    "negative_feedback = ((train < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517621, 382567)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_feedback.sum(), negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback + negative_feedback != zero_mask).all()\n",
    "assert (positive_feedback + negative_feedback == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95.97847414073473, 2.3124349989099473, 1.7090908603553212)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback), get_sparsity(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265267, 2.3124349989099473, 1.7090908603553212, 4.021525859265268)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 - get_sparsity(zero_mask), get_sparsity(positive_feedback), get_sparsity(negative_feedback), get_sparsity(positive_feedback) + get_sparsity(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_random_batch(mat, batch_size=64):\n",
    "#     '''\n",
    "#     returns random rows of size batch_size\n",
    "#     '''\n",
    "#     rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "# #     print(mat.shape, rand_rows)\n",
    "# #     print(mat[rand_rows].shape)\n",
    "#     return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, mat, p=0.5, batch_size=64):\n",
    "        '''\n",
    "        mat is a binary matrix (e.g. positive feedback, or negative feedback)\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.mat = mat\n",
    "        self.p = p\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.mat.shape[0] / self.batch_size))\n",
    "    \n",
    "    def gen_item_GAN(self):\n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y, indexes\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_negative = DataGenerator(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _ = generator_negative.gen_item_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6324203993524016, 1.2534572315164598)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(x), get_sparsity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (x*y == x).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = 0.4\n",
    "# mask_arr_neg = (np.random.rand(negative_feedback_mask.shape[0], negative_feedback_mask.shape[1]) > P)\n",
    "# y_neg = negative_feedback_mask\n",
    "# X_neg = negative_feedback_mask*mask_arr_neg # corrupting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_arr_pos = (np.random.rand(positive_feedback_mask.shape[0], positive_feedback_mask.shape[1]) > P)\n",
    "# y_pos = positive_feedback_mask\n",
    "# X_pos = positive_feedback_mask*mask_arr_pos # corrupting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupted_tr = (mask_arr_neg*(tr>0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sparsity(tr), get_sparsity(X_neg), get_sparsity(y_neg), get_sparsity(X_pos), get_sparsity(y_pos), get_sparsity(corrupted_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg = autoEncoder(tr.shape[1])\n",
    "# model_neg.compile(optimizer = Adam(lr=0.0001), loss='mse')\n",
    "\n",
    "# model_pos = autoEncoder(tr.shape[1])\n",
    "# model_pos.compile(optimizer = Adam(lr=0.0001), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg = keras.models.load_model('./model_neg')\n",
    "model_pos = keras.models.load_model('./model_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "UserScore (InputLayer)       (None, 3706)              0         \n",
      "_________________________________________________________________\n",
      "EncLayer1 (Dense)            (None, 512)               1897984   \n",
      "_________________________________________________________________\n",
      "LatentSpace (Dense)          (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "DecLayer1 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "UserScorePred (Dense)        (None, 3706)              1901178   \n",
      "=================================================================\n",
      "Total params: 4,324,474\n",
      "Trainable params: 4,324,474\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_neg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_tr = np.load('predicted_tr.npy')\n",
    "# augmented_train = np.load('augmented_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_neg = model_neg.fit_generator(\n",
    "#                     generator=generator_negative,\n",
    "#                     epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_pos = model_pos.fit(x=X_pos, y=y_pos,\n",
    "#                   epochs=300,\n",
    "#                   batch_size=128,\n",
    "#                   shuffle=True,\n",
    "#                   validation_split=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# def plot_hist(hist):\n",
    "#     # summarize history for loss\n",
    "#     fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "\n",
    "#     plt.title('model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "#     plt.plot(hist.history['loss'])\n",
    "#     #plt.plot(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(hist_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(hist_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_neg.save('./model_neg')\n",
    "# # model_pos.save('./model_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.5\n",
    "mask_arr_neg = (np.random.rand(negative_feedback.shape[0], negative_feedback.shape[1]) > P)\n",
    "y_neg = negative_feedback\n",
    "X_neg = negative_feedback*mask_arr_neg # corrupting\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "predicted_neg = model_neg.predict(X_neg)\n",
    "# predicted_pos = model_pos.predict(X_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66795, 382567)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_neg > 0.5).sum(), (y_neg == 1).sum() # predicted vs real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (predicted_neg>0.5).sum(), (predicted_pos>0.5).sum() # trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9829927389774684\n"
     ]
    }
   ],
   "source": [
    "print((y_neg * (predicted_neg>0.5)).sum()/(predicted_neg>0.5).sum()) # accuracy on actual \n",
    "# print((y_pos * (predicted_pos>0.4)).sum()/(predicted_pos>0.4).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((y_pos * (predicted_neg>0.4)).sum()/y_pos.sum()) # just to see that it's a low number\n",
    "# print((y_neg * (predicted_pos>0.4)).sum()/y_neg.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829927389774684"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_neg * (predicted_neg>0.5)).sum()/((predicted_neg>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (y_pos * (predicted_pos>0.5)).sum()/((predicted_pos>0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22652"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((predicted_neg>0.5)  * (X_neg<0.5)).sum() # predicted values which were not in the train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((predicted_pos>0.5)  * (X_pos<0.5)).sum() # predicted values which were not in the train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949849902878333"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_neg * (((predicted_neg>0.5)  * (X_neg<0.5)))) == 1).sum()/(((predicted_neg>0.5)  * (X_neg<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((y_pos * (((predicted_pos>0.5)  * (X_pos<0.5)))) == 1).sum()/(((predicted_pos>0.5)  * (X_pos<0.5))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add_negative = model_neg.predict(y_neg)\n",
    "# to_add_positive = model_neg.predict(y_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add_negative = (to_add_negative>0.5).astype(int)\n",
    "# to_add_positive = (to_add_positive>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((y_pos * to_add_negative).sum()/y_pos.sum()) # That's working\n",
    "# print((y_neg * to_add_positive).sum()/y_neg.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_add_negative.sum(), to_add_positive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (y_neg* (to_add_negative>0.8)).sum()/(((to_add_negative>0.8)).sum()), (y_pos* (to_add_positive>0.8)).sum()/(((to_add_positive>0.8)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(to_add_negative * tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(to_add_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14126\n",
      "26343\n",
      "49962\n",
      "103159\n",
      "5398\n",
      "1002\n"
     ]
    }
   ],
   "source": [
    "print((to_add_negative * (tr==0)).sum()) # new values\n",
    "print((to_add_negative * (tr==1)).sum()) \n",
    "print((to_add_negative * (tr==2)).sum())\n",
    "print((to_add_negative * (tr==3)).sum())\n",
    "print((to_add_negative * (tr==4)).sum())\n",
    "print((to_add_negative * (tr==5)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_add_negative = to_add_negative*(tr==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(to_add_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to keep the balance\n",
    "# threshold_neg = 0.2\n",
    "# threshold_pos = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((to_add_negative > threshold_neg) * (tr==0)).sum(), ((to_add_positive > threshold_pos) * (tr==0)).sum() # new values # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_probs_neg = [(tr == 1).sum()/((tr > 0) & (tr < 4)).sum(), (tr == 2).sum()/(((tr > 0) & (tr < 4))).sum(), (tr == 3).sum()/((tr > 0) & (tr < 4)).sum()]\n",
    "# p_probs_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_probs_pos = [(tr == 4).sum()/((tr > 3) & (tr <= 5)).sum(), (tr == 5).sum()/((tr > 3) & (tr <= 5)).sum()]\n",
    "# p_probs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_train = tr + (to_add_negative > threshold_neg) * (tr == 0) * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs_neg) + (to_add_positive > threshold_pos) * (tr == 0) * np.random.choice(np.arange(4, 6), tr.shape, p=p_probs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_train = tr + to_add_negative * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sparsity(tr), get_sparsity(augmented_train) # reduced sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (np.isin(tr, augmented_train)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(5, (5 == tr).sum())\n",
    "# print(4, (4 == tr).sum())\n",
    "# print(3, (3 == tr).sum())\n",
    "# print(2, (2 == tr).sum())\n",
    "# print(1, (1 == tr).sum())\n",
    "# print(0, (0 == tr).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(5, (5 == augmented_train).sum())\n",
    "# print(4, (4 == augmented_train).sum())\n",
    "# print(3, (3 == augmented_train).sum())\n",
    "# print(2, (2 == augmented_train).sum())\n",
    "# print(1, (1 == augmented_train).sum())\n",
    "# print(0, (0 == augmented_train).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((tr == 0) * (augmented_train > 0)).sum() # new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.save('predicted_tr', predicted_tr)\n",
    "# np.save('augmented_train', augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, feat_size):\n",
    "        super(NetD, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "#         self.use_cuda = True\n",
    "#         self.feat_size = feat_size\n",
    "        # top\n",
    "#         print(self.feat_size*2)\n",
    "        self.t1 = torch.nn.Linear(self.feat_size, 512)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(self.feat_size, 512)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 512, self.feat_size)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        \n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "#         if self.use_cuda: \n",
    "        idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "        x = filt * x\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_size):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz + self.feat_size, 512), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "#                                 torch.nn.ReLU(), \n",
    "# #                                 nn.Dropout(0.5),\n",
    "#                                 torch.nn.Linear(2048, 2048),\n",
    "                                torch.nn.ReLU(), \n",
    "#                                 torch.nn.BatchNorm1d(512),\n",
    "#                                 nn.Dropout(0.6),\n",
    "                                torch.nn.Linear(512, features_length), \n",
    "                                torch.nn.Sigmoid()\n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "        \n",
    "    def forward(self, e_mask, x):\n",
    "        x = self.netGen(x)\n",
    "        x = x * e_mask\n",
    "        return x\n",
    "#         return F.dropout(x, 0.7)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_random_batch(mat, batch_size=64):\n",
    "#     '''\n",
    "#     returns random rows of size batch_size\n",
    "#     '''\n",
    "#     rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "# #     print(mat.shape, rand_rows)\n",
    "# #     print(mat[rand_rows].shape)\n",
    "#     return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = torch.autograd.Variable(torch.Tensor(train))\n",
    "# augmented_train = torch.autograd.Variable(torch.Tensor(augmented_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train\n",
    "# del augmented_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sparsity(train.cpu().numpy()), get_sparsity(augmented_train.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx = get_random_batch(train)\n",
    "# xy = get_random_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_my(xx, xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.sum(torch.abs(torch.abs(xx != 0).float()*xy - xy), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx > xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def d_my(x_r, x_g): # custom loss -todo\n",
    "# #     return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r), 1)/x_r.shape[1]\n",
    "\n",
    "# def d_my(x_r, x_g): # custom loss -todo\n",
    "#     return torch.sum(torch.abs(x_g - x_r), 1)/x_r.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_generator(corrupted, original, batch_size=64):\n",
    "#     rand_rows = np.random.randint(corrupted.shape[0], size=batch_size)\n",
    "#     return torch.Tensor(corrupted[rand_rows]).cuda().float(), torch.Tensor(original[rand_rows]).cuda().float(), rand_rows\n",
    "# #     return torch.from_numpy(corrupted[rand_rows]).float(), torch.from_numpy(original[rand_rows]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sparsity(X_neg), get_sparsity(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b,_ = batch_generator(X_neg, (tr > 0).astype(float))\n",
    "\n",
    "# get_sparsity(a.cpu().numpy()), get_sparsity(b.cpu().numpy()), a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_losses = []\n",
    "disc_losses = []\n",
    "def train_GAN(netD, netG, negative, steps_per_epoch = 100, epochs = 5):\n",
    "    d_iter = 5\n",
    "    g_iter = 1\n",
    "    gen_iterations = 0\n",
    "#     gen_losses = []\n",
    "#     disc_losses = []\n",
    "#     train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for c in range(steps_per_epoch):\n",
    "            data_iter = 100\n",
    "            i = 0\n",
    "            while i < 100:\n",
    "                ############################\n",
    "                # (1) Update D network\n",
    "                ###########################\n",
    "                for p in netD.parameters(): # reset requires_grad\n",
    "                    p.requires_grad = True # they are set to False below in netG update\n",
    "    #             d_iter = d_iter\n",
    "                j = 0\n",
    "                while j < d_iter*5:\n",
    "                    j += 1\n",
    "                    # load real data\n",
    "                    i += 1\n",
    "                    if negative:\n",
    "                        condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "#                         condition, X, idxs = batch_generator(X_neg, y_neg)\n",
    "    #                 X, _ = data_iter.next()\n",
    "    #                 X = X.view(X.size(0), -1)\n",
    "    #                 X = (X >= 0.5).float()\n",
    "#                     if cuda: \n",
    "                    X = torch.from_numpy(X).float().cuda()\n",
    "                    condition = torch.from_numpy(condition).float().cuda()\n",
    "    #                 print(condition.shape, X_neg.shape, y_neg.shape)\n",
    "                    real = Variable(X)\n",
    "\n",
    "                    # generate fake data\n",
    "                    noise = torch.randn(batch_size, nz)\n",
    "#                     if cuda: \n",
    "                    noise = noise.cuda()\n",
    "#                     noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "                    with torch.no_grad():\n",
    "                        noisev = Variable(noise) # totally freeze netG\n",
    "                    concated = torch.cat((noisev, condition), 1)\n",
    "    #                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\n",
    "                    e_mask = torch.Tensor(tr[idxs]>0).cuda()\n",
    "                    fake = Variable(netG(e_mask, concated).data)\n",
    "\n",
    "                    # compute gradient, take step\n",
    "                    netD.zero_grad()\n",
    "    #                 concated_real = torch.cat((real, condition), 1)\n",
    "    #                 print(concated_real)\n",
    "                    out = netD(real, fake)\n",
    "                    outputD = torch.mean(out) + lamba * out.norm()\n",
    "                    stdD = torch.std(out)\n",
    "                    outputD.backward(mone)\n",
    "                    optimizerD.step()\n",
    "#                     print('AAAAAAAAA mse:=WWWWWWWWWWWWWWWWWWWWWW')\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "\n",
    "    #         g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "                netG.zero_grad()\n",
    "                # load real data\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "                    \n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "                condition = torch.from_numpy(condition).float().cuda()\n",
    "                real = Variable(X)\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "                noise = noise.cuda()\n",
    "                noisev = Variable(noise)\n",
    "                concated_ = torch.cat((noisev, condition), 1)\n",
    "                e_mask_ = torch.Tensor(tr[idxs]>0).cuda()\n",
    " \n",
    "                fake = netG(e_mask_, concated_)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "                gen_iterations += 1\n",
    "#             print('AAAAAA')\n",
    "            eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "#             eval_losses.append(eval_loss)\n",
    "#             print('mse:', eval_loss)\n",
    "#             print(outputG.item(), outputD.item())\n",
    "            gen_losses.append(outputG.item())\n",
    "            disc_losses.append(outputD.item())\n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, 100, gen_iterations, outputD.item(), outputG.item()))\n",
    "    return gen_losses, disc_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sparsity(X_neg), get_sparsity(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrD = 5e-4\n",
    "# lrG = 5e-4\n",
    "# batch_size = 128\n",
    "# cuda = True\n",
    "# epochs = 1000 #change\n",
    "# seed = 1\n",
    "# nz = 16\n",
    "# d_iter = 5\n",
    "# g_iter = 1\n",
    "# lamba = 2e-4\n",
    "\n",
    "lrD = 2e-4\n",
    "lrG = 2e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 5\n",
    "# device = 5\n",
    "seed = 1\n",
    "nz = 20\n",
    "lamba = 1e-2\n",
    "# lamba = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=512, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=512, bias=True)\n",
      "  (fc): Linear(in_features=1024, out_features=3706, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=3726, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=3706, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_neg = NetD(tr.shape[1]).cuda()\n",
    "netG_neg = NetG(tr.shape[1]).cuda()\n",
    "print(netD_neg)\n",
    "print(netG_neg)\n",
    "optimizerG = optim.RMSprop(netG_neg.parameters(), lr=lrG, weight_decay=2e-5)\n",
    "optimizerD = optim.RMSprop(netD_neg.parameters(), lr=lrD, weight_decay=2e-5)\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = (-1 * one).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG_neg.load_state_dict(torch.load('./netG_neg-1m'))\n",
    "# netG_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][101/100][1] Loss_D: 0.015590 Loss_G: 0.013127 \n",
      "[0/5][101/100][2] Loss_D: 0.012274 Loss_G: 0.019413 \n",
      "[0/5][101/100][3] Loss_D: 0.013818 Loss_G: 0.010213 \n",
      "[0/5][101/100][4] Loss_D: 0.021555 Loss_G: 0.013341 \n",
      "[0/5][101/100][5] Loss_D: 0.012423 Loss_G: 0.015593 \n",
      "[0/5][101/100][6] Loss_D: 0.014124 Loss_G: 0.005573 \n",
      "[0/5][101/100][7] Loss_D: 0.013160 Loss_G: 0.013912 \n",
      "[0/5][101/100][8] Loss_D: 0.007706 Loss_G: 0.010417 \n",
      "[0/5][101/100][9] Loss_D: 0.012324 Loss_G: 0.007507 \n",
      "[0/5][101/100][10] Loss_D: 0.004959 Loss_G: 0.007894 \n",
      "[0/5][101/100][11] Loss_D: 0.004364 Loss_G: 0.004738 \n",
      "[0/5][101/100][12] Loss_D: 0.004807 Loss_G: 0.006066 \n",
      "[0/5][101/100][13] Loss_D: 0.006822 Loss_G: 0.005996 \n",
      "[0/5][101/100][14] Loss_D: 0.005354 Loss_G: 0.005258 \n",
      "[0/5][101/100][15] Loss_D: 0.004356 Loss_G: 0.003513 \n",
      "[0/5][101/100][16] Loss_D: 0.005614 Loss_G: 0.007974 \n",
      "[0/5][101/100][17] Loss_D: 0.004348 Loss_G: 0.003956 \n",
      "[0/5][101/100][18] Loss_D: 0.002884 Loss_G: 0.002734 \n",
      "[0/5][101/100][19] Loss_D: 0.003758 Loss_G: 0.002952 \n",
      "[0/5][101/100][20] Loss_D: 0.004187 Loss_G: 0.003242 \n",
      "[0/5][101/100][21] Loss_D: 0.002329 Loss_G: 0.001085 \n",
      "[0/5][101/100][22] Loss_D: 0.003612 Loss_G: 0.002526 \n",
      "[0/5][101/100][23] Loss_D: 0.002282 Loss_G: 0.003718 \n",
      "[0/5][101/100][24] Loss_D: 0.002429 Loss_G: 0.004080 \n",
      "[0/5][101/100][25] Loss_D: 0.007273 Loss_G: 0.004512 \n",
      "[0/5][101/100][26] Loss_D: 0.004906 Loss_G: 0.004124 \n",
      "[0/5][101/100][27] Loss_D: 0.003304 Loss_G: 0.003272 \n",
      "[0/5][101/100][28] Loss_D: 0.002669 Loss_G: 0.006078 \n",
      "[0/5][101/100][29] Loss_D: 0.006548 Loss_G: 0.005731 \n",
      "[0/5][101/100][30] Loss_D: 0.003732 Loss_G: 0.003628 \n",
      "[0/5][101/100][31] Loss_D: 0.007046 Loss_G: 0.005844 \n",
      "[0/5][101/100][32] Loss_D: 0.002106 Loss_G: 0.001461 \n",
      "[0/5][101/100][33] Loss_D: 0.002938 Loss_G: 0.005070 \n",
      "[0/5][101/100][34] Loss_D: 0.003701 Loss_G: 0.003539 \n",
      "[0/5][101/100][35] Loss_D: 0.003300 Loss_G: 0.004942 \n",
      "[0/5][101/100][36] Loss_D: 0.007917 Loss_G: 0.001557 \n",
      "[0/5][101/100][37] Loss_D: 0.002969 Loss_G: 0.006268 \n",
      "[0/5][101/100][38] Loss_D: 0.001835 Loss_G: 0.001336 \n",
      "[0/5][101/100][39] Loss_D: 0.004503 Loss_G: 0.007212 \n",
      "[0/5][101/100][40] Loss_D: 0.002400 Loss_G: 0.003785 \n",
      "[0/5][101/100][41] Loss_D: 0.005291 Loss_G: 0.006407 \n",
      "[0/5][101/100][42] Loss_D: 0.002428 Loss_G: 0.000883 \n",
      "[0/5][101/100][43] Loss_D: 0.004350 Loss_G: 0.003191 \n",
      "[0/5][101/100][44] Loss_D: 0.005185 Loss_G: 0.004192 \n",
      "[0/5][101/100][45] Loss_D: 0.004314 Loss_G: 0.003390 \n",
      "[0/5][101/100][46] Loss_D: 0.004809 Loss_G: 0.003570 \n",
      "[0/5][101/100][47] Loss_D: 0.001308 Loss_G: 0.004006 \n",
      "[0/5][101/100][48] Loss_D: 0.002018 Loss_G: 0.001325 \n",
      "[0/5][101/100][49] Loss_D: 0.006369 Loss_G: 0.003143 \n",
      "[0/5][101/100][50] Loss_D: 0.002716 Loss_G: 0.003883 \n",
      "[0/5][101/100][51] Loss_D: 0.002978 Loss_G: 0.005915 \n",
      "[0/5][101/100][52] Loss_D: 0.001759 Loss_G: 0.005167 \n",
      "[0/5][101/100][53] Loss_D: 0.003541 Loss_G: 0.005201 \n",
      "[0/5][101/100][54] Loss_D: 0.002830 Loss_G: 0.002243 \n",
      "[0/5][101/100][55] Loss_D: 0.004870 Loss_G: 0.001810 \n",
      "[0/5][101/100][56] Loss_D: 0.003876 Loss_G: 0.003434 \n",
      "[0/5][101/100][57] Loss_D: 0.006175 Loss_G: 0.006216 \n",
      "[0/5][101/100][58] Loss_D: 0.003397 Loss_G: 0.003837 \n",
      "[0/5][101/100][59] Loss_D: 0.008502 Loss_G: 0.005657 \n",
      "[0/5][101/100][60] Loss_D: 0.002173 Loss_G: 0.003482 \n",
      "[0/5][101/100][61] Loss_D: 0.004827 Loss_G: 0.001402 \n",
      "[0/5][101/100][62] Loss_D: 0.005665 Loss_G: 0.002424 \n",
      "[0/5][101/100][63] Loss_D: 0.002043 Loss_G: 0.003636 \n",
      "[0/5][101/100][64] Loss_D: 0.003978 Loss_G: 0.001042 \n",
      "[0/5][101/100][65] Loss_D: 0.002028 Loss_G: 0.004381 \n",
      "[0/5][101/100][66] Loss_D: 0.001951 Loss_G: 0.006548 \n",
      "[0/5][101/100][67] Loss_D: 0.004294 Loss_G: 0.002803 \n",
      "[0/5][101/100][68] Loss_D: 0.005215 Loss_G: 0.004378 \n",
      "[0/5][101/100][69] Loss_D: 0.001213 Loss_G: 0.004228 \n",
      "[0/5][101/100][70] Loss_D: 0.003133 Loss_G: 0.004351 \n",
      "[0/5][101/100][71] Loss_D: 0.004438 Loss_G: 0.007174 \n",
      "[0/5][101/100][72] Loss_D: 0.002645 Loss_G: 0.004667 \n",
      "[0/5][101/100][73] Loss_D: 0.005718 Loss_G: 0.002321 \n",
      "[0/5][101/100][74] Loss_D: 0.003474 Loss_G: 0.003888 \n",
      "[0/5][101/100][75] Loss_D: 0.004802 Loss_G: 0.003054 \n",
      "[0/5][101/100][76] Loss_D: 0.002519 Loss_G: 0.004571 \n",
      "[0/5][101/100][77] Loss_D: 0.005537 Loss_G: 0.005601 \n",
      "[0/5][101/100][78] Loss_D: 0.004529 Loss_G: 0.003133 \n",
      "[0/5][101/100][79] Loss_D: 0.006700 Loss_G: 0.002572 \n",
      "[0/5][101/100][80] Loss_D: 0.004575 Loss_G: 0.005834 \n",
      "[0/5][101/100][81] Loss_D: 0.005199 Loss_G: 0.002566 \n",
      "[0/5][101/100][82] Loss_D: 0.002108 Loss_G: 0.006145 \n",
      "[0/5][101/100][83] Loss_D: 0.005993 Loss_G: 0.003251 \n",
      "[0/5][101/100][84] Loss_D: 0.002154 Loss_G: 0.004267 \n",
      "[0/5][101/100][85] Loss_D: 0.004321 Loss_G: 0.003660 \n",
      "[0/5][101/100][86] Loss_D: 0.001118 Loss_G: 0.008256 \n",
      "[0/5][101/100][87] Loss_D: 0.006736 Loss_G: 0.004595 \n",
      "[0/5][101/100][88] Loss_D: 0.002290 Loss_G: 0.002090 \n",
      "[0/5][101/100][89] Loss_D: 0.000815 Loss_G: 0.006739 \n",
      "[0/5][101/100][90] Loss_D: 0.004969 Loss_G: 0.002689 \n",
      "[0/5][101/100][91] Loss_D: 0.003283 Loss_G: 0.002418 \n",
      "[0/5][101/100][92] Loss_D: 0.002839 Loss_G: 0.004349 \n",
      "[0/5][101/100][93] Loss_D: 0.002059 Loss_G: 0.005641 \n",
      "[0/5][101/100][94] Loss_D: 0.003429 Loss_G: 0.004736 \n",
      "[0/5][101/100][95] Loss_D: 0.006881 Loss_G: 0.004957 \n",
      "[0/5][101/100][96] Loss_D: 0.003679 Loss_G: 0.005180 \n",
      "[0/5][101/100][97] Loss_D: 0.005367 Loss_G: 0.002466 \n",
      "[0/5][101/100][98] Loss_D: 0.007320 Loss_G: 0.003220 \n",
      "[0/5][101/100][99] Loss_D: 0.005608 Loss_G: 0.004697 \n",
      "[0/5][101/100][100] Loss_D: 0.006464 Loss_G: 0.006784 \n",
      "[1/5][101/100][101] Loss_D: 0.004180 Loss_G: 0.000622 \n",
      "[1/5][101/100][102] Loss_D: 0.003001 Loss_G: 0.004697 \n",
      "[1/5][101/100][103] Loss_D: 0.003893 Loss_G: 0.001780 \n",
      "[1/5][101/100][104] Loss_D: 0.001875 Loss_G: 0.002589 \n",
      "[1/5][101/100][105] Loss_D: -0.000420 Loss_G: 0.004829 \n",
      "[1/5][101/100][106] Loss_D: 0.004078 Loss_G: 0.003249 \n",
      "[1/5][101/100][107] Loss_D: 0.001682 Loss_G: 0.004410 \n",
      "[1/5][101/100][108] Loss_D: 0.002901 Loss_G: 0.002755 \n",
      "[1/5][101/100][109] Loss_D: 0.005167 Loss_G: 0.002608 \n",
      "[1/5][101/100][110] Loss_D: 0.004660 Loss_G: 0.003677 \n",
      "[1/5][101/100][111] Loss_D: 0.003196 Loss_G: 0.003308 \n",
      "[1/5][101/100][112] Loss_D: 0.003822 Loss_G: 0.004219 \n",
      "[1/5][101/100][113] Loss_D: 0.004433 Loss_G: 0.003717 \n",
      "[1/5][101/100][114] Loss_D: 0.003352 Loss_G: 0.001009 \n",
      "[1/5][101/100][115] Loss_D: 0.004475 Loss_G: 0.004305 \n",
      "[1/5][101/100][116] Loss_D: 0.002285 Loss_G: 0.007242 \n",
      "[1/5][101/100][117] Loss_D: 0.001643 Loss_G: 0.004712 \n",
      "[1/5][101/100][118] Loss_D: 0.001393 Loss_G: 0.004762 \n",
      "[1/5][101/100][119] Loss_D: 0.002272 Loss_G: 0.002076 \n",
      "[1/5][101/100][120] Loss_D: 0.003690 Loss_G: 0.004113 \n",
      "[1/5][101/100][121] Loss_D: 0.006304 Loss_G: 0.001631 \n",
      "[1/5][101/100][122] Loss_D: 0.004837 Loss_G: 0.001578 \n",
      "[1/5][101/100][123] Loss_D: 0.006877 Loss_G: 0.004546 \n",
      "[1/5][101/100][124] Loss_D: 0.002617 Loss_G: 0.008527 \n",
      "[1/5][101/100][125] Loss_D: 0.002987 Loss_G: 0.002062 \n",
      "[1/5][101/100][126] Loss_D: 0.001526 Loss_G: 0.004214 \n",
      "[1/5][101/100][127] Loss_D: 0.005979 Loss_G: 0.003798 \n",
      "[1/5][101/100][128] Loss_D: 0.003285 Loss_G: 0.004768 \n",
      "[1/5][101/100][129] Loss_D: 0.005654 Loss_G: 0.001752 \n",
      "[1/5][101/100][130] Loss_D: 0.009217 Loss_G: 0.004859 \n",
      "[1/5][101/100][131] Loss_D: 0.004630 Loss_G: 0.006832 \n",
      "[1/5][101/100][132] Loss_D: 0.001844 Loss_G: 0.003173 \n",
      "[1/5][101/100][133] Loss_D: 0.005978 Loss_G: 0.002268 \n",
      "[1/5][101/100][134] Loss_D: 0.005757 Loss_G: 0.006127 \n",
      "[1/5][101/100][135] Loss_D: 0.003897 Loss_G: 0.004721 \n",
      "[1/5][101/100][136] Loss_D: 0.004686 Loss_G: 0.004834 \n",
      "[1/5][101/100][137] Loss_D: 0.004945 Loss_G: 0.006220 \n",
      "[1/5][101/100][138] Loss_D: 0.002295 Loss_G: 0.001517 \n",
      "[1/5][101/100][139] Loss_D: 0.004402 Loss_G: 0.001944 \n",
      "[1/5][101/100][140] Loss_D: 0.003869 Loss_G: 0.003246 \n",
      "[1/5][101/100][141] Loss_D: 0.001645 Loss_G: 0.004205 \n",
      "[1/5][101/100][142] Loss_D: 0.004466 Loss_G: 0.002209 \n",
      "[1/5][101/100][143] Loss_D: 0.001707 Loss_G: -0.000060 \n",
      "[1/5][101/100][144] Loss_D: 0.002104 Loss_G: 0.005940 \n",
      "[1/5][101/100][145] Loss_D: 0.004786 Loss_G: 0.002216 \n",
      "[1/5][101/100][146] Loss_D: 0.002120 Loss_G: 0.001643 \n",
      "[1/5][101/100][147] Loss_D: 0.004553 Loss_G: 0.001032 \n",
      "[1/5][101/100][148] Loss_D: 0.006664 Loss_G: 0.004001 \n",
      "[1/5][101/100][149] Loss_D: 0.001245 Loss_G: 0.005453 \n",
      "[1/5][101/100][150] Loss_D: 0.002357 Loss_G: 0.001817 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5][101/100][151] Loss_D: 0.002669 Loss_G: 0.004350 \n",
      "[1/5][101/100][152] Loss_D: 0.002906 Loss_G: 0.006057 \n",
      "[1/5][101/100][153] Loss_D: 0.003746 Loss_G: 0.004652 \n",
      "[1/5][101/100][154] Loss_D: 0.002167 Loss_G: 0.002453 \n",
      "[1/5][101/100][155] Loss_D: 0.005083 Loss_G: 0.003335 \n",
      "[1/5][101/100][156] Loss_D: 0.007854 Loss_G: 0.002971 \n",
      "[1/5][101/100][157] Loss_D: 0.002389 Loss_G: 0.004417 \n",
      "[1/5][101/100][158] Loss_D: 0.003667 Loss_G: 0.003861 \n",
      "[1/5][101/100][159] Loss_D: 0.003170 Loss_G: 0.003554 \n",
      "[1/5][101/100][160] Loss_D: 0.003458 Loss_G: 0.003553 \n",
      "[1/5][101/100][161] Loss_D: 0.003125 Loss_G: 0.004207 \n",
      "[1/5][101/100][162] Loss_D: 0.003738 Loss_G: 0.005646 \n",
      "[1/5][101/100][163] Loss_D: 0.006502 Loss_G: 0.003769 \n",
      "[1/5][101/100][164] Loss_D: 0.004218 Loss_G: 0.005371 \n",
      "[1/5][101/100][165] Loss_D: 0.005279 Loss_G: 0.003506 \n",
      "[1/5][101/100][166] Loss_D: 0.003706 Loss_G: 0.002547 \n",
      "[1/5][101/100][167] Loss_D: 0.006015 Loss_G: 0.003951 \n",
      "[1/5][101/100][168] Loss_D: 0.003678 Loss_G: 0.002685 \n",
      "[1/5][101/100][169] Loss_D: 0.007667 Loss_G: 0.004460 \n",
      "[1/5][101/100][170] Loss_D: 0.005642 Loss_G: 0.000413 \n",
      "[1/5][101/100][171] Loss_D: 0.003628 Loss_G: 0.002094 \n",
      "[1/5][101/100][172] Loss_D: 0.002957 Loss_G: 0.005984 \n",
      "[1/5][101/100][173] Loss_D: 0.003747 Loss_G: 0.003398 \n",
      "[1/5][101/100][174] Loss_D: 0.006544 Loss_G: 0.004744 \n",
      "[1/5][101/100][175] Loss_D: 0.001247 Loss_G: 0.004141 \n",
      "[1/5][101/100][176] Loss_D: 0.004771 Loss_G: 0.003595 \n",
      "[1/5][101/100][177] Loss_D: 0.001230 Loss_G: 0.004257 \n",
      "[1/5][101/100][178] Loss_D: 0.001026 Loss_G: 0.003863 \n",
      "[1/5][101/100][179] Loss_D: 0.004067 Loss_G: 0.008593 \n",
      "[1/5][101/100][180] Loss_D: 0.004164 Loss_G: 0.005704 \n",
      "[1/5][101/100][181] Loss_D: 0.006498 Loss_G: 0.001162 \n",
      "[1/5][101/100][182] Loss_D: 0.002566 Loss_G: 0.002100 \n",
      "[1/5][101/100][183] Loss_D: 0.001899 Loss_G: 0.005013 \n",
      "[1/5][101/100][184] Loss_D: 0.007072 Loss_G: 0.005862 \n",
      "[1/5][101/100][185] Loss_D: 0.004637 Loss_G: 0.000725 \n",
      "[1/5][101/100][186] Loss_D: 0.007306 Loss_G: 0.005215 \n",
      "[1/5][101/100][187] Loss_D: 0.003241 Loss_G: 0.001552 \n",
      "[1/5][101/100][188] Loss_D: 0.004318 Loss_G: 0.003741 \n",
      "[1/5][101/100][189] Loss_D: 0.005162 Loss_G: 0.005509 \n",
      "[1/5][101/100][190] Loss_D: 0.003357 Loss_G: 0.003685 \n",
      "[1/5][101/100][191] Loss_D: 0.004697 Loss_G: 0.006252 \n",
      "[1/5][101/100][192] Loss_D: 0.001296 Loss_G: 0.002832 \n",
      "[1/5][101/100][193] Loss_D: 0.001545 Loss_G: 0.004033 \n",
      "[1/5][101/100][194] Loss_D: 0.005687 Loss_G: 0.003621 \n",
      "[1/5][101/100][195] Loss_D: 0.007666 Loss_G: 0.001966 \n",
      "[1/5][101/100][196] Loss_D: 0.004273 Loss_G: 0.005936 \n",
      "[1/5][101/100][197] Loss_D: 0.006341 Loss_G: 0.003565 \n",
      "[1/5][101/100][198] Loss_D: 0.004054 Loss_G: 0.002226 \n",
      "[1/5][101/100][199] Loss_D: 0.003554 Loss_G: 0.003237 \n",
      "[1/5][101/100][200] Loss_D: 0.003095 Loss_G: 0.003286 \n",
      "[2/5][101/100][201] Loss_D: 0.002564 Loss_G: 0.004489 \n",
      "[2/5][101/100][202] Loss_D: 0.004123 Loss_G: 0.005487 \n",
      "[2/5][101/100][203] Loss_D: 0.008169 Loss_G: 0.001783 \n",
      "[2/5][101/100][204] Loss_D: 0.002937 Loss_G: 0.002616 \n",
      "[2/5][101/100][205] Loss_D: 0.001771 Loss_G: 0.003671 \n",
      "[2/5][101/100][206] Loss_D: 0.001803 Loss_G: 0.003529 \n",
      "[2/5][101/100][207] Loss_D: 0.003686 Loss_G: 0.004057 \n",
      "[2/5][101/100][208] Loss_D: 0.004188 Loss_G: 0.004161 \n",
      "[2/5][101/100][209] Loss_D: 0.001839 Loss_G: 0.004186 \n",
      "[2/5][101/100][210] Loss_D: 0.001371 Loss_G: 0.003079 \n",
      "[2/5][101/100][211] Loss_D: 0.003745 Loss_G: 0.002270 \n",
      "[2/5][101/100][212] Loss_D: 0.003515 Loss_G: 0.003929 \n",
      "[2/5][101/100][213] Loss_D: 0.003608 Loss_G: 0.001688 \n",
      "[2/5][101/100][214] Loss_D: 0.006337 Loss_G: 0.007626 \n",
      "[2/5][101/100][215] Loss_D: 0.005396 Loss_G: 0.007287 \n",
      "[2/5][101/100][216] Loss_D: 0.007453 Loss_G: 0.003193 \n",
      "[2/5][101/100][217] Loss_D: 0.000783 Loss_G: 0.005818 \n",
      "[2/5][101/100][218] Loss_D: 0.003970 Loss_G: 0.005859 \n",
      "[2/5][101/100][219] Loss_D: 0.003583 Loss_G: 0.001619 \n",
      "[2/5][101/100][220] Loss_D: 0.003256 Loss_G: 0.002563 \n",
      "[2/5][101/100][221] Loss_D: 0.004591 Loss_G: 0.002727 \n",
      "[2/5][101/100][222] Loss_D: 0.001190 Loss_G: 0.004001 \n",
      "[2/5][101/100][223] Loss_D: 0.002343 Loss_G: 0.003018 \n",
      "[2/5][101/100][224] Loss_D: 0.001721 Loss_G: 0.003528 \n",
      "[2/5][101/100][225] Loss_D: 0.002261 Loss_G: 0.006974 \n",
      "[2/5][101/100][226] Loss_D: 0.005230 Loss_G: 0.002906 \n",
      "[2/5][101/100][227] Loss_D: 0.003913 Loss_G: 0.002986 \n",
      "[2/5][101/100][228] Loss_D: 0.002962 Loss_G: 0.002841 \n",
      "[2/5][101/100][229] Loss_D: 0.004407 Loss_G: 0.002741 \n",
      "[2/5][101/100][230] Loss_D: 0.006121 Loss_G: 0.001573 \n",
      "[2/5][101/100][231] Loss_D: 0.002954 Loss_G: 0.004685 \n",
      "[2/5][101/100][232] Loss_D: 0.001875 Loss_G: 0.003979 \n",
      "[2/5][101/100][233] Loss_D: 0.003885 Loss_G: 0.002904 \n",
      "[2/5][101/100][234] Loss_D: 0.001987 Loss_G: 0.001749 \n",
      "[2/5][101/100][235] Loss_D: 0.005563 Loss_G: 0.006341 \n",
      "[2/5][101/100][236] Loss_D: 0.000426 Loss_G: 0.007891 \n",
      "[2/5][101/100][237] Loss_D: 0.006920 Loss_G: 0.002256 \n",
      "[2/5][101/100][238] Loss_D: 0.001819 Loss_G: 0.000989 \n",
      "[2/5][101/100][239] Loss_D: 0.003273 Loss_G: 0.003608 \n",
      "[2/5][101/100][240] Loss_D: 0.002880 Loss_G: 0.005072 \n",
      "[2/5][101/100][241] Loss_D: 0.003005 Loss_G: 0.004477 \n",
      "[2/5][101/100][242] Loss_D: 0.003872 Loss_G: 0.002887 \n",
      "[2/5][101/100][243] Loss_D: 0.002684 Loss_G: 0.004953 \n",
      "[2/5][101/100][244] Loss_D: 0.004793 Loss_G: 0.002390 \n",
      "[2/5][101/100][245] Loss_D: 0.002584 Loss_G: 0.002998 \n",
      "[2/5][101/100][246] Loss_D: 0.002272 Loss_G: 0.003002 \n",
      "[2/5][101/100][247] Loss_D: 0.004868 Loss_G: 0.002722 \n",
      "[2/5][101/100][248] Loss_D: 0.008533 Loss_G: 0.004193 \n",
      "[2/5][101/100][249] Loss_D: 0.004076 Loss_G: 0.003724 \n",
      "[2/5][101/100][250] Loss_D: 0.003779 Loss_G: 0.001670 \n",
      "[2/5][101/100][251] Loss_D: 0.001415 Loss_G: 0.003676 \n",
      "[2/5][101/100][252] Loss_D: 0.002915 Loss_G: 0.003918 \n",
      "[2/5][101/100][253] Loss_D: 0.003496 Loss_G: 0.003045 \n",
      "[2/5][101/100][254] Loss_D: 0.005396 Loss_G: 0.001987 \n",
      "[2/5][101/100][255] Loss_D: 0.004286 Loss_G: 0.004329 \n",
      "[2/5][101/100][256] Loss_D: 0.001847 Loss_G: 0.004352 \n",
      "[2/5][101/100][257] Loss_D: 0.003180 Loss_G: 0.004161 \n",
      "[2/5][101/100][258] Loss_D: 0.002910 Loss_G: 0.003185 \n",
      "[2/5][101/100][259] Loss_D: 0.007705 Loss_G: 0.002540 \n",
      "[2/5][101/100][260] Loss_D: 0.004617 Loss_G: 0.002138 \n",
      "[2/5][101/100][261] Loss_D: 0.007032 Loss_G: 0.003978 \n",
      "[2/5][101/100][262] Loss_D: 0.004047 Loss_G: 0.004099 \n",
      "[2/5][101/100][263] Loss_D: 0.001637 Loss_G: 0.003481 \n",
      "[2/5][101/100][264] Loss_D: 0.002923 Loss_G: 0.008655 \n",
      "[2/5][101/100][265] Loss_D: 0.003707 Loss_G: 0.004050 \n",
      "[2/5][101/100][266] Loss_D: 0.004646 Loss_G: 0.002640 \n",
      "[2/5][101/100][267] Loss_D: 0.003626 Loss_G: 0.003251 \n",
      "[2/5][101/100][268] Loss_D: 0.008972 Loss_G: 0.005516 \n",
      "[2/5][101/100][269] Loss_D: 0.001428 Loss_G: 0.001506 \n",
      "[2/5][101/100][270] Loss_D: 0.004477 Loss_G: 0.004575 \n",
      "[2/5][101/100][271] Loss_D: 0.003562 Loss_G: 0.002422 \n",
      "[2/5][101/100][272] Loss_D: 0.007650 Loss_G: 0.006425 \n",
      "[2/5][101/100][273] Loss_D: 0.002958 Loss_G: 0.002348 \n",
      "[2/5][101/100][274] Loss_D: 0.002591 Loss_G: 0.003925 \n",
      "[2/5][101/100][275] Loss_D: 0.004208 Loss_G: 0.005613 \n",
      "[2/5][101/100][276] Loss_D: 0.000126 Loss_G: 0.004800 \n",
      "[2/5][101/100][277] Loss_D: 0.002071 Loss_G: 0.003708 \n",
      "[2/5][101/100][278] Loss_D: 0.004632 Loss_G: 0.001058 \n",
      "[2/5][101/100][279] Loss_D: 0.003733 Loss_G: 0.003154 \n",
      "[2/5][101/100][280] Loss_D: 0.004241 Loss_G: 0.002815 \n",
      "[2/5][101/100][281] Loss_D: 0.003904 Loss_G: 0.003044 \n",
      "[2/5][101/100][282] Loss_D: 0.005477 Loss_G: 0.001982 \n",
      "[2/5][101/100][283] Loss_D: 0.004623 Loss_G: 0.003752 \n",
      "[2/5][101/100][284] Loss_D: 0.005426 Loss_G: 0.003083 \n",
      "[2/5][101/100][285] Loss_D: 0.002894 Loss_G: 0.002455 \n",
      "[2/5][101/100][286] Loss_D: 0.005006 Loss_G: 0.003624 \n",
      "[2/5][101/100][287] Loss_D: 0.003982 Loss_G: 0.001986 \n",
      "[2/5][101/100][288] Loss_D: 0.002068 Loss_G: 0.002441 \n",
      "[2/5][101/100][289] Loss_D: 0.005457 Loss_G: 0.004239 \n",
      "[2/5][101/100][290] Loss_D: 0.004738 Loss_G: 0.004529 \n",
      "[2/5][101/100][291] Loss_D: 0.000950 Loss_G: 0.005597 \n",
      "[2/5][101/100][292] Loss_D: 0.003895 Loss_G: 0.003627 \n",
      "[2/5][101/100][293] Loss_D: 0.004435 Loss_G: 0.005352 \n",
      "[2/5][101/100][294] Loss_D: -0.000034 Loss_G: 0.002812 \n",
      "[2/5][101/100][295] Loss_D: 0.004539 Loss_G: 0.002535 \n",
      "[2/5][101/100][296] Loss_D: 0.004363 Loss_G: 0.001440 \n",
      "[2/5][101/100][297] Loss_D: 0.000861 Loss_G: 0.004355 \n",
      "[2/5][101/100][298] Loss_D: 0.002838 Loss_G: 0.002985 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/5][101/100][299] Loss_D: 0.002894 Loss_G: 0.003872 \n",
      "[2/5][101/100][300] Loss_D: 0.004521 Loss_G: 0.002055 \n",
      "[3/5][101/100][301] Loss_D: 0.004571 Loss_G: 0.003535 \n",
      "[3/5][101/100][302] Loss_D: 0.003908 Loss_G: 0.001340 \n",
      "[3/5][101/100][303] Loss_D: 0.002382 Loss_G: 0.002868 \n",
      "[3/5][101/100][304] Loss_D: 0.004564 Loss_G: 0.003582 \n",
      "[3/5][101/100][305] Loss_D: 0.002307 Loss_G: 0.003528 \n",
      "[3/5][101/100][306] Loss_D: 0.002649 Loss_G: 0.002088 \n",
      "[3/5][101/100][307] Loss_D: 0.002384 Loss_G: 0.004170 \n",
      "[3/5][101/100][308] Loss_D: 0.009257 Loss_G: 0.003818 \n",
      "[3/5][101/100][309] Loss_D: 0.003922 Loss_G: 0.005050 \n",
      "[3/5][101/100][310] Loss_D: 0.003103 Loss_G: 0.002431 \n",
      "[3/5][101/100][311] Loss_D: 0.002510 Loss_G: 0.002851 \n",
      "[3/5][101/100][312] Loss_D: 0.002067 Loss_G: 0.000484 \n",
      "[3/5][101/100][313] Loss_D: 0.002719 Loss_G: 0.005373 \n",
      "[3/5][101/100][314] Loss_D: 0.005151 Loss_G: 0.001298 \n",
      "[3/5][101/100][315] Loss_D: 0.005617 Loss_G: 0.002096 \n",
      "[3/5][101/100][316] Loss_D: 0.003486 Loss_G: 0.002503 \n",
      "[3/5][101/100][317] Loss_D: 0.003248 Loss_G: 0.004696 \n",
      "[3/5][101/100][318] Loss_D: 0.001810 Loss_G: 0.003097 \n",
      "[3/5][101/100][319] Loss_D: 0.004987 Loss_G: 0.002172 \n",
      "[3/5][101/100][320] Loss_D: 0.002856 Loss_G: 0.001890 \n",
      "[3/5][101/100][321] Loss_D: 0.005197 Loss_G: 0.003515 \n",
      "[3/5][101/100][322] Loss_D: 0.004512 Loss_G: 0.003807 \n",
      "[3/5][101/100][323] Loss_D: 0.003487 Loss_G: 0.002090 \n",
      "[3/5][101/100][324] Loss_D: 0.000175 Loss_G: 0.000716 \n",
      "[3/5][101/100][325] Loss_D: 0.002699 Loss_G: 0.001471 \n",
      "[3/5][101/100][326] Loss_D: 0.003621 Loss_G: 0.002613 \n",
      "[3/5][101/100][327] Loss_D: 0.004528 Loss_G: 0.005404 \n",
      "[3/5][101/100][328] Loss_D: 0.005778 Loss_G: 0.004699 \n",
      "[3/5][101/100][329] Loss_D: 0.001595 Loss_G: 0.003150 \n",
      "[3/5][101/100][330] Loss_D: 0.000444 Loss_G: 0.004058 \n",
      "[3/5][101/100][331] Loss_D: 0.003903 Loss_G: 0.002875 \n",
      "[3/5][101/100][332] Loss_D: 0.003304 Loss_G: 0.005168 \n",
      "[3/5][101/100][333] Loss_D: 0.006093 Loss_G: 0.004933 \n",
      "[3/5][101/100][334] Loss_D: 0.006074 Loss_G: 0.003953 \n",
      "[3/5][101/100][335] Loss_D: 0.003463 Loss_G: 0.001802 \n",
      "[3/5][101/100][336] Loss_D: 0.002773 Loss_G: 0.004567 \n",
      "[3/5][101/100][337] Loss_D: 0.002588 Loss_G: 0.005747 \n",
      "[3/5][101/100][338] Loss_D: -0.001503 Loss_G: 0.002173 \n",
      "[3/5][101/100][339] Loss_D: 0.004736 Loss_G: 0.001868 \n",
      "[3/5][101/100][340] Loss_D: 0.002798 Loss_G: 0.004255 \n",
      "[3/5][101/100][341] Loss_D: 0.003351 Loss_G: 0.005190 \n",
      "[3/5][101/100][342] Loss_D: 0.004600 Loss_G: 0.003584 \n",
      "[3/5][101/100][343] Loss_D: 0.006471 Loss_G: 0.003565 \n",
      "[3/5][101/100][344] Loss_D: 0.003169 Loss_G: 0.002446 \n",
      "[3/5][101/100][345] Loss_D: 0.005567 Loss_G: 0.002479 \n",
      "[3/5][101/100][346] Loss_D: 0.005328 Loss_G: 0.003878 \n",
      "[3/5][101/100][347] Loss_D: 0.005566 Loss_G: 0.002857 \n",
      "[3/5][101/100][348] Loss_D: 0.005357 Loss_G: 0.003832 \n",
      "[3/5][101/100][349] Loss_D: 0.001954 Loss_G: 0.002006 \n",
      "[3/5][101/100][350] Loss_D: 0.002376 Loss_G: 0.003515 \n",
      "[3/5][101/100][351] Loss_D: 0.005825 Loss_G: 0.005593 \n",
      "[3/5][101/100][352] Loss_D: 0.002686 Loss_G: 0.002871 \n",
      "[3/5][101/100][353] Loss_D: 0.006691 Loss_G: 0.002439 \n",
      "[3/5][101/100][354] Loss_D: -0.000426 Loss_G: 0.004426 \n",
      "[3/5][101/100][355] Loss_D: 0.006838 Loss_G: 0.007618 \n",
      "[3/5][101/100][356] Loss_D: 0.004426 Loss_G: 0.004200 \n",
      "[3/5][101/100][357] Loss_D: 0.003545 Loss_G: 0.004246 \n",
      "[3/5][101/100][358] Loss_D: 0.006886 Loss_G: 0.002989 \n",
      "[3/5][101/100][359] Loss_D: 0.006857 Loss_G: 0.007941 \n",
      "[3/5][101/100][360] Loss_D: 0.005366 Loss_G: 0.005312 \n",
      "[3/5][101/100][361] Loss_D: 0.004214 Loss_G: 0.002108 \n",
      "[3/5][101/100][362] Loss_D: 0.001200 Loss_G: 0.005368 \n",
      "[3/5][101/100][363] Loss_D: 0.002676 Loss_G: 0.002777 \n",
      "[3/5][101/100][364] Loss_D: 0.005078 Loss_G: 0.005876 \n",
      "[3/5][101/100][365] Loss_D: 0.002592 Loss_G: 0.004220 \n",
      "[3/5][101/100][366] Loss_D: 0.003322 Loss_G: 0.004828 \n",
      "[3/5][101/100][367] Loss_D: 0.005169 Loss_G: 0.003743 \n",
      "[3/5][101/100][368] Loss_D: 0.002059 Loss_G: 0.004990 \n",
      "[3/5][101/100][369] Loss_D: 0.004149 Loss_G: 0.003583 \n",
      "[3/5][101/100][370] Loss_D: 0.005769 Loss_G: 0.005425 \n",
      "[3/5][101/100][371] Loss_D: 0.006402 Loss_G: 0.002284 \n",
      "[3/5][101/100][372] Loss_D: 0.004273 Loss_G: 0.001410 \n",
      "[3/5][101/100][373] Loss_D: 0.003394 Loss_G: 0.003344 \n",
      "[3/5][101/100][374] Loss_D: 0.003535 Loss_G: 0.005679 \n",
      "[3/5][101/100][375] Loss_D: 0.001337 Loss_G: 0.004336 \n",
      "[3/5][101/100][376] Loss_D: 0.004533 Loss_G: 0.004349 \n",
      "[3/5][101/100][377] Loss_D: 0.003708 Loss_G: 0.002028 \n",
      "[3/5][101/100][378] Loss_D: 0.001638 Loss_G: 0.001487 \n",
      "[3/5][101/100][379] Loss_D: 0.004415 Loss_G: 0.003306 \n",
      "[3/5][101/100][380] Loss_D: 0.006729 Loss_G: 0.002593 \n",
      "[3/5][101/100][381] Loss_D: 0.004810 Loss_G: 0.005327 \n",
      "[3/5][101/100][382] Loss_D: 0.003948 Loss_G: 0.003100 \n",
      "[3/5][101/100][383] Loss_D: 0.005738 Loss_G: 0.003594 \n",
      "[3/5][101/100][384] Loss_D: 0.005889 Loss_G: 0.005285 \n",
      "[3/5][101/100][385] Loss_D: 0.005698 Loss_G: 0.000673 \n",
      "[3/5][101/100][386] Loss_D: 0.002046 Loss_G: 0.002806 \n",
      "[3/5][101/100][387] Loss_D: 0.004711 Loss_G: 0.005316 \n",
      "[3/5][101/100][388] Loss_D: 0.005805 Loss_G: 0.003037 \n",
      "[3/5][101/100][389] Loss_D: 0.003479 Loss_G: 0.006541 \n",
      "[3/5][101/100][390] Loss_D: 0.003219 Loss_G: 0.005788 \n",
      "[3/5][101/100][391] Loss_D: 0.001834 Loss_G: 0.004182 \n",
      "[3/5][101/100][392] Loss_D: 0.004531 Loss_G: 0.004082 \n",
      "[3/5][101/100][393] Loss_D: 0.004468 Loss_G: 0.006809 \n",
      "[3/5][101/100][394] Loss_D: 0.003357 Loss_G: 0.003470 \n",
      "[3/5][101/100][395] Loss_D: 0.001156 Loss_G: 0.000981 \n",
      "[3/5][101/100][396] Loss_D: 0.003872 Loss_G: 0.001231 \n",
      "[3/5][101/100][397] Loss_D: 0.004417 Loss_G: 0.001625 \n",
      "[3/5][101/100][398] Loss_D: 0.002703 Loss_G: 0.004393 \n",
      "[3/5][101/100][399] Loss_D: 0.002802 Loss_G: 0.004276 \n",
      "[3/5][101/100][400] Loss_D: 0.000630 Loss_G: 0.002785 \n",
      "[4/5][101/100][401] Loss_D: 0.004984 Loss_G: 0.005826 \n",
      "[4/5][101/100][402] Loss_D: 0.002806 Loss_G: 0.005017 \n",
      "[4/5][101/100][403] Loss_D: 0.004311 Loss_G: 0.003855 \n",
      "[4/5][101/100][404] Loss_D: 0.003389 Loss_G: 0.002686 \n",
      "[4/5][101/100][405] Loss_D: 0.003358 Loss_G: 0.007239 \n",
      "[4/5][101/100][406] Loss_D: 0.001330 Loss_G: 0.001316 \n",
      "[4/5][101/100][407] Loss_D: 0.004491 Loss_G: 0.004472 \n",
      "[4/5][101/100][408] Loss_D: 0.003987 Loss_G: 0.002057 \n",
      "[4/5][101/100][409] Loss_D: 0.004754 Loss_G: 0.003073 \n",
      "[4/5][101/100][410] Loss_D: 0.005995 Loss_G: 0.002326 \n",
      "[4/5][101/100][411] Loss_D: 0.004187 Loss_G: 0.004903 \n",
      "[4/5][101/100][412] Loss_D: 0.003488 Loss_G: 0.003078 \n",
      "[4/5][101/100][413] Loss_D: 0.003349 Loss_G: 0.003460 \n",
      "[4/5][101/100][414] Loss_D: 0.001809 Loss_G: 0.005930 \n",
      "[4/5][101/100][415] Loss_D: 0.007135 Loss_G: 0.004375 \n",
      "[4/5][101/100][416] Loss_D: 0.004187 Loss_G: 0.003711 \n",
      "[4/5][101/100][417] Loss_D: 0.005756 Loss_G: 0.002726 \n",
      "[4/5][101/100][418] Loss_D: 0.001972 Loss_G: 0.002793 \n",
      "[4/5][101/100][419] Loss_D: 0.006239 Loss_G: 0.003423 \n",
      "[4/5][101/100][420] Loss_D: 0.005639 Loss_G: 0.006269 \n",
      "[4/5][101/100][421] Loss_D: 0.002563 Loss_G: 0.002881 \n",
      "[4/5][101/100][422] Loss_D: 0.003723 Loss_G: 0.002979 \n",
      "[4/5][101/100][423] Loss_D: 0.000955 Loss_G: 0.004411 \n",
      "[4/5][101/100][424] Loss_D: 0.006391 Loss_G: 0.003299 \n",
      "[4/5][101/100][425] Loss_D: 0.004961 Loss_G: 0.003858 \n",
      "[4/5][101/100][426] Loss_D: 0.002857 Loss_G: 0.003726 \n",
      "[4/5][101/100][427] Loss_D: 0.002207 Loss_G: 0.003079 \n",
      "[4/5][101/100][428] Loss_D: 0.003841 Loss_G: 0.002469 \n",
      "[4/5][101/100][429] Loss_D: 0.004465 Loss_G: 0.004625 \n",
      "[4/5][101/100][430] Loss_D: 0.004656 Loss_G: 0.006310 \n",
      "[4/5][101/100][431] Loss_D: 0.006025 Loss_G: 0.003048 \n",
      "[4/5][101/100][432] Loss_D: 0.003520 Loss_G: 0.005091 \n",
      "[4/5][101/100][433] Loss_D: 0.005941 Loss_G: 0.002459 \n",
      "[4/5][101/100][434] Loss_D: 0.001479 Loss_G: 0.001825 \n",
      "[4/5][101/100][435] Loss_D: 0.003214 Loss_G: 0.005063 \n",
      "[4/5][101/100][436] Loss_D: 0.004409 Loss_G: 0.002567 \n",
      "[4/5][101/100][437] Loss_D: 0.003749 Loss_G: 0.004594 \n",
      "[4/5][101/100][438] Loss_D: 0.005396 Loss_G: 0.002888 \n",
      "[4/5][101/100][439] Loss_D: 0.002656 Loss_G: 0.003303 \n",
      "[4/5][101/100][440] Loss_D: 0.003020 Loss_G: 0.002573 \n",
      "[4/5][101/100][441] Loss_D: 0.002674 Loss_G: 0.003467 \n",
      "[4/5][101/100][442] Loss_D: 0.005993 Loss_G: 0.006923 \n",
      "[4/5][101/100][443] Loss_D: 0.005037 Loss_G: 0.006842 \n",
      "[4/5][101/100][444] Loss_D: 0.005032 Loss_G: 0.003422 \n",
      "[4/5][101/100][445] Loss_D: 0.004075 Loss_G: 0.000035 \n",
      "[4/5][101/100][446] Loss_D: 0.003472 Loss_G: 0.006619 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/5][101/100][447] Loss_D: 0.002025 Loss_G: 0.003538 \n",
      "[4/5][101/100][448] Loss_D: 0.004116 Loss_G: 0.004982 \n",
      "[4/5][101/100][449] Loss_D: 0.003722 Loss_G: 0.001591 \n",
      "[4/5][101/100][450] Loss_D: 0.003219 Loss_G: 0.003651 \n",
      "[4/5][101/100][451] Loss_D: 0.002832 Loss_G: 0.004959 \n",
      "[4/5][101/100][452] Loss_D: 0.002039 Loss_G: 0.002903 \n",
      "[4/5][101/100][453] Loss_D: 0.003724 Loss_G: 0.002501 \n",
      "[4/5][101/100][454] Loss_D: 0.005758 Loss_G: 0.001594 \n",
      "[4/5][101/100][455] Loss_D: 0.003557 Loss_G: 0.003743 \n",
      "[4/5][101/100][456] Loss_D: 0.003689 Loss_G: 0.005656 \n",
      "[4/5][101/100][457] Loss_D: 0.003481 Loss_G: 0.004123 \n",
      "[4/5][101/100][458] Loss_D: 0.004131 Loss_G: 0.002128 \n",
      "[4/5][101/100][459] Loss_D: 0.005937 Loss_G: 0.004972 \n",
      "[4/5][101/100][460] Loss_D: 0.003403 Loss_G: 0.006206 \n",
      "[4/5][101/100][461] Loss_D: 0.004255 Loss_G: 0.005876 \n",
      "[4/5][101/100][462] Loss_D: 0.005973 Loss_G: 0.006671 \n",
      "[4/5][101/100][463] Loss_D: 0.002453 Loss_G: 0.005988 \n",
      "[4/5][101/100][464] Loss_D: 0.003944 Loss_G: 0.002791 \n",
      "[4/5][101/100][465] Loss_D: 0.004763 Loss_G: 0.004624 \n",
      "[4/5][101/100][466] Loss_D: 0.004828 Loss_G: 0.006332 \n",
      "[4/5][101/100][467] Loss_D: 0.001327 Loss_G: 0.007866 \n",
      "[4/5][101/100][468] Loss_D: 0.002477 Loss_G: 0.002873 \n",
      "[4/5][101/100][469] Loss_D: 0.001893 Loss_G: 0.005019 \n",
      "[4/5][101/100][470] Loss_D: 0.002420 Loss_G: 0.005958 \n",
      "[4/5][101/100][471] Loss_D: 0.005405 Loss_G: 0.004581 \n",
      "[4/5][101/100][472] Loss_D: 0.006325 Loss_G: 0.000808 \n",
      "[4/5][101/100][473] Loss_D: 0.004692 Loss_G: 0.002681 \n",
      "[4/5][101/100][474] Loss_D: 0.002825 Loss_G: 0.003281 \n",
      "[4/5][101/100][475] Loss_D: 0.005459 Loss_G: 0.002887 \n",
      "[4/5][101/100][476] Loss_D: 0.004221 Loss_G: 0.003548 \n",
      "[4/5][101/100][477] Loss_D: 0.003036 Loss_G: 0.003232 \n",
      "[4/5][101/100][478] Loss_D: 0.004536 Loss_G: 0.003184 \n",
      "[4/5][101/100][479] Loss_D: 0.005155 Loss_G: 0.002785 \n",
      "[4/5][101/100][480] Loss_D: 0.006331 Loss_G: 0.005239 \n",
      "[4/5][101/100][481] Loss_D: 0.004280 Loss_G: 0.002062 \n",
      "[4/5][101/100][482] Loss_D: 0.001217 Loss_G: 0.002710 \n",
      "[4/5][101/100][483] Loss_D: 0.002861 Loss_G: 0.003682 \n",
      "[4/5][101/100][484] Loss_D: 0.000545 Loss_G: 0.004542 \n",
      "[4/5][101/100][485] Loss_D: 0.006916 Loss_G: 0.001149 \n",
      "[4/5][101/100][486] Loss_D: 0.002785 Loss_G: 0.004318 \n",
      "[4/5][101/100][487] Loss_D: 0.003578 Loss_G: 0.002864 \n",
      "[4/5][101/100][488] Loss_D: 0.003266 Loss_G: 0.003428 \n",
      "[4/5][101/100][489] Loss_D: 0.004579 Loss_G: 0.007896 \n",
      "[4/5][101/100][490] Loss_D: 0.004518 Loss_G: 0.000696 \n",
      "[4/5][101/100][491] Loss_D: 0.004190 Loss_G: 0.002506 \n",
      "[4/5][101/100][492] Loss_D: 0.004854 Loss_G: 0.002598 \n",
      "[4/5][101/100][493] Loss_D: 0.001541 Loss_G: 0.002920 \n",
      "[4/5][101/100][494] Loss_D: 0.005023 Loss_G: 0.001981 \n",
      "[4/5][101/100][495] Loss_D: 0.004315 Loss_G: 0.003516 \n",
      "[4/5][101/100][496] Loss_D: 0.004140 Loss_G: 0.003491 \n",
      "[4/5][101/100][497] Loss_D: 0.003007 Loss_G: 0.002710 \n",
      "[4/5][101/100][498] Loss_D: 0.004140 Loss_G: 0.006669 \n",
      "[4/5][101/100][499] Loss_D: 0.003775 Loss_G: 0.004319 \n",
      "[4/5][101/100][500] Loss_D: 0.006978 Loss_G: 0.002972 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.013126597739756107,\n",
       "  0.019412890076637268,\n",
       "  0.010213066823780537,\n",
       "  0.0133413877338171,\n",
       "  0.015593254938721657,\n",
       "  0.0055726016871631145,\n",
       "  0.013911847025156021,\n",
       "  0.010416612960398197,\n",
       "  0.007506988011300564,\n",
       "  0.007893580012023449,\n",
       "  0.004737638868391514,\n",
       "  0.006066109053790569,\n",
       "  0.005995640531182289,\n",
       "  0.0052580744959414005,\n",
       "  0.0035125678405165672,\n",
       "  0.007974263280630112,\n",
       "  0.003955844324082136,\n",
       "  0.002734075766056776,\n",
       "  0.0029523347038775682,\n",
       "  0.003242484061047435,\n",
       "  0.001084849121980369,\n",
       "  0.0025255330838263035,\n",
       "  0.003718000603839755,\n",
       "  0.004080341197550297,\n",
       "  0.004512427374720573,\n",
       "  0.004124290309846401,\n",
       "  0.00327226216904819,\n",
       "  0.006077606230974197,\n",
       "  0.00573092931881547,\n",
       "  0.0036280557978898287,\n",
       "  0.005843624472618103,\n",
       "  0.001460762694478035,\n",
       "  0.005070263985544443,\n",
       "  0.0035387154202908278,\n",
       "  0.00494206789880991,\n",
       "  0.0015567054506391287,\n",
       "  0.006268015131354332,\n",
       "  0.0013356006238609552,\n",
       "  0.007212123367935419,\n",
       "  0.0037851103115826845,\n",
       "  0.006407453212887049,\n",
       "  0.0008828458958305418,\n",
       "  0.003191328840330243,\n",
       "  0.00419207289814949,\n",
       "  0.0033897135872393847,\n",
       "  0.003570458386093378,\n",
       "  0.004006125498563051,\n",
       "  0.0013246997259557247,\n",
       "  0.003142585977911949,\n",
       "  0.0038829054683446884,\n",
       "  0.005914502777159214,\n",
       "  0.005167090799659491,\n",
       "  0.0052014682441949844,\n",
       "  0.0022426219657063484,\n",
       "  0.001810187241062522,\n",
       "  0.003434262238442898,\n",
       "  0.006216403096914291,\n",
       "  0.0038372946437448263,\n",
       "  0.005657447502017021,\n",
       "  0.00348234036937356,\n",
       "  0.0014015974011272192,\n",
       "  0.0024240179918706417,\n",
       "  0.003636422799900174,\n",
       "  0.0010421589249745011,\n",
       "  0.004380940459668636,\n",
       "  0.006547784432768822,\n",
       "  0.002803029492497444,\n",
       "  0.004377841483801603,\n",
       "  0.004227520432323217,\n",
       "  0.004350853152573109,\n",
       "  0.0071741510182619095,\n",
       "  0.0046674697659909725,\n",
       "  0.002321135252714157,\n",
       "  0.0038884487003087997,\n",
       "  0.0030540432780981064,\n",
       "  0.004571347963064909,\n",
       "  0.005601399578154087,\n",
       "  0.003132864832878113,\n",
       "  0.0025723502039909363,\n",
       "  0.005833525210618973,\n",
       "  0.0025660416577011347,\n",
       "  0.006144515238702297,\n",
       "  0.0032512808684259653,\n",
       "  0.004266686737537384,\n",
       "  0.0036595198325812817,\n",
       "  0.00825582817196846,\n",
       "  0.004594982136040926,\n",
       "  0.002090382156893611,\n",
       "  0.006738794036209583,\n",
       "  0.00268884957768023,\n",
       "  0.00241769477725029,\n",
       "  0.0043490114621818066,\n",
       "  0.005640510935336351,\n",
       "  0.004736245144158602,\n",
       "  0.004957377910614014,\n",
       "  0.005180292762815952,\n",
       "  0.002465775003656745,\n",
       "  0.003220419865101576,\n",
       "  0.004697058349847794,\n",
       "  0.006783929653465748,\n",
       "  0.0006222609081305563,\n",
       "  0.00469683762639761,\n",
       "  0.0017802375368773937,\n",
       "  0.002588872564956546,\n",
       "  0.004829024896025658,\n",
       "  0.003248682012781501,\n",
       "  0.004410481546074152,\n",
       "  0.0027552018873393536,\n",
       "  0.002608338836580515,\n",
       "  0.00367702916264534,\n",
       "  0.003308101324364543,\n",
       "  0.0042186579667031765,\n",
       "  0.0037170066498219967,\n",
       "  0.0010090810246765614,\n",
       "  0.004305205307900906,\n",
       "  0.007242203690111637,\n",
       "  0.004711612593382597,\n",
       "  0.00476211030036211,\n",
       "  0.00207646400667727,\n",
       "  0.0041127935983240604,\n",
       "  0.0016305595636367798,\n",
       "  0.0015780860558152199,\n",
       "  0.004545936360955238,\n",
       "  0.008527101017534733,\n",
       "  0.002062416635453701,\n",
       "  0.004214342217892408,\n",
       "  0.0037978431209921837,\n",
       "  0.004767509177327156,\n",
       "  0.0017515209037810564,\n",
       "  0.004859139211475849,\n",
       "  0.006832311861217022,\n",
       "  0.003172691445797682,\n",
       "  0.002268302021548152,\n",
       "  0.006127147935330868,\n",
       "  0.004720564931631088,\n",
       "  0.004833536688238382,\n",
       "  0.006219847593456507,\n",
       "  0.001517062890343368,\n",
       "  0.001944411313161254,\n",
       "  0.003245643340051174,\n",
       "  0.004205040633678436,\n",
       "  0.0022085998207330704,\n",
       "  -6.033474346622825e-05,\n",
       "  0.005939978174865246,\n",
       "  0.0022161267697811127,\n",
       "  0.001643120776861906,\n",
       "  0.00103183148894459,\n",
       "  0.004001125693321228,\n",
       "  0.005452511832118034,\n",
       "  0.0018170748371630907,\n",
       "  0.00435046898201108,\n",
       "  0.0060567669570446014,\n",
       "  0.004652027040719986,\n",
       "  0.0024527909699827433,\n",
       "  0.003335250075906515,\n",
       "  0.0029709660448133945,\n",
       "  0.004416921176016331,\n",
       "  0.0038612207863479853,\n",
       "  0.003554422641173005,\n",
       "  0.0035528067965060472,\n",
       "  0.004207220859825611,\n",
       "  0.005646174773573875,\n",
       "  0.0037693565245717764,\n",
       "  0.005370512139052153,\n",
       "  0.0035056578926742077,\n",
       "  0.0025474149733781815,\n",
       "  0.0039507062174379826,\n",
       "  0.0026845233514904976,\n",
       "  0.004459973890334368,\n",
       "  0.0004130657180212438,\n",
       "  0.0020942373666912317,\n",
       "  0.005984263028949499,\n",
       "  0.0033983150497078896,\n",
       "  0.004743760451674461,\n",
       "  0.004141460172832012,\n",
       "  0.003594766603782773,\n",
       "  0.004256788175553083,\n",
       "  0.0038631060160696507,\n",
       "  0.008592508733272552,\n",
       "  0.005704376846551895,\n",
       "  0.0011618214193731546,\n",
       "  0.002100351033732295,\n",
       "  0.005012667737901211,\n",
       "  0.005862394347786903,\n",
       "  0.0007245680317282677,\n",
       "  0.0052148448303341866,\n",
       "  0.0015515305567532778,\n",
       "  0.00374051695689559,\n",
       "  0.005508901085704565,\n",
       "  0.0036854567006230354,\n",
       "  0.006252117455005646,\n",
       "  0.0028321174904704094,\n",
       "  0.004033195786178112,\n",
       "  0.003620726987719536,\n",
       "  0.0019660743419080973,\n",
       "  0.005936329253017902,\n",
       "  0.003565477207303047,\n",
       "  0.0022264502476900816,\n",
       "  0.0032371217384934425,\n",
       "  0.003285811748355627,\n",
       "  0.004488751292228699,\n",
       "  0.005486646201461554,\n",
       "  0.001782550592906773,\n",
       "  0.0026162201538681984,\n",
       "  0.003670833073556423,\n",
       "  0.0035285649355500937,\n",
       "  0.004056582693010569,\n",
       "  0.004161381162703037,\n",
       "  0.0041856965981423855,\n",
       "  0.0030793752521276474,\n",
       "  0.0022696813102811575,\n",
       "  0.003928578924387693,\n",
       "  0.001687611103989184,\n",
       "  0.007625982165336609,\n",
       "  0.007286840118467808,\n",
       "  0.003193068550899625,\n",
       "  0.005818015895783901,\n",
       "  0.0058591063134372234,\n",
       "  0.001618789043277502,\n",
       "  0.0025625533889979124,\n",
       "  0.002727467566728592,\n",
       "  0.0040011173114180565,\n",
       "  0.003018197137862444,\n",
       "  0.003527931170538068,\n",
       "  0.006973925046622753,\n",
       "  0.0029064666014164686,\n",
       "  0.002985692583024502,\n",
       "  0.002841000910848379,\n",
       "  0.0027405465953052044,\n",
       "  0.0015726499259471893,\n",
       "  0.004684815648943186,\n",
       "  0.003978733904659748,\n",
       "  0.0029036111664026976,\n",
       "  0.0017489905003458261,\n",
       "  0.0063410415314137936,\n",
       "  0.007890705019235611,\n",
       "  0.002256116596981883,\n",
       "  0.0009892628295347095,\n",
       "  0.0036082291044294834,\n",
       "  0.005072163417935371,\n",
       "  0.00447732862085104,\n",
       "  0.0028871027752757072,\n",
       "  0.004953229334205389,\n",
       "  0.002389895496889949,\n",
       "  0.0029978440143167973,\n",
       "  0.003002067096531391,\n",
       "  0.002722301986068487,\n",
       "  0.004192890599370003,\n",
       "  0.003723572939634323,\n",
       "  0.0016695766244083643,\n",
       "  0.003676102263852954,\n",
       "  0.003917536698281765,\n",
       "  0.0030450494959950447,\n",
       "  0.0019868523813784122,\n",
       "  0.004329284653067589,\n",
       "  0.004352407529950142,\n",
       "  0.0041610789485275745,\n",
       "  0.0031853592954576015,\n",
       "  0.002539646113291383,\n",
       "  0.0021380444522947073,\n",
       "  0.003977714106440544,\n",
       "  0.004098963923752308,\n",
       "  0.003480859100818634,\n",
       "  0.008655047975480556,\n",
       "  0.00405003409832716,\n",
       "  0.0026400922797620296,\n",
       "  0.0032508692238479853,\n",
       "  0.005515931639820337,\n",
       "  0.0015060007572174072,\n",
       "  0.004575012251734734,\n",
       "  0.002421631943434477,\n",
       "  0.006424750201404095,\n",
       "  0.0023481426760554314,\n",
       "  0.003924596589058638,\n",
       "  0.005613051354885101,\n",
       "  0.004799807444214821,\n",
       "  0.003707658965140581,\n",
       "  0.0010578816290944815,\n",
       "  0.0031539108604192734,\n",
       "  0.002814777661114931,\n",
       "  0.0030437635723501444,\n",
       "  0.0019818060100078583,\n",
       "  0.0037520085461437702,\n",
       "  0.003082720097154379,\n",
       "  0.0024551276583224535,\n",
       "  0.003624027594923973,\n",
       "  0.0019859413150697947,\n",
       "  0.0024410595651715994,\n",
       "  0.004239129833877087,\n",
       "  0.004529126919806004,\n",
       "  0.005596643779426813,\n",
       "  0.003627009689807892,\n",
       "  0.00535182049497962,\n",
       "  0.0028117545880377293,\n",
       "  0.002535199746489525,\n",
       "  0.0014403065433725715,\n",
       "  0.004354760982096195,\n",
       "  0.0029849796555936337,\n",
       "  0.0038721547462046146,\n",
       "  0.0020550736226141453,\n",
       "  0.0035349158570170403,\n",
       "  0.0013404017081484199,\n",
       "  0.0028679990209639072,\n",
       "  0.0035816719755530357,\n",
       "  0.003528496017679572,\n",
       "  0.0020877087954431772,\n",
       "  0.0041696541011333466,\n",
       "  0.00381846958771348,\n",
       "  0.005049928557127714,\n",
       "  0.0024305859114974737,\n",
       "  0.0028512391727417707,\n",
       "  0.00048398569924756885,\n",
       "  0.005373149644583464,\n",
       "  0.0012978826416656375,\n",
       "  0.0020963624119758606,\n",
       "  0.0025032530538737774,\n",
       "  0.004696069750934839,\n",
       "  0.003096887608990073,\n",
       "  0.002171560190618038,\n",
       "  0.001889814157038927,\n",
       "  0.003514873096719384,\n",
       "  0.0038065901026129723,\n",
       "  0.002089527901262045,\n",
       "  0.0007164759445004165,\n",
       "  0.0014706053771078587,\n",
       "  0.002612873911857605,\n",
       "  0.005403635557740927,\n",
       "  0.0046993037685751915,\n",
       "  0.003149727825075388,\n",
       "  0.004058400169014931,\n",
       "  0.002874684054404497,\n",
       "  0.0051682498306035995,\n",
       "  0.00493302196264267,\n",
       "  0.003952725790441036,\n",
       "  0.001801967155188322,\n",
       "  0.004566963762044907,\n",
       "  0.005747044924646616,\n",
       "  0.002173056360334158,\n",
       "  0.0018678453052416444,\n",
       "  0.004255067091435194,\n",
       "  0.005189809016883373,\n",
       "  0.003584461286664009,\n",
       "  0.003564930520951748,\n",
       "  0.002445773221552372,\n",
       "  0.002479360904544592,\n",
       "  0.003877619281411171,\n",
       "  0.0028570955619215965,\n",
       "  0.0038323807530105114,\n",
       "  0.0020056706853210926,\n",
       "  0.003514745505526662,\n",
       "  0.005593106150627136,\n",
       "  0.0028708227910101414,\n",
       "  0.0024394220672547817,\n",
       "  0.004426231607794762,\n",
       "  0.0076177953742444515,\n",
       "  0.004199715331196785,\n",
       "  0.004245512653142214,\n",
       "  0.0029888437129557133,\n",
       "  0.007940592244267464,\n",
       "  0.005312097724527121,\n",
       "  0.0021082458551973104,\n",
       "  0.0053679123520851135,\n",
       "  0.0027770004235208035,\n",
       "  0.005875512026250362,\n",
       "  0.004219955764710903,\n",
       "  0.004828275181353092,\n",
       "  0.0037425325717777014,\n",
       "  0.004990429617464542,\n",
       "  0.0035831397399306297,\n",
       "  0.0054248543456196785,\n",
       "  0.0022839822340756655,\n",
       "  0.0014102800050750375,\n",
       "  0.003344263182953,\n",
       "  0.005679477471858263,\n",
       "  0.004336001351475716,\n",
       "  0.004349076189100742,\n",
       "  0.0020280482713133097,\n",
       "  0.001486821100115776,\n",
       "  0.0033063064329326153,\n",
       "  0.002592714037746191,\n",
       "  0.005327189341187477,\n",
       "  0.003099738620221615,\n",
       "  0.0035944245755672455,\n",
       "  0.005284589249640703,\n",
       "  0.0006727109430357814,\n",
       "  0.0028056269511580467,\n",
       "  0.005316154100000858,\n",
       "  0.0030366340652108192,\n",
       "  0.006540610920637846,\n",
       "  0.005787873174995184,\n",
       "  0.004182070959359407,\n",
       "  0.0040815239772200584,\n",
       "  0.00680884625762701,\n",
       "  0.003470040624961257,\n",
       "  0.0009806095622479916,\n",
       "  0.001231302274391055,\n",
       "  0.0016247392632067204,\n",
       "  0.004393049981445074,\n",
       "  0.00427614152431488,\n",
       "  0.0027849648613482714,\n",
       "  0.005826265085488558,\n",
       "  0.005016520619392395,\n",
       "  0.0038549364544451237,\n",
       "  0.002686274005100131,\n",
       "  0.007239293772727251,\n",
       "  0.0013157455250620842,\n",
       "  0.004471601452678442,\n",
       "  0.0020566999446600676,\n",
       "  0.0030730441212654114,\n",
       "  0.0023263785988092422,\n",
       "  0.00490281218662858,\n",
       "  0.0030776006169617176,\n",
       "  0.0034596757031977177,\n",
       "  0.005930392071604729,\n",
       "  0.0043747383169829845,\n",
       "  0.003711084369570017,\n",
       "  0.002725627738982439,\n",
       "  0.0027931518852710724,\n",
       "  0.003423270769417286,\n",
       "  0.006269062403589487,\n",
       "  0.002880871994420886,\n",
       "  0.0029786829836666584,\n",
       "  0.0044112554751336575,\n",
       "  0.0032991464249789715,\n",
       "  0.0038578175008296967,\n",
       "  0.003726192517206073,\n",
       "  0.0030786856077611446,\n",
       "  0.0024688742123544216,\n",
       "  0.004625428933650255,\n",
       "  0.006309804040938616,\n",
       "  0.003047526814043522,\n",
       "  0.005090552847832441,\n",
       "  0.0024594501592218876,\n",
       "  0.0018245517276227474,\n",
       "  0.005062861833721399,\n",
       "  0.0025672740302979946,\n",
       "  0.004593942780047655,\n",
       "  0.0028879253659397364,\n",
       "  0.0033026388846337795,\n",
       "  0.002572732511907816,\n",
       "  0.003467331640422344,\n",
       "  0.006922505795955658,\n",
       "  0.006841815542429686,\n",
       "  0.003422187641263008,\n",
       "  3.4964585211127996e-05,\n",
       "  0.006619197782129049,\n",
       "  0.003537594573572278,\n",
       "  0.004982137121260166,\n",
       "  0.0015914600808173418,\n",
       "  0.0036514962557703257,\n",
       "  0.004958715755492449,\n",
       "  0.002902563661336899,\n",
       "  0.0025010304525494576,\n",
       "  0.0015939647564664483,\n",
       "  0.0037425721529871225,\n",
       "  0.005656332243233919,\n",
       "  0.004122697748243809,\n",
       "  0.0021275042090564966,\n",
       "  0.004971646703779697,\n",
       "  0.006205929908901453,\n",
       "  0.005875870585441589,\n",
       "  0.006670746952295303,\n",
       "  0.005987727083265781,\n",
       "  0.0027910408098250628,\n",
       "  0.004624424502253532,\n",
       "  0.0063317446038126945,\n",
       "  0.00786597654223442,\n",
       "  0.002873216290026903,\n",
       "  0.005019233096390963,\n",
       "  0.005958407185971737,\n",
       "  0.004581314045935869,\n",
       "  0.0008075717487372458,\n",
       "  0.0026811729185283184,\n",
       "  0.0032807118259370327,\n",
       "  0.0028865658678114414,\n",
       "  0.0035482023376971483,\n",
       "  0.0032318872399628162,\n",
       "  0.003183966502547264,\n",
       "  0.002785045187920332,\n",
       "  0.0052389176562428474,\n",
       "  0.00206182268448174,\n",
       "  0.0027103926986455917,\n",
       "  0.0036820806562900543,\n",
       "  0.004541551694273949,\n",
       "  0.0011485036229714751,\n",
       "  0.004317933693528175,\n",
       "  0.0028642623219639063,\n",
       "  0.003428456839174032,\n",
       "  0.007896147668361664,\n",
       "  0.0006955260178074241,\n",
       "  0.0025063450448215008,\n",
       "  0.002597599057480693,\n",
       "  0.002920287661254406,\n",
       "  0.001980760833248496,\n",
       "  0.0035160104744136333,\n",
       "  0.0034908130764961243,\n",
       "  0.002710429485887289,\n",
       "  0.006669083144515753,\n",
       "  0.004319498781114817,\n",
       "  0.002972089685499668],\n",
       " [0.015589992515742779,\n",
       "  0.012274431996047497,\n",
       "  0.01381849218159914,\n",
       "  0.021554701030254364,\n",
       "  0.01242266409099102,\n",
       "  0.014123914763331413,\n",
       "  0.013160351663827896,\n",
       "  0.007705667056143284,\n",
       "  0.012324366718530655,\n",
       "  0.0049590254202485085,\n",
       "  0.0043638888746500015,\n",
       "  0.004806672688573599,\n",
       "  0.006821990478783846,\n",
       "  0.005353986751288176,\n",
       "  0.004356119316071272,\n",
       "  0.005614080466330051,\n",
       "  0.004348166286945343,\n",
       "  0.0028835479170084,\n",
       "  0.0037581888027489185,\n",
       "  0.004187386482954025,\n",
       "  0.0023287797812372446,\n",
       "  0.003612448927015066,\n",
       "  0.0022815808188170195,\n",
       "  0.0024285023100674152,\n",
       "  0.007272615563124418,\n",
       "  0.004906121175736189,\n",
       "  0.0033035888336598873,\n",
       "  0.00266901939176023,\n",
       "  0.006547911092638969,\n",
       "  0.0037322218995541334,\n",
       "  0.007045737002044916,\n",
       "  0.0021064290776848793,\n",
       "  0.0029378996696323156,\n",
       "  0.0037012766115367413,\n",
       "  0.003300305688753724,\n",
       "  0.00791651476174593,\n",
       "  0.0029685695189982653,\n",
       "  0.0018348218873143196,\n",
       "  0.004503237083554268,\n",
       "  0.002399624325335026,\n",
       "  0.005290846340358257,\n",
       "  0.0024283663369715214,\n",
       "  0.004349734168499708,\n",
       "  0.005185198970139027,\n",
       "  0.0043136971071362495,\n",
       "  0.004809203092008829,\n",
       "  0.0013075006427243352,\n",
       "  0.0020180793944746256,\n",
       "  0.006369109731167555,\n",
       "  0.0027163990307599306,\n",
       "  0.0029778366442769766,\n",
       "  0.0017591062933206558,\n",
       "  0.003541111247614026,\n",
       "  0.002830331912264228,\n",
       "  0.004870006814599037,\n",
       "  0.003876041853800416,\n",
       "  0.006175289861857891,\n",
       "  0.0033968137577176094,\n",
       "  0.00850178673863411,\n",
       "  0.0021730144508183002,\n",
       "  0.004826564807444811,\n",
       "  0.0056652952916920185,\n",
       "  0.002042892388999462,\n",
       "  0.003977509681135416,\n",
       "  0.002028446178883314,\n",
       "  0.001950823119841516,\n",
       "  0.004294294863939285,\n",
       "  0.0052145677618682384,\n",
       "  0.0012129527749493718,\n",
       "  0.0031331239733844995,\n",
       "  0.0044382004998624325,\n",
       "  0.0026445460971444845,\n",
       "  0.0057181380689144135,\n",
       "  0.003474377328529954,\n",
       "  0.004801548086106777,\n",
       "  0.002518689027056098,\n",
       "  0.005536913406103849,\n",
       "  0.004529075231403112,\n",
       "  0.006700047291815281,\n",
       "  0.004575223661959171,\n",
       "  0.005198624450713396,\n",
       "  0.002108088694512844,\n",
       "  0.005992892198264599,\n",
       "  0.0021538122091442347,\n",
       "  0.004320583771914244,\n",
       "  0.0011175799882039428,\n",
       "  0.006736085284501314,\n",
       "  0.0022901000920683146,\n",
       "  0.0008151775691658258,\n",
       "  0.004969071596860886,\n",
       "  0.0032832035794854164,\n",
       "  0.002839342225342989,\n",
       "  0.002058733720332384,\n",
       "  0.0034293923527002335,\n",
       "  0.006880710832774639,\n",
       "  0.0036792184691876173,\n",
       "  0.005367124453186989,\n",
       "  0.0073198964819312096,\n",
       "  0.005608085077255964,\n",
       "  0.006464079488068819,\n",
       "  0.004180078394711018,\n",
       "  0.0030008764006197453,\n",
       "  0.0038932329043745995,\n",
       "  0.001875389600172639,\n",
       "  -0.0004195199580863118,\n",
       "  0.004077899269759655,\n",
       "  0.0016819878946989775,\n",
       "  0.0029006577096879482,\n",
       "  0.005166538059711456,\n",
       "  0.004659677855670452,\n",
       "  0.003196221310645342,\n",
       "  0.003822125494480133,\n",
       "  0.0044334144331514835,\n",
       "  0.0033519621938467026,\n",
       "  0.00447505246847868,\n",
       "  0.002284562448039651,\n",
       "  0.0016425091307610273,\n",
       "  0.0013929781271144748,\n",
       "  0.002271783072501421,\n",
       "  0.003689704928547144,\n",
       "  0.00630404707044363,\n",
       "  0.0048373425379395485,\n",
       "  0.006876552477478981,\n",
       "  0.002617327030748129,\n",
       "  0.002987267915159464,\n",
       "  0.001525597763247788,\n",
       "  0.005978681147098541,\n",
       "  0.0032851481810212135,\n",
       "  0.00565394526347518,\n",
       "  0.009217442944645882,\n",
       "  0.004630031995475292,\n",
       "  0.0018439767882227898,\n",
       "  0.005978494882583618,\n",
       "  0.005757369566708803,\n",
       "  0.003897080197930336,\n",
       "  0.0046856296248734,\n",
       "  0.004945477936416864,\n",
       "  0.002295364160090685,\n",
       "  0.004401643760502338,\n",
       "  0.0038692671805620193,\n",
       "  0.0016446050722151995,\n",
       "  0.004465987905859947,\n",
       "  0.001707032904960215,\n",
       "  0.002104405779391527,\n",
       "  0.004786464385688305,\n",
       "  0.002120213583111763,\n",
       "  0.004552699159830809,\n",
       "  0.0066644130274653435,\n",
       "  0.0012453871313482523,\n",
       "  0.0023574240040034056,\n",
       "  0.0026693735271692276,\n",
       "  0.00290628126822412,\n",
       "  0.0037459139712154865,\n",
       "  0.0021671345457434654,\n",
       "  0.0050829313695430756,\n",
       "  0.007853863760828972,\n",
       "  0.002388652414083481,\n",
       "  0.003667454468086362,\n",
       "  0.0031704509165138006,\n",
       "  0.0034576435573399067,\n",
       "  0.0031245150603353977,\n",
       "  0.0037380822468549013,\n",
       "  0.006502246484160423,\n",
       "  0.004217903129756451,\n",
       "  0.005279114469885826,\n",
       "  0.003705937648192048,\n",
       "  0.0060150097124278545,\n",
       "  0.0036784978583455086,\n",
       "  0.007666632998734713,\n",
       "  0.005641611758619547,\n",
       "  0.003627869300544262,\n",
       "  0.002957450458779931,\n",
       "  0.003746950766071677,\n",
       "  0.006544324103742838,\n",
       "  0.0012465559411793947,\n",
       "  0.004770602565258741,\n",
       "  0.0012295350898057222,\n",
       "  0.0010261147981509566,\n",
       "  0.00406678719446063,\n",
       "  0.0041638873517513275,\n",
       "  0.006497711408883333,\n",
       "  0.002566415583714843,\n",
       "  0.0018989556701853871,\n",
       "  0.007072220556437969,\n",
       "  0.004637244623154402,\n",
       "  0.007305706851184368,\n",
       "  0.003240672405809164,\n",
       "  0.00431800028309226,\n",
       "  0.00516220647841692,\n",
       "  0.003356641624122858,\n",
       "  0.004696985706686974,\n",
       "  0.001295506488531828,\n",
       "  0.0015454253880307078,\n",
       "  0.005686820484697819,\n",
       "  0.007666229270398617,\n",
       "  0.004273084923624992,\n",
       "  0.006340628024190664,\n",
       "  0.004054093733429909,\n",
       "  0.0035535634960979223,\n",
       "  0.0030950093641877174,\n",
       "  0.0025643217377364635,\n",
       "  0.0041226488538086414,\n",
       "  0.008168785832822323,\n",
       "  0.002937378128990531,\n",
       "  0.0017711344407871366,\n",
       "  0.0018031209474429488,\n",
       "  0.003686477430164814,\n",
       "  0.004188488237559795,\n",
       "  0.001839338568970561,\n",
       "  0.0013714220840483904,\n",
       "  0.0037450788076967,\n",
       "  0.003514716401696205,\n",
       "  0.00360839138738811,\n",
       "  0.0063365246169269085,\n",
       "  0.00539642246440053,\n",
       "  0.007453456521034241,\n",
       "  0.0007830641698092222,\n",
       "  0.003969790879637003,\n",
       "  0.0035829045809805393,\n",
       "  0.003255846444517374,\n",
       "  0.004591462202370167,\n",
       "  0.00119040347635746,\n",
       "  0.002342620864510536,\n",
       "  0.0017211413942277431,\n",
       "  0.0022612796165049076,\n",
       "  0.005230424925684929,\n",
       "  0.003912570886313915,\n",
       "  0.0029622968286275864,\n",
       "  0.004407424945384264,\n",
       "  0.006120596546679735,\n",
       "  0.0029538131784647703,\n",
       "  0.001874752575531602,\n",
       "  0.003885386511683464,\n",
       "  0.0019870877731591463,\n",
       "  0.005562531761825085,\n",
       "  0.0004263108130544424,\n",
       "  0.006919944658875465,\n",
       "  0.001819445751607418,\n",
       "  0.0032730144448578358,\n",
       "  0.002880206797271967,\n",
       "  0.0030054054223001003,\n",
       "  0.003871751017868519,\n",
       "  0.0026836434844881296,\n",
       "  0.004793415777385235,\n",
       "  0.0025844709016382694,\n",
       "  0.0022722159046679735,\n",
       "  0.004867524839937687,\n",
       "  0.00853338185697794,\n",
       "  0.00407622754573822,\n",
       "  0.003778818529099226,\n",
       "  0.001414573984220624,\n",
       "  0.0029146054293960333,\n",
       "  0.003495563752949238,\n",
       "  0.0053956941701471806,\n",
       "  0.004285519476979971,\n",
       "  0.001846767496317625,\n",
       "  0.0031797820702195168,\n",
       "  0.002909635193645954,\n",
       "  0.00770516786724329,\n",
       "  0.004616947844624519,\n",
       "  0.007031999994069338,\n",
       "  0.004047242924571037,\n",
       "  0.0016366168856620789,\n",
       "  0.002923395251855254,\n",
       "  0.003707015421241522,\n",
       "  0.004645804408937693,\n",
       "  0.0036260485649108887,\n",
       "  0.008972056210041046,\n",
       "  0.0014281312469393015,\n",
       "  0.0044774701818823814,\n",
       "  0.003561717690899968,\n",
       "  0.007649551145732403,\n",
       "  0.002958263736218214,\n",
       "  0.002591406460851431,\n",
       "  0.004207625985145569,\n",
       "  0.00012644118396565318,\n",
       "  0.002070682356134057,\n",
       "  0.004632349591702223,\n",
       "  0.003732780460268259,\n",
       "  0.004241218790411949,\n",
       "  0.003904319368302822,\n",
       "  0.005476758815348148,\n",
       "  0.0046231686137616634,\n",
       "  0.005426324438303709,\n",
       "  0.002893569879233837,\n",
       "  0.005005738232284784,\n",
       "  0.003982259891927242,\n",
       "  0.002068408066406846,\n",
       "  0.005456681828945875,\n",
       "  0.0047378260642290115,\n",
       "  0.0009504294721409678,\n",
       "  0.0038952715694904327,\n",
       "  0.004435024224221706,\n",
       "  -3.387697506695986e-05,\n",
       "  0.004539182875305414,\n",
       "  0.004363229498267174,\n",
       "  0.0008609628421254456,\n",
       "  0.0028378297574818134,\n",
       "  0.002893539611250162,\n",
       "  0.004521425347775221,\n",
       "  0.004571021534502506,\n",
       "  0.003908292390406132,\n",
       "  0.002381587168201804,\n",
       "  0.004564027301967144,\n",
       "  0.0023070890456438065,\n",
       "  0.0026485128328204155,\n",
       "  0.0023835799656808376,\n",
       "  0.00925663486123085,\n",
       "  0.003921919967979193,\n",
       "  0.00310272304341197,\n",
       "  0.002510092221200466,\n",
       "  0.002066519344225526,\n",
       "  0.002719202311709523,\n",
       "  0.005151205696165562,\n",
       "  0.005616653710603714,\n",
       "  0.003486044704914093,\n",
       "  0.0032483339309692383,\n",
       "  0.001810331828892231,\n",
       "  0.004986916668713093,\n",
       "  0.002855615457519889,\n",
       "  0.005197108257561922,\n",
       "  0.004511872306466103,\n",
       "  0.0034874160774052143,\n",
       "  0.00017506882431916893,\n",
       "  0.0026994524523615837,\n",
       "  0.0036211649421602488,\n",
       "  0.0045281071215868,\n",
       "  0.005778097081929445,\n",
       "  0.0015949573135003448,\n",
       "  0.0004435988375917077,\n",
       "  0.0039033242501318455,\n",
       "  0.0033044125884771347,\n",
       "  0.006092906929552555,\n",
       "  0.006073547061532736,\n",
       "  0.0034632158931344748,\n",
       "  0.0027731717564165592,\n",
       "  0.0025881982874125242,\n",
       "  -0.001502683968283236,\n",
       "  0.004736257251352072,\n",
       "  0.0027983190957456827,\n",
       "  0.003351160092279315,\n",
       "  0.004600054118782282,\n",
       "  0.006471320521086454,\n",
       "  0.0031691091135144234,\n",
       "  0.005567483138293028,\n",
       "  0.005327626597136259,\n",
       "  0.005566077772527933,\n",
       "  0.0053565045818686485,\n",
       "  0.0019543988164514303,\n",
       "  0.0023757952731102705,\n",
       "  0.005825079046189785,\n",
       "  0.0026857454795390368,\n",
       "  0.00669063301756978,\n",
       "  -0.0004264292074367404,\n",
       "  0.006837752647697926,\n",
       "  0.00442581158131361,\n",
       "  0.0035448255948722363,\n",
       "  0.006885935552418232,\n",
       "  0.006857103202491999,\n",
       "  0.0053659239783883095,\n",
       "  0.004214175511151552,\n",
       "  0.0011996714165434241,\n",
       "  0.00267566810362041,\n",
       "  0.005077884998172522,\n",
       "  0.0025921673513948917,\n",
       "  0.0033221906051039696,\n",
       "  0.005168717820197344,\n",
       "  0.002058902522549033,\n",
       "  0.004149399697780609,\n",
       "  0.005768872797489166,\n",
       "  0.006402111146599054,\n",
       "  0.004272942431271076,\n",
       "  0.0033941271249204874,\n",
       "  0.0035354490391910076,\n",
       "  0.0013367162318900228,\n",
       "  0.00453337375074625,\n",
       "  0.00370846688747406,\n",
       "  0.001638002460822463,\n",
       "  0.004414771683514118,\n",
       "  0.006729287561029196,\n",
       "  0.004809515550732613,\n",
       "  0.003948247991502285,\n",
       "  0.005737815983593464,\n",
       "  0.005889278836548328,\n",
       "  0.005698043387383223,\n",
       "  0.0020455331541597843,\n",
       "  0.004711396060883999,\n",
       "  0.005804711952805519,\n",
       "  0.0034786625765264034,\n",
       "  0.003218982368707657,\n",
       "  0.001834492664784193,\n",
       "  0.00453131552785635,\n",
       "  0.004468027502298355,\n",
       "  0.0033570597879588604,\n",
       "  0.001155874808318913,\n",
       "  0.003871873952448368,\n",
       "  0.00441731046885252,\n",
       "  0.0027028010226786137,\n",
       "  0.0028021857142448425,\n",
       "  0.0006299146916717291,\n",
       "  0.004984219092875719,\n",
       "  0.002805622760206461,\n",
       "  0.004311022814363241,\n",
       "  0.0033887901809066534,\n",
       "  0.003358184825628996,\n",
       "  0.0013296565739437938,\n",
       "  0.004491055384278297,\n",
       "  0.003987226169556379,\n",
       "  0.004754377529025078,\n",
       "  0.005995291750878096,\n",
       "  0.004186537116765976,\n",
       "  0.0034878964070230722,\n",
       "  0.0033494646195322275,\n",
       "  0.0018091538222506642,\n",
       "  0.007135272491723299,\n",
       "  0.004187070298939943,\n",
       "  0.005755915306508541,\n",
       "  0.0019719561096280813,\n",
       "  0.006239301059395075,\n",
       "  0.0056386711075901985,\n",
       "  0.00256253220140934,\n",
       "  0.003722934518009424,\n",
       "  0.0009554576827213168,\n",
       "  0.006390837952494621,\n",
       "  0.004961489234119654,\n",
       "  0.0028574923053383827,\n",
       "  0.0022065257653594017,\n",
       "  0.0038409768603742123,\n",
       "  0.004464591387659311,\n",
       "  0.0046557774767279625,\n",
       "  0.00602544704452157,\n",
       "  0.003519851714372635,\n",
       "  0.005941242910921574,\n",
       "  0.001479435944929719,\n",
       "  0.0032138328533619642,\n",
       "  0.00440949946641922,\n",
       "  0.003748921677470207,\n",
       "  0.005396259482949972,\n",
       "  0.0026557110249996185,\n",
       "  0.003019712632521987,\n",
       "  0.0026736310683190823,\n",
       "  0.005993339233100414,\n",
       "  0.0050372164696455,\n",
       "  0.005031803157180548,\n",
       "  0.004075152799487114,\n",
       "  0.0034719582181423903,\n",
       "  0.0020250624511390924,\n",
       "  0.004116457886993885,\n",
       "  0.0037219305522739887,\n",
       "  0.0032193681690841913,\n",
       "  0.002831723541021347,\n",
       "  0.00203884718939662,\n",
       "  0.00372427050024271,\n",
       "  0.0057582249864935875,\n",
       "  0.0035572464112192392,\n",
       "  0.0036894790828227997,\n",
       "  0.0034811743535101414,\n",
       "  0.004131005611270666,\n",
       "  0.005936621222645044,\n",
       "  0.0034032659605145454,\n",
       "  0.004254605155438185,\n",
       "  0.005973411723971367,\n",
       "  0.002452879911288619,\n",
       "  0.003944260999560356,\n",
       "  0.004762789234519005,\n",
       "  0.004828459583222866,\n",
       "  0.0013272197684273124,\n",
       "  0.00247689220122993,\n",
       "  0.001892512897029519,\n",
       "  0.0024202209897339344,\n",
       "  0.00540492357686162,\n",
       "  0.006325318943709135,\n",
       "  0.004692484624683857,\n",
       "  0.0028249670285731554,\n",
       "  0.0054588038474321365,\n",
       "  0.004221286624670029,\n",
       "  0.003036226611584425,\n",
       "  0.004535532556474209,\n",
       "  0.005154656711965799,\n",
       "  0.006331350654363632,\n",
       "  0.004279837943613529,\n",
       "  0.0012168842367827892,\n",
       "  0.0028606648556888103,\n",
       "  0.0005447951843962073,\n",
       "  0.006916376296430826,\n",
       "  0.0027852552011609077,\n",
       "  0.00357814971357584,\n",
       "  0.003265759442001581,\n",
       "  0.004578747786581516,\n",
       "  0.0045178914442658424,\n",
       "  0.0041895704343914986,\n",
       "  0.00485426327213645,\n",
       "  0.001540598226711154,\n",
       "  0.005023444537073374,\n",
       "  0.004315191879868507,\n",
       "  0.0041398596949875355,\n",
       "  0.0030074703972786665,\n",
       "  0.004139769822359085,\n",
       "  0.0037749388720840216,\n",
       "  0.006977583281695843])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.train()\n",
    "netG_neg.train()\n",
    "# gen_losses, disc_losses = train_GAN(netD_neg, netG_neg, negative=True)\n",
    "train_GAN(netD_neg, netG_neg, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(gen_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(disc_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD_neg.state_dict(), './netD_neg-1m')\n",
    "torch.save(netG_neg.state_dict(), './netG_neg-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=3726, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=3706, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test without train\n",
    "netD_neg_test = NetD(tr.shape[1]).cuda()\n",
    "netG_neg_test = NetG(tr.shape[1]).cuda()\n",
    "\n",
    "netD_neg_test.eval()\n",
    "netG_neg_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=3726, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=3706, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.eval()\n",
    "netG_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuraccy\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(tr > 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake_accur_check = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without train\n",
    "fake_test_accur_check = netG_neg_test(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_accur_check_ = (fake_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_accur_check = (fake_test_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230186, 130432)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_accur_check_ * negative_feedback).sum(), (fake_accur_check_ * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192938, 260072)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_test_accur_check * negative_feedback).sum(), (fake_test_accur_check * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6016880703249365"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items\n",
    "(fake_accur_check_ * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504324732661207"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items - WITHOUT TRAIN \n",
    "(fake_test_accur_check * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7480164058258841"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items\n",
    "((1-fake_accur_check_) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4975628886772368"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4975628886772368"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7211"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fake_accur_check\n",
    "del fake_test_accur_check \n",
    "del e_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "\n",
    "# X = torch.from_numpy(X).float().cuda()\n",
    "# condition = torch.from_numpy(condition).float().cuda()\n",
    "# # real = Variable(X)\n",
    "\n",
    "# noise = torch.randn(64, nz).cuda()\n",
    "# e_mask = torch.Tensor(tr[idxs]>0).cuda()\n",
    "\n",
    "# concated = torch.cat((noise, condition), 1)\n",
    "# fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_test = netG_neg_test(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fake > 0.5).sum(), ((fake > 0.5) * (condition==0)).sum(), condition.sum(), X.sum(), e_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake = (fake >= 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_test = (fake_test > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fake * condition).sum()/condition.sum(), (fake * X).sum()/X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fake * condition).sum()/condition.sum(), (fake * X).sum()/fake.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fake_test * condition).sum()/condition.sum(), (fake_test * X).sum()/X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # accuraccy on zeros (positive feedbacks)\n",
    "# (fake * (1-condition)*X).sum()/(X * (1 - condition)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fake_test * (1-condition)*X).sum()/(X * (1 - condition)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # accuraccy on zeros (positive feedbacks)\n",
    "# ((1 - fake)*(1-condition)*(torch.from_numpy(positive_feedback[idxs]).float().cuda())).cpu().numpy().sum()/((positive_feedback[idxs]).astype(float)*(1-condition).cpu().numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((1 - fake_test)*(1-condition)*(torch.from_numpy(positive_feedback[idxs]).float().cuda())).cpu().numpy().sum()/((positive_feedback[idxs]).astype(float)*(1-condition).cpu().numpy()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(X_neg, y_neg, batch_size=256)\n",
    "# # predicting on ngetavie feedbacks\n",
    "# condition = torch.from_numpy(X_neg).float().cuda()\n",
    "# X = torch.from_numpy(y_neg).cuda()\n",
    "\n",
    "\n",
    "# noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(tr>0).cuda()\n",
    "\n",
    "# concated = torch.cat((noise, condition), 1)\n",
    "# fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake = (fake > 0.4).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake = fake.int().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake = fake.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake.sum(), negative_feedback_mask.sum(), to_add_negative.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fake * negative_feedback_mask).sum()/negative_feedback_mask.sum() #checking negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (fake * to_add_negative).sum()/to_add_negative.sum() #checking negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((1-fake) * positive_feedback_mask).sum()/positive_feedback_mask.sum() #checking positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_add_negative = to_add_negative.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sparsity(to_add_negative), get_sparsity(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (y_neg*to_add_negative).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# predicting on ngetavie feedbacks\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(tr == 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = (fake > 0.9).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.sum(), to_add_negative.sum(), (to_add_negative*(tr==0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fake * to_add_negative ).sum()   # checking accuracy with autoencoder generated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DELETE\n",
    "# # condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# # predicting on ngetavie feedbacks\n",
    "# condition_ = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# # X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "# noise_ = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask_ = torch.Tensor((tr == 0).astype(float)).cuda()\n",
    "\n",
    "# concated_ = torch.cat((noise_, condition_), 1)\n",
    "# fake_ = netG_neg(e_mask_, concated_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_train = tr + fake * to_add_negative * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs_neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking\n",
    "# print((positive_feedback * (to_add_negative*fake)).sum()/(to_add_negative*fake).sum()) \n",
    "# print((positive_feedback * to_add_negative).sum()/to_add_negative.sum()) \n",
    "# print((positive_feedback * fake).sum()/fake.sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_augment_negative = to_add_negative*fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4010"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(to_augment_negative *(1-negative_feedback)).sum() #new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_aug = to_augment_negative *(1-negative_feedback_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_aug.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter \n",
    "import matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13207882540835983, 0.25313474502505445, 0.6147864295665857]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_probs = [(tr == 1).sum()/((tr > 0) & (tr < 4)).sum(), (tr == 2).sum()/(((tr > 0) & (tr < 4))).sum(), (tr == 3).sum()/((tr > 0) & (tr < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(np.arange(1, 4), tr.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_augment_negative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-3fbf54f26b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mto_augment_negative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'to_augment_negative' is not defined"
     ]
    }
   ],
   "source": [
    "to_augment_negative.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train = tr + to_augment_negative * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.021525859265269, 4.039440249032355)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr), get_sparsity(augmented_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60, 80]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF_SGD_aug = matrix_factorization.ExplicitMF(augmented_train, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 60, 80]\n",
    "\n",
    "# # iter_array = [10]\n",
    "# # iter_array = [1, 2, 5, 10, 25]\n",
    "# MF_SGD_aug.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.0054055644765958\n",
      "Test mse: 1.0074799937156813\n",
      "Iteration: 2\n",
      "Train mse: 0.9326729588309529\n",
      "Test mse: 0.9371270949857945\n",
      "Iteration: 5\n",
      "Train mse: 0.863053787676433\n",
      "Test mse: 0.8712224075071392\n",
      "Iteration: 10\n",
      "Train mse: 0.8314646291620809\n",
      "Test mse: 0.8427646602946745\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8086777896278844\n",
      "Test mse: 0.8258189313710902\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7567334141442847\n",
      "Test mse: 0.7945486725462679\n",
      "Iteration: 60\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.7180278806643992\n",
      "Test mse: 0.7748495304136693\n",
      "Iteration: 80\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6326583864648311\n",
      "Test mse: 0.7467872145590981\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_train, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60, 80]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.0082582099738886\n",
      "Test mse: 1.0097977337604331\n",
      "Iteration: 2\n",
      "Train mse: 0.9352860075426364\n",
      "Test mse: 0.9386757464338958\n",
      "Iteration: 5\n",
      "Train mse: 0.8643467507935484\n",
      "Test mse: 0.8714117810539594\n",
      "Iteration: 10\n",
      "Train mse: 0.8321931726441352\n",
      "Test mse: 0.842914561236109\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.809157638151105\n",
      "Test mse: 0.8259229587679571\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7566791660185568\n",
      "Test mse: 0.7945160499133725\n",
      "Iteration: 60\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.7174679561985765\n",
      "Test mse: 0.7751236816396795\n",
      "Iteration: 80\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6322252028820194\n",
      "Test mse: 0.74880699888832\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60, 80]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.55689833063505\n",
      "Test mse: 0.7469134432578997\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_train, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 60, 80]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([100], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.5575207270968554\n",
      "Test mse: 0.7471432503263868\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 60, 80]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve([100], vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# predicting on ngetavie feedbacks\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(tr == 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# predicting on ngetavie feedbacks\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(tr > 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake_accur_check = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_accur_check_ = (fake_accur_check.detach().cpu().numpy() > 0.95).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fake_accur_check_ * negative_feedback).sum(), (fake_accur_check_ * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fake_accur_check_ * negative_feedback).sum() / ((fake_accur_check_ * negative_feedback).sum() + (fake_accur_check_ * positive_feedback).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fake > 0.9).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = (fake > 0.9).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake*(tr>0)).sum() # should be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_only_gan = tr + fake * np.random.choice(np.arange(1, 4), tr.shape, p=p_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.3281567745878355, 4.021525859265269)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(augmented_train_only_gan), get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 0.9643356591125679\n",
      "Test mse: 0.9850378723821315\n",
      "Iteration: 2\n",
      "Train mse: 0.8972174387773421\n",
      "Test mse: 0.9234190988918609\n",
      "Iteration: 5\n",
      "Train mse: 0.8436268874891254\n",
      "Test mse: 0.8696809850570631\n",
      "Iteration: 10\n",
      "Train mse: 0.8187371017454731\n",
      "Test mse: 0.8453382532264735\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.7993064678519609\n",
      "Test mse: 0.8297496314734026\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7502206107099286\n",
      "Test mse: 0.8000756978954462\n",
      "Iteration: 60\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.7134464984377021\n",
      "Test mse: 0.7819751921442637\n",
      "Iteration: 80\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6289449955486643\n",
      "Test mse: 0.7565968759774923\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(augmented_train_only_gan, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 60, 80]\n",
    "\n",
    "# iter_array = [10]\n",
    "# iter_array = [1, 2, 5, 10, 25]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
