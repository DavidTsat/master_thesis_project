{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "written by Lorenz Muller\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import sys\n",
    "from dataLoader import loadData\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Downloading Movielens-1m\n",
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "# #     http://www.grouplens.org/system/files/ml-1m.zip\n",
    "# !unzip ml-1m.zip\n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 4.956859111785889 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(seed)\n",
    "# path = './ml-1m/ratings.dat'\n",
    "# delimiter='::'\n",
    "\n",
    "# tic = time()\n",
    "# print('reading data...')\n",
    "# data = np.loadtxt(path, skiprows=0, delimiter=delimiter).astype('int32')\n",
    "# print('data read in', time() - tic, 'seconds')\n",
    "\n",
    "# n_u = np.unique(data[:, 0]).shape[0]  # number of users\n",
    "# n_m = np.unique(data[:, 1]).shape[0]  # number of movies\n",
    "# n_r = data.shape[0]  # number of ratings\n",
    "\n",
    "# # these dictionaries define a mapping from user/movie id to to user/movie number (contiguous from zero)\n",
    "# udict = {}\n",
    "# for i, u in enumerate(np.unique(data[:, 0]).tolist()):\n",
    "#     udict[u] = i\n",
    "# mdict = {}\n",
    "# for i, m in enumerate(np.unique(data[:, 1]).tolist()):\n",
    "#     mdict[m] = i\n",
    "\n",
    "# # shuffle indices\n",
    "# idx = np.arange(n_r)\n",
    "# np.random.shuffle(idx)\n",
    "\n",
    "# trainRatings = np.zeros((n_u, n_m), dtype='float32')\n",
    "# validRatings = np.zeros((n_u, n_m), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "\n",
    "# ratings_list = [i.strip().split(\"::\") for i in open('./ml-1m/ratings.dat', 'r').readlines()]\n",
    "# users_list = [i.strip().split(\"::\") for i in open('./ml-1m/users.dat', 'r').readlines()]\n",
    "# movies_list = [i.strip().split(\"::\") for i in open('./ml-1m/movies.dat', 'r').readlines()]\n",
    "\n",
    "# ratings_df = pd.DataFrame(ratings_list, columns = ['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype = int)\n",
    "# movies_df = pd.DataFrame(movies_list, columns = ['MovieID', 'Title', 'Genres'])\n",
    "# movies_df['MovieID'] = movies_df['MovieID'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n",
    "# R_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings = np.array(R_df.values, dtype=int)\n",
    "# n_users = ratings.shape[0]\n",
    "# n_items = ratings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = tr + vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100021"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vr > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6040, 3706), (6040, 3706))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, vr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from os.path import isfile, isdir, join\n",
    "import os\n",
    "# from tensorboard_logger import configure, log_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f92fc0d830>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 100\n",
    "cuda = True\n",
    "epochs = 400 #change\n",
    "device = 5\n",
    "seed = 1\n",
    "nz = 20\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 1e-2 # constant for L2 penalty (diversity)\n",
    "name = \"mnist-experiment\"\n",
    "# configure(\"runs/run-\" + args.name, flush_secs=5)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#     transform=transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     ])), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Dropout(p=0.5)\n",
      "    (5): Linear(in_features=1024, out_features=3706, bias=True)\n",
      "    (6): Sigmoid()\n",
      "    (7): Dropout(p=0.6)\n",
      "  )\n",
      ")\n",
      "NetD(\n",
      "  (t1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=3706, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=3706, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "#         filt = (torch.abs((real > 0.3).float() * fake - real))/real.shape[0]\n",
    "        filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())/(real != 0).sum()\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "        x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.net = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz, 1024), \n",
    "                                torch.nn.ReLU(), \n",
    "                                torch.nn.Linear(1024, 1024), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.5),\n",
    "                                torch.nn.Linear(1024, features_length), \n",
    "                                torch.nn.Sigmoid(), \n",
    "                                nn.Dropout(0.6)\n",
    "                                )\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#                                  nn.Linear(nz,1024),\n",
    "# #                                  nn.Dropout(0.3)\n",
    "#                                  nn.ReLU(),\n",
    "#                                  nn.Linear(1024,2048),\n",
    "#                                  nn.Sigmoid(),\n",
    "#                                  nn.Dropout(0.3),\n",
    "#                                  nn.Linear(2048,features_length),\n",
    "# #                                  nn.Sigmoid()\n",
    "#                                  nn.Dropout(0.5)\n",
    "#                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "#         return x\n",
    "        return x*5 # to get values in range [0,5]\n",
    "    \n",
    "# networks\n",
    "netD = NetD(use_cuda=True)\n",
    "netG = NetG()\n",
    "print(netG)\n",
    "print(netD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "cuda = True\n",
    "if cuda is True:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    one, mone = one.cuda(), mone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in netD.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #\n",
    "    \n",
    "for p in netG.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRealSample(length=6):\n",
    "     return Variable(torch.IntTensor(np.random.choice([0, 1], size=(batch_size, length))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3706)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_batch(train, batch_size=batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g):\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "ratings_list = [i.strip().split(\"::\") for i in open('./ml-1m/ratings.dat', 'r').readlines()]\n",
    "users_list = [i.strip().split(\"::\") for i in open('./ml-1m/users.dat', 'r').readlines()]\n",
    "movies_list = [i.strip().split(\"::\") for i in open('./ml-1m/movies.dat', 'r').readlines()]\n",
    "\n",
    "ratings_df = pd.DataFrame(ratings_list, columns = ['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype = int)\n",
    "movies_df = pd.DataFrame(movies_list, columns = ['MovieID', 'Title', 'Genres'])\n",
    "movies_df['MovieID'] = movies_df['MovieID'].apply(pd.to_numeric)\n",
    "\n",
    "R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n",
    "\n",
    "ratings = np.array(R_df.values, dtype=int)\n",
    "n_users = ratings.shape[0]\n",
    "n_items = ratings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 0, 0, ..., 0, 0, 0],\n",
       "       [5, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [4, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.468362562231285"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 3. my distance between random real and fake samples 42916.7734375\n",
      "Epoch number 3. MSE distance between random real and fake samples 10.16589069366455\n",
      "Epoch number 7. my distance between random real and fake samples 34558.19140625\n",
      "Epoch number 7. MSE distance between random real and fake samples 8.85556411743164\n",
      "Epoch number 11. my distance between random real and fake samples 38496.1015625\n",
      "Epoch number 11. MSE distance between random real and fake samples 8.10557746887207\n",
      "Epoch number 15. my distance between random real and fake samples 36116.75\n",
      "Epoch number 15. MSE distance between random real and fake samples 7.6949334144592285\n",
      "Epoch number 19. my distance between random real and fake samples 28026.5546875\n",
      "Epoch number 19. MSE distance between random real and fake samples 7.422409534454346\n",
      "Epoch number 23. my distance between random real and fake samples 32105.15625\n",
      "Epoch number 23. MSE distance between random real and fake samples 7.229992389678955\n",
      "Epoch number 27. my distance between random real and fake samples 32968.56640625\n",
      "Epoch number 27. MSE distance between random real and fake samples 7.061685085296631\n",
      "Epoch number 31. my distance between random real and fake samples 39565.1484375\n",
      "Epoch number 31. MSE distance between random real and fake samples 6.997607707977295\n",
      "Epoch number 35. my distance between random real and fake samples 42375.33984375\n",
      "Epoch number 35. MSE distance between random real and fake samples 6.904023170471191\n",
      "Epoch number 39. my distance between random real and fake samples 40959.8671875\n",
      "Epoch number 39. MSE distance between random real and fake samples 6.785863399505615\n",
      "Epoch number 43. my distance between random real and fake samples 38891.6796875\n",
      "Epoch number 43. MSE distance between random real and fake samples 6.615298271179199\n",
      "Epoch number 47. my distance between random real and fake samples 36142.578125\n",
      "Epoch number 47. MSE distance between random real and fake samples 6.615091800689697\n",
      "Epoch number 51. my distance between random real and fake samples 44584.94140625\n",
      "Epoch number 51. MSE distance between random real and fake samples 6.590579986572266\n",
      "Epoch number 55. my distance between random real and fake samples 39760.9609375\n",
      "Epoch number 55. MSE distance between random real and fake samples 6.598241329193115\n",
      "Epoch number 59. my distance between random real and fake samples 39230.5859375\n",
      "Epoch number 59. MSE distance between random real and fake samples 6.467867851257324\n",
      "Epoch number 63. my distance between random real and fake samples 34878.265625\n",
      "Epoch number 63. MSE distance between random real and fake samples 6.391635417938232\n",
      "Epoch number 67. my distance between random real and fake samples 38086.43359375\n",
      "Epoch number 67. MSE distance between random real and fake samples 6.431432723999023\n",
      "Epoch number 71. my distance between random real and fake samples 37647.1875\n",
      "Epoch number 71. MSE distance between random real and fake samples 6.420580863952637\n",
      "Epoch number 75. my distance between random real and fake samples 37843.265625\n",
      "Epoch number 75. MSE distance between random real and fake samples 6.3713202476501465\n",
      "Epoch number 79. my distance between random real and fake samples 31247.63671875\n",
      "Epoch number 79. MSE distance between random real and fake samples 6.339123725891113\n",
      "Epoch number 83. my distance between random real and fake samples 45093.19921875\n",
      "Epoch number 83. MSE distance between random real and fake samples 6.265430450439453\n",
      "Epoch number 87. my distance between random real and fake samples 38935.7578125\n",
      "Epoch number 87. MSE distance between random real and fake samples 6.2629618644714355\n",
      "Epoch number 91. my distance between random real and fake samples 33140.8203125\n",
      "Epoch number 91. MSE distance between random real and fake samples 6.248418807983398\n",
      "Epoch number 95. my distance between random real and fake samples 36840.1796875\n",
      "Epoch number 95. MSE distance between random real and fake samples 6.207701683044434\n",
      "Epoch number 99. my distance between random real and fake samples 38041.6015625\n",
      "Epoch number 99. MSE distance between random real and fake samples 6.191335678100586\n",
      "Epoch number 103. my distance between random real and fake samples 35485.19140625\n",
      "Epoch number 103. MSE distance between random real and fake samples 6.158731937408447\n",
      "Epoch number 107. my distance between random real and fake samples 39077.22265625\n",
      "Epoch number 107. MSE distance between random real and fake samples 6.189393997192383\n",
      "Epoch number 111. my distance between random real and fake samples 32927.26953125\n",
      "Epoch number 111. MSE distance between random real and fake samples 6.0411601066589355\n",
      "Epoch number 115. my distance between random real and fake samples 32405.6875\n",
      "Epoch number 115. MSE distance between random real and fake samples 6.116573333740234\n",
      "Epoch number 119. my distance between random real and fake samples 36287.1953125\n",
      "Epoch number 119. MSE distance between random real and fake samples 6.014150142669678\n",
      "Epoch number 123. my distance between random real and fake samples 47067.953125\n",
      "Epoch number 123. MSE distance between random real and fake samples 6.1484808921813965\n",
      "Epoch number 127. my distance between random real and fake samples 37210.078125\n",
      "Epoch number 127. MSE distance between random real and fake samples 6.085209846496582\n",
      "Epoch number 131. my distance between random real and fake samples 40264.69140625\n",
      "Epoch number 131. MSE distance between random real and fake samples 6.1624040603637695\n",
      "Epoch number 135. my distance between random real and fake samples 33401.75\n",
      "Epoch number 135. MSE distance between random real and fake samples 5.996057510375977\n",
      "Epoch number 139. my distance between random real and fake samples 35088.8125\n",
      "Epoch number 139. MSE distance between random real and fake samples 6.000080108642578\n",
      "Epoch number 143. my distance between random real and fake samples 39297.7734375\n",
      "Epoch number 143. MSE distance between random real and fake samples 6.043071269989014\n",
      "Epoch number 147. my distance between random real and fake samples 34422.671875\n",
      "Epoch number 147. MSE distance between random real and fake samples 6.082133769989014\n",
      "Epoch number 151. my distance between random real and fake samples 34681.3203125\n",
      "Epoch number 151. MSE distance between random real and fake samples 5.99552059173584\n",
      "Epoch number 155. my distance between random real and fake samples 41757.9609375\n",
      "Epoch number 155. MSE distance between random real and fake samples 6.065366744995117\n",
      "Epoch number 159. my distance between random real and fake samples 40720.23828125\n",
      "Epoch number 159. MSE distance between random real and fake samples 5.994631767272949\n",
      "Epoch number 163. my distance between random real and fake samples 34145.48828125\n",
      "Epoch number 163. MSE distance between random real and fake samples 6.009753227233887\n",
      "Epoch number 167. my distance between random real and fake samples 37665.140625\n",
      "Epoch number 167. MSE distance between random real and fake samples 5.9921674728393555\n",
      "Epoch number 171. my distance between random real and fake samples 36612.75390625\n",
      "Epoch number 171. MSE distance between random real and fake samples 5.98386287689209\n",
      "Epoch number 175. my distance between random real and fake samples 37464.82421875\n",
      "Epoch number 175. MSE distance between random real and fake samples 5.906786918640137\n",
      "Epoch number 179. my distance between random real and fake samples 33305.1796875\n",
      "Epoch number 179. MSE distance between random real and fake samples 5.889012336730957\n",
      "Epoch number 183. my distance between random real and fake samples 30901.552734375\n",
      "Epoch number 183. MSE distance between random real and fake samples 5.953599452972412\n",
      "Epoch number 187. my distance between random real and fake samples 38704.12109375\n",
      "Epoch number 187. MSE distance between random real and fake samples 5.964677333831787\n",
      "Epoch number 191. my distance between random real and fake samples 37183.2109375\n",
      "Epoch number 191. MSE distance between random real and fake samples 5.930996894836426\n",
      "Epoch number 195. my distance between random real and fake samples 45242.28125\n",
      "Epoch number 195. MSE distance between random real and fake samples 5.832322120666504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 199. my distance between random real and fake samples 40020.421875\n",
      "Epoch number 199. MSE distance between random real and fake samples 5.817627429962158\n",
      "Epoch number 203. my distance between random real and fake samples 34299.80078125\n",
      "Epoch number 203. MSE distance between random real and fake samples 5.930644989013672\n",
      "Epoch number 207. my distance between random real and fake samples 34953.1796875\n",
      "Epoch number 207. MSE distance between random real and fake samples 5.935337066650391\n",
      "Epoch number 211. my distance between random real and fake samples 36271.5546875\n",
      "Epoch number 211. MSE distance between random real and fake samples 5.8728108406066895\n",
      "Epoch number 215. my distance between random real and fake samples 30598.20703125\n",
      "Epoch number 215. MSE distance between random real and fake samples 5.869164943695068\n",
      "Epoch number 219. my distance between random real and fake samples 37352.13671875\n",
      "Epoch number 219. MSE distance between random real and fake samples 5.8663530349731445\n",
      "Epoch number 223. my distance between random real and fake samples 43996.30859375\n",
      "Epoch number 223. MSE distance between random real and fake samples 5.783070087432861\n",
      "Epoch number 227. my distance between random real and fake samples 37756.15625\n",
      "Epoch number 227. MSE distance between random real and fake samples 5.894072532653809\n",
      "Epoch number 231. my distance between random real and fake samples 29996.84765625\n",
      "Epoch number 231. MSE distance between random real and fake samples 5.736064434051514\n",
      "Epoch number 235. my distance between random real and fake samples 34115.2734375\n",
      "Epoch number 235. MSE distance between random real and fake samples 5.884888648986816\n",
      "Epoch number 239. my distance between random real and fake samples 32266.53125\n",
      "Epoch number 239. MSE distance between random real and fake samples 5.840763568878174\n",
      "Epoch number 243. my distance between random real and fake samples 33688.46484375\n",
      "Epoch number 243. MSE distance between random real and fake samples 5.773222923278809\n",
      "Epoch number 247. my distance between random real and fake samples 40363.6953125\n",
      "Epoch number 247. MSE distance between random real and fake samples 5.878448486328125\n",
      "Epoch number 251. my distance between random real and fake samples 33400.98046875\n",
      "Epoch number 251. MSE distance between random real and fake samples 5.735082149505615\n",
      "Epoch number 255. my distance between random real and fake samples 37246.9140625\n",
      "Epoch number 255. MSE distance between random real and fake samples 5.806543350219727\n",
      "Epoch number 259. my distance between random real and fake samples 38035.60546875\n",
      "Epoch number 259. MSE distance between random real and fake samples 5.773155212402344\n",
      "Epoch number 263. my distance between random real and fake samples 31909.4453125\n",
      "Epoch number 263. MSE distance between random real and fake samples 5.758020877838135\n",
      "Epoch number 267. my distance between random real and fake samples 33847.5234375\n",
      "Epoch number 267. MSE distance between random real and fake samples 5.716183185577393\n",
      "Epoch number 271. my distance between random real and fake samples 29262.845703125\n",
      "Epoch number 271. MSE distance between random real and fake samples 5.602036476135254\n",
      "Epoch number 275. my distance between random real and fake samples 35331.2421875\n",
      "Epoch number 275. MSE distance between random real and fake samples 5.780309677124023\n",
      "Epoch number 279. my distance between random real and fake samples 37008.578125\n",
      "Epoch number 279. MSE distance between random real and fake samples 5.702278137207031\n",
      "Epoch number 283. my distance between random real and fake samples 38119.67578125\n",
      "Epoch number 283. MSE distance between random real and fake samples 5.682126045227051\n",
      "Epoch number 287. my distance between random real and fake samples 37470.12109375\n",
      "Epoch number 287. MSE distance between random real and fake samples 5.810766220092773\n",
      "Epoch number 291. my distance between random real and fake samples 39694.6953125\n",
      "Epoch number 291. MSE distance between random real and fake samples 5.711463928222656\n",
      "Epoch number 295. my distance between random real and fake samples 32275.921875\n",
      "Epoch number 295. MSE distance between random real and fake samples 5.686830997467041\n",
      "Epoch number 299. my distance between random real and fake samples 31759.626953125\n",
      "Epoch number 299. MSE distance between random real and fake samples 5.792844772338867\n",
      "Epoch number 303. my distance between random real and fake samples 34334.1328125\n",
      "Epoch number 303. MSE distance between random real and fake samples 5.664026737213135\n",
      "Epoch number 307. my distance between random real and fake samples 36918.921875\n",
      "Epoch number 307. MSE distance between random real and fake samples 5.762014389038086\n",
      "Epoch number 311. my distance between random real and fake samples 32783.90625\n",
      "Epoch number 311. MSE distance between random real and fake samples 5.737360954284668\n",
      "Epoch number 315. my distance between random real and fake samples 43466.4140625\n",
      "Epoch number 315. MSE distance between random real and fake samples 5.729104995727539\n",
      "Epoch number 319. my distance between random real and fake samples 39091.484375\n",
      "Epoch number 319. MSE distance between random real and fake samples 5.659003734588623\n",
      "Epoch number 323. my distance between random real and fake samples 35830.9765625\n",
      "Epoch number 323. MSE distance between random real and fake samples 5.710907936096191\n",
      "Epoch number 327. my distance between random real and fake samples 38974.1328125\n",
      "Epoch number 327. MSE distance between random real and fake samples 5.783687114715576\n",
      "Epoch number 331. my distance between random real and fake samples 36974.046875\n",
      "Epoch number 331. MSE distance between random real and fake samples 5.629632472991943\n",
      "Epoch number 335. my distance between random real and fake samples 36232.2421875\n",
      "Epoch number 335. MSE distance between random real and fake samples 5.655613422393799\n",
      "Epoch number 339. my distance between random real and fake samples 41270.1328125\n",
      "Epoch number 339. MSE distance between random real and fake samples 5.607487678527832\n",
      "Epoch number 343. my distance between random real and fake samples 34420.625\n",
      "Epoch number 343. MSE distance between random real and fake samples 5.532899856567383\n",
      "Epoch number 347. my distance between random real and fake samples 39648.6328125\n",
      "Epoch number 347. MSE distance between random real and fake samples 5.651731014251709\n",
      "Epoch number 351. my distance between random real and fake samples 35792.7890625\n",
      "Epoch number 351. MSE distance between random real and fake samples 5.677252292633057\n",
      "Epoch number 355. my distance between random real and fake samples 30650.296875\n",
      "Epoch number 355. MSE distance between random real and fake samples 5.641417503356934\n",
      "Epoch number 359. my distance between random real and fake samples 33349.421875\n",
      "Epoch number 359. MSE distance between random real and fake samples 5.654482841491699\n",
      "Epoch number 363. my distance between random real and fake samples 36265.2890625\n",
      "Epoch number 363. MSE distance between random real and fake samples 5.608371734619141\n",
      "Epoch number 367. my distance between random real and fake samples 27265.978515625\n",
      "Epoch number 367. MSE distance between random real and fake samples 5.59713077545166\n",
      "Epoch number 371. my distance between random real and fake samples 29504.26171875\n",
      "Epoch number 371. MSE distance between random real and fake samples 5.468850135803223\n",
      "Epoch number 375. my distance between random real and fake samples 41893.1484375\n",
      "Epoch number 375. MSE distance between random real and fake samples 5.600508213043213\n",
      "Epoch number 379. my distance between random real and fake samples 38034.125\n",
      "Epoch number 379. MSE distance between random real and fake samples 5.665369033813477\n",
      "Epoch number 383. my distance between random real and fake samples 47053.859375\n",
      "Epoch number 383. MSE distance between random real and fake samples 5.702467441558838\n",
      "Epoch number 387. my distance between random real and fake samples 38830.38671875\n",
      "Epoch number 387. MSE distance between random real and fake samples 5.616556644439697\n",
      "Epoch number 391. my distance between random real and fake samples 32749.736328125\n",
      "Epoch number 391. MSE distance between random real and fake samples 5.5063581466674805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 395. my distance between random real and fake samples 38255.19921875\n",
      "Epoch number 395. MSE distance between random real and fake samples 5.610450744628906\n",
      "Epoch number 399. my distance between random real and fake samples 31917.599609375\n",
      "Epoch number 399. MSE distance between random real and fake samples 5.498556613922119\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 150\n",
    "gen_iterations = 0\n",
    "eval_losses = []\n",
    "for epoch in range(epochs):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < steps_per_epoch:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter:\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "#             X, _ = data_iter.next()\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             print(X >= 0.5)\n",
    "# #             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "#             print(real.shape, fake.shape)\n",
    "    \n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "#             print('real', real)\n",
    "#             print('fake', fake[:,0].sum())\n",
    "            out = netD(real, fake)\n",
    "            \n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "#             print(out.shape)\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        g_iter = g_iter\n",
    "        j = 0\n",
    "        while j < g_iter:\n",
    "            j += 1\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X += torch.Tensor(np.random.normal(0, 0.2, X.shape))\n",
    "#             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            \n",
    "            # update generator\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            noisev = Variable(noise)\n",
    "            fake = netG(noisev)\n",
    "            out = netD(real, fake)\n",
    "            outputG = torch.mean(out) + lamba * out.norm()\n",
    "            stdG = torch.std(out)\n",
    "            outputG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "#             print('output_D', outputD.item(), gen_iterations)\n",
    "#             print('output_G', outputG.item(), gen_iterations)\n",
    "#             print('std_D', stdD.item(), gen_iterations)\n",
    "#             print('std_G', stdG.item(), gen_iterations)\n",
    "            \n",
    "            # evaluation\n",
    "            if gen_iterations % 100 == 0: # todo- to change\n",
    "#                 gen.eval()\n",
    "#                 z_vector_eval = make_some_noise(128)\n",
    "#                 fake_rows_eval = gen(z_vector_eval)\n",
    "#                 real_rows_eval = get_random_batch(train, 128)\n",
    "        #         print(fake_rows[0][:10]) enable to see some results\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                eval_losses.append(eval_loss)\n",
    "                print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0677, 3.0749, 3.1720,  ..., 0.0000, 3.2269, 0.0000],\n",
       "        [4.0740, 0.0000, 3.0261,  ..., 0.0000, 0.0000, 3.7588],\n",
       "        [0.0000, 3.4719, 0.0000,  ..., 3.7493, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [4.1807, 3.0051, 0.0000,  ..., 4.5524, 3.6993, 0.0000],\n",
       "        [0.0000, 3.0363, 0.0000,  ..., 0.0000, 0.0000, 3.1808],\n",
       "        [0.0000, 2.8532, 0.0000,  ..., 0.0000, 0.0000, 3.5762]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = [c.item() for c in eval_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(tr.shape[0], nz)\n",
    "if cuda: \n",
    "    noise = noise.cuda()\n",
    "noisev = Variable(noise)\n",
    "fake = netG(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6040, 3706])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor(851505, device='cuda:0') 226310\n",
      "4 tensor(3571705, device='cuda:0') 348971\n",
      "3 tensor(3212831, device='cuda:0') 261197\n",
      "2 tensor(925470, device='cuda:0') 107557\n",
      "1 tensor(87883, device='cuda:0') 56174\n",
      "0 tensor(13434032, device='cuda:0') 21384031\n"
     ]
    }
   ],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr).round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr).round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr).round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr).round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr).round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr).round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./fake', fake.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tr + vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake.cpu().detach().int().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.984417608102845"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.468362562231285"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr+vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 3, 0, ..., 0, 4, 4],\n",
       "       [0, 0, 0, ..., 0, 5, 0],\n",
       "       [4, 0, 3, ..., 4, 5, 4],\n",
       "       ...,\n",
       "       [4, 3, 3, ..., 0, 3, 0],\n",
       "       [0, 3, 0, ..., 3, 4, 4],\n",
       "       [5, 0, 3, ..., 0, 4, 4]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(fake, 0, 5, out=fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./fake', fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ix = np.random.randint(0, fake.shape[0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake = fake[rand_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0, 0, ..., 4, 0, 4],\n",
       "       [0, 3, 3, ..., 5, 0, 0],\n",
       "       [4, 0, 3, ..., 0, 0, 4],\n",
       "       ...,\n",
       "       [0, 4, 0, ..., 4, 0, 0],\n",
       "       [4, 3, 3, ..., 4, 5, 0],\n",
       "       [4, 3, 3, ..., 4, 0, 4]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adding_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 5])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(adding_fake[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 4])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(fake[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tr[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.779468289293665"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 4.607970714569092 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "tr_orig, vr_1 = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed,  transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = np.append(tr_orig, adding_fake, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.606681479632465"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 4, 5])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(adding_fake[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./fake', fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat = np.load('./fake.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 3, 0, ..., 0, 4, 4],\n",
       "       [0, 0, 0, ..., 0, 5, 0],\n",
       "       [4, 0, 3, ..., 4, 5, 4],\n",
       "       ...,\n",
       "       [4, 3, 3, ..., 0, 3, 0],\n",
       "       [0, 3, 0, ..., 3, 4, 4],\n",
       "       [5, 0, 3, ..., 0, 4, 4]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_concat.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seed = 47\n",
    "\n",
    "# # load data\n",
    "# tr_1, vr_1 = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed,\n",
    "#  transpose=True, valfrac=0.1)\n",
    "\n",
    "# tm_1 = np.greater(tr_1, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "# vm_1 = np.greater(vr_1, 1e-12).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (vr_1 == vr).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tr_1 == tr).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_2, vr_2 = loadData('./ml-1m/ratings.dat', delimiter='::',\n",
    "#                   seed=seed, transpose=False, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter \n",
    "import matrix_factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6140, 3706), (6040, 3706))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, tr_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.606681479632465"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7288213535627215\n",
      "Test mse: 0.8133318867936199\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "iter_array = [40]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6140, 3706), (6040, 3706))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, tr_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8135750792748816\n",
      "Test mse: 0.8141703075881768\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr_orig, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "iter_array = [20]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6240, 3706)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_2 = np.append(tr, to_concat[:2000,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.3203302278807397\n",
      "Test mse: 0.8314406623619328\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(tr_2, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "iter_array = [10]\n",
    "MF_SGD.calculate_learning_curve(iter_array, vr, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "tm_orig = np.greater(tr_orig, 1e-12).astype('float32')\n",
    "vm = np.greater(vr, 1e-12).astype('float32')\n",
    "\n",
    "n_m = tr.shape[0]  # number of movies\n",
    "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n",
    "\n",
    "# Set hyper-parameters\n",
    "n_hid = 500\n",
    "# lambda_2 = float(sys.argv[1]) if len(sys.argv) > 1 else 60.\n",
    "# lambda_s = float(sys.argv[2]) if len(sys.argv) > 2 else 0.013\n",
    "n_layers = 2\n",
    "output_every = 50  # evaluate performance on test set; breaks l-bfgs loop\n",
    "n_epoch = n_layers * 10 * output_every\n",
    "verbose_bfgs = True\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-c043f3e4d66b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Input placeholders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_u\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "if not use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    \n",
    "# Input placeholders\n",
    "R = tf.placeholder(\"float\", [None, n_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network functions\n",
    "def kernel(u, v):\n",
    "    \"\"\"\n",
    "    Sparsifying kernel function\n",
    "\n",
    "    :param u: input vectors [n_in, 1, n_dim]\n",
    "    :param v: output vectors [1, n_hid, n_dim]\n",
    "    :return: input to output connection matrix\n",
    "    \"\"\"\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "    return hat\n",
    "\n",
    "\n",
    "def kernel_layer(x, n_hid=500, n_dim=5, activation=tf.nn.sigmoid, name=''):\n",
    "    \"\"\"\n",
    "    a kernel sparsified layer\n",
    "\n",
    "    :param x: input [batch, channels]\n",
    "    :param n_hid: number of hidden units\n",
    "    :param n_dim: number of dimensions to embed for kernelization\n",
    "    :param activation: output activation\n",
    "    :param name: layer name for scoping\n",
    "    :return: layer output, regularization term\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
    "        n_in = x.get_shape().as_list()[1]\n",
    "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
    "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
    "        b = tf.get_variable('b', [n_hid])\n",
    "\n",
    "    # compute sparsifying kernel\n",
    "    # as u and v move further from each other for some given pair of neurons, their connection\n",
    "    # decreases in strength and eventually goes to zero.\n",
    "    w_hat = kernel(u, v)\n",
    "\n",
    "    # compute regularization terms\n",
    "#     sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
    "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
    "\n",
    "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
    "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
    "\n",
    "    # compute output\n",
    "    W_eff = W * w_hat\n",
    "    y = tf.matmul(x, W_eff) + b\n",
    "    y = activation(y)\n",
    "    return y, sparse_reg_term + l2_reg_term\n",
    "\n",
    "\n",
    "# Instantiate network\n",
    "y = R\n",
    "reg_losses = None\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = kernel_layer(y, n_hid, name=str(i))\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "prediction, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
    "reg_losses = reg_losses + reg_loss\n",
    "\n",
    "# Compute loss (symbolic)\n",
    "diff = tm*(R - prediction)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss = sqE + reg_losses\n",
    "\n",
    "# Instantiate L-BFGS Optimizer\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': output_every,\n",
    "                                                                  'disp': verbose_bfgs,\n",
    "                                                                  'maxcor': 10},\n",
    "                                                   method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loop\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(int(n_epoch / output_every)):\n",
    "        optimizer.minimize(sess, feed_dict={R: tr}) #do maxiter optimization steps\n",
    "        pre = sess.run(prediction, feed_dict={R: tr}) #predict ratings\n",
    "\n",
    "        error = (vm * (np.clip(pre, 1., 5.) - vr) ** 2).sum() / vm.sum() #compute validation error\n",
    "        error_train = (tm * (np.clip(pre, 1., 5.) - tr) ** 2).sum() / tm.sum() #compute train error\n",
    "        error_train_orig = (tm_orig * (np.clip(pre, 1., 5.) - tr_orig) ** 2).sum() / tm_orig.sum() #compute train error\n",
    "\n",
    "        print('.-^-._' * 12)\n",
    "        print('epoch:', i, 'validation rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train), 'train orig rmse:', np.sqrt(error_train_orig))\n",
    "        print('.-^-._' * 12)\n",
    "\n",
    "    with open('summary_ml1m.txt', 'a') as file:\n",
    "        for a in sys.argv[1:]:\n",
    "            file.write(a + ' ')\n",
    "        file.write(str(np.sqrt(error)) + ' ' + str(np.sqrt(error_train), + ' ' + str(np.sqrt(error_train))\n",
    "                   + ' ' + str(seed) + '\\n')\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
