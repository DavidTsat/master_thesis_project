{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "written by Lorenz Muller\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import sys\n",
    "from dataLoader import loadData\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "data read in 4.97664999961853 seconds\n",
      "loaded dense data matrix\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tr, vr = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed, transpose=True, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 3.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3706, 6040), (3706, 6040))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, vr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.021525859265269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44683670296601535"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from os.path import isfile, isdir, join\n",
    "import os\n",
    "# from tensorboard_logger import configure, log_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20695727b30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 100\n",
    "cuda = True\n",
    "epochs = 300 #change\n",
    "device = 5\n",
    "seed = 1\n",
    "nz = 200\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 1e-2 # constant for L2 penalty (diversity)\n",
    "name = \"mnist-experiment\"\n",
    "# configure(\"runs/run-\" + args.name, flush_secs=5)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#     transform=transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     ])), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=6040, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Dropout(p=0.5)\n",
      "  )\n",
      ")\n",
      "NetD(\n",
      "  (t1): Linear(in_features=6040, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=6040, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=6040, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "features_length = train.shape[1]\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(features_length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(features_length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, features_length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        filt = (torch.abs((real != 0).float() * fake - real))/real.shape[0]\n",
    "#         filt = torch.abs((real != 0).float().cuda() * fake.cuda() - real.cuda())\n",
    "#         filt = torch.abs((xr != 0).int() * xf - xr)\n",
    "#         filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "#         print(filt.type(), x.type())\n",
    "        x = filt * x\n",
    "\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "        \n",
    "# netG = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(nz, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, features_length),\n",
    "#     torch.nn.Sigmoid()*5\n",
    "#     )\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "                                 nn.Linear(nz,1024),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(1024,features_length),\n",
    "                                 nn.Sigmoid(),\n",
    "                                 nn.Dropout(0.5)\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x*5 # to get values in range [0,5]\n",
    "    \n",
    "# networks\n",
    "netD = NetD(use_cuda=False)\n",
    "netG = NetG()\n",
    "print(netG)\n",
    "print(netD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "cuda = False\n",
    "if cuda is True:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    one, mone = one.cuda(), mone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in netD.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #\n",
    "    \n",
    "for p in netG.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRealSample(length=6):\n",
    "     return Variable(torch.IntTensor(np.random.choice([0, 1], size=(batch_size, length))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6040)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_random_batch(train, batch_size=batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_my(x_r, x_g):\n",
    "    return torch.sum(torch.abs((x_r != 0).float() * x_g - x_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 5. my distance between random real and fake samples 64486.98828125\n",
      "Epoch number 5. MSE distance between random real and fake samples 9.120088577270508\n",
      "Epoch number 11. my distance between random real and fake samples 47131.28125\n",
      "Epoch number 11. MSE distance between random real and fake samples 8.834625244140625\n",
      "Epoch number 17. my distance between random real and fake samples 51363.6875\n",
      "Epoch number 17. MSE distance between random real and fake samples 8.556299209594727\n",
      "Epoch number 23. my distance between random real and fake samples 46855.2109375\n",
      "Epoch number 23. MSE distance between random real and fake samples 8.422072410583496\n",
      "Epoch number 29. my distance between random real and fake samples 58706.0625\n",
      "Epoch number 29. MSE distance between random real and fake samples 8.181174278259277\n",
      "Epoch number 35. my distance between random real and fake samples 49674.34765625\n",
      "Epoch number 35. MSE distance between random real and fake samples 8.306154251098633\n",
      "Epoch number 41. my distance between random real and fake samples 69893.3515625\n",
      "Epoch number 41. MSE distance between random real and fake samples 8.252363204956055\n",
      "Epoch number 47. my distance between random real and fake samples 42326.453125\n",
      "Epoch number 47. MSE distance between random real and fake samples 8.166743278503418\n",
      "Epoch number 52. my distance between random real and fake samples 53823.78125\n",
      "Epoch number 52. MSE distance between random real and fake samples 8.121761322021484\n",
      "Epoch number 58. my distance between random real and fake samples 47139.8984375\n",
      "Epoch number 58. MSE distance between random real and fake samples 8.161216735839844\n",
      "Epoch number 64. my distance between random real and fake samples 57389.7578125\n",
      "Epoch number 64. MSE distance between random real and fake samples 8.235028266906738\n",
      "Epoch number 70. my distance between random real and fake samples 61641.3203125\n",
      "Epoch number 70. MSE distance between random real and fake samples 8.0856351852417\n",
      "Epoch number 76. my distance between random real and fake samples 53915.953125\n",
      "Epoch number 76. MSE distance between random real and fake samples 8.165562629699707\n",
      "Epoch number 82. my distance between random real and fake samples 47362.87109375\n",
      "Epoch number 82. MSE distance between random real and fake samples 8.0332612991333\n",
      "Epoch number 88. my distance between random real and fake samples 43290.74609375\n",
      "Epoch number 88. MSE distance between random real and fake samples 8.12916374206543\n",
      "Epoch number 94. my distance between random real and fake samples 50400.9765625\n",
      "Epoch number 94. MSE distance between random real and fake samples 8.058619499206543\n",
      "Epoch number 99. my distance between random real and fake samples 59918.86328125\n",
      "Epoch number 99. MSE distance between random real and fake samples 8.082275390625\n",
      "Epoch number 105. my distance between random real and fake samples 42989.47265625\n",
      "Epoch number 105. MSE distance between random real and fake samples 8.065574645996094\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 100\n",
    "gen_iterations = 0\n",
    "eval_losses = []\n",
    "for epoch in range(epochs):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < steps_per_epoch:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter:\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "#             X, _ = data_iter.next()\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             print(X >= 0.5)\n",
    "# #             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "#             print(real.shape, fake.shape)\n",
    "    \n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "#             print('real', real)\n",
    "#             print('fake', fake[:,0].sum())\n",
    "            out = netD(real, fake)\n",
    "            \n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "#             print(out.shape)\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        g_iter = g_iter\n",
    "        j = 0\n",
    "        while j < g_iter:\n",
    "            j += 1\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False # to avoid computation\n",
    "            netG.zero_grad()\n",
    "            \n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = get_random_batch(train, batch_size=batch_size)\n",
    "#             X = X.view(X.size(0), -1)\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            \n",
    "            # update generator\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            noisev = Variable(noise)\n",
    "            fake = netG(noisev)\n",
    "            out = netD(real, fake)\n",
    "            outputG = torch.mean(out) + lamba * out.norm()\n",
    "            stdG = torch.std(out)\n",
    "            outputG.backward(one)\n",
    "            optimizerG.step()\n",
    "\n",
    "            gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, len(data_loader), gen_iterations, outputD.item(), outputG.item()))\n",
    "#             print('output_D', outputD.item(), gen_iterations)\n",
    "#             print('output_G', outputG.item(), gen_iterations)\n",
    "#             print('std_D', stdD.item(), gen_iterations)\n",
    "#             print('std_G', stdG.item(), gen_iterations)\n",
    "            \n",
    "            # evaluation\n",
    "            if gen_iterations % 100 == 0: # todo- to change\n",
    "#                 gen.eval()\n",
    "#                 z_vector_eval = make_some_noise(128)\n",
    "#                 fake_rows_eval = gen(z_vector_eval)\n",
    "#                 real_rows_eval = get_random_batch(train, 128)\n",
    "        #         print(fake_rows[0][:10]) enable to see some results\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "                eval_losses.append(eval_loss)\n",
    "                print('Epoch number {}. my distance between random real and fake samples {}'.format(epoch, d_my(real, fake)))\n",
    "                print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = [c.item() for c in eval_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(tr.shape[0], nz)\n",
    "if cuda: \n",
    "    noise = noise.cuda()\n",
    "noisev = Variable(noise)\n",
    "fake = netG(noisev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5, (5 == fake.round()).sum(), (5 == (tr + vr).round()).sum())\n",
    "print(4, (4 == fake.round()).sum(), (4 == (tr + vr).round()).sum())\n",
    "print(3, (3 == fake.round()).sum(), (3 == (tr + vr).round()).sum())\n",
    "print(2, (2 == fake.round()).sum(), (2 == (tr + vr).round()).sum())\n",
    "print(1, (1 == fake.round()).sum(), (1 == (tr + vr).round()).sum())\n",
    "print(0, (0 == fake.round()).sum(), (0 == (tr + vr).round()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./fake', fake.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tr + vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = fake.detach().int().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr+vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip(fake, 0, 5, out=fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./fake', fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ix = np.random.randint(0, fake.shape[0], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake = fake[rand_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adding_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(adding_fake[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fake[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(tr[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_orig = tr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = np.append(tr, adding_fake, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sparsity(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./fake', fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat = np.load('./fake.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seed = 47\n",
    "\n",
    "# # load data\n",
    "# tr_1, vr_1 = loadData('./ml-1m/ratings.dat', delimiter='::', seed=seed,\n",
    "#  transpose=True, valfrac=0.1)\n",
    "\n",
    "# tm_1 = np.greater(tr_1, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "# vm_1 = np.greater(vr_1, 1e-12).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (vr_1 == vr).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (tr_1 == tr).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_2, vr_2 = loadData('./ml-1m/ratings.dat', delimiter='::',\n",
    "                  seed=seed, transpose=True, valfrac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vr_1 == vr_2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "tm_orig = np.greater(tr_orig, 1e-12).astype('float32')\n",
    "vm = np.greater(vr, 1e-12).astype('float32')\n",
    "\n",
    "n_m = tr.shape[0]  # number of movies\n",
    "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n",
    "\n",
    "# Set hyper-parameters\n",
    "n_hid = 500\n",
    "# lambda_2 = float(sys.argv[1]) if len(sys.argv) > 1 else 60.\n",
    "# lambda_s = float(sys.argv[2]) if len(sys.argv) > 2 else 0.013\n",
    "n_layers = 2\n",
    "output_every = 50  # evaluate performance on test set; breaks l-bfgs loop\n",
    "n_epoch = n_layers * 10 * output_every\n",
    "verbose_bfgs = True\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    \n",
    "# Input placeholders\n",
    "R = tf.placeholder(\"float\", [None, n_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network functions\n",
    "def kernel(u, v):\n",
    "    \"\"\"\n",
    "    Sparsifying kernel function\n",
    "\n",
    "    :param u: input vectors [n_in, 1, n_dim]\n",
    "    :param v: output vectors [1, n_hid, n_dim]\n",
    "    :return: input to output connection matrix\n",
    "    \"\"\"\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "    return hat\n",
    "\n",
    "\n",
    "def kernel_layer(x, n_hid=500, n_dim=5, activation=tf.nn.sigmoid, name=''):\n",
    "    \"\"\"\n",
    "    a kernel sparsified layer\n",
    "\n",
    "    :param x: input [batch, channels]\n",
    "    :param n_hid: number of hidden units\n",
    "    :param n_dim: number of dimensions to embed for kernelization\n",
    "    :param activation: output activation\n",
    "    :param name: layer name for scoping\n",
    "    :return: layer output, regularization term\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
    "        n_in = x.get_shape().as_list()[1]\n",
    "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
    "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
    "        b = tf.get_variable('b', [n_hid])\n",
    "\n",
    "    # compute sparsifying kernel\n",
    "    # as u and v move further from each other for some given pair of neurons, their connection\n",
    "    # decreases in strength and eventually goes to zero.\n",
    "    w_hat = kernel(u, v)\n",
    "\n",
    "    # compute regularization terms\n",
    "#     sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
    "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
    "\n",
    "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
    "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
    "\n",
    "    # compute output\n",
    "    W_eff = W * w_hat\n",
    "    y = tf.matmul(x, W_eff) + b\n",
    "    y = activation(y)\n",
    "    return y, sparse_reg_term + l2_reg_term\n",
    "\n",
    "\n",
    "# Instantiate network\n",
    "y = R\n",
    "reg_losses = None\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = kernel_layer(y, n_hid, name=str(i))\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "prediction, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
    "reg_losses = reg_losses + reg_loss\n",
    "\n",
    "# Compute loss (symbolic)\n",
    "diff = tm*(R - prediction)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss = sqE + reg_losses\n",
    "\n",
    "# Instantiate L-BFGS Optimizer\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': output_every,\n",
    "                                                                  'disp': verbose_bfgs,\n",
    "                                                                  'maxcor': 10},\n",
    "                                                   method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loop\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(int(n_epoch / output_every)):\n",
    "        optimizer.minimize(sess, feed_dict={R: tr}) #do maxiter optimization steps\n",
    "        pre = sess.run(prediction, feed_dict={R: tr}) #predict ratings\n",
    "\n",
    "        error = (vm * (np.clip(pre, 1., 5.) - vr) ** 2).sum() / vm.sum() #compute validation error\n",
    "        error_train = (tm * (np.clip(pre, 1., 5.) - tr) ** 2).sum() / tm.sum() #compute train error\n",
    "        error_train_orig = (tm_orig * (np.clip(pre, 1., 5.) - tr_orig) ** 2).sum() / tm_orig.sum() #compute train error\n",
    "\n",
    "        print('.-^-._' * 12)\n",
    "        print('epoch:', i, 'validation rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train), 'train orig rmse:', np.sqrt(error_train_orig))\n",
    "        print('.-^-._' * 12)\n",
    "\n",
    "    with open('summary_ml1m.txt', 'a') as file:\n",
    "        for a in sys.argv[1:]:\n",
    "            file.write(a + ' ')\n",
    "        file.write(str(np.sqrt(error)) + ' ' + str(np.sqrt(error_train), + ' ' + str(np.sqrt(error_train))\n",
    "                   + ' ' + str(seed) + '\\n')\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
