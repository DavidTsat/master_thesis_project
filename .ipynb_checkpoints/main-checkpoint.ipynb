{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbimporter\n",
      "  Downloading https://files.pythonhosted.org/packages/39/a1/9771984f60c4ea6d0c36215b44a5c1537cfda845c08e7ee5cf748c43d7cf/nbimporter-0.3.1.tar.gz\n",
      "Building wheels for collected packages: nbimporter\n",
      "  Building wheel for nbimporter (setup.py): started\n",
      "  Building wheel for nbimporter (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\davidt\\AppData\\Local\\pip\\Cache\\wheels\\44\\7d\\40\\f48fe3f0995a282d582ca73a6807b9bb23c8cc3f1525e62429\n",
      "Successfully built nbimporter\n",
      "Installing collected packages: nbimporter\n",
      "Successfully installed nbimporter-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from matrix_factorization.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nbimporter \n",
    "import matrix_factorization\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 4808k    0  2641    0     0   2060      0  0:39:50  0:00:01  0:39:49  2058\n",
      "  3 4808k    3  192k    0     0  91715      0  0:00:53  0:00:02  0:00:51 91672\n",
      " 20 4808k   20  980k    0     0   312k      0  0:00:15  0:00:03  0:00:12  312k\n",
      " 31 4808k   31 1529k    0     0   368k      0  0:00:13  0:00:04  0:00:09  367k\n",
      " 43 4808k   43 2068k    0     0   400k      0  0:00:12  0:00:05  0:00:07  409k\n",
      " 50 4808k   50 2417k    0     0   390k      0  0:00:12  0:00:06  0:00:06  492k\n",
      " 54 4808k   54 2644k    0     0   366k      0  0:00:13  0:00:07  0:00:06  484k\n",
      " 59 4808k   59 2866k    0     0   354k      0  0:00:13  0:00:08  0:00:05  380k\n",
      " 62 4808k   62 3013k    0     0   328k      0  0:00:14  0:00:09  0:00:05  296k\n",
      " 66 4808k   66 3207k    0     0   314k      0  0:00:15  0:00:10  0:00:05  225k\n",
      " 70 4808k   70 3394k    0     0   302k      0  0:00:15  0:00:11  0:00:04  193k\n",
      " 74 4808k   74 3602k    0     0   297k      0  0:00:16  0:00:12  0:00:04  196k\n",
      " 79 4808k   79 3816k    0     0   291k      0  0:00:16  0:00:13  0:00:03  189k\n",
      " 83 4808k   83 4026k    0     0   285k      0  0:00:16  0:00:14  0:00:02  205k\n",
      " 87 4808k   87 4198k    0     0   277k      0  0:00:17  0:00:15  0:00:02  201k\n",
      " 91 4808k   91 4378k    0     0   271k      0  0:00:17  0:00:16  0:00:01  200k\n",
      " 95 4808k   95 4576k    0     0   266k      0  0:00:18  0:00:17  0:00:01  192k\n",
      " 99 4808k   99 4791k    0     0   264k      0  0:00:18  0:00:18 --:--:--  194k\n",
      "100 4808k  100 4808k    0     0   264k      0  0:00:18  0:00:18 --:--:--  191k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    }
   ],
   "source": [
    "# Downloading Movielens-100k\n",
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip ml-100k.zip\n",
    "!cd ml-100k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    print (str(n_users) + ' users')\n",
    "    print (str(n_items) + ' items')\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n",
      "Sparsity: 6.30%\n"
     ]
    }
   ],
   "source": [
    "print ('Sparsity: {:4.2f}%'.format(get_sparsity(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], size=10, replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.9179628673300955\n",
      "Test mse: 1.0113053267188021\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = matrix_factorization.ExplicitMF(train, 40, learning='sgd', verbose=True)\n",
    "# iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "iter_array = [10]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.304669364224531"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.710139043178159"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5945303210463734"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ####################################### GANS ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as t_data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_some_noise(batch_size):\n",
    "    return torch.rand(batch_size,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_some_noise(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining generator class\n",
    "\n",
    "class generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, out):\n",
    "        \n",
    "        super(generator, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                 nn.Linear(inp,300),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(300,1000),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(1000,800),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(800,out),\n",
    "                                 nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining discriminator class\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp, out):\n",
    "        \n",
    "        super(discriminator, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                 nn.Linear(inp,300),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(300,300),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(300,200),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Linear(200,out),\n",
    "                                 nn.Sigmoid()\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = discriminator(ratings.shape[1], 1)\n",
    "gen = generator(100, ratings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=1682, out_features=300, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=300, out_features=200, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=200, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=300, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=300, out_features=1000, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=1000, out_features=800, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=800, out_features=1682, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_steps = 300\n",
    "g_steps = 300\n",
    "\n",
    "criteriond1 = nn.BCELoss()\n",
    "optimizerd1 = optim.SGD(dis.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "criteriond2 = nn.BCELoss()\n",
    "optimizerd2 = optim.SGD(gen.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# printing_steps = 200\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_batch(mat, batch_size=16):\n",
    "    rand_rows = np.random.randint(mat.shape[0], size=batch_size)\n",
    "#     print(mat.shape, rand_rows)\n",
    "#     print(mat[rand_rows].shape)\n",
    "    return mat[rand_rows]\n",
    "    \n",
    "get_random_batch(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.autograd.Variable(torch.Tensor(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 1. MSE distance between random real and fake samples 1286161.0\n",
      "Epoch number 2. MSE distance between random real and fake samples 1297580.625\n",
      "Epoch number 3. MSE distance between random real and fake samples 1257361.875\n",
      "Epoch number 4. MSE distance between random real and fake samples 1239742.75\n",
      "Epoch number 5. MSE distance between random real and fake samples 1117233.5\n",
      "Epoch number 6. MSE distance between random real and fake samples 981990.3125\n",
      "Epoch number 7. MSE distance between random real and fake samples 813074.25\n",
      "Epoch number 8. MSE distance between random real and fake samples 682110.9375\n",
      "Epoch number 9. MSE distance between random real and fake samples 566130.5625\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "eval_losses = []\n",
    "for epoch in range(10):\n",
    "#     print (epoch)\n",
    "\n",
    "    # training discriminator\n",
    "    gen.train()\n",
    "    dis.train()\n",
    "    for d_step in range(d_steps):\n",
    "        dis.zero_grad()\n",
    "        \n",
    "        # training discriminator on real data\n",
    "        real_rows = get_random_batch(train, batch_size)\n",
    "        discriminator_real_outputs = dis(real_rows)\n",
    "   \n",
    "        dis_real_loss = criteriond1(discriminator_real_outputs, Variable(torch.ones(batch_size,1)))\n",
    "    \n",
    "        dis_real_loss.backward()\n",
    "\n",
    "        # training discriminator on data produced by generator\n",
    "        z_vector = make_some_noise(batch_size)\n",
    "        #output from generator is generated        \n",
    "        fake_rows = gen(z_vector).detach()\n",
    "#         print(fake_rows[:20])\n",
    "        dis_fake_out = dis(fake_rows)\n",
    "        dis_fake_loss = criteriond1(dis_fake_out, Variable(torch.zeros(batch_size,1)))\n",
    "        dis_fake_loss.backward()\n",
    "\n",
    "        optimizerd1.step()\n",
    "        \n",
    "    # training generator\n",
    "    for g_step in range(g_steps):\n",
    "        gen.zero_grad()\n",
    "        \n",
    "        #generating data for input for generator\n",
    "        z_vector = make_some_noise(batch_size)\n",
    "        \n",
    "        fake_rows = gen(z_vector)\n",
    "#         print(fake_rows.shape, z_vector.shape)\n",
    "#         print(fake_rows[:20])\n",
    "        dis_out_gen_training = dis(fake_rows)\n",
    "        gen_loss = criteriond2(dis_out_gen_training, Variable(torch.ones(batch_size,1)))\n",
    "        gen_loss.backward()\n",
    "        \n",
    "        optimizerd2.step()\n",
    "\n",
    "    # evaluation\n",
    "    if epoch % 10: # todo- to change\n",
    "        gen.eval()\n",
    "        z_vector_eval = make_some_noise(128)\n",
    "        fake_rows_eval = gen(z_vector_eval)\n",
    "        real_rows_eval = get_random_batch(train, 128)\n",
    "#         print(fake_rows[0][:10]) enable to see some results\n",
    "        eval_loss = F.mse_loss(fake_rows_eval, real_rows_eval, reduction='sum')\n",
    "        eval_losses.append(eval_loss)\n",
    "        print('Epoch number {}. MSE distance between random real and fake samples {}'.format(epoch, eval_loss))\n",
    "#         print('Epoch number {}. L1 distance between random real and fake samples {}'.format(epoch, torch.sum(torch.abs(fake_rows_eval - real_rows_eval))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(eval_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vector = make_some_noise(16)\n",
    "fake_rows = gen(z_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9384, 2.0510, 2.0211, 4.0023, 0.6754, 0.1114, 4.9678, 4.7714, 4.8160],\n",
       "        [4.9397, 2.0485, 2.0191, 4.0074, 0.6688, 0.1109, 4.9686, 4.7733, 4.8177],\n",
       "        [4.9389, 2.0423, 2.0070, 4.0117, 0.6723, 0.1116, 4.9681, 4.7724, 4.8184],\n",
       "        [4.9389, 2.0523, 2.0147, 4.0046, 0.6737, 0.1110, 4.9678, 4.7735, 4.8175],\n",
       "        [4.9396, 2.0421, 2.0103, 4.0109, 0.6721, 0.1100, 4.9689, 4.7744, 4.8187],\n",
       "        [4.9393, 2.0508, 2.0170, 4.0109, 0.6668, 0.1112, 4.9684, 4.7729, 4.8177],\n",
       "        [4.9398, 2.0380, 2.0102, 4.0165, 0.6683, 0.1093, 4.9688, 4.7744, 4.8201],\n",
       "        [4.9397, 2.0419, 2.0164, 4.0165, 0.6701, 0.1106, 4.9689, 4.7755, 4.8203],\n",
       "        [4.9387, 2.0582, 2.0154, 4.0064, 0.6702, 0.1116, 4.9678, 4.7723, 4.8177],\n",
       "        [4.9377, 2.0616, 2.0197, 4.0116, 0.6763, 0.1131, 4.9671, 4.7697, 4.8149],\n",
       "        [4.9390, 2.0456, 2.0093, 4.0066, 0.6786, 0.1118, 4.9679, 4.7713, 4.8184],\n",
       "        [4.9383, 2.0484, 2.0099, 4.0102, 0.6814, 0.1126, 4.9678, 4.7715, 4.8152],\n",
       "        [4.9384, 2.0467, 2.0242, 4.0071, 0.6754, 0.1114, 4.9679, 4.7722, 4.8171],\n",
       "        [4.9394, 2.0328, 2.0167, 4.0151, 0.6719, 0.1098, 4.9686, 4.7734, 4.8198],\n",
       "        [4.9391, 2.0451, 2.0228, 4.0117, 0.6713, 0.1115, 4.9681, 4.7735, 4.8175],\n",
       "        [4.9388, 2.0450, 2.0164, 4.0118, 0.6747, 0.1118, 4.9683, 4.7703, 4.8167]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we see generator produces very similar vectors \n",
    "fake_rows[:,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "from os.path import isfile, isdir, join\n",
    "import os\n",
    "# from tensorboard_logger import configure, log_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20c79f43930>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 100\n",
    "cuda = True\n",
    "epochs = 1000\n",
    "device = 5\n",
    "seed = 42\n",
    "nz = 100\n",
    "d_iter = 5\n",
    "g_iter = 1\n",
    "lamba = 1e-2 # constant for L2 penalty (diversity)\n",
    "name = \"mnist-experiment\"\n",
    "# configure(\"runs/run-\" + args.name, flush_secs=5)\n",
    "torch.manual_seed(seed)\n",
    "# if cuda:\n",
    "# #     torch.cuda.set_device('cuda')\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "# datasets.MNIST('../data', train=True, download=True,\n",
    "# transform=transforms.Compose([transforms.ToTensor(),])), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=6\n",
    "batch_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(NetD, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        # top\n",
    "        self.t1 = torch.nn.Linear(length, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(length, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, length)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        print(\"##########\"*40)\n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "#         if (filt == xr * xf).all():\n",
    "#             print('AAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "            \n",
    "#         print('xr & xf', xr * xf)\n",
    "#         print('filt', filt)\n",
    "#         print('xr.shape, xf.shape', xr.shape, xf.shape)\n",
    "#         print('filt.shape', filt.shape)\n",
    "        print('xr', xr)\n",
    "        print('xf', xf)\n",
    "        print('filt', filt)\n",
    "#         return filt\n",
    "#         # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "#         print(idr.shape)\n",
    "        print('idr', idr)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "        print('idrx', idrx)\n",
    "#         print('idrx.shape', idrx.shape)\n",
    "#         print('idrx', idrx[10:20, 100:200])\n",
    "        if self.use_cuda: \n",
    "            idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "#         print('xr.shape', xr.shape)\n",
    "#         print('xr', xr[10:20, 100:200])\n",
    "#         print('xr*idrx.shape', (xr*idrx).shape)\n",
    "#         print('xr*idrx', (xr*idrx)[10:20, 100:200])\n",
    "#         print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', xr * idrx == xr)\n",
    "#         for c in xr * idrx == xr:\n",
    "#             print(c)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        print('xt', xt)\n",
    "        print('xb', xb)\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "#         print(xt.shape, xb.shape)\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        print('x', x)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "#         print('xxxx', x.shape)\n",
    "#         print('x[:, :10]', x[:10, :10])\n",
    "#         print('filt', filt[:10, :10])\n",
    "        # apply filter, aggregate\n",
    "#         print('x[:, :10]', x[:10, :10])\n",
    "#         print('filt', filt[:10, :10])\n",
    "        x = filt * x\n",
    "        print('x', x)\n",
    "#         print('x[:, :10]', x[:10, :10])\n",
    "#         print('xxxx', x.shape)\n",
    "#         print('x', x[:10, :10])\n",
    "#         print(x.mean(dim = 1).shape, x.mean(dim = 1))\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        print('x', x)\n",
    "#         print('xxxx', x.shape)\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        if self.use_cuda: \n",
    "            sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        print('x', x)\n",
    "        print(\"##########\"*40)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_vec_size = 100\n",
    "# vec_size = 1000\n",
    "\n",
    "netG = torch.nn.Sequential(\n",
    "    torch.nn.Linear(nz, 1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1024, length),\n",
    "    torch.nn.Sigmoid()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=1024, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1024, out_features=6, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n",
      "NetD(\n",
      "  (t1): Linear(in_features=6, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=6, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD = NetD()\n",
    "print(netG)\n",
    "print(netD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lrG)\n",
    "optimizerD = optim.RMSprop(netD.parameters(), lr=lrD)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "if cuda is True:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    one, mone = one.cuda(), mone.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in netD.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #\n",
    "    \n",
    "for p in netG.parameters(): # reset requires_grad\n",
    "    p.requires_grad = True #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRealSample(length=6):\n",
    "     return Variable(torch.IntTensor(np.random.choice([0, 1], size=(batch_size, length))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 0],\n",
       "        [1, 0, 1, 0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getRealSample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6]) torch.Size([5, 6])\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "xr tensor([[1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 1., 1., 1.]], device='cuda:0')\n",
      "xf tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "filt tensor([[1., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 1., 0., 1.]], device='cuda:0')\n",
      "idr tensor([1, 0, 0, 0, 1])\n",
      "idrx tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "xt tensor([[1., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 1.]], device='cuda:0')\n",
      "xb tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "x tensor([[0.0000, 0.0000, 1.1909,  ..., 0.0000, 0.0523, 0.2128],\n",
      "        [0.0000, 0.0000, 0.9391,  ..., 0.0000, 0.1477, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3715,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0646, 0.0000, 0.1769,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4637,  ..., 0.0000, 0.0609, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n",
      "x tensor([[ 0.8516, -0.8205,  0.0000, -0.0000, -0.0000, -0.5008],\n",
      "        [ 0.0000, -0.9268,  0.7131, -0.0000, -0.9805, -0.0000],\n",
      "        [-0.0000, -0.0000,  0.4230, -0.0000, -0.9413, -0.0000],\n",
      "        [ 0.0000, -0.9588,  0.0000, -0.8685, -0.9914, -0.0000],\n",
      "        [ 0.0000, -0.9435,  0.0000, -0.5974, -0.0000, -0.8545]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n",
      "x tensor([-0.0783, -0.1990, -0.0864, -0.4698, -0.3993], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "x tensor([-0.0783,  0.1990,  0.0864,  0.4698, -0.3993], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n",
      "################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "torch.Size([5, 6]) torch.Size([5, 6]) torch.Size([5])\n",
      "outputD.shape tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gen_iterations = 0\n",
    "for epoch in range(epochs):\n",
    "#     data_iter = iter(data_loader)\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        d_iter = d_iter\n",
    "        j = 0\n",
    "        while j < d_iter and i < len(data_loader):\n",
    "            j += 1\n",
    "            # load real data\n",
    "            i += 1\n",
    "            X = getRealSample()\n",
    "            X = X.view(X.size(0), -1).float()\n",
    "#             X = (X >= 0.5).float()\n",
    "            if cuda: \n",
    "                X = X.cuda()\n",
    "            real = Variable(X)\n",
    "            # generate fake data\n",
    "            noise = torch.randn(batch_size, nz)\n",
    "            if cuda: \n",
    "                noise = noise.cuda()\n",
    "            with torch.no_grad():\n",
    "                noisev = Variable(noise) # totally freeze netG\n",
    "            fake = (Variable(netG(noisev), requires_grad=True) > 0.5).float()\n",
    "            # compute gradient, take step\n",
    "            netD.zero_grad()\n",
    "            print(real.shape, fake.shape)\n",
    "            out = netD(real, fake)\n",
    "            print(real.shape, fake.shape, out.shape)\n",
    "            outputD = torch.mean(out) + lamba * out.norm()\n",
    "            stdD = torch.std(out)\n",
    "            print('outputD.shape', outputD)\n",
    "            outputD.backward(mone)\n",
    "            optimizerD.step()\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "\n",
    "#             g_iter = g_iter\n",
    "#             j = 0\n",
    "#             while j < g_iter and i < len(data_loader):\n",
    "#                 j += 1\n",
    "#             for p in netD.parameters():\n",
    "#                 p.requires_grad = False # to avoid computation\n",
    "#                 netG.zero_grad()\n",
    "#                 # load real data\n",
    "#                 i += 1\n",
    "#                 try:\n",
    "#                     X = getRealSample()\n",
    "#                 except:\n",
    "#                     continue\n",
    "#                 X = X.view(X.size(0), -1)\n",
    "#                 X = (X >= 0.5).float()\n",
    "#                 if cuda: \n",
    "#                     X = X.cuda()\n",
    "#                 real = Variable(X)\n",
    "#                 # update generator\n",
    "#                 noise = torch.randn(batch_size, nz)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "#                 noisev = Variable(noise)\n",
    "#                 fake = netG(noisev)\n",
    "#                 print(real.shape, fake.shape)\n",
    "#                 out = netD(real, fake)\n",
    "#                 outputG = torch.mean(out) + lamba * out.norm()\n",
    "#                 stdG = torch.std(out)\n",
    "#                 outputG.backward(one)\n",
    "#                 optimizerG.step()\n",
    "#                 gen_iterations += 1\n",
    "\n",
    "#             print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f ' % (epoch, epochs, i, len(data_loader), gen_iterations, outputD.data.item(), outputG.data.item()))\n",
    "#             print('output_D', outputD.data.item(), gen_iterations)\n",
    "#             print('output_G', outputG.data.item(), gen_iterations)\n",
    "#             print('std_D', stdD.data.item(), gen_iterations)\n",
    "#             print('std_G', stdG.data.item(), gen_iterations)\n",
    "#             if gen_iterations % 100 == 0:\n",
    "#                 if not isdir('./images/{0}'.format(name)):\n",
    "#                     os.mkdir('./images/{0}'.format(name))\n",
    "#                 real = real.data[0:100,:]\n",
    "#                 real = real.view(real.size(0), 1, 28, 28)\n",
    "#                 vutils.save_image(real, './images/{0}/real_samples.png'.format(name, gen_iterations))\n",
    "#                 noise = torch.randn(min(100, batch_size), nz)\n",
    "#                 if cuda: \n",
    "#                     noise = noise.cuda()\n",
    "#                 fake = netG(Variable(noise, volatile=True))\n",
    "#                 # fake = (fake.data >= 0.5).float()\n",
    "#                 R = torch.rand(fake.size())\n",
    "#                 fake = (fake.data.cpu() >= R).float()\n",
    "#                 fake = fake.view(fake.size(0), 1, 28, 28)\n",
    "#                 vutils.save_image(fake, './images/{0}/fake_samples_{1}.png'.format(name, gen_iterations))\n",
    "\n",
    "#             # do checkpointing\n",
    "#             if not isdir('./checkpoint/{0}'.format(name)):\n",
    "#                 os.mkdir('./checkpoint/{0}'.format(name))\n",
    "#             torch.save(netG.state_dict(), './checkpoint/{0}/netG_epoch_{1}.pth'.format(name, epoch))\n",
    "#             torch.save(netD.state_dict(), './checkpoint/{0}/netD_epoch_{1}.pth'.format(name, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
