{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')\n",
    "    \n",
    "# # !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')\n",
    "    \n",
    "# # !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following blogpost's independent code to benchmark our experiments https://blog.insightdatascience.com/explicit-matrix-factorization-als-sgd-and-all-that-jazz-b00e4d9b21ea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent MF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1     1193       5\n",
       "1        1      661       3\n",
       "2        1      914       3\n",
       "3        1     3408       4\n",
       "4        1     2355       5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating']\n",
    "df = pd.read_csv('./ml-1m/ratings.dat', sep='::', usecols = [0, 1, 2], names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, (3706,), 3952)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.item_id.unique().max(), df.item_id.unique().shape, df.item_id.unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().max()\n",
    "n_items = df.item_id.unique().max()\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "#     print(row)\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.190220560634904"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    np.random.seed(seed)\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1m, test_1m = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.93718412338794, 0.25303643724696356)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_1m), get_sparsity(test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movielens-100k dataset\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_100k, test_100k = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 0.5945303210463734)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((943, 1682), (6040, 3952))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_100k.shape, train_1m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        np.random.seed(seed)\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * \\\n",
    "                                (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.8411868244782985\n",
      "Test mse: 10.956557787912471\n",
      "Iteration: 2\n",
      "Train mse: 5.731156257503645\n",
      "Test mse: 8.66466855942882\n",
      "Iteration: 5\n",
      "Train mse: 5.41223275733234\n",
      "Test mse: 8.225124472876551\n",
      "Iteration: 10\n",
      "Train mse: 5.394598612851955\n",
      "Test mse: 8.198143635604692\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.3944670279169324\n",
      "Test mse: 8.195114378325407\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394455951420811\n",
      "Test mse: 8.194896519679713\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.394255047026079\n",
      "Test mse: 8.194688553798981\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def plot_learning_curve(iter_array, model):\n",
    "    plt.plot(iter_array, model.train_mse, \\\n",
    "             label='Training', linewidth=5)\n",
    "    plt.plot(iter_array, model.test_mse, \\\n",
    "             label='Test', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('MSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(iter_array, MF_ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 8.133017313467287\n",
      "Test mse: 11.385478952329686\n",
      "Iteration: 2\n",
      "Train mse: 6.181422331570321\n",
      "Test mse: 9.510432053967698\n",
      "Iteration: 5\n",
      "Train mse: 5.945868319505223\n",
      "Test mse: 9.181499199125527\n",
      "Iteration: 10\n",
      "Train mse: 5.929921005207816\n",
      "Test mse: 9.163591900058499\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.922879579194776\n",
      "Test mse: 9.155637570112932\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.921741164281515\n",
      "Test mse: 9.152673442345948\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.921434636456923\n",
      "Test mse: 9.152593151189302\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_1m, n_factors=20, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142807793060997\n",
      "Test mse: 1.1796269767247503\n",
      "Iteration: 2\n",
      "Train mse: 1.0730325987217215\n",
      "Test mse: 1.1283621352778417\n",
      "Iteration: 5\n",
      "Train mse: 0.9767157588922124\n",
      "Test mse: 1.0499014864558258\n",
      "Iteration: 10\n",
      "Train mse: 0.9190727366615857\n",
      "Test mse: 0.9988387536431501\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8669457204379917\n",
      "Test mse: 0.9525818812444498\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8387251669562131\n",
      "Test mse: 0.9343161812537658\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7485848531412769\n",
      "Test mse: 0.9170005429005551\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40410579696377263\n",
      "Test mse: 0.9148070379768369\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Proposed GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train_100k == 0)\n",
    "positive_feedback = (train_100k > 3)\n",
    "negative_feedback = ((train_100k < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49901, 40669)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_feedback.sum(), negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback + negative_feedback != zero_mask).all()\n",
    "assert (positive_feedback + negative_feedback == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94.28986095682184, 3.146093059441684, 2.5640459837364746)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback), get_sparsity(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, mat, p=0.25, batch_size=128):\n",
    "        '''\n",
    "        mat is a binary matrix (e.g. positive feedback, or negative feedback)\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.mat = mat\n",
    "        self.p = p\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.mat.shape[0] / self.batch_size))\n",
    "    \n",
    "    def gen_item_GAN(self):\n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y, indexes\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_negative = DataGenerator(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _ = generator_negative.gen_item_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7064878121284186, 2.283832491082045)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(x), get_sparsity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, feat_size):\n",
    "        super(NetD, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "#         self.use_cuda = True\n",
    "#         self.feat_size = feat_size\n",
    "        # top\n",
    "#         print(self.feat_size*2)\n",
    "        self.t1 = torch.nn.Linear(self.feat_size, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(self.feat_size, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, self.feat_size)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        \n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "#         if self.use_cuda: \n",
    "        idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "        x = filt * x\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_size):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz + self.feat_size, 1024), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "#                                 torch.nn.ReLU(), \n",
    "# #                                 nn.Dropout(0.5),\n",
    "#                                 torch.nn.Linear(2048, 2048),\n",
    "                                torch.nn.ReLU(), \n",
    "#                                 torch.nn.BatchNorm1d(512),\n",
    "#                                 nn.Dropout(0.6),\n",
    "                                torch.nn.Linear(1024, self.feat_size), \n",
    "                                torch.nn.Sigmoid()\n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "        \n",
    "    def forward(self, e_mask, x):\n",
    "        x = self.netGen(x)\n",
    "#         print(x.shape, )\n",
    "        x = x * e_mask\n",
    "        return x\n",
    "#         return F.dropout(x, 0.7)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_losses = []\n",
    "disc_losses = []\n",
    "def train_GAN(netD, netG, negative, tr, steps_per_epoch = 200, epochs = 10):\n",
    "    d_iter = 10\n",
    "    g_iter = 1\n",
    "    gen_iterations = 0\n",
    "#     gen_losses = []\n",
    "#     disc_losses = []\n",
    "#     train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for c in range(steps_per_epoch):\n",
    "#             data_iter = 100\n",
    "            i = 0\n",
    "#             while i < 100:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in netD.parameters(): # reset requires_grad\n",
    "                p.requires_grad = True # they are set to False below in netG update\n",
    "#             d_iter = d_iter\n",
    "            j = 0\n",
    "            while j < d_iter:\n",
    "                j += 1\n",
    "                # load real data\n",
    "                i += 1\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "#                         condition, X, idxs = batch_generator(X_neg, y_neg)\n",
    "#                 X, _ = data_iter.next()\n",
    "#                 X = X.view(X.size(0), -1)\n",
    "#                 X = (X >= 0.5).float()\n",
    "#                     if cuda: \n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "#                     X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "#     + torch.randn(X.size()).cuda() * 0.2\n",
    "#                 print(condition.shape, X_neg.shape, y_neg.shape)\n",
    "                real = Variable(X)\n",
    "\n",
    "                # generate fake data\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "#                     if cuda: \n",
    "                noise = noise.cuda()\n",
    "#                     noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "                with torch.no_grad():\n",
    "                    noisev = Variable(noise) # totally freeze netG\n",
    "                concated = torch.cat((noisev, condition), 1)\n",
    "#                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\n",
    "                e_mask = torch.Tensor(tr[idxs]>0).cuda()\n",
    "#                     print(e_mask.shape, concated.shape, condition.shape)\n",
    "                fake = Variable(netG(e_mask, concated).data)\n",
    "\n",
    "                # compute gradient, take step\n",
    "                netD.zero_grad()\n",
    "#                 concated_real = torch.cat((real, condition), 1)\n",
    "#                 print(concated_real)\n",
    "                out = netD(real, fake)\n",
    "                outputD = torch.mean(out) + lamba * out.norm()\n",
    "                stdD = torch.std(out)\n",
    "                outputD.backward(mone)\n",
    "                optimizerD.step()\n",
    "#                     print('AAAAAAAAA mse:=WWWWWWWWWWWWWWWWWWWWWW')\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "\n",
    "#         g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "                netG.zero_grad()\n",
    "                # load real data\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "\n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "    #                 X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "                real = Variable(X)\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "                noise = noise.cuda()\n",
    "                noisev = Variable(noise)\n",
    "                concated_ = torch.cat((noisev, condition), 1)\n",
    "                e_mask_ = torch.Tensor(tr[idxs]>0).cuda()\n",
    "\n",
    "                fake = netG(e_mask_, concated_)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "                gen_iterations += 1\n",
    "    #             print('AAAAAA')\n",
    "#                 eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "    #             eval_losses.append(eval_loss)\n",
    "    #             print('mse:', eval_loss)\n",
    "    #             print(outputG.item(), outputD.item())\n",
    "                gen_losses.append(outputG.item())\n",
    "                disc_losses.append(outputD.item())\n",
    "                print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, 100, gen_iterations, outputD.item(), outputG.item()))\n",
    "    return gen_losses, disc_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 128\n",
    "cuda = True\n",
    "epochs = 12\n",
    "# device = 5\n",
    "# seed = 1\n",
    "nz = 8\n",
    "lamba = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=1682, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=1682, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=1682, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=1690, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1682, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_neg = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg = NetG(train_100k.shape[1]).cuda()\n",
    "print(netD_neg)\n",
    "print(netG_neg)\n",
    "optimizerG = optim.Adam(netG_neg.parameters(), lr=lrG, weight_decay=1e-4)\n",
    "optimizerD = optim.Adam(netD_neg.parameters(), lr=lrD, weight_decay=1e-4)\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = (-1 * one).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][11/100][1] Loss_D: 0.004238 Loss_G: 0.003247 \n",
      "[0/10][11/100][2] Loss_D: 0.003354 Loss_G: 0.002189 \n",
      "[0/10][11/100][3] Loss_D: 0.003859 Loss_G: 0.001554 \n",
      "[0/10][11/100][4] Loss_D: 0.002782 Loss_G: 0.003311 \n",
      "[0/10][11/100][5] Loss_D: 0.002363 Loss_G: 0.003818 \n",
      "[0/10][11/100][6] Loss_D: 0.001835 Loss_G: 0.004114 \n",
      "[0/10][11/100][7] Loss_D: 0.002326 Loss_G: 0.002153 \n",
      "[0/10][11/100][8] Loss_D: 0.003438 Loss_G: 0.004790 \n",
      "[0/10][11/100][9] Loss_D: 0.001720 Loss_G: 0.003622 \n",
      "[0/10][11/100][10] Loss_D: 0.002345 Loss_G: 0.003075 \n",
      "[0/10][11/100][11] Loss_D: 0.003198 Loss_G: 0.001515 \n",
      "[0/10][11/100][12] Loss_D: 0.004261 Loss_G: 0.002830 \n",
      "[0/10][11/100][13] Loss_D: 0.002000 Loss_G: 0.002137 \n",
      "[0/10][11/100][14] Loss_D: 0.001597 Loss_G: 0.001335 \n",
      "[0/10][11/100][15] Loss_D: 0.004945 Loss_G: 0.001234 \n",
      "[0/10][11/100][16] Loss_D: 0.002594 Loss_G: 0.002286 \n",
      "[0/10][11/100][17] Loss_D: 0.003068 Loss_G: 0.002366 \n",
      "[0/10][11/100][18] Loss_D: 0.001817 Loss_G: 0.002292 \n",
      "[0/10][11/100][19] Loss_D: 0.002174 Loss_G: 0.002211 \n",
      "[0/10][11/100][20] Loss_D: 0.004233 Loss_G: 0.003130 \n",
      "[0/10][11/100][21] Loss_D: 0.002438 Loss_G: 0.003136 \n",
      "[0/10][11/100][22] Loss_D: 0.002820 Loss_G: 0.002346 \n",
      "[0/10][11/100][23] Loss_D: 0.003763 Loss_G: 0.002954 \n",
      "[0/10][11/100][24] Loss_D: 0.003153 Loss_G: 0.002925 \n",
      "[0/10][11/100][25] Loss_D: 0.002721 Loss_G: 0.002186 \n",
      "[0/10][11/100][26] Loss_D: 0.001689 Loss_G: 0.002724 \n",
      "[0/10][11/100][27] Loss_D: 0.003150 Loss_G: 0.003013 \n",
      "[0/10][11/100][28] Loss_D: 0.002644 Loss_G: 0.003455 \n",
      "[0/10][11/100][29] Loss_D: 0.003632 Loss_G: 0.002749 \n",
      "[0/10][11/100][30] Loss_D: 0.002338 Loss_G: 0.002988 \n",
      "[0/10][11/100][31] Loss_D: 0.002009 Loss_G: 0.001525 \n",
      "[0/10][11/100][32] Loss_D: 0.003683 Loss_G: 0.002951 \n",
      "[0/10][11/100][33] Loss_D: 0.002379 Loss_G: 0.004004 \n",
      "[0/10][11/100][34] Loss_D: 0.003078 Loss_G: 0.001617 \n",
      "[0/10][11/100][35] Loss_D: 0.001814 Loss_G: 0.003493 \n",
      "[0/10][11/100][36] Loss_D: 0.003146 Loss_G: 0.003254 \n",
      "[0/10][11/100][37] Loss_D: 0.003825 Loss_G: 0.004359 \n",
      "[0/10][11/100][38] Loss_D: 0.003060 Loss_G: 0.003599 \n",
      "[0/10][11/100][39] Loss_D: 0.003168 Loss_G: 0.001456 \n",
      "[0/10][11/100][40] Loss_D: 0.002482 Loss_G: 0.003877 \n",
      "[0/10][11/100][41] Loss_D: 0.004176 Loss_G: 0.003195 \n",
      "[0/10][11/100][42] Loss_D: 0.002275 Loss_G: 0.002784 \n",
      "[0/10][11/100][43] Loss_D: 0.002859 Loss_G: 0.001458 \n",
      "[0/10][11/100][44] Loss_D: 0.002254 Loss_G: 0.002082 \n",
      "[0/10][11/100][45] Loss_D: 0.002609 Loss_G: 0.003144 \n",
      "[0/10][11/100][46] Loss_D: 0.001816 Loss_G: 0.004174 \n",
      "[0/10][11/100][47] Loss_D: 0.002633 Loss_G: 0.003274 \n",
      "[0/10][11/100][48] Loss_D: 0.002708 Loss_G: 0.002657 \n",
      "[0/10][11/100][49] Loss_D: 0.003077 Loss_G: 0.004463 \n",
      "[0/10][11/100][50] Loss_D: 0.006189 Loss_G: 0.005038 \n",
      "[0/10][11/100][51] Loss_D: 0.005039 Loss_G: 0.004714 \n",
      "[0/10][11/100][52] Loss_D: 0.005172 Loss_G: 0.006362 \n",
      "[0/10][11/100][53] Loss_D: 0.005605 Loss_G: 0.008220 \n",
      "[0/10][11/100][54] Loss_D: 0.006184 Loss_G: 0.003702 \n",
      "[0/10][11/100][55] Loss_D: 0.005222 Loss_G: 0.005952 \n",
      "[0/10][11/100][56] Loss_D: 0.005219 Loss_G: 0.006041 \n",
      "[0/10][11/100][57] Loss_D: 0.005423 Loss_G: 0.005425 \n",
      "[0/10][11/100][58] Loss_D: 0.003212 Loss_G: 0.003700 \n",
      "[0/10][11/100][59] Loss_D: 0.004003 Loss_G: 0.003878 \n",
      "[0/10][11/100][60] Loss_D: 0.003880 Loss_G: 0.003315 \n",
      "[0/10][11/100][61] Loss_D: 0.004787 Loss_G: 0.002876 \n",
      "[0/10][11/100][62] Loss_D: 0.004237 Loss_G: 0.004030 \n",
      "[0/10][11/100][63] Loss_D: 0.003543 Loss_G: 0.002770 \n",
      "[0/10][11/100][64] Loss_D: 0.001903 Loss_G: 0.002290 \n",
      "[0/10][11/100][65] Loss_D: 0.003180 Loss_G: 0.002682 \n",
      "[0/10][11/100][66] Loss_D: 0.001331 Loss_G: 0.002734 \n",
      "[0/10][11/100][67] Loss_D: 0.002591 Loss_G: 0.000872 \n",
      "[0/10][11/100][68] Loss_D: 0.001788 Loss_G: 0.002837 \n",
      "[0/10][11/100][69] Loss_D: 0.002923 Loss_G: 0.002054 \n",
      "[0/10][11/100][70] Loss_D: 0.001846 Loss_G: 0.001265 \n",
      "[0/10][11/100][71] Loss_D: 0.001547 Loss_G: 0.002663 \n",
      "[0/10][11/100][72] Loss_D: 0.002322 Loss_G: 0.000681 \n",
      "[0/10][11/100][73] Loss_D: 0.003900 Loss_G: 0.003282 \n",
      "[0/10][11/100][74] Loss_D: 0.003705 Loss_G: 0.002735 \n",
      "[0/10][11/100][75] Loss_D: 0.001411 Loss_G: 0.002828 \n",
      "[0/10][11/100][76] Loss_D: 0.002335 Loss_G: 0.002299 \n",
      "[0/10][11/100][77] Loss_D: 0.002508 Loss_G: 0.003068 \n",
      "[0/10][11/100][78] Loss_D: 0.003751 Loss_G: 0.004273 \n",
      "[0/10][11/100][79] Loss_D: 0.002496 Loss_G: 0.003022 \n",
      "[0/10][11/100][80] Loss_D: 0.002241 Loss_G: 0.002961 \n",
      "[0/10][11/100][81] Loss_D: 0.001525 Loss_G: 0.002532 \n",
      "[0/10][11/100][82] Loss_D: 0.002841 Loss_G: 0.002016 \n",
      "[0/10][11/100][83] Loss_D: 0.002487 Loss_G: 0.002371 \n",
      "[0/10][11/100][84] Loss_D: 0.003462 Loss_G: 0.002186 \n",
      "[0/10][11/100][85] Loss_D: 0.003044 Loss_G: 0.002104 \n",
      "[0/10][11/100][86] Loss_D: 0.001736 Loss_G: 0.003697 \n",
      "[0/10][11/100][87] Loss_D: 0.002103 Loss_G: 0.002186 \n",
      "[0/10][11/100][88] Loss_D: 0.002254 Loss_G: 0.002095 \n",
      "[0/10][11/100][89] Loss_D: 0.004103 Loss_G: 0.002663 \n",
      "[0/10][11/100][90] Loss_D: 0.002339 Loss_G: 0.002525 \n",
      "[0/10][11/100][91] Loss_D: 0.002991 Loss_G: 0.001451 \n",
      "[0/10][11/100][92] Loss_D: 0.001624 Loss_G: 0.002793 \n",
      "[0/10][11/100][93] Loss_D: 0.003180 Loss_G: 0.002644 \n",
      "[0/10][11/100][94] Loss_D: 0.002198 Loss_G: 0.002288 \n",
      "[0/10][11/100][95] Loss_D: 0.002040 Loss_G: 0.001737 \n",
      "[0/10][11/100][96] Loss_D: 0.003715 Loss_G: 0.003777 \n",
      "[0/10][11/100][97] Loss_D: 0.002389 Loss_G: 0.002254 \n",
      "[0/10][11/100][98] Loss_D: 0.001820 Loss_G: 0.002653 \n",
      "[0/10][11/100][99] Loss_D: 0.003153 Loss_G: 0.002712 \n",
      "[0/10][11/100][100] Loss_D: 0.002375 Loss_G: 0.004553 \n",
      "[0/10][11/100][101] Loss_D: 0.003471 Loss_G: 0.004268 \n",
      "[0/10][11/100][102] Loss_D: 0.002482 Loss_G: 0.002404 \n",
      "[0/10][11/100][103] Loss_D: 0.004156 Loss_G: 0.003559 \n",
      "[0/10][11/100][104] Loss_D: 0.003711 Loss_G: 0.005567 \n",
      "[0/10][11/100][105] Loss_D: 0.004359 Loss_G: 0.002798 \n",
      "[0/10][11/100][106] Loss_D: 0.004859 Loss_G: 0.005086 \n",
      "[0/10][11/100][107] Loss_D: 0.005850 Loss_G: 0.004492 \n",
      "[0/10][11/100][108] Loss_D: 0.004032 Loss_G: 0.003131 \n",
      "[0/10][11/100][109] Loss_D: 0.003891 Loss_G: 0.004575 \n",
      "[0/10][11/100][110] Loss_D: 0.004282 Loss_G: 0.003629 \n",
      "[0/10][11/100][111] Loss_D: 0.003627 Loss_G: 0.005340 \n",
      "[0/10][11/100][112] Loss_D: 0.005655 Loss_G: 0.003242 \n",
      "[0/10][11/100][113] Loss_D: 0.006106 Loss_G: 0.003212 \n",
      "[0/10][11/100][114] Loss_D: 0.004513 Loss_G: 0.003594 \n",
      "[0/10][11/100][115] Loss_D: 0.002836 Loss_G: 0.004282 \n",
      "[0/10][11/100][116] Loss_D: 0.002738 Loss_G: 0.004278 \n",
      "[0/10][11/100][117] Loss_D: 0.005005 Loss_G: 0.003070 \n",
      "[0/10][11/100][118] Loss_D: 0.003092 Loss_G: 0.002062 \n",
      "[0/10][11/100][119] Loss_D: 0.002815 Loss_G: 0.003321 \n",
      "[0/10][11/100][120] Loss_D: 0.001835 Loss_G: 0.002757 \n",
      "[0/10][11/100][121] Loss_D: 0.001775 Loss_G: 0.002440 \n",
      "[0/10][11/100][122] Loss_D: 0.000791 Loss_G: 0.003262 \n",
      "[0/10][11/100][123] Loss_D: 0.001619 Loss_G: 0.001999 \n",
      "[0/10][11/100][124] Loss_D: 0.002965 Loss_G: 0.004522 \n",
      "[0/10][11/100][125] Loss_D: 0.003372 Loss_G: 0.002622 \n",
      "[0/10][11/100][126] Loss_D: 0.002231 Loss_G: 0.003113 \n",
      "[0/10][11/100][127] Loss_D: 0.003108 Loss_G: 0.004891 \n",
      "[0/10][11/100][128] Loss_D: 0.004179 Loss_G: 0.003291 \n",
      "[0/10][11/100][129] Loss_D: 0.002397 Loss_G: 0.001080 \n",
      "[0/10][11/100][130] Loss_D: 0.002750 Loss_G: 0.003272 \n",
      "[0/10][11/100][131] Loss_D: 0.003182 Loss_G: 0.001650 \n",
      "[0/10][11/100][132] Loss_D: 0.001455 Loss_G: 0.003335 \n",
      "[0/10][11/100][133] Loss_D: 0.003197 Loss_G: 0.003381 \n",
      "[0/10][11/100][134] Loss_D: 0.002254 Loss_G: 0.003075 \n",
      "[0/10][11/100][135] Loss_D: 0.002319 Loss_G: 0.003268 \n",
      "[0/10][11/100][136] Loss_D: 0.002942 Loss_G: 0.001622 \n",
      "[0/10][11/100][137] Loss_D: 0.002496 Loss_G: 0.002316 \n",
      "[0/10][11/100][138] Loss_D: 0.003585 Loss_G: 0.003385 \n",
      "[0/10][11/100][139] Loss_D: 0.003872 Loss_G: 0.001539 \n",
      "[0/10][11/100][140] Loss_D: 0.002978 Loss_G: 0.003041 \n",
      "[0/10][11/100][141] Loss_D: 0.003441 Loss_G: 0.001444 \n",
      "[0/10][11/100][142] Loss_D: 0.003213 Loss_G: 0.002850 \n",
      "[0/10][11/100][143] Loss_D: 0.003825 Loss_G: 0.002703 \n",
      "[0/10][11/100][144] Loss_D: 0.002763 Loss_G: 0.003093 \n",
      "[0/10][11/100][145] Loss_D: 0.002792 Loss_G: 0.003510 \n",
      "[0/10][11/100][146] Loss_D: 0.002409 Loss_G: 0.003159 \n",
      "[0/10][11/100][147] Loss_D: 0.005176 Loss_G: 0.001930 \n",
      "[0/10][11/100][148] Loss_D: 0.003705 Loss_G: 0.003627 \n",
      "[0/10][11/100][149] Loss_D: 0.003281 Loss_G: 0.003663 \n",
      "[0/10][11/100][150] Loss_D: 0.004909 Loss_G: 0.004097 \n",
      "[0/10][11/100][151] Loss_D: 0.003719 Loss_G: 0.004170 \n",
      "[0/10][11/100][152] Loss_D: 0.002827 Loss_G: 0.004778 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][11/100][153] Loss_D: 0.004338 Loss_G: 0.002358 \n",
      "[0/10][11/100][154] Loss_D: 0.003062 Loss_G: 0.002737 \n",
      "[0/10][11/100][155] Loss_D: 0.003775 Loss_G: 0.004760 \n",
      "[0/10][11/100][156] Loss_D: 0.003848 Loss_G: 0.004169 \n",
      "[0/10][11/100][157] Loss_D: 0.005942 Loss_G: 0.004774 \n",
      "[0/10][11/100][158] Loss_D: 0.003313 Loss_G: 0.004943 \n",
      "[0/10][11/100][159] Loss_D: 0.005686 Loss_G: 0.003100 \n",
      "[0/10][11/100][160] Loss_D: 0.003512 Loss_G: 0.003946 \n",
      "[0/10][11/100][161] Loss_D: 0.005492 Loss_G: 0.006995 \n",
      "[0/10][11/100][162] Loss_D: 0.005241 Loss_G: 0.004142 \n",
      "[0/10][11/100][163] Loss_D: 0.004584 Loss_G: 0.003344 \n",
      "[0/10][11/100][164] Loss_D: 0.004096 Loss_G: 0.005668 \n",
      "[0/10][11/100][165] Loss_D: 0.004782 Loss_G: 0.004973 \n",
      "[0/10][11/100][166] Loss_D: 0.002818 Loss_G: 0.004860 \n",
      "[0/10][11/100][167] Loss_D: 0.002709 Loss_G: 0.003706 \n",
      "[0/10][11/100][168] Loss_D: 0.002961 Loss_G: 0.004316 \n",
      "[0/10][11/100][169] Loss_D: 0.002991 Loss_G: 0.004239 \n",
      "[0/10][11/100][170] Loss_D: 0.003084 Loss_G: 0.004922 \n",
      "[0/10][11/100][171] Loss_D: 0.003692 Loss_G: 0.005452 \n",
      "[0/10][11/100][172] Loss_D: 0.002341 Loss_G: 0.001230 \n",
      "[0/10][11/100][173] Loss_D: 0.003240 Loss_G: 0.004424 \n",
      "[0/10][11/100][174] Loss_D: 0.002398 Loss_G: 0.002440 \n",
      "[0/10][11/100][175] Loss_D: 0.004146 Loss_G: 0.002464 \n",
      "[0/10][11/100][176] Loss_D: 0.002753 Loss_G: 0.005247 \n",
      "[0/10][11/100][177] Loss_D: 0.004317 Loss_G: 0.002773 \n",
      "[0/10][11/100][178] Loss_D: 0.002492 Loss_G: 0.003365 \n",
      "[0/10][11/100][179] Loss_D: 0.002364 Loss_G: 0.002812 \n",
      "[0/10][11/100][180] Loss_D: 0.002720 Loss_G: 0.002743 \n",
      "[0/10][11/100][181] Loss_D: 0.002071 Loss_G: 0.004357 \n",
      "[0/10][11/100][182] Loss_D: 0.003083 Loss_G: 0.002471 \n",
      "[0/10][11/100][183] Loss_D: 0.004103 Loss_G: 0.003300 \n",
      "[0/10][11/100][184] Loss_D: 0.003055 Loss_G: 0.003225 \n",
      "[0/10][11/100][185] Loss_D: 0.002237 Loss_G: 0.001673 \n",
      "[0/10][11/100][186] Loss_D: 0.001763 Loss_G: 0.002593 \n",
      "[0/10][11/100][187] Loss_D: 0.004463 Loss_G: 0.002920 \n",
      "[0/10][11/100][188] Loss_D: 0.002720 Loss_G: 0.004194 \n",
      "[0/10][11/100][189] Loss_D: 0.003591 Loss_G: 0.002258 \n",
      "[0/10][11/100][190] Loss_D: 0.002636 Loss_G: 0.002494 \n",
      "[0/10][11/100][191] Loss_D: 0.004133 Loss_G: 0.003409 \n",
      "[0/10][11/100][192] Loss_D: 0.003301 Loss_G: 0.002829 \n",
      "[0/10][11/100][193] Loss_D: 0.003306 Loss_G: 0.002749 \n",
      "[0/10][11/100][194] Loss_D: 0.002626 Loss_G: 0.002977 \n",
      "[0/10][11/100][195] Loss_D: 0.004131 Loss_G: 0.002504 \n",
      "[0/10][11/100][196] Loss_D: 0.002633 Loss_G: 0.003546 \n",
      "[0/10][11/100][197] Loss_D: 0.003055 Loss_G: 0.002926 \n",
      "[0/10][11/100][198] Loss_D: 0.002133 Loss_G: 0.002303 \n",
      "[0/10][11/100][199] Loss_D: 0.002069 Loss_G: 0.002310 \n",
      "[0/10][11/100][200] Loss_D: 0.003512 Loss_G: 0.002260 \n",
      "[1/10][11/100][201] Loss_D: 0.005916 Loss_G: 0.002040 \n",
      "[1/10][11/100][202] Loss_D: 0.002913 Loss_G: 0.002553 \n",
      "[1/10][11/100][203] Loss_D: 0.002789 Loss_G: 0.003456 \n",
      "[1/10][11/100][204] Loss_D: 0.003711 Loss_G: 0.003801 \n",
      "[1/10][11/100][205] Loss_D: 0.005078 Loss_G: 0.004006 \n",
      "[1/10][11/100][206] Loss_D: 0.002812 Loss_G: 0.002839 \n",
      "[1/10][11/100][207] Loss_D: 0.002647 Loss_G: 0.002708 \n",
      "[1/10][11/100][208] Loss_D: 0.004650 Loss_G: 0.003829 \n",
      "[1/10][11/100][209] Loss_D: 0.000918 Loss_G: 0.003273 \n",
      "[1/10][11/100][210] Loss_D: 0.003518 Loss_G: 0.003009 \n",
      "[1/10][11/100][211] Loss_D: 0.004141 Loss_G: 0.003432 \n",
      "[1/10][11/100][212] Loss_D: 0.004531 Loss_G: 0.006455 \n",
      "[1/10][11/100][213] Loss_D: 0.003205 Loss_G: 0.003088 \n",
      "[1/10][11/100][214] Loss_D: 0.003429 Loss_G: 0.003711 \n",
      "[1/10][11/100][215] Loss_D: 0.004386 Loss_G: 0.003434 \n",
      "[1/10][11/100][216] Loss_D: 0.003098 Loss_G: 0.004147 \n",
      "[1/10][11/100][217] Loss_D: 0.004127 Loss_G: 0.005527 \n",
      "[1/10][11/100][218] Loss_D: 0.005254 Loss_G: 0.003664 \n",
      "[1/10][11/100][219] Loss_D: 0.005260 Loss_G: 0.005415 \n",
      "[1/10][11/100][220] Loss_D: 0.003250 Loss_G: 0.004622 \n",
      "[1/10][11/100][221] Loss_D: 0.003606 Loss_G: 0.004300 \n",
      "[1/10][11/100][222] Loss_D: 0.005524 Loss_G: 0.004713 \n",
      "[1/10][11/100][223] Loss_D: 0.003774 Loss_G: 0.003690 \n",
      "[1/10][11/100][224] Loss_D: 0.002949 Loss_G: 0.004010 \n",
      "[1/10][11/100][225] Loss_D: 0.003345 Loss_G: 0.003459 \n",
      "[1/10][11/100][226] Loss_D: 0.003613 Loss_G: 0.002102 \n",
      "[1/10][11/100][227] Loss_D: 0.002749 Loss_G: 0.003487 \n",
      "[1/10][11/100][228] Loss_D: 0.003437 Loss_G: 0.002036 \n",
      "[1/10][11/100][229] Loss_D: 0.002812 Loss_G: 0.003418 \n",
      "[1/10][11/100][230] Loss_D: 0.002189 Loss_G: 0.002950 \n",
      "[1/10][11/100][231] Loss_D: 0.002771 Loss_G: 0.002624 \n",
      "[1/10][11/100][232] Loss_D: 0.002605 Loss_G: 0.003344 \n",
      "[1/10][11/100][233] Loss_D: 0.003218 Loss_G: 0.002156 \n",
      "[1/10][11/100][234] Loss_D: 0.002695 Loss_G: 0.002302 \n",
      "[1/10][11/100][235] Loss_D: 0.002491 Loss_G: 0.002899 \n",
      "[1/10][11/100][236] Loss_D: 0.001853 Loss_G: 0.003081 \n",
      "[1/10][11/100][237] Loss_D: 0.003928 Loss_G: 0.006856 \n",
      "[1/10][11/100][238] Loss_D: 0.001647 Loss_G: 0.001783 \n",
      "[1/10][11/100][239] Loss_D: 0.002814 Loss_G: 0.002969 \n",
      "[1/10][11/100][240] Loss_D: 0.002276 Loss_G: 0.002036 \n",
      "[1/10][11/100][241] Loss_D: 0.002242 Loss_G: 0.003300 \n",
      "[1/10][11/100][242] Loss_D: 0.003132 Loss_G: 0.004416 \n",
      "[1/10][11/100][243] Loss_D: 0.002732 Loss_G: 0.003372 \n",
      "[1/10][11/100][244] Loss_D: 0.001974 Loss_G: 0.004078 \n",
      "[1/10][11/100][245] Loss_D: 0.002842 Loss_G: 0.003312 \n",
      "[1/10][11/100][246] Loss_D: 0.002909 Loss_G: 0.002235 \n",
      "[1/10][11/100][247] Loss_D: 0.002782 Loss_G: 0.002042 \n",
      "[1/10][11/100][248] Loss_D: 0.002428 Loss_G: 0.002902 \n",
      "[1/10][11/100][249] Loss_D: 0.003139 Loss_G: 0.003433 \n",
      "[1/10][11/100][250] Loss_D: 0.002719 Loss_G: 0.004098 \n",
      "[1/10][11/100][251] Loss_D: 0.003823 Loss_G: 0.003068 \n",
      "[1/10][11/100][252] Loss_D: 0.002487 Loss_G: 0.002943 \n",
      "[1/10][11/100][253] Loss_D: 0.002700 Loss_G: 0.002874 \n",
      "[1/10][11/100][254] Loss_D: 0.003246 Loss_G: 0.001493 \n",
      "[1/10][11/100][255] Loss_D: 0.002011 Loss_G: 0.002405 \n",
      "[1/10][11/100][256] Loss_D: 0.003104 Loss_G: 0.002452 \n",
      "[1/10][11/100][257] Loss_D: 0.001940 Loss_G: 0.004309 \n",
      "[1/10][11/100][258] Loss_D: 0.002526 Loss_G: 0.003072 \n",
      "[1/10][11/100][259] Loss_D: 0.003641 Loss_G: 0.002608 \n",
      "[1/10][11/100][260] Loss_D: 0.002340 Loss_G: 0.003758 \n",
      "[1/10][11/100][261] Loss_D: 0.003271 Loss_G: 0.002434 \n",
      "[1/10][11/100][262] Loss_D: 0.003603 Loss_G: 0.003790 \n",
      "[1/10][11/100][263] Loss_D: 0.005258 Loss_G: 0.002702 \n",
      "[1/10][11/100][264] Loss_D: 0.004667 Loss_G: 0.003804 \n",
      "[1/10][11/100][265] Loss_D: 0.003098 Loss_G: 0.005710 \n",
      "[1/10][11/100][266] Loss_D: 0.005698 Loss_G: 0.004565 \n",
      "[1/10][11/100][267] Loss_D: 0.006310 Loss_G: 0.004885 \n",
      "[1/10][11/100][268] Loss_D: 0.003780 Loss_G: 0.003114 \n",
      "[1/10][11/100][269] Loss_D: 0.002339 Loss_G: 0.004425 \n",
      "[1/10][11/100][270] Loss_D: 0.003022 Loss_G: 0.004346 \n",
      "[1/10][11/100][271] Loss_D: 0.003996 Loss_G: 0.003840 \n",
      "[1/10][11/100][272] Loss_D: 0.005790 Loss_G: 0.004492 \n",
      "[1/10][11/100][273] Loss_D: 0.005318 Loss_G: 0.005106 \n",
      "[1/10][11/100][274] Loss_D: 0.004705 Loss_G: 0.005995 \n",
      "[1/10][11/100][275] Loss_D: 0.004089 Loss_G: 0.003473 \n",
      "[1/10][11/100][276] Loss_D: 0.004518 Loss_G: 0.005001 \n",
      "[1/10][11/100][277] Loss_D: 0.004408 Loss_G: 0.005055 \n",
      "[1/10][11/100][278] Loss_D: 0.005237 Loss_G: 0.002934 \n",
      "[1/10][11/100][279] Loss_D: 0.003360 Loss_G: 0.002890 \n",
      "[1/10][11/100][280] Loss_D: 0.003380 Loss_G: 0.003306 \n",
      "[1/10][11/100][281] Loss_D: 0.002467 Loss_G: 0.003224 \n",
      "[1/10][11/100][282] Loss_D: 0.003106 Loss_G: 0.002015 \n",
      "[1/10][11/100][283] Loss_D: 0.004019 Loss_G: 0.003983 \n",
      "[1/10][11/100][284] Loss_D: 0.002611 Loss_G: 0.004438 \n",
      "[1/10][11/100][285] Loss_D: 0.002619 Loss_G: 0.002014 \n",
      "[1/10][11/100][286] Loss_D: 0.003373 Loss_G: 0.002136 \n",
      "[1/10][11/100][287] Loss_D: 0.001637 Loss_G: 0.002313 \n",
      "[1/10][11/100][288] Loss_D: 0.002171 Loss_G: 0.002648 \n",
      "[1/10][11/100][289] Loss_D: 0.001531 Loss_G: 0.001129 \n",
      "[1/10][11/100][290] Loss_D: 0.003616 Loss_G: 0.002175 \n",
      "[1/10][11/100][291] Loss_D: 0.002664 Loss_G: 0.004601 \n",
      "[1/10][11/100][292] Loss_D: 0.003480 Loss_G: 0.001592 \n",
      "[1/10][11/100][293] Loss_D: 0.002429 Loss_G: 0.003602 \n",
      "[1/10][11/100][294] Loss_D: 0.002258 Loss_G: 0.001501 \n",
      "[1/10][11/100][295] Loss_D: 0.003537 Loss_G: 0.002678 \n",
      "[1/10][11/100][296] Loss_D: 0.001968 Loss_G: 0.003184 \n",
      "[1/10][11/100][297] Loss_D: 0.002281 Loss_G: 0.003042 \n",
      "[1/10][11/100][298] Loss_D: 0.004214 Loss_G: 0.002890 \n",
      "[1/10][11/100][299] Loss_D: 0.002786 Loss_G: 0.002195 \n",
      "[1/10][11/100][300] Loss_D: 0.002142 Loss_G: 0.002517 \n",
      "[1/10][11/100][301] Loss_D: 0.002643 Loss_G: 0.002394 \n",
      "[1/10][11/100][302] Loss_D: 0.004193 Loss_G: 0.003692 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][11/100][303] Loss_D: 0.001898 Loss_G: 0.002703 \n",
      "[1/10][11/100][304] Loss_D: 0.003607 Loss_G: 0.004385 \n",
      "[1/10][11/100][305] Loss_D: 0.003177 Loss_G: 0.006187 \n",
      "[1/10][11/100][306] Loss_D: 0.003957 Loss_G: 0.002928 \n",
      "[1/10][11/100][307] Loss_D: 0.001238 Loss_G: 0.003382 \n",
      "[1/10][11/100][308] Loss_D: 0.003089 Loss_G: 0.002683 \n",
      "[1/10][11/100][309] Loss_D: 0.002815 Loss_G: 0.002643 \n",
      "[1/10][11/100][310] Loss_D: 0.002947 Loss_G: 0.004726 \n",
      "[1/10][11/100][311] Loss_D: 0.002225 Loss_G: 0.002794 \n",
      "[1/10][11/100][312] Loss_D: 0.004134 Loss_G: 0.002416 \n",
      "[1/10][11/100][313] Loss_D: 0.002899 Loss_G: 0.002664 \n",
      "[1/10][11/100][314] Loss_D: 0.001712 Loss_G: 0.003780 \n",
      "[1/10][11/100][315] Loss_D: 0.004186 Loss_G: 0.002543 \n",
      "[1/10][11/100][316] Loss_D: 0.003426 Loss_G: 0.002766 \n",
      "[1/10][11/100][317] Loss_D: 0.004465 Loss_G: 0.003030 \n",
      "[1/10][11/100][318] Loss_D: 0.002698 Loss_G: 0.003535 \n",
      "[1/10][11/100][319] Loss_D: 0.002767 Loss_G: 0.003796 \n",
      "[1/10][11/100][320] Loss_D: 0.002395 Loss_G: 0.004099 \n",
      "[1/10][11/100][321] Loss_D: 0.004074 Loss_G: 0.004630 \n",
      "[1/10][11/100][322] Loss_D: 0.004194 Loss_G: 0.003433 \n",
      "[1/10][11/100][323] Loss_D: 0.002446 Loss_G: 0.003729 \n",
      "[1/10][11/100][324] Loss_D: 0.004265 Loss_G: 0.004378 \n",
      "[1/10][11/100][325] Loss_D: 0.003216 Loss_G: 0.003330 \n",
      "[1/10][11/100][326] Loss_D: 0.003212 Loss_G: 0.002800 \n",
      "[1/10][11/100][327] Loss_D: 0.003111 Loss_G: 0.004102 \n",
      "[1/10][11/100][328] Loss_D: 0.003225 Loss_G: 0.003794 \n",
      "[1/10][11/100][329] Loss_D: 0.004828 Loss_G: 0.002966 \n",
      "[1/10][11/100][330] Loss_D: 0.004037 Loss_G: 0.003388 \n",
      "[1/10][11/100][331] Loss_D: 0.005195 Loss_G: 0.004827 \n",
      "[1/10][11/100][332] Loss_D: 0.005124 Loss_G: 0.004453 \n",
      "[1/10][11/100][333] Loss_D: 0.003482 Loss_G: 0.004235 \n",
      "[1/10][11/100][334] Loss_D: 0.004223 Loss_G: 0.003705 \n",
      "[1/10][11/100][335] Loss_D: 0.004761 Loss_G: 0.003109 \n",
      "[1/10][11/100][336] Loss_D: 0.006097 Loss_G: 0.004032 \n",
      "[1/10][11/100][337] Loss_D: 0.005233 Loss_G: 0.004017 \n",
      "[1/10][11/100][338] Loss_D: 0.005368 Loss_G: 0.003961 \n",
      "[1/10][11/100][339] Loss_D: 0.003772 Loss_G: 0.003930 \n",
      "[1/10][11/100][340] Loss_D: 0.004860 Loss_G: 0.003633 \n",
      "[1/10][11/100][341] Loss_D: 0.003125 Loss_G: 0.002810 \n",
      "[1/10][11/100][342] Loss_D: 0.003330 Loss_G: 0.005095 \n",
      "[1/10][11/100][343] Loss_D: 0.004032 Loss_G: 0.003567 \n",
      "[1/10][11/100][344] Loss_D: 0.002374 Loss_G: 0.003448 \n",
      "[1/10][11/100][345] Loss_D: 0.002178 Loss_G: 0.002597 \n",
      "[1/10][11/100][346] Loss_D: 0.002223 Loss_G: 0.002544 \n",
      "[1/10][11/100][347] Loss_D: 0.002981 Loss_G: 0.001962 \n",
      "[1/10][11/100][348] Loss_D: 0.002503 Loss_G: 0.001783 \n",
      "[1/10][11/100][349] Loss_D: 0.002423 Loss_G: 0.003755 \n",
      "[1/10][11/100][350] Loss_D: 0.001775 Loss_G: 0.001969 \n",
      "[1/10][11/100][351] Loss_D: 0.001783 Loss_G: 0.002225 \n",
      "[1/10][11/100][352] Loss_D: 0.002760 Loss_G: 0.001938 \n",
      "[1/10][11/100][353] Loss_D: 0.002502 Loss_G: 0.002761 \n",
      "[1/10][11/100][354] Loss_D: 0.002564 Loss_G: 0.002699 \n",
      "[1/10][11/100][355] Loss_D: 0.002449 Loss_G: 0.002944 \n",
      "[1/10][11/100][356] Loss_D: 0.002872 Loss_G: 0.002832 \n",
      "[1/10][11/100][357] Loss_D: 0.003944 Loss_G: 0.003241 \n",
      "[1/10][11/100][358] Loss_D: 0.002436 Loss_G: 0.003294 \n",
      "[1/10][11/100][359] Loss_D: 0.002263 Loss_G: 0.004107 \n",
      "[1/10][11/100][360] Loss_D: 0.004065 Loss_G: 0.003514 \n",
      "[1/10][11/100][361] Loss_D: 0.003011 Loss_G: 0.003273 \n",
      "[1/10][11/100][362] Loss_D: 0.002643 Loss_G: 0.001809 \n",
      "[1/10][11/100][363] Loss_D: 0.002561 Loss_G: 0.002673 \n",
      "[1/10][11/100][364] Loss_D: 0.003024 Loss_G: 0.002251 \n",
      "[1/10][11/100][365] Loss_D: 0.002974 Loss_G: 0.003138 \n",
      "[1/10][11/100][366] Loss_D: 0.002980 Loss_G: 0.002238 \n",
      "[1/10][11/100][367] Loss_D: 0.005286 Loss_G: 0.002723 \n",
      "[1/10][11/100][368] Loss_D: 0.002830 Loss_G: 0.003309 \n",
      "[1/10][11/100][369] Loss_D: 0.004428 Loss_G: 0.003886 \n",
      "[1/10][11/100][370] Loss_D: 0.002532 Loss_G: 0.003084 \n",
      "[1/10][11/100][371] Loss_D: 0.003839 Loss_G: 0.002126 \n",
      "[1/10][11/100][372] Loss_D: 0.003207 Loss_G: 0.002687 \n",
      "[1/10][11/100][373] Loss_D: 0.002286 Loss_G: 0.002872 \n",
      "[1/10][11/100][374] Loss_D: 0.001500 Loss_G: 0.001774 \n",
      "[1/10][11/100][375] Loss_D: 0.003997 Loss_G: 0.004874 \n",
      "[1/10][11/100][376] Loss_D: 0.001973 Loss_G: 0.002529 \n",
      "[1/10][11/100][377] Loss_D: 0.004006 Loss_G: 0.002808 \n",
      "[1/10][11/100][378] Loss_D: 0.002939 Loss_G: 0.003432 \n",
      "[1/10][11/100][379] Loss_D: 0.003412 Loss_G: 0.002945 \n",
      "[1/10][11/100][380] Loss_D: 0.003593 Loss_G: 0.004597 \n",
      "[1/10][11/100][381] Loss_D: 0.001717 Loss_G: 0.002574 \n",
      "[1/10][11/100][382] Loss_D: 0.004136 Loss_G: 0.002512 \n",
      "[1/10][11/100][383] Loss_D: 0.003321 Loss_G: 0.003135 \n",
      "[1/10][11/100][384] Loss_D: 0.003445 Loss_G: 0.003355 \n",
      "[1/10][11/100][385] Loss_D: 0.003252 Loss_G: 0.004607 \n",
      "[1/10][11/100][386] Loss_D: 0.004749 Loss_G: 0.003265 \n",
      "[1/10][11/100][387] Loss_D: 0.003111 Loss_G: 0.004951 \n",
      "[1/10][11/100][388] Loss_D: 0.003204 Loss_G: 0.003608 \n",
      "[1/10][11/100][389] Loss_D: 0.002145 Loss_G: 0.004968 \n",
      "[1/10][11/100][390] Loss_D: 0.003035 Loss_G: 0.003940 \n",
      "[1/10][11/100][391] Loss_D: 0.003700 Loss_G: 0.003129 \n",
      "[1/10][11/100][392] Loss_D: 0.004819 Loss_G: 0.003365 \n",
      "[1/10][11/100][393] Loss_D: 0.003872 Loss_G: 0.004664 \n",
      "[1/10][11/100][394] Loss_D: 0.004040 Loss_G: 0.003908 \n",
      "[1/10][11/100][395] Loss_D: 0.003065 Loss_G: 0.003831 \n",
      "[1/10][11/100][396] Loss_D: 0.005246 Loss_G: 0.002612 \n",
      "[1/10][11/100][397] Loss_D: 0.003621 Loss_G: 0.004332 \n",
      "[1/10][11/100][398] Loss_D: 0.003726 Loss_G: 0.004155 \n",
      "[1/10][11/100][399] Loss_D: 0.003920 Loss_G: 0.004330 \n",
      "[1/10][11/100][400] Loss_D: 0.003401 Loss_G: 0.003942 \n",
      "[2/10][11/100][401] Loss_D: 0.005016 Loss_G: 0.003216 \n",
      "[2/10][11/100][402] Loss_D: 0.002635 Loss_G: 0.003280 \n",
      "[2/10][11/100][403] Loss_D: 0.004439 Loss_G: 0.003260 \n",
      "[2/10][11/100][404] Loss_D: 0.003144 Loss_G: 0.002760 \n",
      "[2/10][11/100][405] Loss_D: 0.003520 Loss_G: 0.003853 \n",
      "[2/10][11/100][406] Loss_D: 0.002099 Loss_G: 0.002582 \n",
      "[2/10][11/100][407] Loss_D: 0.001759 Loss_G: 0.004043 \n",
      "[2/10][11/100][408] Loss_D: 0.002322 Loss_G: 0.002992 \n",
      "[2/10][11/100][409] Loss_D: 0.001645 Loss_G: 0.002795 \n",
      "[2/10][11/100][410] Loss_D: 0.004404 Loss_G: 0.002769 \n",
      "[2/10][11/100][411] Loss_D: 0.005008 Loss_G: 0.003594 \n",
      "[2/10][11/100][412] Loss_D: 0.001354 Loss_G: 0.003019 \n",
      "[2/10][11/100][413] Loss_D: 0.003224 Loss_G: 0.004874 \n",
      "[2/10][11/100][414] Loss_D: 0.002328 Loss_G: 0.002695 \n",
      "[2/10][11/100][415] Loss_D: 0.002972 Loss_G: 0.002805 \n",
      "[2/10][11/100][416] Loss_D: 0.002969 Loss_G: 0.003635 \n",
      "[2/10][11/100][417] Loss_D: 0.003846 Loss_G: 0.005022 \n",
      "[2/10][11/100][418] Loss_D: 0.003081 Loss_G: 0.004511 \n",
      "[2/10][11/100][419] Loss_D: 0.002693 Loss_G: 0.003095 \n",
      "[2/10][11/100][420] Loss_D: 0.003267 Loss_G: 0.003427 \n",
      "[2/10][11/100][421] Loss_D: 0.003810 Loss_G: 0.003789 \n",
      "[2/10][11/100][422] Loss_D: 0.002970 Loss_G: 0.002281 \n",
      "[2/10][11/100][423] Loss_D: 0.001585 Loss_G: 0.003569 \n",
      "[2/10][11/100][424] Loss_D: 0.003482 Loss_G: 0.003324 \n",
      "[2/10][11/100][425] Loss_D: 0.002617 Loss_G: 0.002784 \n",
      "[2/10][11/100][426] Loss_D: 0.003673 Loss_G: 0.002307 \n",
      "[2/10][11/100][427] Loss_D: 0.003697 Loss_G: 0.002129 \n",
      "[2/10][11/100][428] Loss_D: 0.003501 Loss_G: 0.003716 \n",
      "[2/10][11/100][429] Loss_D: 0.002433 Loss_G: 0.002204 \n",
      "[2/10][11/100][430] Loss_D: 0.003418 Loss_G: 0.002750 \n",
      "[2/10][11/100][431] Loss_D: 0.001615 Loss_G: 0.003280 \n",
      "[2/10][11/100][432] Loss_D: 0.003398 Loss_G: 0.004354 \n",
      "[2/10][11/100][433] Loss_D: 0.001888 Loss_G: 0.002792 \n",
      "[2/10][11/100][434] Loss_D: 0.003794 Loss_G: 0.003877 \n",
      "[2/10][11/100][435] Loss_D: 0.002637 Loss_G: 0.002816 \n",
      "[2/10][11/100][436] Loss_D: 0.002452 Loss_G: 0.003998 \n",
      "[2/10][11/100][437] Loss_D: 0.002390 Loss_G: 0.003855 \n",
      "[2/10][11/100][438] Loss_D: 0.004528 Loss_G: 0.003113 \n",
      "[2/10][11/100][439] Loss_D: 0.004279 Loss_G: 0.002116 \n",
      "[2/10][11/100][440] Loss_D: 0.004767 Loss_G: 0.004339 \n",
      "[2/10][11/100][441] Loss_D: 0.001287 Loss_G: 0.002425 \n",
      "[2/10][11/100][442] Loss_D: 0.003899 Loss_G: 0.003689 \n",
      "[2/10][11/100][443] Loss_D: 0.003775 Loss_G: 0.002844 \n",
      "[2/10][11/100][444] Loss_D: 0.003508 Loss_G: 0.002533 \n",
      "[2/10][11/100][445] Loss_D: 0.003228 Loss_G: 0.002399 \n",
      "[2/10][11/100][446] Loss_D: 0.002200 Loss_G: 0.004907 \n",
      "[2/10][11/100][447] Loss_D: 0.004040 Loss_G: 0.002888 \n",
      "[2/10][11/100][448] Loss_D: 0.002093 Loss_G: 0.003481 \n",
      "[2/10][11/100][449] Loss_D: 0.002778 Loss_G: 0.002967 \n",
      "[2/10][11/100][450] Loss_D: 0.001885 Loss_G: 0.002520 \n",
      "[2/10][11/100][451] Loss_D: 0.003823 Loss_G: 0.003115 \n",
      "[2/10][11/100][452] Loss_D: 0.003528 Loss_G: 0.003217 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][11/100][453] Loss_D: 0.002915 Loss_G: 0.003249 \n",
      "[2/10][11/100][454] Loss_D: 0.003032 Loss_G: 0.001766 \n",
      "[2/10][11/100][455] Loss_D: 0.003261 Loss_G: 0.002840 \n",
      "[2/10][11/100][456] Loss_D: 0.003137 Loss_G: 0.003503 \n",
      "[2/10][11/100][457] Loss_D: 0.003353 Loss_G: 0.003791 \n",
      "[2/10][11/100][458] Loss_D: 0.004809 Loss_G: 0.003747 \n",
      "[2/10][11/100][459] Loss_D: 0.003217 Loss_G: 0.002765 \n",
      "[2/10][11/100][460] Loss_D: 0.002565 Loss_G: 0.003037 \n",
      "[2/10][11/100][461] Loss_D: 0.002931 Loss_G: 0.003637 \n",
      "[2/10][11/100][462] Loss_D: 0.002070 Loss_G: 0.002801 \n",
      "[2/10][11/100][463] Loss_D: 0.001829 Loss_G: 0.003709 \n",
      "[2/10][11/100][464] Loss_D: 0.003076 Loss_G: 0.002904 \n",
      "[2/10][11/100][465] Loss_D: 0.003130 Loss_G: 0.001723 \n",
      "[2/10][11/100][466] Loss_D: 0.002970 Loss_G: 0.003368 \n",
      "[2/10][11/100][467] Loss_D: 0.001963 Loss_G: 0.003493 \n",
      "[2/10][11/100][468] Loss_D: 0.002592 Loss_G: 0.004019 \n",
      "[2/10][11/100][469] Loss_D: 0.002758 Loss_G: 0.003161 \n",
      "[2/10][11/100][470] Loss_D: 0.003403 Loss_G: 0.003273 \n",
      "[2/10][11/100][471] Loss_D: 0.003812 Loss_G: 0.004036 \n",
      "[2/10][11/100][472] Loss_D: 0.003152 Loss_G: 0.003061 \n",
      "[2/10][11/100][473] Loss_D: 0.004909 Loss_G: 0.004836 \n",
      "[2/10][11/100][474] Loss_D: 0.003695 Loss_G: 0.003061 \n",
      "[2/10][11/100][475] Loss_D: 0.002355 Loss_G: 0.004324 \n",
      "[2/10][11/100][476] Loss_D: 0.002931 Loss_G: 0.002605 \n",
      "[2/10][11/100][477] Loss_D: 0.003817 Loss_G: 0.003837 \n",
      "[2/10][11/100][478] Loss_D: 0.003750 Loss_G: 0.002548 \n",
      "[2/10][11/100][479] Loss_D: 0.003534 Loss_G: 0.002605 \n",
      "[2/10][11/100][480] Loss_D: 0.006118 Loss_G: 0.002838 \n",
      "[2/10][11/100][481] Loss_D: 0.002821 Loss_G: 0.002532 \n",
      "[2/10][11/100][482] Loss_D: 0.003951 Loss_G: 0.001471 \n",
      "[2/10][11/100][483] Loss_D: 0.001953 Loss_G: 0.001881 \n",
      "[2/10][11/100][484] Loss_D: 0.003841 Loss_G: 0.001555 \n",
      "[2/10][11/100][485] Loss_D: 0.001790 Loss_G: 0.002703 \n",
      "[2/10][11/100][486] Loss_D: 0.002256 Loss_G: 0.003376 \n",
      "[2/10][11/100][487] Loss_D: 0.002819 Loss_G: 0.003517 \n",
      "[2/10][11/100][488] Loss_D: 0.002817 Loss_G: 0.001301 \n",
      "[2/10][11/100][489] Loss_D: 0.003010 Loss_G: 0.001287 \n",
      "[2/10][11/100][490] Loss_D: 0.001546 Loss_G: 0.002486 \n",
      "[2/10][11/100][491] Loss_D: 0.002689 Loss_G: 0.000022 \n",
      "[2/10][11/100][492] Loss_D: 0.002589 Loss_G: 0.001391 \n",
      "[2/10][11/100][493] Loss_D: 0.001564 Loss_G: 0.001895 \n",
      "[2/10][11/100][494] Loss_D: 0.002728 Loss_G: 0.002758 \n",
      "[2/10][11/100][495] Loss_D: 0.001783 Loss_G: 0.002494 \n",
      "[2/10][11/100][496] Loss_D: 0.002641 Loss_G: 0.002908 \n",
      "[2/10][11/100][497] Loss_D: 0.001860 Loss_G: 0.002202 \n",
      "[2/10][11/100][498] Loss_D: 0.003395 Loss_G: 0.002749 \n",
      "[2/10][11/100][499] Loss_D: 0.003369 Loss_G: 0.001046 \n",
      "[2/10][11/100][500] Loss_D: 0.002489 Loss_G: 0.001645 \n",
      "[2/10][11/100][501] Loss_D: 0.002581 Loss_G: 0.004108 \n",
      "[2/10][11/100][502] Loss_D: 0.004058 Loss_G: 0.004208 \n",
      "[2/10][11/100][503] Loss_D: 0.001667 Loss_G: 0.003698 \n",
      "[2/10][11/100][504] Loss_D: 0.004367 Loss_G: 0.005224 \n",
      "[2/10][11/100][505] Loss_D: 0.002677 Loss_G: 0.004421 \n",
      "[2/10][11/100][506] Loss_D: 0.004648 Loss_G: 0.003839 \n",
      "[2/10][11/100][507] Loss_D: 0.004020 Loss_G: 0.002606 \n",
      "[2/10][11/100][508] Loss_D: 0.003390 Loss_G: 0.002636 \n",
      "[2/10][11/100][509] Loss_D: 0.002600 Loss_G: 0.003145 \n",
      "[2/10][11/100][510] Loss_D: 0.004303 Loss_G: 0.002942 \n",
      "[2/10][11/100][511] Loss_D: 0.002331 Loss_G: 0.001563 \n",
      "[2/10][11/100][512] Loss_D: 0.002179 Loss_G: 0.002667 \n",
      "[2/10][11/100][513] Loss_D: 0.003160 Loss_G: 0.002632 \n",
      "[2/10][11/100][514] Loss_D: 0.001662 Loss_G: 0.002749 \n",
      "[2/10][11/100][515] Loss_D: 0.003591 Loss_G: 0.001121 \n",
      "[2/10][11/100][516] Loss_D: 0.002405 Loss_G: 0.003701 \n",
      "[2/10][11/100][517] Loss_D: 0.001988 Loss_G: 0.002527 \n",
      "[2/10][11/100][518] Loss_D: 0.003486 Loss_G: 0.001955 \n",
      "[2/10][11/100][519] Loss_D: 0.005426 Loss_G: 0.002734 \n",
      "[2/10][11/100][520] Loss_D: 0.003354 Loss_G: 0.001800 \n",
      "[2/10][11/100][521] Loss_D: 0.001885 Loss_G: 0.002111 \n",
      "[2/10][11/100][522] Loss_D: 0.002813 Loss_G: 0.005457 \n",
      "[2/10][11/100][523] Loss_D: 0.002237 Loss_G: 0.002801 \n",
      "[2/10][11/100][524] Loss_D: 0.002418 Loss_G: 0.002413 \n",
      "[2/10][11/100][525] Loss_D: 0.003013 Loss_G: 0.003117 \n",
      "[2/10][11/100][526] Loss_D: 0.002559 Loss_G: 0.002190 \n",
      "[2/10][11/100][527] Loss_D: 0.003593 Loss_G: 0.003781 \n",
      "[2/10][11/100][528] Loss_D: 0.001614 Loss_G: 0.002749 \n",
      "[2/10][11/100][529] Loss_D: 0.001976 Loss_G: 0.002863 \n",
      "[2/10][11/100][530] Loss_D: 0.002072 Loss_G: 0.001322 \n",
      "[2/10][11/100][531] Loss_D: 0.002833 Loss_G: 0.002997 \n",
      "[2/10][11/100][532] Loss_D: 0.002589 Loss_G: 0.002377 \n",
      "[2/10][11/100][533] Loss_D: 0.003486 Loss_G: 0.004338 \n",
      "[2/10][11/100][534] Loss_D: 0.003534 Loss_G: 0.004672 \n",
      "[2/10][11/100][535] Loss_D: 0.006676 Loss_G: 0.005345 \n",
      "[2/10][11/100][536] Loss_D: 0.003586 Loss_G: 0.006163 \n",
      "[2/10][11/100][537] Loss_D: 0.005384 Loss_G: 0.003657 \n",
      "[2/10][11/100][538] Loss_D: 0.003598 Loss_G: 0.003731 \n",
      "[2/10][11/100][539] Loss_D: 0.004581 Loss_G: 0.002204 \n",
      "[2/10][11/100][540] Loss_D: 0.003349 Loss_G: 0.002825 \n",
      "[2/10][11/100][541] Loss_D: 0.002134 Loss_G: 0.003197 \n",
      "[2/10][11/100][542] Loss_D: 0.002735 Loss_G: 0.002088 \n",
      "[2/10][11/100][543] Loss_D: 0.001942 Loss_G: 0.002433 \n",
      "[2/10][11/100][544] Loss_D: 0.002073 Loss_G: 0.001868 \n",
      "[2/10][11/100][545] Loss_D: 0.002977 Loss_G: 0.003604 \n",
      "[2/10][11/100][546] Loss_D: 0.004155 Loss_G: 0.002667 \n",
      "[2/10][11/100][547] Loss_D: 0.002357 Loss_G: 0.002837 \n",
      "[2/10][11/100][548] Loss_D: 0.002431 Loss_G: 0.003096 \n",
      "[2/10][11/100][549] Loss_D: 0.002540 Loss_G: 0.003366 \n",
      "[2/10][11/100][550] Loss_D: 0.003482 Loss_G: 0.004158 \n",
      "[2/10][11/100][551] Loss_D: 0.002471 Loss_G: 0.003515 \n",
      "[2/10][11/100][552] Loss_D: 0.001270 Loss_G: 0.001886 \n",
      "[2/10][11/100][553] Loss_D: 0.002086 Loss_G: 0.002871 \n",
      "[2/10][11/100][554] Loss_D: 0.002931 Loss_G: 0.001961 \n",
      "[2/10][11/100][555] Loss_D: 0.002383 Loss_G: 0.001916 \n",
      "[2/10][11/100][556] Loss_D: 0.003375 Loss_G: 0.002765 \n",
      "[2/10][11/100][557] Loss_D: 0.001050 Loss_G: 0.003808 \n",
      "[2/10][11/100][558] Loss_D: 0.001854 Loss_G: 0.002056 \n",
      "[2/10][11/100][559] Loss_D: 0.002791 Loss_G: 0.002570 \n",
      "[2/10][11/100][560] Loss_D: 0.002894 Loss_G: 0.002366 \n",
      "[2/10][11/100][561] Loss_D: 0.002941 Loss_G: 0.003035 \n",
      "[2/10][11/100][562] Loss_D: 0.002804 Loss_G: 0.002147 \n",
      "[2/10][11/100][563] Loss_D: 0.003922 Loss_G: 0.003826 \n",
      "[2/10][11/100][564] Loss_D: 0.002374 Loss_G: 0.003250 \n",
      "[2/10][11/100][565] Loss_D: 0.002348 Loss_G: 0.004486 \n",
      "[2/10][11/100][566] Loss_D: 0.002382 Loss_G: 0.002968 \n",
      "[2/10][11/100][567] Loss_D: 0.001977 Loss_G: 0.003892 \n",
      "[2/10][11/100][568] Loss_D: 0.004037 Loss_G: 0.003512 \n",
      "[2/10][11/100][569] Loss_D: 0.003845 Loss_G: 0.003364 \n",
      "[2/10][11/100][570] Loss_D: 0.002182 Loss_G: 0.003125 \n",
      "[2/10][11/100][571] Loss_D: 0.003916 Loss_G: 0.003100 \n",
      "[2/10][11/100][572] Loss_D: 0.002620 Loss_G: 0.002127 \n",
      "[2/10][11/100][573] Loss_D: 0.002734 Loss_G: 0.002879 \n",
      "[2/10][11/100][574] Loss_D: 0.002603 Loss_G: 0.004320 \n",
      "[2/10][11/100][575] Loss_D: 0.004718 Loss_G: 0.000669 \n",
      "[2/10][11/100][576] Loss_D: 0.002178 Loss_G: 0.002423 \n",
      "[2/10][11/100][577] Loss_D: 0.002316 Loss_G: 0.002496 \n",
      "[2/10][11/100][578] Loss_D: 0.003035 Loss_G: 0.003077 \n",
      "[2/10][11/100][579] Loss_D: 0.002120 Loss_G: 0.003051 \n",
      "[2/10][11/100][580] Loss_D: 0.004168 Loss_G: 0.004204 \n",
      "[2/10][11/100][581] Loss_D: 0.005280 Loss_G: 0.004399 \n",
      "[2/10][11/100][582] Loss_D: 0.003630 Loss_G: 0.004085 \n",
      "[2/10][11/100][583] Loss_D: 0.003120 Loss_G: 0.006603 \n",
      "[2/10][11/100][584] Loss_D: 0.005782 Loss_G: 0.005139 \n",
      "[2/10][11/100][585] Loss_D: 0.007694 Loss_G: 0.003306 \n",
      "[2/10][11/100][586] Loss_D: 0.005608 Loss_G: 0.004359 \n",
      "[2/10][11/100][587] Loss_D: 0.005069 Loss_G: 0.004279 \n",
      "[2/10][11/100][588] Loss_D: 0.004116 Loss_G: 0.004795 \n",
      "[2/10][11/100][589] Loss_D: 0.003238 Loss_G: 0.003555 \n",
      "[2/10][11/100][590] Loss_D: 0.004533 Loss_G: 0.003641 \n",
      "[2/10][11/100][591] Loss_D: 0.003552 Loss_G: 0.004817 \n",
      "[2/10][11/100][592] Loss_D: 0.001411 Loss_G: 0.000987 \n",
      "[2/10][11/100][593] Loss_D: 0.002775 Loss_G: 0.002927 \n",
      "[2/10][11/100][594] Loss_D: 0.002900 Loss_G: 0.002888 \n",
      "[2/10][11/100][595] Loss_D: 0.002300 Loss_G: 0.002425 \n",
      "[2/10][11/100][596] Loss_D: 0.000120 Loss_G: 0.003029 \n",
      "[2/10][11/100][597] Loss_D: 0.001922 Loss_G: 0.005208 \n",
      "[2/10][11/100][598] Loss_D: 0.002935 Loss_G: 0.001889 \n",
      "[2/10][11/100][599] Loss_D: 0.002111 Loss_G: 0.001934 \n",
      "[2/10][11/100][600] Loss_D: 0.003499 Loss_G: 0.001551 \n",
      "[3/10][11/100][601] Loss_D: 0.001721 Loss_G: 0.002539 \n",
      "[3/10][11/100][602] Loss_D: 0.003890 Loss_G: 0.003504 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][11/100][603] Loss_D: 0.003806 Loss_G: 0.003470 \n",
      "[3/10][11/100][604] Loss_D: 0.004312 Loss_G: 0.002197 \n",
      "[3/10][11/100][605] Loss_D: 0.002454 Loss_G: 0.002310 \n",
      "[3/10][11/100][606] Loss_D: 0.002587 Loss_G: 0.004470 \n",
      "[3/10][11/100][607] Loss_D: 0.001683 Loss_G: 0.002261 \n",
      "[3/10][11/100][608] Loss_D: 0.004363 Loss_G: 0.003782 \n",
      "[3/10][11/100][609] Loss_D: 0.002086 Loss_G: 0.002401 \n",
      "[3/10][11/100][610] Loss_D: 0.001942 Loss_G: 0.003366 \n",
      "[3/10][11/100][611] Loss_D: 0.003219 Loss_G: 0.002925 \n",
      "[3/10][11/100][612] Loss_D: 0.002401 Loss_G: 0.003578 \n",
      "[3/10][11/100][613] Loss_D: 0.004321 Loss_G: 0.001694 \n",
      "[3/10][11/100][614] Loss_D: 0.003034 Loss_G: 0.002993 \n",
      "[3/10][11/100][615] Loss_D: 0.003584 Loss_G: 0.003321 \n",
      "[3/10][11/100][616] Loss_D: 0.003291 Loss_G: 0.004547 \n",
      "[3/10][11/100][617] Loss_D: 0.001638 Loss_G: 0.004049 \n",
      "[3/10][11/100][618] Loss_D: 0.003142 Loss_G: 0.003064 \n",
      "[3/10][11/100][619] Loss_D: 0.001670 Loss_G: 0.001645 \n",
      "[3/10][11/100][620] Loss_D: 0.002835 Loss_G: 0.003431 \n",
      "[3/10][11/100][621] Loss_D: 0.002571 Loss_G: 0.002533 \n",
      "[3/10][11/100][622] Loss_D: 0.002980 Loss_G: 0.002689 \n",
      "[3/10][11/100][623] Loss_D: 0.004374 Loss_G: 0.004879 \n",
      "[3/10][11/100][624] Loss_D: 0.003289 Loss_G: 0.002827 \n",
      "[3/10][11/100][625] Loss_D: 0.005083 Loss_G: 0.002794 \n",
      "[3/10][11/100][626] Loss_D: 0.004863 Loss_G: 0.003290 \n",
      "[3/10][11/100][627] Loss_D: 0.003427 Loss_G: 0.003259 \n",
      "[3/10][11/100][628] Loss_D: 0.002450 Loss_G: 0.004514 \n",
      "[3/10][11/100][629] Loss_D: 0.003307 Loss_G: 0.002931 \n",
      "[3/10][11/100][630] Loss_D: 0.002850 Loss_G: 0.005532 \n",
      "[3/10][11/100][631] Loss_D: 0.005657 Loss_G: 0.003139 \n",
      "[3/10][11/100][632] Loss_D: 0.005290 Loss_G: 0.003081 \n",
      "[3/10][11/100][633] Loss_D: 0.004327 Loss_G: 0.005390 \n",
      "[3/10][11/100][634] Loss_D: 0.005658 Loss_G: 0.003527 \n",
      "[3/10][11/100][635] Loss_D: 0.004582 Loss_G: 0.004501 \n",
      "[3/10][11/100][636] Loss_D: 0.001358 Loss_G: 0.005297 \n",
      "[3/10][11/100][637] Loss_D: 0.003448 Loss_G: 0.004864 \n",
      "[3/10][11/100][638] Loss_D: 0.003082 Loss_G: 0.004444 \n",
      "[3/10][11/100][639] Loss_D: 0.003431 Loss_G: 0.002922 \n",
      "[3/10][11/100][640] Loss_D: 0.004758 Loss_G: 0.002863 \n",
      "[3/10][11/100][641] Loss_D: 0.001601 Loss_G: 0.004642 \n",
      "[3/10][11/100][642] Loss_D: 0.002624 Loss_G: 0.004267 \n",
      "[3/10][11/100][643] Loss_D: 0.003982 Loss_G: 0.001890 \n",
      "[3/10][11/100][644] Loss_D: 0.003234 Loss_G: 0.003092 \n",
      "[3/10][11/100][645] Loss_D: 0.002772 Loss_G: 0.001016 \n",
      "[3/10][11/100][646] Loss_D: 0.003173 Loss_G: 0.002919 \n",
      "[3/10][11/100][647] Loss_D: 0.003393 Loss_G: 0.001941 \n",
      "[3/10][11/100][648] Loss_D: 0.002003 Loss_G: 0.002975 \n",
      "[3/10][11/100][649] Loss_D: 0.001776 Loss_G: 0.003808 \n",
      "[3/10][11/100][650] Loss_D: 0.001711 Loss_G: 0.003624 \n",
      "[3/10][11/100][651] Loss_D: 0.001913 Loss_G: 0.001258 \n",
      "[3/10][11/100][652] Loss_D: 0.000836 Loss_G: 0.001244 \n",
      "[3/10][11/100][653] Loss_D: 0.001694 Loss_G: 0.001592 \n",
      "[3/10][11/100][654] Loss_D: 0.002742 Loss_G: 0.003371 \n",
      "[3/10][11/100][655] Loss_D: 0.003449 Loss_G: 0.002585 \n",
      "[3/10][11/100][656] Loss_D: 0.002767 Loss_G: 0.003049 \n",
      "[3/10][11/100][657] Loss_D: 0.003423 Loss_G: 0.005776 \n",
      "[3/10][11/100][658] Loss_D: 0.002086 Loss_G: 0.001801 \n",
      "[3/10][11/100][659] Loss_D: 0.003821 Loss_G: 0.001945 \n",
      "[3/10][11/100][660] Loss_D: 0.002887 Loss_G: 0.002905 \n",
      "[3/10][11/100][661] Loss_D: 0.002595 Loss_G: 0.003548 \n",
      "[3/10][11/100][662] Loss_D: 0.000811 Loss_G: 0.002224 \n",
      "[3/10][11/100][663] Loss_D: 0.003490 Loss_G: 0.002829 \n",
      "[3/10][11/100][664] Loss_D: 0.003264 Loss_G: 0.001982 \n",
      "[3/10][11/100][665] Loss_D: 0.003737 Loss_G: 0.002790 \n",
      "[3/10][11/100][666] Loss_D: 0.001647 Loss_G: 0.001972 \n",
      "[3/10][11/100][667] Loss_D: 0.002549 Loss_G: 0.004017 \n",
      "[3/10][11/100][668] Loss_D: 0.002208 Loss_G: 0.003019 \n",
      "[3/10][11/100][669] Loss_D: 0.003111 Loss_G: 0.002564 \n",
      "[3/10][11/100][670] Loss_D: 0.002504 Loss_G: 0.003474 \n",
      "[3/10][11/100][671] Loss_D: 0.001847 Loss_G: 0.003105 \n",
      "[3/10][11/100][672] Loss_D: 0.004377 Loss_G: 0.002234 \n",
      "[3/10][11/100][673] Loss_D: 0.002072 Loss_G: 0.002767 \n",
      "[3/10][11/100][674] Loss_D: 0.002891 Loss_G: 0.004265 \n",
      "[3/10][11/100][675] Loss_D: 0.004017 Loss_G: 0.003153 \n",
      "[3/10][11/100][676] Loss_D: 0.003101 Loss_G: 0.004252 \n",
      "[3/10][11/100][677] Loss_D: 0.003754 Loss_G: 0.002318 \n",
      "[3/10][11/100][678] Loss_D: 0.003826 Loss_G: 0.002792 \n",
      "[3/10][11/100][679] Loss_D: 0.003574 Loss_G: 0.005242 \n",
      "[3/10][11/100][680] Loss_D: 0.004421 Loss_G: 0.004598 \n",
      "[3/10][11/100][681] Loss_D: 0.003929 Loss_G: 0.004677 \n",
      "[3/10][11/100][682] Loss_D: 0.004184 Loss_G: 0.003952 \n",
      "[3/10][11/100][683] Loss_D: 0.002768 Loss_G: 0.003901 \n",
      "[3/10][11/100][684] Loss_D: 0.004431 Loss_G: 0.004861 \n",
      "[3/10][11/100][685] Loss_D: 0.004069 Loss_G: 0.005440 \n",
      "[3/10][11/100][686] Loss_D: 0.002983 Loss_G: 0.004398 \n",
      "[3/10][11/100][687] Loss_D: 0.004224 Loss_G: 0.002397 \n",
      "[3/10][11/100][688] Loss_D: 0.003788 Loss_G: 0.005100 \n",
      "[3/10][11/100][689] Loss_D: 0.003936 Loss_G: 0.003749 \n",
      "[3/10][11/100][690] Loss_D: 0.005810 Loss_G: 0.003177 \n",
      "[3/10][11/100][691] Loss_D: 0.002587 Loss_G: 0.003967 \n",
      "[3/10][11/100][692] Loss_D: 0.003882 Loss_G: 0.005001 \n",
      "[3/10][11/100][693] Loss_D: 0.002708 Loss_G: 0.004734 \n",
      "[3/10][11/100][694] Loss_D: 0.005004 Loss_G: 0.003200 \n",
      "[3/10][11/100][695] Loss_D: 0.004091 Loss_G: 0.004442 \n",
      "[3/10][11/100][696] Loss_D: 0.004331 Loss_G: 0.002393 \n",
      "[3/10][11/100][697] Loss_D: 0.001214 Loss_G: 0.002703 \n",
      "[3/10][11/100][698] Loss_D: 0.001613 Loss_G: 0.003466 \n",
      "[3/10][11/100][699] Loss_D: 0.002510 Loss_G: 0.003787 \n",
      "[3/10][11/100][700] Loss_D: 0.002078 Loss_G: 0.002773 \n",
      "[3/10][11/100][701] Loss_D: 0.002924 Loss_G: 0.003281 \n",
      "[3/10][11/100][702] Loss_D: 0.002468 Loss_G: 0.002743 \n",
      "[3/10][11/100][703] Loss_D: 0.002875 Loss_G: 0.003043 \n",
      "[3/10][11/100][704] Loss_D: 0.002204 Loss_G: 0.002090 \n",
      "[3/10][11/100][705] Loss_D: 0.003778 Loss_G: 0.003015 \n",
      "[3/10][11/100][706] Loss_D: 0.003070 Loss_G: 0.002335 \n",
      "[3/10][11/100][707] Loss_D: 0.002847 Loss_G: 0.002789 \n",
      "[3/10][11/100][708] Loss_D: 0.002629 Loss_G: 0.002520 \n",
      "[3/10][11/100][709] Loss_D: 0.002942 Loss_G: 0.003355 \n",
      "[3/10][11/100][710] Loss_D: 0.002664 Loss_G: 0.002284 \n",
      "[3/10][11/100][711] Loss_D: 0.001675 Loss_G: 0.002448 \n",
      "[3/10][11/100][712] Loss_D: 0.002051 Loss_G: 0.002165 \n",
      "[3/10][11/100][713] Loss_D: 0.003421 Loss_G: 0.001973 \n",
      "[3/10][11/100][714] Loss_D: 0.002512 Loss_G: 0.003582 \n",
      "[3/10][11/100][715] Loss_D: 0.002027 Loss_G: 0.001753 \n",
      "[3/10][11/100][716] Loss_D: 0.004096 Loss_G: 0.003924 \n",
      "[3/10][11/100][717] Loss_D: 0.004471 Loss_G: 0.003257 \n",
      "[3/10][11/100][718] Loss_D: 0.003734 Loss_G: 0.003904 \n",
      "[3/10][11/100][719] Loss_D: 0.003566 Loss_G: 0.001825 \n",
      "[3/10][11/100][720] Loss_D: 0.002827 Loss_G: 0.002077 \n",
      "[3/10][11/100][721] Loss_D: 0.002037 Loss_G: 0.003019 \n",
      "[3/10][11/100][722] Loss_D: 0.003330 Loss_G: 0.004011 \n",
      "[3/10][11/100][723] Loss_D: 0.004454 Loss_G: 0.002094 \n",
      "[3/10][11/100][724] Loss_D: 0.002317 Loss_G: 0.002743 \n",
      "[3/10][11/100][725] Loss_D: 0.004196 Loss_G: 0.003372 \n",
      "[3/10][11/100][726] Loss_D: 0.001662 Loss_G: 0.004885 \n",
      "[3/10][11/100][727] Loss_D: 0.003593 Loss_G: 0.001521 \n",
      "[3/10][11/100][728] Loss_D: 0.002800 Loss_G: 0.004275 \n",
      "[3/10][11/100][729] Loss_D: 0.002264 Loss_G: 0.002338 \n",
      "[3/10][11/100][730] Loss_D: 0.001993 Loss_G: 0.001652 \n",
      "[3/10][11/100][731] Loss_D: 0.002196 Loss_G: 0.003021 \n",
      "[3/10][11/100][732] Loss_D: 0.002122 Loss_G: 0.001837 \n",
      "[3/10][11/100][733] Loss_D: 0.002873 Loss_G: 0.004343 \n",
      "[3/10][11/100][734] Loss_D: 0.002298 Loss_G: 0.003619 \n",
      "[3/10][11/100][735] Loss_D: 0.002851 Loss_G: 0.002994 \n",
      "[3/10][11/100][736] Loss_D: 0.004615 Loss_G: 0.002469 \n",
      "[3/10][11/100][737] Loss_D: 0.002711 Loss_G: 0.001680 \n",
      "[3/10][11/100][738] Loss_D: 0.003492 Loss_G: 0.002084 \n",
      "[3/10][11/100][739] Loss_D: 0.003248 Loss_G: 0.003138 \n",
      "[3/10][11/100][740] Loss_D: 0.003709 Loss_G: 0.002781 \n",
      "[3/10][11/100][741] Loss_D: 0.004022 Loss_G: 0.004415 \n",
      "[3/10][11/100][742] Loss_D: 0.004004 Loss_G: 0.003298 \n",
      "[3/10][11/100][743] Loss_D: 0.004609 Loss_G: 0.005137 \n",
      "[3/10][11/100][744] Loss_D: 0.003501 Loss_G: 0.004363 \n",
      "[3/10][11/100][745] Loss_D: 0.004511 Loss_G: 0.004227 \n",
      "[3/10][11/100][746] Loss_D: 0.004734 Loss_G: 0.004148 \n",
      "[3/10][11/100][747] Loss_D: 0.003858 Loss_G: 0.005119 \n",
      "[3/10][11/100][748] Loss_D: 0.003132 Loss_G: 0.004405 \n",
      "[3/10][11/100][749] Loss_D: 0.004312 Loss_G: 0.004265 \n",
      "[3/10][11/100][750] Loss_D: 0.003720 Loss_G: 0.005379 \n",
      "[3/10][11/100][751] Loss_D: 0.004903 Loss_G: 0.003529 \n",
      "[3/10][11/100][752] Loss_D: 0.003153 Loss_G: 0.002838 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][11/100][753] Loss_D: 0.003223 Loss_G: 0.003464 \n",
      "[3/10][11/100][754] Loss_D: 0.003760 Loss_G: 0.003489 \n",
      "[3/10][11/100][755] Loss_D: 0.002884 Loss_G: 0.003578 \n",
      "[3/10][11/100][756] Loss_D: 0.005358 Loss_G: 0.004040 \n",
      "[3/10][11/100][757] Loss_D: 0.003323 Loss_G: 0.004329 \n",
      "[3/10][11/100][758] Loss_D: 0.003952 Loss_G: 0.004590 \n",
      "[3/10][11/100][759] Loss_D: 0.001465 Loss_G: 0.000437 \n",
      "[3/10][11/100][760] Loss_D: 0.002631 Loss_G: 0.004141 \n",
      "[3/10][11/100][761] Loss_D: 0.003408 Loss_G: 0.001749 \n",
      "[3/10][11/100][762] Loss_D: 0.003063 Loss_G: 0.002749 \n",
      "[3/10][11/100][763] Loss_D: 0.004329 Loss_G: 0.003175 \n",
      "[3/10][11/100][764] Loss_D: 0.003329 Loss_G: 0.002402 \n",
      "[3/10][11/100][765] Loss_D: 0.003660 Loss_G: 0.003443 \n",
      "[3/10][11/100][766] Loss_D: 0.002806 Loss_G: 0.003586 \n",
      "[3/10][11/100][767] Loss_D: 0.001878 Loss_G: 0.002374 \n",
      "[3/10][11/100][768] Loss_D: 0.001956 Loss_G: 0.001958 \n",
      "[3/10][11/100][769] Loss_D: 0.004310 Loss_G: 0.002998 \n",
      "[3/10][11/100][770] Loss_D: 0.002216 Loss_G: 0.003905 \n",
      "[3/10][11/100][771] Loss_D: 0.001971 Loss_G: 0.002533 \n",
      "[3/10][11/100][772] Loss_D: 0.004378 Loss_G: 0.001335 \n",
      "[3/10][11/100][773] Loss_D: 0.001941 Loss_G: 0.005198 \n",
      "[3/10][11/100][774] Loss_D: 0.002305 Loss_G: 0.002977 \n",
      "[3/10][11/100][775] Loss_D: 0.002056 Loss_G: 0.003152 \n",
      "[3/10][11/100][776] Loss_D: 0.001883 Loss_G: 0.001880 \n",
      "[3/10][11/100][777] Loss_D: 0.003832 Loss_G: 0.002514 \n",
      "[3/10][11/100][778] Loss_D: 0.002105 Loss_G: 0.003070 \n",
      "[3/10][11/100][779] Loss_D: 0.003883 Loss_G: 0.003098 \n",
      "[3/10][11/100][780] Loss_D: 0.001722 Loss_G: 0.001826 \n",
      "[3/10][11/100][781] Loss_D: 0.002541 Loss_G: 0.003876 \n",
      "[3/10][11/100][782] Loss_D: 0.003162 Loss_G: 0.003072 \n",
      "[3/10][11/100][783] Loss_D: 0.002803 Loss_G: 0.001702 \n",
      "[3/10][11/100][784] Loss_D: 0.003093 Loss_G: 0.003477 \n",
      "[3/10][11/100][785] Loss_D: 0.005055 Loss_G: 0.002769 \n",
      "[3/10][11/100][786] Loss_D: 0.003046 Loss_G: 0.005025 \n",
      "[3/10][11/100][787] Loss_D: 0.002691 Loss_G: 0.004959 \n",
      "[3/10][11/100][788] Loss_D: 0.004124 Loss_G: 0.002024 \n",
      "[3/10][11/100][789] Loss_D: 0.003410 Loss_G: 0.002868 \n",
      "[3/10][11/100][790] Loss_D: 0.003239 Loss_G: 0.003527 \n",
      "[3/10][11/100][791] Loss_D: 0.004024 Loss_G: 0.003679 \n",
      "[3/10][11/100][792] Loss_D: 0.002689 Loss_G: 0.005075 \n",
      "[3/10][11/100][793] Loss_D: 0.004205 Loss_G: 0.003555 \n",
      "[3/10][11/100][794] Loss_D: 0.003172 Loss_G: 0.003476 \n",
      "[3/10][11/100][795] Loss_D: 0.003392 Loss_G: 0.003991 \n",
      "[3/10][11/100][796] Loss_D: 0.005148 Loss_G: 0.003525 \n",
      "[3/10][11/100][797] Loss_D: 0.003796 Loss_G: 0.004790 \n",
      "[3/10][11/100][798] Loss_D: 0.004601 Loss_G: 0.004778 \n",
      "[3/10][11/100][799] Loss_D: 0.002731 Loss_G: 0.004699 \n",
      "[3/10][11/100][800] Loss_D: 0.003923 Loss_G: 0.004192 \n",
      "[4/10][11/100][801] Loss_D: 0.003993 Loss_G: 0.004183 \n",
      "[4/10][11/100][802] Loss_D: 0.004913 Loss_G: 0.004574 \n",
      "[4/10][11/100][803] Loss_D: 0.005074 Loss_G: 0.004315 \n",
      "[4/10][11/100][804] Loss_D: 0.003957 Loss_G: 0.004024 \n",
      "[4/10][11/100][805] Loss_D: 0.003868 Loss_G: 0.003866 \n",
      "[4/10][11/100][806] Loss_D: 0.004195 Loss_G: 0.003782 \n",
      "[4/10][11/100][807] Loss_D: 0.003579 Loss_G: 0.002788 \n",
      "[4/10][11/100][808] Loss_D: 0.002621 Loss_G: 0.003494 \n",
      "[4/10][11/100][809] Loss_D: 0.002662 Loss_G: 0.002632 \n",
      "[4/10][11/100][810] Loss_D: 0.003327 Loss_G: 0.003475 \n",
      "[4/10][11/100][811] Loss_D: 0.002713 Loss_G: 0.002096 \n",
      "[4/10][11/100][812] Loss_D: 0.002994 Loss_G: 0.002664 \n",
      "[4/10][11/100][813] Loss_D: 0.003186 Loss_G: 0.003442 \n",
      "[4/10][11/100][814] Loss_D: 0.002668 Loss_G: 0.002106 \n",
      "[4/10][11/100][815] Loss_D: 0.002822 Loss_G: 0.002326 \n",
      "[4/10][11/100][816] Loss_D: 0.004064 Loss_G: 0.002745 \n",
      "[4/10][11/100][817] Loss_D: 0.002376 Loss_G: 0.002161 \n",
      "[4/10][11/100][818] Loss_D: 0.004205 Loss_G: 0.004326 \n",
      "[4/10][11/100][819] Loss_D: 0.002397 Loss_G: 0.001628 \n",
      "[4/10][11/100][820] Loss_D: 0.003489 Loss_G: 0.002216 \n",
      "[4/10][11/100][821] Loss_D: 0.002919 Loss_G: 0.003425 \n",
      "[4/10][11/100][822] Loss_D: 0.004711 Loss_G: 0.003697 \n",
      "[4/10][11/100][823] Loss_D: 0.002734 Loss_G: 0.003941 \n",
      "[4/10][11/100][824] Loss_D: 0.002385 Loss_G: 0.004702 \n",
      "[4/10][11/100][825] Loss_D: 0.005123 Loss_G: 0.003142 \n",
      "[4/10][11/100][826] Loss_D: 0.003257 Loss_G: 0.003440 \n",
      "[4/10][11/100][827] Loss_D: 0.002337 Loss_G: 0.004439 \n",
      "[4/10][11/100][828] Loss_D: 0.002616 Loss_G: 0.002373 \n",
      "[4/10][11/100][829] Loss_D: 0.004846 Loss_G: 0.002607 \n",
      "[4/10][11/100][830] Loss_D: 0.004827 Loss_G: 0.002628 \n",
      "[4/10][11/100][831] Loss_D: 0.003006 Loss_G: 0.002955 \n",
      "[4/10][11/100][832] Loss_D: 0.004208 Loss_G: 0.002688 \n",
      "[4/10][11/100][833] Loss_D: 0.001637 Loss_G: 0.003522 \n",
      "[4/10][11/100][834] Loss_D: 0.003090 Loss_G: 0.002144 \n",
      "[4/10][11/100][835] Loss_D: 0.004429 Loss_G: 0.002731 \n",
      "[4/10][11/100][836] Loss_D: 0.003162 Loss_G: 0.002881 \n",
      "[4/10][11/100][837] Loss_D: 0.002488 Loss_G: 0.003397 \n",
      "[4/10][11/100][838] Loss_D: 0.002823 Loss_G: 0.002278 \n",
      "[4/10][11/100][839] Loss_D: 0.002126 Loss_G: 0.002866 \n",
      "[4/10][11/100][840] Loss_D: 0.004068 Loss_G: 0.002855 \n",
      "[4/10][11/100][841] Loss_D: 0.001960 Loss_G: 0.002969 \n",
      "[4/10][11/100][842] Loss_D: 0.003397 Loss_G: 0.004671 \n",
      "[4/10][11/100][843] Loss_D: 0.003554 Loss_G: 0.002408 \n",
      "[4/10][11/100][844] Loss_D: 0.003865 Loss_G: 0.004460 \n",
      "[4/10][11/100][845] Loss_D: 0.003244 Loss_G: 0.003809 \n",
      "[4/10][11/100][846] Loss_D: 0.003934 Loss_G: 0.003545 \n",
      "[4/10][11/100][847] Loss_D: 0.002978 Loss_G: 0.004281 \n",
      "[4/10][11/100][848] Loss_D: 0.003972 Loss_G: 0.004473 \n",
      "[4/10][11/100][849] Loss_D: 0.004726 Loss_G: 0.003436 \n",
      "[4/10][11/100][850] Loss_D: 0.002693 Loss_G: 0.004855 \n",
      "[4/10][11/100][851] Loss_D: 0.003380 Loss_G: 0.005296 \n",
      "[4/10][11/100][852] Loss_D: 0.003207 Loss_G: 0.005260 \n",
      "[4/10][11/100][853] Loss_D: 0.004435 Loss_G: 0.003300 \n",
      "[4/10][11/100][854] Loss_D: 0.005069 Loss_G: 0.003759 \n",
      "[4/10][11/100][855] Loss_D: 0.003184 Loss_G: 0.004017 \n",
      "[4/10][11/100][856] Loss_D: 0.005277 Loss_G: 0.005137 \n",
      "[4/10][11/100][857] Loss_D: 0.004990 Loss_G: 0.003966 \n",
      "[4/10][11/100][858] Loss_D: 0.005254 Loss_G: 0.003082 \n",
      "[4/10][11/100][859] Loss_D: 0.004519 Loss_G: 0.003857 \n",
      "[4/10][11/100][860] Loss_D: 0.003782 Loss_G: 0.003542 \n",
      "[4/10][11/100][861] Loss_D: 0.004662 Loss_G: 0.005258 \n",
      "[4/10][11/100][862] Loss_D: 0.004478 Loss_G: 0.004163 \n",
      "[4/10][11/100][863] Loss_D: 0.004943 Loss_G: 0.005157 \n",
      "[4/10][11/100][864] Loss_D: 0.003930 Loss_G: 0.004092 \n",
      "[4/10][11/100][865] Loss_D: 0.002868 Loss_G: 0.003639 \n",
      "[4/10][11/100][866] Loss_D: 0.002946 Loss_G: 0.002545 \n",
      "[4/10][11/100][867] Loss_D: 0.003648 Loss_G: 0.003240 \n",
      "[4/10][11/100][868] Loss_D: 0.003160 Loss_G: 0.004490 \n",
      "[4/10][11/100][869] Loss_D: 0.003308 Loss_G: 0.002659 \n",
      "[4/10][11/100][870] Loss_D: 0.003074 Loss_G: 0.001748 \n",
      "[4/10][11/100][871] Loss_D: 0.002411 Loss_G: 0.003682 \n",
      "[4/10][11/100][872] Loss_D: 0.001973 Loss_G: 0.002150 \n",
      "[4/10][11/100][873] Loss_D: 0.002912 Loss_G: 0.002537 \n",
      "[4/10][11/100][874] Loss_D: 0.002801 Loss_G: 0.001844 \n",
      "[4/10][11/100][875] Loss_D: 0.002670 Loss_G: 0.003500 \n",
      "[4/10][11/100][876] Loss_D: 0.003297 Loss_G: 0.002847 \n",
      "[4/10][11/100][877] Loss_D: 0.003084 Loss_G: 0.001713 \n",
      "[4/10][11/100][878] Loss_D: 0.002900 Loss_G: 0.002750 \n",
      "[4/10][11/100][879] Loss_D: 0.002650 Loss_G: 0.002281 \n",
      "[4/10][11/100][880] Loss_D: 0.002743 Loss_G: 0.002677 \n",
      "[4/10][11/100][881] Loss_D: 0.003494 Loss_G: 0.003029 \n",
      "[4/10][11/100][882] Loss_D: 0.002466 Loss_G: 0.003925 \n",
      "[4/10][11/100][883] Loss_D: 0.002222 Loss_G: 0.003790 \n",
      "[4/10][11/100][884] Loss_D: 0.003003 Loss_G: 0.002304 \n",
      "[4/10][11/100][885] Loss_D: 0.002652 Loss_G: 0.003367 \n",
      "[4/10][11/100][886] Loss_D: 0.002772 Loss_G: 0.003202 \n",
      "[4/10][11/100][887] Loss_D: 0.003835 Loss_G: 0.002971 \n",
      "[4/10][11/100][888] Loss_D: 0.002550 Loss_G: 0.004820 \n",
      "[4/10][11/100][889] Loss_D: 0.002419 Loss_G: 0.002335 \n",
      "[4/10][11/100][890] Loss_D: 0.001722 Loss_G: 0.001567 \n",
      "[4/10][11/100][891] Loss_D: 0.003206 Loss_G: 0.003254 \n",
      "[4/10][11/100][892] Loss_D: 0.002521 Loss_G: 0.002861 \n",
      "[4/10][11/100][893] Loss_D: 0.004371 Loss_G: 0.003586 \n",
      "[4/10][11/100][894] Loss_D: 0.002440 Loss_G: 0.003596 \n",
      "[4/10][11/100][895] Loss_D: 0.002250 Loss_G: 0.003381 \n",
      "[4/10][11/100][896] Loss_D: 0.003805 Loss_G: 0.004053 \n",
      "[4/10][11/100][897] Loss_D: 0.003100 Loss_G: 0.002394 \n",
      "[4/10][11/100][898] Loss_D: 0.002096 Loss_G: 0.002884 \n",
      "[4/10][11/100][899] Loss_D: 0.004129 Loss_G: 0.003409 \n",
      "[4/10][11/100][900] Loss_D: 0.001901 Loss_G: 0.004112 \n",
      "[4/10][11/100][901] Loss_D: 0.002123 Loss_G: 0.003187 \n",
      "[4/10][11/100][902] Loss_D: 0.003278 Loss_G: 0.004152 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10][11/100][903] Loss_D: 0.003203 Loss_G: 0.004509 \n",
      "[4/10][11/100][904] Loss_D: 0.003070 Loss_G: 0.002912 \n",
      "[4/10][11/100][905] Loss_D: 0.002976 Loss_G: 0.003951 \n",
      "[4/10][11/100][906] Loss_D: 0.003830 Loss_G: 0.004532 \n",
      "[4/10][11/100][907] Loss_D: 0.003159 Loss_G: 0.003535 \n",
      "[4/10][11/100][908] Loss_D: 0.003722 Loss_G: 0.004807 \n",
      "[4/10][11/100][909] Loss_D: 0.006174 Loss_G: 0.004066 \n",
      "[4/10][11/100][910] Loss_D: 0.005075 Loss_G: 0.004289 \n",
      "[4/10][11/100][911] Loss_D: 0.005449 Loss_G: 0.004307 \n",
      "[4/10][11/100][912] Loss_D: 0.004555 Loss_G: 0.005070 \n",
      "[4/10][11/100][913] Loss_D: 0.005007 Loss_G: 0.004348 \n",
      "[4/10][11/100][914] Loss_D: 0.004912 Loss_G: 0.003295 \n",
      "[4/10][11/100][915] Loss_D: 0.003522 Loss_G: 0.003512 \n",
      "[4/10][11/100][916] Loss_D: 0.003724 Loss_G: 0.004621 \n",
      "[4/10][11/100][917] Loss_D: 0.003996 Loss_G: 0.003015 \n",
      "[4/10][11/100][918] Loss_D: 0.004107 Loss_G: 0.003592 \n",
      "[4/10][11/100][919] Loss_D: 0.004591 Loss_G: 0.003653 \n",
      "[4/10][11/100][920] Loss_D: 0.002532 Loss_G: 0.003366 \n",
      "[4/10][11/100][921] Loss_D: 0.003386 Loss_G: 0.003098 \n",
      "[4/10][11/100][922] Loss_D: 0.002220 Loss_G: 0.002926 \n",
      "[4/10][11/100][923] Loss_D: 0.003200 Loss_G: 0.003050 \n",
      "[4/10][11/100][924] Loss_D: 0.001137 Loss_G: 0.003357 \n",
      "[4/10][11/100][925] Loss_D: 0.003208 Loss_G: 0.002727 \n",
      "[4/10][11/100][926] Loss_D: 0.002993 Loss_G: 0.004400 \n",
      "[4/10][11/100][927] Loss_D: 0.003631 Loss_G: 0.002833 \n",
      "[4/10][11/100][928] Loss_D: 0.003494 Loss_G: 0.004449 \n",
      "[4/10][11/100][929] Loss_D: 0.004727 Loss_G: 0.003282 \n",
      "[4/10][11/100][930] Loss_D: 0.003011 Loss_G: 0.003728 \n",
      "[4/10][11/100][931] Loss_D: 0.003095 Loss_G: 0.002962 \n",
      "[4/10][11/100][932] Loss_D: 0.002947 Loss_G: 0.003438 \n",
      "[4/10][11/100][933] Loss_D: 0.003805 Loss_G: 0.002761 \n",
      "[4/10][11/100][934] Loss_D: 0.004330 Loss_G: 0.003675 \n",
      "[4/10][11/100][935] Loss_D: 0.004332 Loss_G: 0.002467 \n",
      "[4/10][11/100][936] Loss_D: 0.002413 Loss_G: 0.004846 \n",
      "[4/10][11/100][937] Loss_D: 0.001562 Loss_G: 0.003623 \n",
      "[4/10][11/100][938] Loss_D: 0.002374 Loss_G: 0.003409 \n",
      "[4/10][11/100][939] Loss_D: 0.002527 Loss_G: 0.002654 \n",
      "[4/10][11/100][940] Loss_D: 0.002171 Loss_G: 0.006215 \n",
      "[4/10][11/100][941] Loss_D: 0.003267 Loss_G: 0.004048 \n",
      "[4/10][11/100][942] Loss_D: 0.003409 Loss_G: 0.002150 \n",
      "[4/10][11/100][943] Loss_D: 0.002668 Loss_G: 0.003115 \n",
      "[4/10][11/100][944] Loss_D: 0.001726 Loss_G: 0.003646 \n",
      "[4/10][11/100][945] Loss_D: 0.001168 Loss_G: 0.001873 \n",
      "[4/10][11/100][946] Loss_D: 0.002487 Loss_G: 0.003455 \n",
      "[4/10][11/100][947] Loss_D: 0.004582 Loss_G: 0.002811 \n",
      "[4/10][11/100][948] Loss_D: 0.001919 Loss_G: 0.004003 \n",
      "[4/10][11/100][949] Loss_D: 0.003187 Loss_G: 0.003580 \n",
      "[4/10][11/100][950] Loss_D: 0.002096 Loss_G: 0.003404 \n",
      "[4/10][11/100][951] Loss_D: 0.001232 Loss_G: 0.002694 \n",
      "[4/10][11/100][952] Loss_D: 0.002908 Loss_G: 0.003092 \n",
      "[4/10][11/100][953] Loss_D: 0.003251 Loss_G: 0.003536 \n",
      "[4/10][11/100][954] Loss_D: 0.001870 Loss_G: 0.001947 \n",
      "[4/10][11/100][955] Loss_D: 0.002846 Loss_G: 0.002555 \n",
      "[4/10][11/100][956] Loss_D: 0.002568 Loss_G: 0.002901 \n",
      "[4/10][11/100][957] Loss_D: 0.003510 Loss_G: 0.002796 \n",
      "[4/10][11/100][958] Loss_D: 0.004010 Loss_G: 0.002420 \n",
      "[4/10][11/100][959] Loss_D: 0.002523 Loss_G: 0.004117 \n",
      "[4/10][11/100][960] Loss_D: 0.003609 Loss_G: 0.003645 \n",
      "[4/10][11/100][961] Loss_D: 0.002465 Loss_G: 0.002718 \n",
      "[4/10][11/100][962] Loss_D: 0.004889 Loss_G: 0.002611 \n",
      "[4/10][11/100][963] Loss_D: 0.003686 Loss_G: 0.003234 \n",
      "[4/10][11/100][964] Loss_D: 0.003435 Loss_G: 0.003805 \n",
      "[4/10][11/100][965] Loss_D: 0.004324 Loss_G: 0.002360 \n",
      "[4/10][11/100][966] Loss_D: 0.002977 Loss_G: 0.004605 \n",
      "[4/10][11/100][967] Loss_D: 0.004585 Loss_G: 0.002841 \n",
      "[4/10][11/100][968] Loss_D: 0.003091 Loss_G: 0.003275 \n",
      "[4/10][11/100][969] Loss_D: 0.003716 Loss_G: 0.002618 \n",
      "[4/10][11/100][970] Loss_D: 0.002813 Loss_G: 0.003108 \n",
      "[4/10][11/100][971] Loss_D: 0.002782 Loss_G: 0.003639 \n",
      "[4/10][11/100][972] Loss_D: 0.003890 Loss_G: 0.002646 \n",
      "[4/10][11/100][973] Loss_D: 0.003443 Loss_G: 0.003819 \n",
      "[4/10][11/100][974] Loss_D: 0.003106 Loss_G: 0.003028 \n",
      "[4/10][11/100][975] Loss_D: 0.003098 Loss_G: 0.002138 \n",
      "[4/10][11/100][976] Loss_D: 0.003720 Loss_G: 0.003619 \n",
      "[4/10][11/100][977] Loss_D: 0.003802 Loss_G: 0.003612 \n",
      "[4/10][11/100][978] Loss_D: 0.003190 Loss_G: 0.003346 \n",
      "[4/10][11/100][979] Loss_D: 0.005350 Loss_G: 0.002275 \n",
      "[4/10][11/100][980] Loss_D: 0.003026 Loss_G: 0.004179 \n",
      "[4/10][11/100][981] Loss_D: 0.003039 Loss_G: 0.002654 \n",
      "[4/10][11/100][982] Loss_D: 0.003465 Loss_G: 0.002424 \n",
      "[4/10][11/100][983] Loss_D: 0.002102 Loss_G: 0.003205 \n",
      "[4/10][11/100][984] Loss_D: 0.002795 Loss_G: 0.003989 \n",
      "[4/10][11/100][985] Loss_D: 0.001745 Loss_G: 0.004025 \n",
      "[4/10][11/100][986] Loss_D: 0.005288 Loss_G: 0.002183 \n",
      "[4/10][11/100][987] Loss_D: 0.002342 Loss_G: 0.002433 \n",
      "[4/10][11/100][988] Loss_D: 0.002822 Loss_G: 0.002055 \n",
      "[4/10][11/100][989] Loss_D: 0.003194 Loss_G: 0.003056 \n",
      "[4/10][11/100][990] Loss_D: 0.003936 Loss_G: 0.002549 \n",
      "[4/10][11/100][991] Loss_D: 0.004285 Loss_G: 0.005354 \n",
      "[4/10][11/100][992] Loss_D: 0.002249 Loss_G: 0.002653 \n",
      "[4/10][11/100][993] Loss_D: 0.001664 Loss_G: 0.004002 \n",
      "[4/10][11/100][994] Loss_D: 0.004804 Loss_G: 0.003954 \n",
      "[4/10][11/100][995] Loss_D: 0.003322 Loss_G: 0.002362 \n",
      "[4/10][11/100][996] Loss_D: 0.002007 Loss_G: 0.003814 \n",
      "[4/10][11/100][997] Loss_D: 0.003831 Loss_G: 0.003005 \n",
      "[4/10][11/100][998] Loss_D: 0.002977 Loss_G: 0.002997 \n",
      "[4/10][11/100][999] Loss_D: 0.003995 Loss_G: 0.003193 \n",
      "[4/10][11/100][1000] Loss_D: 0.003128 Loss_G: 0.002882 \n",
      "[5/10][11/100][1001] Loss_D: 0.001952 Loss_G: 0.002483 \n",
      "[5/10][11/100][1002] Loss_D: 0.003046 Loss_G: 0.003774 \n",
      "[5/10][11/100][1003] Loss_D: 0.002426 Loss_G: 0.001416 \n",
      "[5/10][11/100][1004] Loss_D: 0.003845 Loss_G: 0.002493 \n",
      "[5/10][11/100][1005] Loss_D: 0.004965 Loss_G: 0.001882 \n",
      "[5/10][11/100][1006] Loss_D: 0.004615 Loss_G: 0.004072 \n",
      "[5/10][11/100][1007] Loss_D: 0.002074 Loss_G: 0.003393 \n",
      "[5/10][11/100][1008] Loss_D: 0.004005 Loss_G: 0.003123 \n",
      "[5/10][11/100][1009] Loss_D: 0.004282 Loss_G: 0.003338 \n",
      "[5/10][11/100][1010] Loss_D: 0.003852 Loss_G: 0.004531 \n",
      "[5/10][11/100][1011] Loss_D: 0.002955 Loss_G: 0.002506 \n",
      "[5/10][11/100][1012] Loss_D: 0.004775 Loss_G: 0.005551 \n",
      "[5/10][11/100][1013] Loss_D: 0.003636 Loss_G: 0.003041 \n",
      "[5/10][11/100][1014] Loss_D: 0.003698 Loss_G: 0.003929 \n",
      "[5/10][11/100][1015] Loss_D: 0.003339 Loss_G: 0.002913 \n",
      "[5/10][11/100][1016] Loss_D: 0.002453 Loss_G: 0.004458 \n",
      "[5/10][11/100][1017] Loss_D: 0.003313 Loss_G: 0.002744 \n",
      "[5/10][11/100][1018] Loss_D: 0.002771 Loss_G: 0.003392 \n",
      "[5/10][11/100][1019] Loss_D: 0.002939 Loss_G: 0.002129 \n",
      "[5/10][11/100][1020] Loss_D: 0.002855 Loss_G: 0.003609 \n",
      "[5/10][11/100][1021] Loss_D: 0.003714 Loss_G: 0.003717 \n",
      "[5/10][11/100][1022] Loss_D: 0.002835 Loss_G: 0.003405 \n",
      "[5/10][11/100][1023] Loss_D: 0.003458 Loss_G: 0.002338 \n",
      "[5/10][11/100][1024] Loss_D: 0.003682 Loss_G: 0.001664 \n",
      "[5/10][11/100][1025] Loss_D: 0.003338 Loss_G: 0.002136 \n",
      "[5/10][11/100][1026] Loss_D: 0.002351 Loss_G: 0.001687 \n",
      "[5/10][11/100][1027] Loss_D: 0.002865 Loss_G: 0.003996 \n",
      "[5/10][11/100][1028] Loss_D: 0.002157 Loss_G: 0.002586 \n",
      "[5/10][11/100][1029] Loss_D: 0.003180 Loss_G: 0.001176 \n",
      "[5/10][11/100][1030] Loss_D: 0.003368 Loss_G: 0.003310 \n",
      "[5/10][11/100][1031] Loss_D: 0.001472 Loss_G: 0.002821 \n",
      "[5/10][11/100][1032] Loss_D: 0.003514 Loss_G: 0.002427 \n",
      "[5/10][11/100][1033] Loss_D: 0.002664 Loss_G: 0.002987 \n",
      "[5/10][11/100][1034] Loss_D: 0.002583 Loss_G: 0.001754 \n",
      "[5/10][11/100][1035] Loss_D: 0.003306 Loss_G: 0.001643 \n",
      "[5/10][11/100][1036] Loss_D: 0.002337 Loss_G: 0.001768 \n",
      "[5/10][11/100][1037] Loss_D: 0.002073 Loss_G: 0.003277 \n",
      "[5/10][11/100][1038] Loss_D: 0.002638 Loss_G: 0.004598 \n",
      "[5/10][11/100][1039] Loss_D: 0.002084 Loss_G: 0.001470 \n",
      "[5/10][11/100][1040] Loss_D: 0.002403 Loss_G: 0.002586 \n",
      "[5/10][11/100][1041] Loss_D: 0.004280 Loss_G: 0.002342 \n",
      "[5/10][11/100][1042] Loss_D: 0.002226 Loss_G: 0.003858 \n",
      "[5/10][11/100][1043] Loss_D: 0.002699 Loss_G: 0.002223 \n",
      "[5/10][11/100][1044] Loss_D: 0.002706 Loss_G: 0.003202 \n",
      "[5/10][11/100][1045] Loss_D: 0.004617 Loss_G: 0.002675 \n",
      "[5/10][11/100][1046] Loss_D: 0.002751 Loss_G: 0.003303 \n",
      "[5/10][11/100][1047] Loss_D: 0.002931 Loss_G: 0.004860 \n",
      "[5/10][11/100][1048] Loss_D: 0.003139 Loss_G: 0.003125 \n",
      "[5/10][11/100][1049] Loss_D: 0.004430 Loss_G: 0.004484 \n",
      "[5/10][11/100][1050] Loss_D: 0.004072 Loss_G: 0.003373 \n",
      "[5/10][11/100][1051] Loss_D: 0.005055 Loss_G: 0.003915 \n",
      "[5/10][11/100][1052] Loss_D: 0.005883 Loss_G: 0.005593 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10][11/100][1053] Loss_D: 0.001874 Loss_G: 0.003102 \n",
      "[5/10][11/100][1054] Loss_D: 0.004117 Loss_G: 0.004005 \n",
      "[5/10][11/100][1055] Loss_D: 0.004209 Loss_G: 0.002813 \n",
      "[5/10][11/100][1056] Loss_D: 0.002867 Loss_G: 0.002357 \n",
      "[5/10][11/100][1057] Loss_D: 0.003532 Loss_G: 0.003963 \n",
      "[5/10][11/100][1058] Loss_D: 0.003119 Loss_G: 0.004137 \n",
      "[5/10][11/100][1059] Loss_D: 0.005322 Loss_G: 0.002599 \n",
      "[5/10][11/100][1060] Loss_D: 0.003540 Loss_G: 0.002226 \n",
      "[5/10][11/100][1061] Loss_D: 0.003813 Loss_G: 0.002406 \n",
      "[5/10][11/100][1062] Loss_D: 0.002782 Loss_G: 0.004986 \n",
      "[5/10][11/100][1063] Loss_D: 0.004640 Loss_G: 0.004368 \n",
      "[5/10][11/100][1064] Loss_D: 0.002459 Loss_G: 0.001296 \n",
      "[5/10][11/100][1065] Loss_D: 0.003797 Loss_G: 0.003085 \n",
      "[5/10][11/100][1066] Loss_D: 0.001457 Loss_G: 0.002723 \n",
      "[5/10][11/100][1067] Loss_D: 0.002488 Loss_G: 0.002980 \n",
      "[5/10][11/100][1068] Loss_D: 0.003246 Loss_G: 0.002701 \n",
      "[5/10][11/100][1069] Loss_D: 0.001494 Loss_G: 0.003149 \n",
      "[5/10][11/100][1070] Loss_D: 0.002596 Loss_G: 0.002162 \n",
      "[5/10][11/100][1071] Loss_D: 0.002905 Loss_G: 0.003399 \n",
      "[5/10][11/100][1072] Loss_D: 0.003042 Loss_G: 0.003623 \n",
      "[5/10][11/100][1073] Loss_D: 0.003517 Loss_G: 0.002992 \n",
      "[5/10][11/100][1074] Loss_D: 0.003423 Loss_G: 0.001342 \n",
      "[5/10][11/100][1075] Loss_D: 0.002743 Loss_G: 0.001216 \n",
      "[5/10][11/100][1076] Loss_D: 0.005023 Loss_G: 0.003623 \n",
      "[5/10][11/100][1077] Loss_D: 0.002510 Loss_G: 0.001409 \n",
      "[5/10][11/100][1078] Loss_D: 0.002960 Loss_G: 0.002747 \n",
      "[5/10][11/100][1079] Loss_D: 0.004217 Loss_G: 0.002838 \n",
      "[5/10][11/100][1080] Loss_D: 0.002544 Loss_G: 0.002085 \n",
      "[5/10][11/100][1081] Loss_D: 0.002694 Loss_G: 0.002670 \n",
      "[5/10][11/100][1082] Loss_D: 0.002827 Loss_G: 0.002818 \n",
      "[5/10][11/100][1083] Loss_D: 0.002614 Loss_G: 0.002909 \n",
      "[5/10][11/100][1084] Loss_D: 0.003342 Loss_G: 0.001699 \n",
      "[5/10][11/100][1085] Loss_D: 0.003786 Loss_G: 0.002284 \n",
      "[5/10][11/100][1086] Loss_D: 0.004482 Loss_G: 0.003485 \n",
      "[5/10][11/100][1087] Loss_D: 0.002185 Loss_G: 0.004218 \n",
      "[5/10][11/100][1088] Loss_D: 0.002342 Loss_G: 0.002478 \n",
      "[5/10][11/100][1089] Loss_D: 0.003941 Loss_G: 0.001770 \n",
      "[5/10][11/100][1090] Loss_D: 0.002451 Loss_G: 0.003683 \n",
      "[5/10][11/100][1091] Loss_D: 0.001940 Loss_G: 0.004309 \n",
      "[5/10][11/100][1092] Loss_D: 0.003186 Loss_G: 0.002236 \n",
      "[5/10][11/100][1093] Loss_D: 0.002761 Loss_G: 0.001633 \n",
      "[5/10][11/100][1094] Loss_D: 0.004661 Loss_G: 0.002775 \n",
      "[5/10][11/100][1095] Loss_D: 0.002777 Loss_G: 0.002678 \n",
      "[5/10][11/100][1096] Loss_D: 0.003160 Loss_G: 0.002457 \n",
      "[5/10][11/100][1097] Loss_D: 0.002424 Loss_G: 0.001464 \n",
      "[5/10][11/100][1098] Loss_D: 0.001715 Loss_G: 0.002691 \n",
      "[5/10][11/100][1099] Loss_D: 0.003240 Loss_G: 0.001776 \n",
      "[5/10][11/100][1100] Loss_D: 0.001936 Loss_G: 0.002445 \n",
      "[5/10][11/100][1101] Loss_D: 0.003499 Loss_G: 0.002041 \n",
      "[5/10][11/100][1102] Loss_D: 0.001769 Loss_G: 0.003806 \n",
      "[5/10][11/100][1103] Loss_D: 0.002762 Loss_G: 0.002100 \n",
      "[5/10][11/100][1104] Loss_D: 0.002634 Loss_G: 0.003288 \n",
      "[5/10][11/100][1105] Loss_D: 0.001979 Loss_G: 0.002336 \n",
      "[5/10][11/100][1106] Loss_D: 0.003530 Loss_G: 0.002265 \n",
      "[5/10][11/100][1107] Loss_D: 0.003628 Loss_G: 0.001859 \n",
      "[5/10][11/100][1108] Loss_D: 0.002856 Loss_G: 0.001883 \n",
      "[5/10][11/100][1109] Loss_D: 0.003580 Loss_G: 0.001513 \n",
      "[5/10][11/100][1110] Loss_D: 0.004773 Loss_G: 0.002812 \n",
      "[5/10][11/100][1111] Loss_D: 0.001945 Loss_G: 0.002680 \n",
      "[5/10][11/100][1112] Loss_D: 0.001073 Loss_G: 0.001386 \n",
      "[5/10][11/100][1113] Loss_D: 0.004874 Loss_G: 0.002695 \n",
      "[5/10][11/100][1114] Loss_D: 0.001870 Loss_G: 0.001935 \n",
      "[5/10][11/100][1115] Loss_D: 0.002297 Loss_G: 0.002219 \n",
      "[5/10][11/100][1116] Loss_D: 0.001204 Loss_G: 0.001905 \n",
      "[5/10][11/100][1117] Loss_D: 0.003390 Loss_G: 0.004465 \n",
      "[5/10][11/100][1118] Loss_D: 0.003246 Loss_G: 0.002736 \n",
      "[5/10][11/100][1119] Loss_D: 0.002294 Loss_G: 0.001409 \n",
      "[5/10][11/100][1120] Loss_D: 0.002679 Loss_G: 0.002068 \n",
      "[5/10][11/100][1121] Loss_D: 0.002626 Loss_G: 0.002538 \n",
      "[5/10][11/100][1122] Loss_D: 0.002265 Loss_G: 0.003758 \n",
      "[5/10][11/100][1123] Loss_D: 0.003749 Loss_G: 0.002304 \n",
      "[5/10][11/100][1124] Loss_D: 0.003519 Loss_G: 0.003601 \n",
      "[5/10][11/100][1125] Loss_D: 0.002157 Loss_G: 0.004413 \n",
      "[5/10][11/100][1126] Loss_D: 0.005122 Loss_G: 0.003517 \n",
      "[5/10][11/100][1127] Loss_D: 0.005456 Loss_G: 0.003306 \n",
      "[5/10][11/100][1128] Loss_D: 0.002667 Loss_G: 0.003159 \n",
      "[5/10][11/100][1129] Loss_D: 0.003841 Loss_G: 0.003982 \n",
      "[5/10][11/100][1130] Loss_D: 0.002259 Loss_G: 0.003020 \n",
      "[5/10][11/100][1131] Loss_D: 0.002218 Loss_G: 0.002691 \n",
      "[5/10][11/100][1132] Loss_D: 0.002684 Loss_G: 0.002864 \n",
      "[5/10][11/100][1133] Loss_D: 0.003099 Loss_G: 0.002876 \n",
      "[5/10][11/100][1134] Loss_D: 0.002531 Loss_G: 0.002254 \n",
      "[5/10][11/100][1135] Loss_D: 0.003672 Loss_G: 0.002971 \n",
      "[5/10][11/100][1136] Loss_D: 0.001672 Loss_G: 0.002637 \n",
      "[5/10][11/100][1137] Loss_D: 0.001919 Loss_G: 0.001630 \n",
      "[5/10][11/100][1138] Loss_D: 0.002481 Loss_G: 0.002137 \n",
      "[5/10][11/100][1139] Loss_D: 0.001608 Loss_G: 0.002067 \n",
      "[5/10][11/100][1140] Loss_D: 0.002819 Loss_G: 0.001462 \n",
      "[5/10][11/100][1141] Loss_D: 0.003698 Loss_G: 0.005046 \n",
      "[5/10][11/100][1142] Loss_D: 0.004107 Loss_G: 0.002926 \n",
      "[5/10][11/100][1143] Loss_D: 0.002815 Loss_G: 0.002038 \n",
      "[5/10][11/100][1144] Loss_D: 0.003071 Loss_G: 0.003813 \n",
      "[5/10][11/100][1145] Loss_D: 0.003618 Loss_G: 0.002413 \n",
      "[5/10][11/100][1146] Loss_D: 0.003094 Loss_G: 0.003612 \n",
      "[5/10][11/100][1147] Loss_D: 0.003524 Loss_G: 0.005211 \n",
      "[5/10][11/100][1148] Loss_D: 0.004079 Loss_G: 0.002095 \n",
      "[5/10][11/100][1149] Loss_D: 0.004218 Loss_G: 0.003136 \n",
      "[5/10][11/100][1150] Loss_D: 0.002780 Loss_G: 0.003571 \n",
      "[5/10][11/100][1151] Loss_D: 0.000966 Loss_G: 0.003429 \n",
      "[5/10][11/100][1152] Loss_D: 0.003480 Loss_G: 0.003331 \n",
      "[5/10][11/100][1153] Loss_D: 0.004168 Loss_G: 0.002097 \n",
      "[5/10][11/100][1154] Loss_D: 0.002603 Loss_G: 0.002352 \n",
      "[5/10][11/100][1155] Loss_D: 0.002807 Loss_G: 0.001875 \n",
      "[5/10][11/100][1156] Loss_D: 0.005024 Loss_G: 0.002297 \n",
      "[5/10][11/100][1157] Loss_D: 0.003528 Loss_G: 0.002849 \n",
      "[5/10][11/100][1158] Loss_D: 0.003778 Loss_G: 0.002811 \n",
      "[5/10][11/100][1159] Loss_D: 0.002289 Loss_G: 0.001704 \n",
      "[5/10][11/100][1160] Loss_D: 0.002717 Loss_G: 0.001837 \n",
      "[5/10][11/100][1161] Loss_D: 0.001665 Loss_G: 0.002722 \n",
      "[5/10][11/100][1162] Loss_D: 0.001919 Loss_G: 0.001298 \n",
      "[5/10][11/100][1163] Loss_D: 0.002403 Loss_G: 0.001781 \n",
      "[5/10][11/100][1164] Loss_D: 0.003828 Loss_G: 0.004171 \n",
      "[5/10][11/100][1165] Loss_D: 0.005486 Loss_G: 0.005241 \n",
      "[5/10][11/100][1166] Loss_D: 0.007076 Loss_G: 0.005698 \n",
      "[5/10][11/100][1167] Loss_D: 0.007706 Loss_G: 0.007143 \n",
      "[5/10][11/100][1168] Loss_D: 0.006519 Loss_G: 0.006848 \n",
      "[5/10][11/100][1169] Loss_D: 0.004744 Loss_G: 0.005239 \n",
      "[5/10][11/100][1170] Loss_D: 0.004366 Loss_G: 0.007076 \n",
      "[5/10][11/100][1171] Loss_D: 0.002086 Loss_G: 0.002783 \n",
      "[5/10][11/100][1172] Loss_D: 0.004191 Loss_G: 0.005288 \n",
      "[5/10][11/100][1173] Loss_D: 0.003743 Loss_G: 0.002487 \n",
      "[5/10][11/100][1174] Loss_D: 0.001878 Loss_G: 0.004430 \n",
      "[5/10][11/100][1175] Loss_D: 0.002586 Loss_G: 0.002990 \n",
      "[5/10][11/100][1176] Loss_D: 0.002253 Loss_G: 0.002264 \n",
      "[5/10][11/100][1177] Loss_D: 0.002964 Loss_G: 0.001499 \n",
      "[5/10][11/100][1178] Loss_D: 0.002183 Loss_G: 0.003159 \n",
      "[5/10][11/100][1179] Loss_D: 0.000612 Loss_G: 0.002349 \n",
      "[5/10][11/100][1180] Loss_D: 0.000397 Loss_G: 0.002073 \n",
      "[5/10][11/100][1181] Loss_D: 0.001084 Loss_G: 0.001821 \n",
      "[5/10][11/100][1182] Loss_D: 0.003469 Loss_G: 0.001989 \n",
      "[5/10][11/100][1183] Loss_D: 0.004843 Loss_G: 0.002670 \n",
      "[5/10][11/100][1184] Loss_D: 0.002105 Loss_G: 0.003623 \n",
      "[5/10][11/100][1185] Loss_D: 0.004473 Loss_G: 0.002415 \n",
      "[5/10][11/100][1186] Loss_D: 0.004343 Loss_G: 0.002322 \n",
      "[5/10][11/100][1187] Loss_D: 0.006061 Loss_G: 0.003758 \n",
      "[5/10][11/100][1188] Loss_D: 0.002107 Loss_G: 0.003556 \n",
      "[5/10][11/100][1189] Loss_D: 0.002505 Loss_G: 0.003274 \n",
      "[5/10][11/100][1190] Loss_D: 0.004358 Loss_G: 0.005208 \n",
      "[5/10][11/100][1191] Loss_D: 0.002618 Loss_G: 0.005094 \n",
      "[5/10][11/100][1192] Loss_D: 0.005480 Loss_G: 0.002773 \n",
      "[5/10][11/100][1193] Loss_D: 0.005761 Loss_G: 0.002901 \n",
      "[5/10][11/100][1194] Loss_D: 0.002614 Loss_G: 0.004852 \n",
      "[5/10][11/100][1195] Loss_D: 0.003426 Loss_G: 0.003439 \n",
      "[5/10][11/100][1196] Loss_D: 0.002996 Loss_G: 0.003530 \n",
      "[5/10][11/100][1197] Loss_D: 0.002656 Loss_G: 0.004269 \n",
      "[5/10][11/100][1198] Loss_D: 0.002943 Loss_G: 0.001680 \n",
      "[5/10][11/100][1199] Loss_D: 0.003144 Loss_G: 0.002881 \n",
      "[5/10][11/100][1200] Loss_D: 0.004189 Loss_G: 0.001774 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][11/100][1201] Loss_D: 0.002471 Loss_G: 0.004180 \n",
      "[6/10][11/100][1202] Loss_D: 0.002219 Loss_G: 0.002348 \n",
      "[6/10][11/100][1203] Loss_D: 0.002914 Loss_G: 0.002865 \n",
      "[6/10][11/100][1204] Loss_D: 0.003758 Loss_G: 0.002435 \n",
      "[6/10][11/100][1205] Loss_D: 0.003467 Loss_G: 0.003599 \n",
      "[6/10][11/100][1206] Loss_D: 0.003510 Loss_G: 0.003689 \n",
      "[6/10][11/100][1207] Loss_D: 0.004559 Loss_G: 0.003061 \n",
      "[6/10][11/100][1208] Loss_D: 0.003526 Loss_G: 0.003992 \n",
      "[6/10][11/100][1209] Loss_D: 0.003307 Loss_G: 0.002821 \n",
      "[6/10][11/100][1210] Loss_D: 0.003016 Loss_G: 0.003798 \n",
      "[6/10][11/100][1211] Loss_D: 0.004131 Loss_G: 0.003578 \n",
      "[6/10][11/100][1212] Loss_D: 0.003097 Loss_G: 0.003580 \n",
      "[6/10][11/100][1213] Loss_D: 0.003622 Loss_G: 0.006106 \n",
      "[6/10][11/100][1214] Loss_D: 0.003020 Loss_G: 0.003851 \n",
      "[6/10][11/100][1215] Loss_D: 0.002959 Loss_G: 0.004514 \n",
      "[6/10][11/100][1216] Loss_D: 0.002693 Loss_G: 0.004617 \n",
      "[6/10][11/100][1217] Loss_D: 0.004667 Loss_G: 0.005040 \n",
      "[6/10][11/100][1218] Loss_D: 0.003311 Loss_G: 0.003963 \n",
      "[6/10][11/100][1219] Loss_D: 0.004127 Loss_G: 0.005506 \n",
      "[6/10][11/100][1220] Loss_D: 0.005150 Loss_G: 0.004973 \n",
      "[6/10][11/100][1221] Loss_D: 0.005209 Loss_G: 0.004697 \n",
      "[6/10][11/100][1222] Loss_D: 0.004654 Loss_G: 0.002875 \n",
      "[6/10][11/100][1223] Loss_D: 0.004209 Loss_G: 0.002804 \n",
      "[6/10][11/100][1224] Loss_D: 0.003831 Loss_G: 0.004063 \n",
      "[6/10][11/100][1225] Loss_D: 0.003861 Loss_G: 0.001811 \n",
      "[6/10][11/100][1226] Loss_D: 0.002635 Loss_G: 0.002431 \n",
      "[6/10][11/100][1227] Loss_D: 0.001173 Loss_G: 0.001083 \n",
      "[6/10][11/100][1228] Loss_D: 0.001891 Loss_G: 0.002081 \n",
      "[6/10][11/100][1229] Loss_D: 0.003507 Loss_G: 0.002540 \n",
      "[6/10][11/100][1230] Loss_D: 0.003054 Loss_G: 0.003547 \n",
      "[6/10][11/100][1231] Loss_D: 0.002103 Loss_G: 0.001702 \n",
      "[6/10][11/100][1232] Loss_D: -0.000390 Loss_G: 0.002243 \n",
      "[6/10][11/100][1233] Loss_D: 0.002727 Loss_G: 0.003845 \n",
      "[6/10][11/100][1234] Loss_D: 0.003949 Loss_G: 0.004962 \n",
      "[6/10][11/100][1235] Loss_D: 0.001798 Loss_G: 0.003935 \n",
      "[6/10][11/100][1236] Loss_D: 0.004177 Loss_G: 0.001894 \n",
      "[6/10][11/100][1237] Loss_D: 0.000840 Loss_G: 0.000468 \n",
      "[6/10][11/100][1238] Loss_D: 0.002229 Loss_G: 0.002341 \n",
      "[6/10][11/100][1239] Loss_D: 0.002630 Loss_G: 0.002486 \n",
      "[6/10][11/100][1240] Loss_D: 0.002782 Loss_G: 0.002428 \n",
      "[6/10][11/100][1241] Loss_D: 0.002440 Loss_G: 0.003161 \n",
      "[6/10][11/100][1242] Loss_D: 0.003118 Loss_G: 0.003253 \n",
      "[6/10][11/100][1243] Loss_D: 0.004361 Loss_G: 0.002413 \n",
      "[6/10][11/100][1244] Loss_D: 0.003979 Loss_G: 0.003913 \n",
      "[6/10][11/100][1245] Loss_D: 0.002166 Loss_G: 0.002479 \n",
      "[6/10][11/100][1246] Loss_D: 0.002313 Loss_G: 0.001680 \n",
      "[6/10][11/100][1247] Loss_D: 0.001884 Loss_G: 0.002489 \n",
      "[6/10][11/100][1248] Loss_D: 0.003140 Loss_G: 0.004203 \n",
      "[6/10][11/100][1249] Loss_D: 0.003680 Loss_G: 0.003476 \n",
      "[6/10][11/100][1250] Loss_D: 0.003138 Loss_G: 0.003361 \n",
      "[6/10][11/100][1251] Loss_D: 0.002458 Loss_G: 0.004870 \n",
      "[6/10][11/100][1252] Loss_D: 0.003313 Loss_G: 0.004038 \n",
      "[6/10][11/100][1253] Loss_D: 0.003174 Loss_G: 0.003237 \n",
      "[6/10][11/100][1254] Loss_D: 0.004695 Loss_G: 0.002466 \n",
      "[6/10][11/100][1255] Loss_D: 0.002603 Loss_G: 0.002391 \n",
      "[6/10][11/100][1256] Loss_D: 0.004368 Loss_G: 0.004791 \n",
      "[6/10][11/100][1257] Loss_D: 0.003623 Loss_G: 0.002611 \n",
      "[6/10][11/100][1258] Loss_D: 0.003798 Loss_G: 0.004410 \n",
      "[6/10][11/100][1259] Loss_D: 0.003683 Loss_G: 0.003263 \n",
      "[6/10][11/100][1260] Loss_D: 0.003365 Loss_G: 0.002988 \n",
      "[6/10][11/100][1261] Loss_D: 0.004407 Loss_G: 0.003158 \n",
      "[6/10][11/100][1262] Loss_D: 0.003835 Loss_G: 0.004277 \n",
      "[6/10][11/100][1263] Loss_D: 0.004779 Loss_G: 0.002780 \n",
      "[6/10][11/100][1264] Loss_D: 0.004878 Loss_G: 0.004171 \n",
      "[6/10][11/100][1265] Loss_D: 0.004803 Loss_G: 0.003715 \n",
      "[6/10][11/100][1266] Loss_D: 0.002963 Loss_G: 0.005176 \n",
      "[6/10][11/100][1267] Loss_D: 0.003181 Loss_G: 0.004216 \n",
      "[6/10][11/100][1268] Loss_D: 0.003349 Loss_G: 0.004867 \n",
      "[6/10][11/100][1269] Loss_D: 0.003357 Loss_G: 0.002888 \n",
      "[6/10][11/100][1270] Loss_D: 0.003211 Loss_G: 0.003422 \n",
      "[6/10][11/100][1271] Loss_D: 0.003413 Loss_G: 0.003643 \n",
      "[6/10][11/100][1272] Loss_D: 0.005044 Loss_G: 0.003269 \n",
      "[6/10][11/100][1273] Loss_D: 0.004124 Loss_G: 0.003371 \n",
      "[6/10][11/100][1274] Loss_D: 0.001629 Loss_G: 0.005760 \n",
      "[6/10][11/100][1275] Loss_D: 0.004517 Loss_G: 0.003608 \n",
      "[6/10][11/100][1276] Loss_D: 0.004363 Loss_G: 0.004192 \n",
      "[6/10][11/100][1277] Loss_D: 0.002623 Loss_G: 0.003341 \n",
      "[6/10][11/100][1278] Loss_D: 0.003401 Loss_G: 0.003485 \n",
      "[6/10][11/100][1279] Loss_D: 0.003036 Loss_G: 0.002780 \n",
      "[6/10][11/100][1280] Loss_D: 0.003342 Loss_G: 0.005383 \n",
      "[6/10][11/100][1281] Loss_D: 0.001721 Loss_G: 0.002701 \n",
      "[6/10][11/100][1282] Loss_D: 0.001982 Loss_G: 0.002676 \n",
      "[6/10][11/100][1283] Loss_D: 0.001977 Loss_G: 0.002256 \n",
      "[6/10][11/100][1284] Loss_D: 0.001856 Loss_G: 0.003964 \n",
      "[6/10][11/100][1285] Loss_D: 0.002852 Loss_G: 0.002012 \n",
      "[6/10][11/100][1286] Loss_D: 0.002497 Loss_G: 0.002453 \n",
      "[6/10][11/100][1287] Loss_D: 0.002209 Loss_G: 0.002452 \n",
      "[6/10][11/100][1288] Loss_D: 0.004091 Loss_G: 0.004178 \n",
      "[6/10][11/100][1289] Loss_D: 0.002839 Loss_G: 0.003577 \n",
      "[6/10][11/100][1290] Loss_D: 0.002068 Loss_G: 0.001285 \n",
      "[6/10][11/100][1291] Loss_D: 0.002335 Loss_G: 0.002405 \n",
      "[6/10][11/100][1292] Loss_D: 0.002778 Loss_G: 0.004092 \n",
      "[6/10][11/100][1293] Loss_D: 0.002807 Loss_G: 0.003032 \n",
      "[6/10][11/100][1294] Loss_D: 0.004100 Loss_G: 0.000822 \n",
      "[6/10][11/100][1295] Loss_D: 0.001730 Loss_G: 0.001382 \n",
      "[6/10][11/100][1296] Loss_D: 0.003931 Loss_G: 0.003537 \n",
      "[6/10][11/100][1297] Loss_D: 0.003837 Loss_G: 0.002991 \n",
      "[6/10][11/100][1298] Loss_D: 0.002750 Loss_G: 0.003409 \n",
      "[6/10][11/100][1299] Loss_D: 0.005172 Loss_G: 0.001970 \n",
      "[6/10][11/100][1300] Loss_D: 0.000893 Loss_G: 0.002983 \n",
      "[6/10][11/100][1301] Loss_D: 0.002651 Loss_G: 0.003121 \n",
      "[6/10][11/100][1302] Loss_D: 0.002735 Loss_G: 0.001742 \n",
      "[6/10][11/100][1303] Loss_D: 0.003168 Loss_G: 0.001892 \n",
      "[6/10][11/100][1304] Loss_D: 0.002290 Loss_G: 0.002526 \n",
      "[6/10][11/100][1305] Loss_D: 0.002150 Loss_G: 0.000879 \n",
      "[6/10][11/100][1306] Loss_D: 0.003708 Loss_G: 0.002267 \n",
      "[6/10][11/100][1307] Loss_D: 0.003756 Loss_G: 0.004246 \n",
      "[6/10][11/100][1308] Loss_D: 0.003318 Loss_G: 0.001933 \n",
      "[6/10][11/100][1309] Loss_D: 0.003231 Loss_G: 0.003221 \n",
      "[6/10][11/100][1310] Loss_D: 0.004122 Loss_G: 0.005036 \n",
      "[6/10][11/100][1311] Loss_D: 0.002581 Loss_G: 0.004729 \n",
      "[6/10][11/100][1312] Loss_D: 0.002325 Loss_G: 0.003691 \n",
      "[6/10][11/100][1313] Loss_D: 0.003282 Loss_G: 0.003870 \n",
      "[6/10][11/100][1314] Loss_D: 0.005698 Loss_G: 0.002286 \n",
      "[6/10][11/100][1315] Loss_D: 0.002353 Loss_G: 0.005869 \n",
      "[6/10][11/100][1316] Loss_D: 0.004700 Loss_G: 0.002756 \n",
      "[6/10][11/100][1317] Loss_D: 0.004551 Loss_G: 0.004320 \n",
      "[6/10][11/100][1318] Loss_D: 0.003856 Loss_G: 0.005696 \n",
      "[6/10][11/100][1319] Loss_D: 0.004375 Loss_G: 0.004371 \n",
      "[6/10][11/100][1320] Loss_D: 0.004268 Loss_G: 0.004217 \n",
      "[6/10][11/100][1321] Loss_D: 0.005454 Loss_G: 0.005146 \n",
      "[6/10][11/100][1322] Loss_D: 0.004977 Loss_G: 0.004497 \n",
      "[6/10][11/100][1323] Loss_D: 0.002220 Loss_G: 0.003208 \n",
      "[6/10][11/100][1324] Loss_D: 0.003649 Loss_G: 0.004106 \n",
      "[6/10][11/100][1325] Loss_D: 0.004311 Loss_G: 0.002277 \n",
      "[6/10][11/100][1326] Loss_D: 0.005122 Loss_G: 0.005818 \n",
      "[6/10][11/100][1327] Loss_D: 0.004289 Loss_G: 0.004467 \n",
      "[6/10][11/100][1328] Loss_D: 0.003366 Loss_G: 0.002498 \n",
      "[6/10][11/100][1329] Loss_D: 0.003474 Loss_G: 0.003662 \n",
      "[6/10][11/100][1330] Loss_D: 0.002992 Loss_G: 0.003383 \n",
      "[6/10][11/100][1331] Loss_D: 0.003334 Loss_G: 0.003416 \n",
      "[6/10][11/100][1332] Loss_D: 0.002662 Loss_G: 0.003126 \n",
      "[6/10][11/100][1333] Loss_D: 0.002633 Loss_G: 0.002889 \n",
      "[6/10][11/100][1334] Loss_D: 0.003969 Loss_G: 0.001686 \n",
      "[6/10][11/100][1335] Loss_D: 0.001786 Loss_G: 0.002842 \n",
      "[6/10][11/100][1336] Loss_D: 0.003781 Loss_G: 0.002873 \n",
      "[6/10][11/100][1337] Loss_D: 0.003223 Loss_G: 0.004033 \n",
      "[6/10][11/100][1338] Loss_D: 0.002849 Loss_G: 0.002192 \n",
      "[6/10][11/100][1339] Loss_D: 0.001701 Loss_G: 0.002911 \n",
      "[6/10][11/100][1340] Loss_D: 0.002425 Loss_G: 0.004374 \n",
      "[6/10][11/100][1341] Loss_D: 0.002099 Loss_G: 0.000136 \n",
      "[6/10][11/100][1342] Loss_D: 0.001713 Loss_G: 0.000688 \n",
      "[6/10][11/100][1343] Loss_D: 0.001852 Loss_G: 0.001709 \n",
      "[6/10][11/100][1344] Loss_D: 0.001569 Loss_G: 0.002300 \n",
      "[6/10][11/100][1345] Loss_D: 0.003211 Loss_G: 0.003356 \n",
      "[6/10][11/100][1346] Loss_D: 0.001433 Loss_G: 0.003417 \n",
      "[6/10][11/100][1347] Loss_D: 0.002331 Loss_G: 0.002820 \n",
      "[6/10][11/100][1348] Loss_D: 0.002481 Loss_G: 0.005502 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][11/100][1349] Loss_D: 0.001347 Loss_G: 0.003414 \n",
      "[6/10][11/100][1350] Loss_D: 0.003001 Loss_G: 0.003942 \n",
      "[6/10][11/100][1351] Loss_D: 0.004221 Loss_G: 0.000942 \n",
      "[6/10][11/100][1352] Loss_D: 0.003919 Loss_G: 0.003287 \n",
      "[6/10][11/100][1353] Loss_D: 0.003058 Loss_G: 0.002458 \n",
      "[6/10][11/100][1354] Loss_D: 0.001247 Loss_G: 0.002523 \n",
      "[6/10][11/100][1355] Loss_D: 0.003482 Loss_G: 0.003209 \n",
      "[6/10][11/100][1356] Loss_D: 0.002712 Loss_G: 0.002068 \n",
      "[6/10][11/100][1357] Loss_D: 0.003390 Loss_G: 0.002662 \n",
      "[6/10][11/100][1358] Loss_D: 0.002665 Loss_G: 0.001344 \n",
      "[6/10][11/100][1359] Loss_D: 0.003333 Loss_G: 0.001458 \n",
      "[6/10][11/100][1360] Loss_D: 0.002593 Loss_G: 0.000307 \n",
      "[6/10][11/100][1361] Loss_D: 0.002656 Loss_G: 0.003989 \n",
      "[6/10][11/100][1362] Loss_D: 0.002644 Loss_G: 0.003747 \n",
      "[6/10][11/100][1363] Loss_D: 0.002966 Loss_G: 0.003598 \n",
      "[6/10][11/100][1364] Loss_D: 0.004070 Loss_G: 0.003157 \n",
      "[6/10][11/100][1365] Loss_D: 0.003836 Loss_G: 0.002298 \n",
      "[6/10][11/100][1366] Loss_D: 0.002563 Loss_G: 0.002987 \n",
      "[6/10][11/100][1367] Loss_D: 0.002514 Loss_G: 0.003261 \n",
      "[6/10][11/100][1368] Loss_D: 0.001822 Loss_G: 0.001997 \n",
      "[6/10][11/100][1369] Loss_D: 0.005796 Loss_G: 0.002287 \n",
      "[6/10][11/100][1370] Loss_D: 0.004278 Loss_G: 0.003041 \n",
      "[6/10][11/100][1371] Loss_D: 0.003098 Loss_G: 0.005188 \n",
      "[6/10][11/100][1372] Loss_D: 0.002720 Loss_G: 0.003383 \n",
      "[6/10][11/100][1373] Loss_D: 0.002633 Loss_G: 0.004003 \n",
      "[6/10][11/100][1374] Loss_D: 0.004532 Loss_G: 0.004168 \n",
      "[6/10][11/100][1375] Loss_D: 0.001885 Loss_G: 0.003960 \n",
      "[6/10][11/100][1376] Loss_D: 0.003852 Loss_G: 0.002510 \n",
      "[6/10][11/100][1377] Loss_D: 0.004738 Loss_G: 0.005689 \n",
      "[6/10][11/100][1378] Loss_D: 0.004442 Loss_G: 0.003872 \n",
      "[6/10][11/100][1379] Loss_D: 0.003871 Loss_G: 0.004385 \n",
      "[6/10][11/100][1380] Loss_D: 0.003458 Loss_G: 0.004189 \n",
      "[6/10][11/100][1381] Loss_D: 0.003812 Loss_G: 0.003854 \n",
      "[6/10][11/100][1382] Loss_D: 0.004106 Loss_G: 0.001832 \n",
      "[6/10][11/100][1383] Loss_D: 0.002563 Loss_G: 0.004113 \n",
      "[6/10][11/100][1384] Loss_D: 0.004053 Loss_G: 0.004716 \n",
      "[6/10][11/100][1385] Loss_D: 0.003845 Loss_G: 0.003719 \n",
      "[6/10][11/100][1386] Loss_D: 0.002637 Loss_G: 0.004144 \n",
      "[6/10][11/100][1387] Loss_D: 0.003166 Loss_G: 0.002934 \n",
      "[6/10][11/100][1388] Loss_D: 0.003164 Loss_G: 0.003325 \n",
      "[6/10][11/100][1389] Loss_D: 0.003719 Loss_G: 0.003431 \n",
      "[6/10][11/100][1390] Loss_D: 0.004743 Loss_G: 0.002686 \n",
      "[6/10][11/100][1391] Loss_D: 0.003298 Loss_G: 0.004382 \n",
      "[6/10][11/100][1392] Loss_D: 0.003412 Loss_G: 0.001809 \n",
      "[6/10][11/100][1393] Loss_D: 0.002259 Loss_G: 0.004250 \n",
      "[6/10][11/100][1394] Loss_D: 0.002440 Loss_G: 0.003107 \n",
      "[6/10][11/100][1395] Loss_D: 0.004544 Loss_G: 0.003040 \n",
      "[6/10][11/100][1396] Loss_D: 0.002634 Loss_G: 0.003870 \n",
      "[6/10][11/100][1397] Loss_D: 0.005097 Loss_G: 0.002911 \n",
      "[6/10][11/100][1398] Loss_D: 0.004040 Loss_G: 0.004009 \n",
      "[6/10][11/100][1399] Loss_D: 0.002976 Loss_G: 0.004247 \n",
      "[6/10][11/100][1400] Loss_D: 0.002866 Loss_G: 0.003879 \n",
      "[7/10][11/100][1401] Loss_D: 0.002970 Loss_G: 0.001407 \n",
      "[7/10][11/100][1402] Loss_D: 0.002701 Loss_G: 0.002028 \n",
      "[7/10][11/100][1403] Loss_D: 0.003244 Loss_G: 0.001223 \n",
      "[7/10][11/100][1404] Loss_D: 0.001430 Loss_G: 0.003584 \n",
      "[7/10][11/100][1405] Loss_D: 0.005717 Loss_G: 0.001187 \n",
      "[7/10][11/100][1406] Loss_D: 0.002033 Loss_G: 0.002021 \n",
      "[7/10][11/100][1407] Loss_D: 0.003750 Loss_G: 0.004468 \n",
      "[7/10][11/100][1408] Loss_D: 0.002927 Loss_G: 0.002565 \n",
      "[7/10][11/100][1409] Loss_D: 0.004086 Loss_G: 0.002906 \n",
      "[7/10][11/100][1410] Loss_D: 0.003031 Loss_G: 0.002118 \n",
      "[7/10][11/100][1411] Loss_D: 0.002076 Loss_G: 0.003043 \n",
      "[7/10][11/100][1412] Loss_D: 0.003084 Loss_G: 0.003026 \n",
      "[7/10][11/100][1413] Loss_D: 0.004015 Loss_G: 0.001954 \n",
      "[7/10][11/100][1414] Loss_D: 0.001660 Loss_G: 0.002597 \n",
      "[7/10][11/100][1415] Loss_D: 0.002612 Loss_G: 0.004289 \n",
      "[7/10][11/100][1416] Loss_D: 0.002072 Loss_G: 0.003313 \n",
      "[7/10][11/100][1417] Loss_D: 0.004358 Loss_G: 0.004189 \n",
      "[7/10][11/100][1418] Loss_D: 0.002770 Loss_G: 0.003211 \n",
      "[7/10][11/100][1419] Loss_D: 0.003920 Loss_G: 0.002618 \n",
      "[7/10][11/100][1420] Loss_D: 0.003030 Loss_G: 0.003487 \n",
      "[7/10][11/100][1421] Loss_D: 0.004272 Loss_G: 0.003568 \n",
      "[7/10][11/100][1422] Loss_D: 0.003402 Loss_G: 0.002589 \n",
      "[7/10][11/100][1423] Loss_D: 0.002892 Loss_G: 0.002705 \n",
      "[7/10][11/100][1424] Loss_D: 0.003168 Loss_G: 0.004533 \n",
      "[7/10][11/100][1425] Loss_D: 0.003041 Loss_G: 0.003726 \n",
      "[7/10][11/100][1426] Loss_D: 0.001399 Loss_G: 0.003294 \n",
      "[7/10][11/100][1427] Loss_D: 0.003041 Loss_G: 0.003328 \n",
      "[7/10][11/100][1428] Loss_D: 0.003445 Loss_G: 0.002988 \n",
      "[7/10][11/100][1429] Loss_D: 0.003091 Loss_G: 0.003348 \n",
      "[7/10][11/100][1430] Loss_D: 0.004239 Loss_G: 0.004108 \n",
      "[7/10][11/100][1431] Loss_D: 0.003002 Loss_G: 0.003943 \n",
      "[7/10][11/100][1432] Loss_D: 0.004526 Loss_G: 0.003439 \n",
      "[7/10][11/100][1433] Loss_D: 0.002439 Loss_G: 0.005048 \n",
      "[7/10][11/100][1434] Loss_D: 0.004585 Loss_G: 0.003290 \n",
      "[7/10][11/100][1435] Loss_D: 0.003830 Loss_G: 0.003874 \n",
      "[7/10][11/100][1436] Loss_D: 0.003325 Loss_G: 0.004048 \n",
      "[7/10][11/100][1437] Loss_D: 0.002898 Loss_G: 0.003456 \n",
      "[7/10][11/100][1438] Loss_D: 0.003444 Loss_G: 0.003931 \n",
      "[7/10][11/100][1439] Loss_D: 0.002676 Loss_G: 0.004559 \n",
      "[7/10][11/100][1440] Loss_D: 0.002609 Loss_G: 0.002570 \n",
      "[7/10][11/100][1441] Loss_D: 0.003160 Loss_G: 0.002307 \n",
      "[7/10][11/100][1442] Loss_D: 0.001917 Loss_G: 0.002115 \n",
      "[7/10][11/100][1443] Loss_D: 0.002901 Loss_G: 0.003290 \n",
      "[7/10][11/100][1444] Loss_D: 0.002869 Loss_G: 0.003161 \n",
      "[7/10][11/100][1445] Loss_D: 0.003076 Loss_G: 0.002472 \n",
      "[7/10][11/100][1446] Loss_D: 0.002157 Loss_G: 0.003877 \n",
      "[7/10][11/100][1447] Loss_D: 0.002908 Loss_G: 0.001503 \n",
      "[7/10][11/100][1448] Loss_D: 0.002650 Loss_G: 0.003748 \n",
      "[7/10][11/100][1449] Loss_D: 0.001617 Loss_G: 0.003139 \n",
      "[7/10][11/100][1450] Loss_D: 0.002936 Loss_G: 0.002552 \n",
      "[7/10][11/100][1451] Loss_D: 0.002111 Loss_G: 0.001706 \n",
      "[7/10][11/100][1452] Loss_D: 0.002706 Loss_G: 0.003786 \n",
      "[7/10][11/100][1453] Loss_D: 0.002701 Loss_G: 0.003807 \n",
      "[7/10][11/100][1454] Loss_D: 0.001868 Loss_G: 0.001345 \n",
      "[7/10][11/100][1455] Loss_D: 0.002387 Loss_G: 0.001976 \n",
      "[7/10][11/100][1456] Loss_D: 0.003506 Loss_G: 0.003297 \n",
      "[7/10][11/100][1457] Loss_D: 0.001176 Loss_G: 0.001911 \n",
      "[7/10][11/100][1458] Loss_D: 0.003370 Loss_G: 0.004497 \n",
      "[7/10][11/100][1459] Loss_D: 0.003565 Loss_G: 0.003049 \n",
      "[7/10][11/100][1460] Loss_D: 0.003908 Loss_G: 0.003245 \n",
      "[7/10][11/100][1461] Loss_D: 0.003614 Loss_G: 0.002610 \n",
      "[7/10][11/100][1462] Loss_D: 0.004706 Loss_G: 0.002061 \n",
      "[7/10][11/100][1463] Loss_D: 0.004580 Loss_G: 0.001570 \n",
      "[7/10][11/100][1464] Loss_D: 0.005734 Loss_G: 0.004917 \n",
      "[7/10][11/100][1465] Loss_D: 0.003640 Loss_G: 0.002809 \n",
      "[7/10][11/100][1466] Loss_D: 0.004864 Loss_G: 0.002491 \n",
      "[7/10][11/100][1467] Loss_D: 0.003956 Loss_G: 0.004254 \n",
      "[7/10][11/100][1468] Loss_D: 0.004222 Loss_G: 0.004432 \n",
      "[7/10][11/100][1469] Loss_D: 0.003325 Loss_G: 0.003356 \n",
      "[7/10][11/100][1470] Loss_D: 0.003355 Loss_G: 0.001220 \n",
      "[7/10][11/100][1471] Loss_D: 0.002448 Loss_G: 0.002215 \n",
      "[7/10][11/100][1472] Loss_D: 0.002175 Loss_G: 0.003084 \n",
      "[7/10][11/100][1473] Loss_D: 0.002185 Loss_G: 0.002984 \n",
      "[7/10][11/100][1474] Loss_D: 0.002275 Loss_G: 0.002127 \n",
      "[7/10][11/100][1475] Loss_D: 0.002305 Loss_G: 0.001698 \n",
      "[7/10][11/100][1476] Loss_D: 0.002449 Loss_G: 0.002186 \n",
      "[7/10][11/100][1477] Loss_D: 0.003316 Loss_G: 0.002789 \n",
      "[7/10][11/100][1478] Loss_D: 0.004071 Loss_G: 0.002091 \n",
      "[7/10][11/100][1479] Loss_D: 0.001584 Loss_G: 0.003677 \n",
      "[7/10][11/100][1480] Loss_D: 0.001403 Loss_G: 0.002418 \n",
      "[7/10][11/100][1481] Loss_D: 0.004821 Loss_G: 0.002378 \n",
      "[7/10][11/100][1482] Loss_D: 0.003237 Loss_G: 0.002711 \n",
      "[7/10][11/100][1483] Loss_D: 0.003402 Loss_G: 0.002924 \n",
      "[7/10][11/100][1484] Loss_D: 0.001704 Loss_G: 0.002873 \n",
      "[7/10][11/100][1485] Loss_D: 0.002008 Loss_G: 0.001616 \n",
      "[7/10][11/100][1486] Loss_D: 0.003021 Loss_G: 0.003637 \n",
      "[7/10][11/100][1487] Loss_D: 0.002014 Loss_G: 0.004885 \n",
      "[7/10][11/100][1488] Loss_D: 0.002942 Loss_G: 0.002774 \n",
      "[7/10][11/100][1489] Loss_D: 0.002165 Loss_G: 0.002675 \n",
      "[7/10][11/100][1490] Loss_D: 0.004924 Loss_G: 0.004080 \n",
      "[7/10][11/100][1491] Loss_D: 0.004816 Loss_G: 0.004161 \n",
      "[7/10][11/100][1492] Loss_D: 0.002332 Loss_G: 0.003587 \n",
      "[7/10][11/100][1493] Loss_D: 0.003000 Loss_G: 0.002766 \n",
      "[7/10][11/100][1494] Loss_D: 0.003193 Loss_G: 0.002090 \n",
      "[7/10][11/100][1495] Loss_D: 0.002026 Loss_G: 0.002201 \n",
      "[7/10][11/100][1496] Loss_D: 0.003194 Loss_G: 0.001586 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10][11/100][1497] Loss_D: 0.001694 Loss_G: 0.002080 \n",
      "[7/10][11/100][1498] Loss_D: 0.002773 Loss_G: 0.002419 \n",
      "[7/10][11/100][1499] Loss_D: 0.003760 Loss_G: 0.002220 \n",
      "[7/10][11/100][1500] Loss_D: 0.002288 Loss_G: 0.002416 \n",
      "[7/10][11/100][1501] Loss_D: 0.002654 Loss_G: 0.002655 \n",
      "[7/10][11/100][1502] Loss_D: 0.003748 Loss_G: 0.003773 \n",
      "[7/10][11/100][1503] Loss_D: 0.002632 Loss_G: 0.005084 \n",
      "[7/10][11/100][1504] Loss_D: 0.003519 Loss_G: 0.004040 \n",
      "[7/10][11/100][1505] Loss_D: 0.003852 Loss_G: 0.004184 \n",
      "[7/10][11/100][1506] Loss_D: 0.005631 Loss_G: 0.006200 \n",
      "[7/10][11/100][1507] Loss_D: 0.004256 Loss_G: 0.003621 \n",
      "[7/10][11/100][1508] Loss_D: 0.006370 Loss_G: 0.003931 \n",
      "[7/10][11/100][1509] Loss_D: 0.003889 Loss_G: 0.003092 \n",
      "[7/10][11/100][1510] Loss_D: 0.003152 Loss_G: 0.006321 \n",
      "[7/10][11/100][1511] Loss_D: 0.004020 Loss_G: 0.003961 \n",
      "[7/10][11/100][1512] Loss_D: 0.002312 Loss_G: 0.003237 \n",
      "[7/10][11/100][1513] Loss_D: 0.001964 Loss_G: 0.003351 \n",
      "[7/10][11/100][1514] Loss_D: 0.001673 Loss_G: 0.003479 \n",
      "[7/10][11/100][1515] Loss_D: 0.002847 Loss_G: 0.004600 \n",
      "[7/10][11/100][1516] Loss_D: 0.003102 Loss_G: 0.002065 \n",
      "[7/10][11/100][1517] Loss_D: 0.002494 Loss_G: 0.003066 \n",
      "[7/10][11/100][1518] Loss_D: 0.001851 Loss_G: 0.002066 \n",
      "[7/10][11/100][1519] Loss_D: 0.001696 Loss_G: 0.001799 \n",
      "[7/10][11/100][1520] Loss_D: 0.002116 Loss_G: 0.001819 \n",
      "[7/10][11/100][1521] Loss_D: 0.001515 Loss_G: 0.001976 \n",
      "[7/10][11/100][1522] Loss_D: 0.003110 Loss_G: 0.001949 \n",
      "[7/10][11/100][1523] Loss_D: 0.004066 Loss_G: 0.001993 \n",
      "[7/10][11/100][1524] Loss_D: 0.002763 Loss_G: 0.002287 \n",
      "[7/10][11/100][1525] Loss_D: 0.001474 Loss_G: 0.001908 \n",
      "[7/10][11/100][1526] Loss_D: 0.003240 Loss_G: 0.002200 \n",
      "[7/10][11/100][1527] Loss_D: 0.003113 Loss_G: 0.002319 \n",
      "[7/10][11/100][1528] Loss_D: 0.003053 Loss_G: 0.002511 \n",
      "[7/10][11/100][1529] Loss_D: 0.004566 Loss_G: 0.004000 \n",
      "[7/10][11/100][1530] Loss_D: 0.002520 Loss_G: 0.003681 \n",
      "[7/10][11/100][1531] Loss_D: 0.004530 Loss_G: 0.002086 \n",
      "[7/10][11/100][1532] Loss_D: 0.003511 Loss_G: 0.002555 \n",
      "[7/10][11/100][1533] Loss_D: 0.004848 Loss_G: 0.002946 \n",
      "[7/10][11/100][1534] Loss_D: 0.002569 Loss_G: 0.002044 \n",
      "[7/10][11/100][1535] Loss_D: 0.003427 Loss_G: 0.003158 \n",
      "[7/10][11/100][1536] Loss_D: 0.003741 Loss_G: 0.002556 \n",
      "[7/10][11/100][1537] Loss_D: 0.002527 Loss_G: 0.003125 \n",
      "[7/10][11/100][1538] Loss_D: 0.003979 Loss_G: 0.002325 \n",
      "[7/10][11/100][1539] Loss_D: 0.003501 Loss_G: 0.002507 \n",
      "[7/10][11/100][1540] Loss_D: 0.003618 Loss_G: 0.003679 \n",
      "[7/10][11/100][1541] Loss_D: 0.004208 Loss_G: 0.002830 \n",
      "[7/10][11/100][1542] Loss_D: 0.002423 Loss_G: 0.003812 \n",
      "[7/10][11/100][1543] Loss_D: 0.004024 Loss_G: 0.002798 \n",
      "[7/10][11/100][1544] Loss_D: 0.003380 Loss_G: 0.001677 \n",
      "[7/10][11/100][1545] Loss_D: 0.002572 Loss_G: 0.002023 \n",
      "[7/10][11/100][1546] Loss_D: 0.001903 Loss_G: 0.002340 \n",
      "[7/10][11/100][1547] Loss_D: 0.003197 Loss_G: 0.003227 \n",
      "[7/10][11/100][1548] Loss_D: 0.002718 Loss_G: 0.002275 \n",
      "[7/10][11/100][1549] Loss_D: 0.003594 Loss_G: 0.003257 \n",
      "[7/10][11/100][1550] Loss_D: 0.002388 Loss_G: 0.000873 \n",
      "[7/10][11/100][1551] Loss_D: 0.002953 Loss_G: 0.003588 \n",
      "[7/10][11/100][1552] Loss_D: 0.003332 Loss_G: 0.001708 \n",
      "[7/10][11/100][1553] Loss_D: 0.002014 Loss_G: 0.002299 \n",
      "[7/10][11/100][1554] Loss_D: 0.002612 Loss_G: 0.003182 \n",
      "[7/10][11/100][1555] Loss_D: 0.004243 Loss_G: 0.002255 \n",
      "[7/10][11/100][1556] Loss_D: 0.004026 Loss_G: 0.004477 \n",
      "[7/10][11/100][1557] Loss_D: 0.004439 Loss_G: 0.004160 \n",
      "[7/10][11/100][1558] Loss_D: 0.004415 Loss_G: 0.003555 \n",
      "[7/10][11/100][1559] Loss_D: 0.004015 Loss_G: 0.004975 \n",
      "[7/10][11/100][1560] Loss_D: 0.004216 Loss_G: 0.003530 \n",
      "[7/10][11/100][1561] Loss_D: 0.004934 Loss_G: 0.004036 \n",
      "[7/10][11/100][1562] Loss_D: 0.005030 Loss_G: 0.005151 \n",
      "[7/10][11/100][1563] Loss_D: 0.003961 Loss_G: 0.004503 \n",
      "[7/10][11/100][1564] Loss_D: 0.005329 Loss_G: 0.004491 \n",
      "[7/10][11/100][1565] Loss_D: 0.005185 Loss_G: 0.003797 \n",
      "[7/10][11/100][1566] Loss_D: 0.004590 Loss_G: 0.003454 \n",
      "[7/10][11/100][1567] Loss_D: 0.002717 Loss_G: 0.003189 \n",
      "[7/10][11/100][1568] Loss_D: 0.002771 Loss_G: 0.002586 \n",
      "[7/10][11/100][1569] Loss_D: 0.005863 Loss_G: 0.003240 \n",
      "[7/10][11/100][1570] Loss_D: 0.003582 Loss_G: 0.003652 \n",
      "[7/10][11/100][1571] Loss_D: 0.001483 Loss_G: 0.002750 \n",
      "[7/10][11/100][1572] Loss_D: 0.002226 Loss_G: 0.003200 \n",
      "[7/10][11/100][1573] Loss_D: 0.002781 Loss_G: 0.002546 \n",
      "[7/10][11/100][1574] Loss_D: 0.002027 Loss_G: 0.002553 \n",
      "[7/10][11/100][1575] Loss_D: 0.004655 Loss_G: 0.001660 \n",
      "[7/10][11/100][1576] Loss_D: 0.003594 Loss_G: 0.001722 \n",
      "[7/10][11/100][1577] Loss_D: 0.003360 Loss_G: 0.002872 \n",
      "[7/10][11/100][1578] Loss_D: 0.002875 Loss_G: 0.002490 \n",
      "[7/10][11/100][1579] Loss_D: 0.002325 Loss_G: 0.001360 \n",
      "[7/10][11/100][1580] Loss_D: 0.004361 Loss_G: 0.002675 \n",
      "[7/10][11/100][1581] Loss_D: 0.002809 Loss_G: 0.003199 \n",
      "[7/10][11/100][1582] Loss_D: 0.003091 Loss_G: 0.004901 \n",
      "[7/10][11/100][1583] Loss_D: 0.001905 Loss_G: 0.003312 \n",
      "[7/10][11/100][1584] Loss_D: 0.003424 Loss_G: 0.002393 \n",
      "[7/10][11/100][1585] Loss_D: 0.002018 Loss_G: 0.002120 \n",
      "[7/10][11/100][1586] Loss_D: 0.004357 Loss_G: 0.002961 \n",
      "[7/10][11/100][1587] Loss_D: 0.004107 Loss_G: 0.003980 \n",
      "[7/10][11/100][1588] Loss_D: 0.001609 Loss_G: 0.001836 \n",
      "[7/10][11/100][1589] Loss_D: 0.002491 Loss_G: 0.002142 \n",
      "[7/10][11/100][1590] Loss_D: 0.002486 Loss_G: 0.003272 \n",
      "[7/10][11/100][1591] Loss_D: 0.003182 Loss_G: 0.004084 \n",
      "[7/10][11/100][1592] Loss_D: 0.002649 Loss_G: 0.003272 \n",
      "[7/10][11/100][1593] Loss_D: 0.005362 Loss_G: 0.004209 \n",
      "[7/10][11/100][1594] Loss_D: 0.002511 Loss_G: 0.003273 \n",
      "[7/10][11/100][1595] Loss_D: 0.004041 Loss_G: 0.003897 \n",
      "[7/10][11/100][1596] Loss_D: 0.003351 Loss_G: 0.003674 \n",
      "[7/10][11/100][1597] Loss_D: 0.003098 Loss_G: 0.002768 \n",
      "[7/10][11/100][1598] Loss_D: 0.003379 Loss_G: 0.004145 \n",
      "[7/10][11/100][1599] Loss_D: 0.003504 Loss_G: 0.004328 \n",
      "[7/10][11/100][1600] Loss_D: 0.001715 Loss_G: 0.003473 \n",
      "[8/10][11/100][1601] Loss_D: 0.002774 Loss_G: 0.003970 \n",
      "[8/10][11/100][1602] Loss_D: 0.005098 Loss_G: 0.003863 \n",
      "[8/10][11/100][1603] Loss_D: 0.004243 Loss_G: 0.005602 \n",
      "[8/10][11/100][1604] Loss_D: 0.005485 Loss_G: 0.003472 \n",
      "[8/10][11/100][1605] Loss_D: 0.005771 Loss_G: 0.004406 \n",
      "[8/10][11/100][1606] Loss_D: 0.002850 Loss_G: 0.004537 \n",
      "[8/10][11/100][1607] Loss_D: 0.005946 Loss_G: 0.004393 \n",
      "[8/10][11/100][1608] Loss_D: 0.003138 Loss_G: 0.003578 \n",
      "[8/10][11/100][1609] Loss_D: 0.005450 Loss_G: 0.004586 \n",
      "[8/10][11/100][1610] Loss_D: 0.004362 Loss_G: 0.003736 \n",
      "[8/10][11/100][1611] Loss_D: 0.002574 Loss_G: 0.005903 \n",
      "[8/10][11/100][1612] Loss_D: 0.004606 Loss_G: 0.002602 \n",
      "[8/10][11/100][1613] Loss_D: 0.002882 Loss_G: 0.004160 \n",
      "[8/10][11/100][1614] Loss_D: 0.002716 Loss_G: 0.004308 \n",
      "[8/10][11/100][1615] Loss_D: 0.003046 Loss_G: 0.002892 \n",
      "[8/10][11/100][1616] Loss_D: 0.004475 Loss_G: 0.004620 \n",
      "[8/10][11/100][1617] Loss_D: 0.001526 Loss_G: 0.003363 \n",
      "[8/10][11/100][1618] Loss_D: 0.003545 Loss_G: 0.003368 \n",
      "[8/10][11/100][1619] Loss_D: 0.002967 Loss_G: 0.002968 \n",
      "[8/10][11/100][1620] Loss_D: 0.003614 Loss_G: 0.003755 \n",
      "[8/10][11/100][1621] Loss_D: 0.002423 Loss_G: 0.001686 \n",
      "[8/10][11/100][1622] Loss_D: 0.003775 Loss_G: 0.001643 \n",
      "[8/10][11/100][1623] Loss_D: 0.003419 Loss_G: 0.002295 \n",
      "[8/10][11/100][1624] Loss_D: 0.002954 Loss_G: 0.002960 \n",
      "[8/10][11/100][1625] Loss_D: 0.002236 Loss_G: 0.002008 \n",
      "[8/10][11/100][1626] Loss_D: 0.002974 Loss_G: 0.002913 \n",
      "[8/10][11/100][1627] Loss_D: 0.001794 Loss_G: 0.003498 \n",
      "[8/10][11/100][1628] Loss_D: 0.003293 Loss_G: 0.001934 \n",
      "[8/10][11/100][1629] Loss_D: 0.001613 Loss_G: 0.002889 \n",
      "[8/10][11/100][1630] Loss_D: 0.003763 Loss_G: 0.002266 \n",
      "[8/10][11/100][1631] Loss_D: 0.000684 Loss_G: 0.003696 \n",
      "[8/10][11/100][1632] Loss_D: 0.003958 Loss_G: 0.003137 \n",
      "[8/10][11/100][1633] Loss_D: 0.002685 Loss_G: 0.002866 \n",
      "[8/10][11/100][1634] Loss_D: 0.002475 Loss_G: 0.002617 \n",
      "[8/10][11/100][1635] Loss_D: 0.002618 Loss_G: 0.004821 \n",
      "[8/10][11/100][1636] Loss_D: 0.002418 Loss_G: 0.004379 \n",
      "[8/10][11/100][1637] Loss_D: 0.001712 Loss_G: 0.003381 \n",
      "[8/10][11/100][1638] Loss_D: 0.002314 Loss_G: 0.002401 \n",
      "[8/10][11/100][1639] Loss_D: 0.001942 Loss_G: 0.002648 \n",
      "[8/10][11/100][1640] Loss_D: 0.003261 Loss_G: 0.001861 \n",
      "[8/10][11/100][1641] Loss_D: 0.003181 Loss_G: 0.002357 \n",
      "[8/10][11/100][1642] Loss_D: 0.002885 Loss_G: 0.001875 \n",
      "[8/10][11/100][1643] Loss_D: 0.003329 Loss_G: 0.002658 \n",
      "[8/10][11/100][1644] Loss_D: 0.002493 Loss_G: 0.003285 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][11/100][1645] Loss_D: 0.002737 Loss_G: 0.003208 \n",
      "[8/10][11/100][1646] Loss_D: 0.003050 Loss_G: 0.003263 \n",
      "[8/10][11/100][1647] Loss_D: 0.003883 Loss_G: 0.002293 \n",
      "[8/10][11/100][1648] Loss_D: 0.000977 Loss_G: 0.002944 \n",
      "[8/10][11/100][1649] Loss_D: 0.005316 Loss_G: 0.003150 \n",
      "[8/10][11/100][1650] Loss_D: 0.001823 Loss_G: 0.003467 \n",
      "[8/10][11/100][1651] Loss_D: 0.003625 Loss_G: 0.004991 \n",
      "[8/10][11/100][1652] Loss_D: 0.004279 Loss_G: 0.004054 \n",
      "[8/10][11/100][1653] Loss_D: 0.002709 Loss_G: 0.004567 \n",
      "[8/10][11/100][1654] Loss_D: 0.004076 Loss_G: 0.001789 \n",
      "[8/10][11/100][1655] Loss_D: 0.003765 Loss_G: 0.004729 \n",
      "[8/10][11/100][1656] Loss_D: 0.003247 Loss_G: 0.002956 \n",
      "[8/10][11/100][1657] Loss_D: 0.003602 Loss_G: 0.003800 \n",
      "[8/10][11/100][1658] Loss_D: 0.002763 Loss_G: 0.003273 \n",
      "[8/10][11/100][1659] Loss_D: 0.002551 Loss_G: 0.004810 \n",
      "[8/10][11/100][1660] Loss_D: 0.002762 Loss_G: 0.004380 \n",
      "[8/10][11/100][1661] Loss_D: 0.002849 Loss_G: 0.003520 \n",
      "[8/10][11/100][1662] Loss_D: 0.003982 Loss_G: 0.002626 \n",
      "[8/10][11/100][1663] Loss_D: 0.004384 Loss_G: 0.003421 \n",
      "[8/10][11/100][1664] Loss_D: 0.004332 Loss_G: 0.003951 \n",
      "[8/10][11/100][1665] Loss_D: 0.002606 Loss_G: 0.003611 \n",
      "[8/10][11/100][1666] Loss_D: 0.003670 Loss_G: 0.004799 \n",
      "[8/10][11/100][1667] Loss_D: 0.003971 Loss_G: 0.004069 \n",
      "[8/10][11/100][1668] Loss_D: 0.003007 Loss_G: 0.002940 \n",
      "[8/10][11/100][1669] Loss_D: 0.004624 Loss_G: 0.006068 \n",
      "[8/10][11/100][1670] Loss_D: 0.003798 Loss_G: 0.003532 \n",
      "[8/10][11/100][1671] Loss_D: 0.007118 Loss_G: 0.004110 \n",
      "[8/10][11/100][1672] Loss_D: 0.004675 Loss_G: 0.004272 \n",
      "[8/10][11/100][1673] Loss_D: 0.004104 Loss_G: 0.004347 \n",
      "[8/10][11/100][1674] Loss_D: 0.003358 Loss_G: 0.005964 \n",
      "[8/10][11/100][1675] Loss_D: 0.003578 Loss_G: 0.003029 \n",
      "[8/10][11/100][1676] Loss_D: 0.004062 Loss_G: 0.002897 \n",
      "[8/10][11/100][1677] Loss_D: 0.003073 Loss_G: 0.004926 \n",
      "[8/10][11/100][1678] Loss_D: 0.002925 Loss_G: 0.004151 \n",
      "[8/10][11/100][1679] Loss_D: 0.005174 Loss_G: 0.003517 \n",
      "[8/10][11/100][1680] Loss_D: 0.002664 Loss_G: 0.004236 \n",
      "[8/10][11/100][1681] Loss_D: 0.003227 Loss_G: 0.003983 \n",
      "[8/10][11/100][1682] Loss_D: 0.002734 Loss_G: 0.003861 \n",
      "[8/10][11/100][1683] Loss_D: 0.003649 Loss_G: 0.002777 \n",
      "[8/10][11/100][1684] Loss_D: 0.002521 Loss_G: 0.003389 \n",
      "[8/10][11/100][1685] Loss_D: 0.002267 Loss_G: 0.001840 \n",
      "[8/10][11/100][1686] Loss_D: 0.002882 Loss_G: 0.003059 \n",
      "[8/10][11/100][1687] Loss_D: 0.002997 Loss_G: 0.002329 \n",
      "[8/10][11/100][1688] Loss_D: 0.004142 Loss_G: 0.004310 \n",
      "[8/10][11/100][1689] Loss_D: 0.002736 Loss_G: 0.003852 \n",
      "[8/10][11/100][1690] Loss_D: 0.002344 Loss_G: 0.003676 \n",
      "[8/10][11/100][1691] Loss_D: 0.002729 Loss_G: 0.004605 \n",
      "[8/10][11/100][1692] Loss_D: 0.003497 Loss_G: 0.003715 \n",
      "[8/10][11/100][1693] Loss_D: 0.004525 Loss_G: 0.004029 \n",
      "[8/10][11/100][1694] Loss_D: 0.002313 Loss_G: 0.001621 \n",
      "[8/10][11/100][1695] Loss_D: 0.003081 Loss_G: 0.003809 \n",
      "[8/10][11/100][1696] Loss_D: 0.001607 Loss_G: 0.002158 \n",
      "[8/10][11/100][1697] Loss_D: 0.004063 Loss_G: 0.002577 \n",
      "[8/10][11/100][1698] Loss_D: 0.002767 Loss_G: 0.003715 \n",
      "[8/10][11/100][1699] Loss_D: 0.004515 Loss_G: 0.003189 \n",
      "[8/10][11/100][1700] Loss_D: 0.002394 Loss_G: 0.002625 \n",
      "[8/10][11/100][1701] Loss_D: 0.002644 Loss_G: 0.002079 \n",
      "[8/10][11/100][1702] Loss_D: 0.002918 Loss_G: 0.002716 \n",
      "[8/10][11/100][1703] Loss_D: 0.002291 Loss_G: 0.003486 \n",
      "[8/10][11/100][1704] Loss_D: 0.004219 Loss_G: 0.004130 \n",
      "[8/10][11/100][1705] Loss_D: 0.002757 Loss_G: 0.002105 \n",
      "[8/10][11/100][1706] Loss_D: 0.003786 Loss_G: 0.003064 \n",
      "[8/10][11/100][1707] Loss_D: 0.002830 Loss_G: 0.002212 \n",
      "[8/10][11/100][1708] Loss_D: 0.002495 Loss_G: 0.003241 \n",
      "[8/10][11/100][1709] Loss_D: 0.002189 Loss_G: 0.003571 \n",
      "[8/10][11/100][1710] Loss_D: 0.003588 Loss_G: 0.002421 \n",
      "[8/10][11/100][1711] Loss_D: 0.003198 Loss_G: 0.002602 \n",
      "[8/10][11/100][1712] Loss_D: 0.003244 Loss_G: 0.002385 \n",
      "[8/10][11/100][1713] Loss_D: 0.003063 Loss_G: 0.002338 \n",
      "[8/10][11/100][1714] Loss_D: 0.004018 Loss_G: 0.005410 \n",
      "[8/10][11/100][1715] Loss_D: 0.002973 Loss_G: 0.002546 \n",
      "[8/10][11/100][1716] Loss_D: 0.004751 Loss_G: 0.004303 \n",
      "[8/10][11/100][1717] Loss_D: 0.005064 Loss_G: 0.004375 \n",
      "[8/10][11/100][1718] Loss_D: 0.003520 Loss_G: 0.003227 \n",
      "[8/10][11/100][1719] Loss_D: 0.003539 Loss_G: 0.004580 \n",
      "[8/10][11/100][1720] Loss_D: 0.004061 Loss_G: 0.002986 \n",
      "[8/10][11/100][1721] Loss_D: 0.004455 Loss_G: 0.003652 \n",
      "[8/10][11/100][1722] Loss_D: 0.005139 Loss_G: 0.003787 \n",
      "[8/10][11/100][1723] Loss_D: 0.003995 Loss_G: 0.003693 \n",
      "[8/10][11/100][1724] Loss_D: 0.002469 Loss_G: 0.005687 \n",
      "[8/10][11/100][1725] Loss_D: 0.003791 Loss_G: 0.004026 \n",
      "[8/10][11/100][1726] Loss_D: 0.004498 Loss_G: 0.004530 \n",
      "[8/10][11/100][1727] Loss_D: 0.003477 Loss_G: 0.002153 \n",
      "[8/10][11/100][1728] Loss_D: 0.003817 Loss_G: 0.003234 \n",
      "[8/10][11/100][1729] Loss_D: 0.006090 Loss_G: 0.001851 \n",
      "[8/10][11/100][1730] Loss_D: 0.002776 Loss_G: 0.002948 \n",
      "[8/10][11/100][1731] Loss_D: 0.003313 Loss_G: 0.004194 \n",
      "[8/10][11/100][1732] Loss_D: 0.004025 Loss_G: 0.004088 \n",
      "[8/10][11/100][1733] Loss_D: 0.003204 Loss_G: 0.003716 \n",
      "[8/10][11/100][1734] Loss_D: 0.002724 Loss_G: 0.004200 \n",
      "[8/10][11/100][1735] Loss_D: 0.005163 Loss_G: 0.003271 \n",
      "[8/10][11/100][1736] Loss_D: 0.002130 Loss_G: 0.002985 \n",
      "[8/10][11/100][1737] Loss_D: 0.002202 Loss_G: 0.003255 \n",
      "[8/10][11/100][1738] Loss_D: 0.003441 Loss_G: 0.003651 \n",
      "[8/10][11/100][1739] Loss_D: 0.004131 Loss_G: 0.004451 \n",
      "[8/10][11/100][1740] Loss_D: 0.003532 Loss_G: 0.002431 \n",
      "[8/10][11/100][1741] Loss_D: 0.004562 Loss_G: 0.005182 \n",
      "[8/10][11/100][1742] Loss_D: 0.003592 Loss_G: 0.003073 \n",
      "[8/10][11/100][1743] Loss_D: 0.003656 Loss_G: 0.001835 \n",
      "[8/10][11/100][1744] Loss_D: 0.003027 Loss_G: 0.003431 \n",
      "[8/10][11/100][1745] Loss_D: 0.003560 Loss_G: 0.002027 \n",
      "[8/10][11/100][1746] Loss_D: 0.001760 Loss_G: 0.002456 \n",
      "[8/10][11/100][1747] Loss_D: 0.002388 Loss_G: 0.001781 \n",
      "[8/10][11/100][1748] Loss_D: 0.004020 Loss_G: 0.005140 \n",
      "[8/10][11/100][1749] Loss_D: 0.004725 Loss_G: 0.002809 \n",
      "[8/10][11/100][1750] Loss_D: 0.002761 Loss_G: 0.002617 \n",
      "[8/10][11/100][1751] Loss_D: 0.003876 Loss_G: 0.003977 \n",
      "[8/10][11/100][1752] Loss_D: 0.004999 Loss_G: 0.001443 \n",
      "[8/10][11/100][1753] Loss_D: 0.004758 Loss_G: 0.003528 \n",
      "[8/10][11/100][1754] Loss_D: 0.005004 Loss_G: 0.002375 \n",
      "[8/10][11/100][1755] Loss_D: 0.001787 Loss_G: 0.002176 \n",
      "[8/10][11/100][1756] Loss_D: 0.002936 Loss_G: 0.002392 \n",
      "[8/10][11/100][1757] Loss_D: 0.005164 Loss_G: 0.001597 \n",
      "[8/10][11/100][1758] Loss_D: 0.003446 Loss_G: 0.003985 \n",
      "[8/10][11/100][1759] Loss_D: 0.003722 Loss_G: 0.003863 \n",
      "[8/10][11/100][1760] Loss_D: 0.003386 Loss_G: 0.003291 \n",
      "[8/10][11/100][1761] Loss_D: 0.003496 Loss_G: 0.003412 \n",
      "[8/10][11/100][1762] Loss_D: 0.002613 Loss_G: 0.003866 \n",
      "[8/10][11/100][1763] Loss_D: 0.004448 Loss_G: 0.003491 \n",
      "[8/10][11/100][1764] Loss_D: 0.001830 Loss_G: 0.003182 \n",
      "[8/10][11/100][1765] Loss_D: 0.002596 Loss_G: 0.002768 \n",
      "[8/10][11/100][1766] Loss_D: 0.002514 Loss_G: 0.004158 \n",
      "[8/10][11/100][1767] Loss_D: 0.003491 Loss_G: 0.003131 \n",
      "[8/10][11/100][1768] Loss_D: 0.003659 Loss_G: 0.004108 \n",
      "[8/10][11/100][1769] Loss_D: 0.003181 Loss_G: 0.002967 \n",
      "[8/10][11/100][1770] Loss_D: 0.003396 Loss_G: 0.003531 \n",
      "[8/10][11/100][1771] Loss_D: 0.002537 Loss_G: 0.004231 \n",
      "[8/10][11/100][1772] Loss_D: 0.004006 Loss_G: 0.003416 \n",
      "[8/10][11/100][1773] Loss_D: 0.003065 Loss_G: 0.003322 \n",
      "[8/10][11/100][1774] Loss_D: 0.000428 Loss_G: 0.002041 \n",
      "[8/10][11/100][1775] Loss_D: 0.003747 Loss_G: 0.001953 \n",
      "[8/10][11/100][1776] Loss_D: 0.003905 Loss_G: 0.001613 \n",
      "[8/10][11/100][1777] Loss_D: 0.003134 Loss_G: 0.002244 \n",
      "[8/10][11/100][1778] Loss_D: 0.002772 Loss_G: 0.003184 \n",
      "[8/10][11/100][1779] Loss_D: 0.002847 Loss_G: 0.002905 \n",
      "[8/10][11/100][1780] Loss_D: 0.003013 Loss_G: 0.001684 \n",
      "[8/10][11/100][1781] Loss_D: 0.002463 Loss_G: 0.001532 \n",
      "[8/10][11/100][1782] Loss_D: 0.001831 Loss_G: 0.003619 \n",
      "[8/10][11/100][1783] Loss_D: 0.004131 Loss_G: -0.000302 \n",
      "[8/10][11/100][1784] Loss_D: 0.002433 Loss_G: 0.002271 \n",
      "[8/10][11/100][1785] Loss_D: 0.002283 Loss_G: 0.002066 \n",
      "[8/10][11/100][1786] Loss_D: 0.003004 Loss_G: 0.003604 \n",
      "[8/10][11/100][1787] Loss_D: 0.002747 Loss_G: 0.004392 \n",
      "[8/10][11/100][1788] Loss_D: 0.003010 Loss_G: 0.001822 \n",
      "[8/10][11/100][1789] Loss_D: 0.003259 Loss_G: 0.001480 \n",
      "[8/10][11/100][1790] Loss_D: 0.002968 Loss_G: 0.001656 \n",
      "[8/10][11/100][1791] Loss_D: 0.004734 Loss_G: 0.002220 \n",
      "[8/10][11/100][1792] Loss_D: 0.001871 Loss_G: 0.001683 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][11/100][1793] Loss_D: 0.002270 Loss_G: 0.004592 \n",
      "[8/10][11/100][1794] Loss_D: 0.002361 Loss_G: 0.002302 \n",
      "[8/10][11/100][1795] Loss_D: 0.003013 Loss_G: 0.001353 \n",
      "[8/10][11/100][1796] Loss_D: 0.003171 Loss_G: 0.003820 \n",
      "[8/10][11/100][1797] Loss_D: 0.005329 Loss_G: 0.004345 \n",
      "[8/10][11/100][1798] Loss_D: 0.002046 Loss_G: 0.004375 \n",
      "[8/10][11/100][1799] Loss_D: 0.002563 Loss_G: 0.003619 \n",
      "[8/10][11/100][1800] Loss_D: 0.003167 Loss_G: 0.002901 \n",
      "[9/10][11/100][1801] Loss_D: 0.003308 Loss_G: 0.003465 \n",
      "[9/10][11/100][1802] Loss_D: 0.002612 Loss_G: 0.003167 \n",
      "[9/10][11/100][1803] Loss_D: 0.003425 Loss_G: 0.003104 \n",
      "[9/10][11/100][1804] Loss_D: 0.002869 Loss_G: 0.003032 \n",
      "[9/10][11/100][1805] Loss_D: 0.003463 Loss_G: 0.003659 \n",
      "[9/10][11/100][1806] Loss_D: 0.003003 Loss_G: 0.002576 \n",
      "[9/10][11/100][1807] Loss_D: 0.005032 Loss_G: 0.003860 \n",
      "[9/10][11/100][1808] Loss_D: 0.004527 Loss_G: 0.005125 \n",
      "[9/10][11/100][1809] Loss_D: 0.003548 Loss_G: 0.004331 \n",
      "[9/10][11/100][1810] Loss_D: 0.004645 Loss_G: 0.003787 \n",
      "[9/10][11/100][1811] Loss_D: 0.003124 Loss_G: 0.003385 \n",
      "[9/10][11/100][1812] Loss_D: 0.005405 Loss_G: 0.002998 \n",
      "[9/10][11/100][1813] Loss_D: 0.002247 Loss_G: 0.003244 \n",
      "[9/10][11/100][1814] Loss_D: 0.003932 Loss_G: 0.003324 \n",
      "[9/10][11/100][1815] Loss_D: 0.002015 Loss_G: 0.004137 \n",
      "[9/10][11/100][1816] Loss_D: 0.002260 Loss_G: 0.002417 \n",
      "[9/10][11/100][1817] Loss_D: 0.001113 Loss_G: 0.002459 \n",
      "[9/10][11/100][1818] Loss_D: 0.004369 Loss_G: 0.002452 \n",
      "[9/10][11/100][1819] Loss_D: 0.002908 Loss_G: 0.004059 \n",
      "[9/10][11/100][1820] Loss_D: 0.003706 Loss_G: 0.002167 \n",
      "[9/10][11/100][1821] Loss_D: 0.002874 Loss_G: 0.002717 \n",
      "[9/10][11/100][1822] Loss_D: 0.001941 Loss_G: 0.003015 \n",
      "[9/10][11/100][1823] Loss_D: 0.003055 Loss_G: 0.001974 \n",
      "[9/10][11/100][1824] Loss_D: 0.002390 Loss_G: 0.001557 \n",
      "[9/10][11/100][1825] Loss_D: 0.003272 Loss_G: 0.002516 \n",
      "[9/10][11/100][1826] Loss_D: 0.002763 Loss_G: 0.001113 \n",
      "[9/10][11/100][1827] Loss_D: 0.002624 Loss_G: 0.003565 \n",
      "[9/10][11/100][1828] Loss_D: 0.002560 Loss_G: 0.000928 \n",
      "[9/10][11/100][1829] Loss_D: 0.002512 Loss_G: 0.004015 \n",
      "[9/10][11/100][1830] Loss_D: 0.004247 Loss_G: 0.002759 \n",
      "[9/10][11/100][1831] Loss_D: 0.002750 Loss_G: 0.002500 \n",
      "[9/10][11/100][1832] Loss_D: 0.003003 Loss_G: 0.003152 \n",
      "[9/10][11/100][1833] Loss_D: 0.003280 Loss_G: 0.002244 \n",
      "[9/10][11/100][1834] Loss_D: 0.003195 Loss_G: 0.000856 \n",
      "[9/10][11/100][1835] Loss_D: 0.004754 Loss_G: 0.001966 \n",
      "[9/10][11/100][1836] Loss_D: 0.003598 Loss_G: 0.004239 \n",
      "[9/10][11/100][1837] Loss_D: 0.004284 Loss_G: 0.002849 \n",
      "[9/10][11/100][1838] Loss_D: 0.003697 Loss_G: 0.002416 \n",
      "[9/10][11/100][1839] Loss_D: 0.003178 Loss_G: 0.003023 \n",
      "[9/10][11/100][1840] Loss_D: 0.002177 Loss_G: 0.002672 \n",
      "[9/10][11/100][1841] Loss_D: 0.003503 Loss_G: 0.002796 \n",
      "[9/10][11/100][1842] Loss_D: 0.004072 Loss_G: 0.004576 \n",
      "[9/10][11/100][1843] Loss_D: 0.004364 Loss_G: 0.002766 \n",
      "[9/10][11/100][1844] Loss_D: 0.001436 Loss_G: 0.004017 \n",
      "[9/10][11/100][1845] Loss_D: 0.003670 Loss_G: 0.004847 \n",
      "[9/10][11/100][1846] Loss_D: 0.002484 Loss_G: 0.002233 \n",
      "[9/10][11/100][1847] Loss_D: 0.002755 Loss_G: 0.002390 \n",
      "[9/10][11/100][1848] Loss_D: 0.003202 Loss_G: 0.005581 \n",
      "[9/10][11/100][1849] Loss_D: 0.003051 Loss_G: 0.003549 \n",
      "[9/10][11/100][1850] Loss_D: 0.002801 Loss_G: 0.004511 \n",
      "[9/10][11/100][1851] Loss_D: 0.002115 Loss_G: 0.002760 \n",
      "[9/10][11/100][1852] Loss_D: 0.003836 Loss_G: 0.003075 \n",
      "[9/10][11/100][1853] Loss_D: 0.002349 Loss_G: 0.002323 \n",
      "[9/10][11/100][1854] Loss_D: 0.002873 Loss_G: 0.002441 \n",
      "[9/10][11/100][1855] Loss_D: 0.001680 Loss_G: 0.004673 \n",
      "[9/10][11/100][1856] Loss_D: 0.004959 Loss_G: 0.002403 \n",
      "[9/10][11/100][1857] Loss_D: 0.003994 Loss_G: 0.002264 \n",
      "[9/10][11/100][1858] Loss_D: 0.004341 Loss_G: 0.002461 \n",
      "[9/10][11/100][1859] Loss_D: 0.004360 Loss_G: 0.003018 \n",
      "[9/10][11/100][1860] Loss_D: 0.005010 Loss_G: 0.004904 \n",
      "[9/10][11/100][1861] Loss_D: 0.003667 Loss_G: 0.003423 \n",
      "[9/10][11/100][1862] Loss_D: 0.003355 Loss_G: 0.003380 \n",
      "[9/10][11/100][1863] Loss_D: 0.006191 Loss_G: 0.003522 \n",
      "[9/10][11/100][1864] Loss_D: 0.004340 Loss_G: 0.004033 \n",
      "[9/10][11/100][1865] Loss_D: 0.004791 Loss_G: 0.005518 \n",
      "[9/10][11/100][1866] Loss_D: 0.003564 Loss_G: 0.004207 \n",
      "[9/10][11/100][1867] Loss_D: 0.003102 Loss_G: 0.003808 \n",
      "[9/10][11/100][1868] Loss_D: 0.005045 Loss_G: 0.003543 \n",
      "[9/10][11/100][1869] Loss_D: 0.005658 Loss_G: 0.003223 \n",
      "[9/10][11/100][1870] Loss_D: 0.003881 Loss_G: 0.007311 \n",
      "[9/10][11/100][1871] Loss_D: 0.004135 Loss_G: 0.002791 \n",
      "[9/10][11/100][1872] Loss_D: 0.002888 Loss_G: 0.003208 \n",
      "[9/10][11/100][1873] Loss_D: 0.002265 Loss_G: 0.003368 \n",
      "[9/10][11/100][1874] Loss_D: 0.003111 Loss_G: 0.003641 \n",
      "[9/10][11/100][1875] Loss_D: 0.003076 Loss_G: 0.004969 \n",
      "[9/10][11/100][1876] Loss_D: 0.002145 Loss_G: 0.003781 \n",
      "[9/10][11/100][1877] Loss_D: 0.003794 Loss_G: 0.001651 \n",
      "[9/10][11/100][1878] Loss_D: 0.003897 Loss_G: 0.003278 \n",
      "[9/10][11/100][1879] Loss_D: 0.002833 Loss_G: 0.003563 \n",
      "[9/10][11/100][1880] Loss_D: 0.003422 Loss_G: 0.003461 \n",
      "[9/10][11/100][1881] Loss_D: 0.003838 Loss_G: 0.003484 \n",
      "[9/10][11/100][1882] Loss_D: 0.003078 Loss_G: 0.003446 \n",
      "[9/10][11/100][1883] Loss_D: 0.001413 Loss_G: 0.002702 \n",
      "[9/10][11/100][1884] Loss_D: 0.002591 Loss_G: 0.003037 \n",
      "[9/10][11/100][1885] Loss_D: 0.002660 Loss_G: 0.004377 \n",
      "[9/10][11/100][1886] Loss_D: 0.002547 Loss_G: 0.004520 \n",
      "[9/10][11/100][1887] Loss_D: 0.003669 Loss_G: 0.003872 \n",
      "[9/10][11/100][1888] Loss_D: 0.001807 Loss_G: 0.003233 \n",
      "[9/10][11/100][1889] Loss_D: 0.002567 Loss_G: 0.002864 \n",
      "[9/10][11/100][1890] Loss_D: 0.001882 Loss_G: 0.003209 \n",
      "[9/10][11/100][1891] Loss_D: 0.002201 Loss_G: 0.001988 \n",
      "[9/10][11/100][1892] Loss_D: 0.003685 Loss_G: 0.001701 \n",
      "[9/10][11/100][1893] Loss_D: 0.003287 Loss_G: 0.003880 \n",
      "[9/10][11/100][1894] Loss_D: 0.003794 Loss_G: 0.002763 \n",
      "[9/10][11/100][1895] Loss_D: 0.003511 Loss_G: 0.004266 \n",
      "[9/10][11/100][1896] Loss_D: 0.002920 Loss_G: 0.003390 \n",
      "[9/10][11/100][1897] Loss_D: 0.003229 Loss_G: 0.003539 \n",
      "[9/10][11/100][1898] Loss_D: 0.003803 Loss_G: 0.003270 \n",
      "[9/10][11/100][1899] Loss_D: 0.003606 Loss_G: 0.003605 \n",
      "[9/10][11/100][1900] Loss_D: 0.002860 Loss_G: 0.004281 \n",
      "[9/10][11/100][1901] Loss_D: 0.003602 Loss_G: 0.002766 \n",
      "[9/10][11/100][1902] Loss_D: 0.003385 Loss_G: 0.004068 \n",
      "[9/10][11/100][1903] Loss_D: 0.004814 Loss_G: 0.002163 \n",
      "[9/10][11/100][1904] Loss_D: 0.003418 Loss_G: 0.005655 \n",
      "[9/10][11/100][1905] Loss_D: 0.001306 Loss_G: 0.003607 \n",
      "[9/10][11/100][1906] Loss_D: 0.002525 Loss_G: 0.002464 \n",
      "[9/10][11/100][1907] Loss_D: 0.002780 Loss_G: 0.004014 \n",
      "[9/10][11/100][1908] Loss_D: 0.003175 Loss_G: 0.003685 \n",
      "[9/10][11/100][1909] Loss_D: 0.001801 Loss_G: 0.002812 \n",
      "[9/10][11/100][1910] Loss_D: 0.004055 Loss_G: 0.003010 \n",
      "[9/10][11/100][1911] Loss_D: 0.003047 Loss_G: 0.003882 \n",
      "[9/10][11/100][1912] Loss_D: 0.003345 Loss_G: 0.003298 \n",
      "[9/10][11/100][1913] Loss_D: 0.002755 Loss_G: 0.004414 \n",
      "[9/10][11/100][1914] Loss_D: 0.004195 Loss_G: 0.004243 \n",
      "[9/10][11/100][1915] Loss_D: 0.005096 Loss_G: 0.004219 \n",
      "[9/10][11/100][1916] Loss_D: 0.004385 Loss_G: 0.003941 \n",
      "[9/10][11/100][1917] Loss_D: 0.004531 Loss_G: 0.004254 \n",
      "[9/10][11/100][1918] Loss_D: 0.003138 Loss_G: 0.003457 \n",
      "[9/10][11/100][1919] Loss_D: 0.005492 Loss_G: 0.004253 \n",
      "[9/10][11/100][1920] Loss_D: 0.003698 Loss_G: 0.004631 \n",
      "[9/10][11/100][1921] Loss_D: 0.004485 Loss_G: 0.003487 \n",
      "[9/10][11/100][1922] Loss_D: 0.003772 Loss_G: 0.002453 \n",
      "[9/10][11/100][1923] Loss_D: 0.004293 Loss_G: 0.003001 \n",
      "[9/10][11/100][1924] Loss_D: 0.003701 Loss_G: 0.004081 \n",
      "[9/10][11/100][1925] Loss_D: 0.003835 Loss_G: 0.004417 \n",
      "[9/10][11/100][1926] Loss_D: 0.002767 Loss_G: 0.001648 \n",
      "[9/10][11/100][1927] Loss_D: 0.002891 Loss_G: 0.002258 \n",
      "[9/10][11/100][1928] Loss_D: 0.003496 Loss_G: 0.002687 \n",
      "[9/10][11/100][1929] Loss_D: 0.002603 Loss_G: 0.003103 \n",
      "[9/10][11/100][1930] Loss_D: 0.004413 Loss_G: 0.003036 \n",
      "[9/10][11/100][1931] Loss_D: 0.004249 Loss_G: 0.001824 \n",
      "[9/10][11/100][1932] Loss_D: 0.007974 Loss_G: 0.002564 \n",
      "[9/10][11/100][1933] Loss_D: 0.004268 Loss_G: 0.002070 \n",
      "[9/10][11/100][1934] Loss_D: 0.002775 Loss_G: 0.002593 \n",
      "[9/10][11/100][1935] Loss_D: 0.002442 Loss_G: 0.002451 \n",
      "[9/10][11/100][1936] Loss_D: 0.003768 Loss_G: 0.002526 \n",
      "[9/10][11/100][1937] Loss_D: 0.003553 Loss_G: 0.005052 \n",
      "[9/10][11/100][1938] Loss_D: 0.002420 Loss_G: 0.002298 \n",
      "[9/10][11/100][1939] Loss_D: 0.001637 Loss_G: 0.003130 \n",
      "[9/10][11/100][1940] Loss_D: 0.002692 Loss_G: 0.004884 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10][11/100][1941] Loss_D: 0.003283 Loss_G: 0.003295 \n",
      "[9/10][11/100][1942] Loss_D: 0.004272 Loss_G: 0.002184 \n",
      "[9/10][11/100][1943] Loss_D: 0.002407 Loss_G: 0.002249 \n",
      "[9/10][11/100][1944] Loss_D: 0.004759 Loss_G: 0.001907 \n",
      "[9/10][11/100][1945] Loss_D: 0.003230 Loss_G: 0.001496 \n",
      "[9/10][11/100][1946] Loss_D: 0.003647 Loss_G: 0.003503 \n",
      "[9/10][11/100][1947] Loss_D: 0.003739 Loss_G: 0.002903 \n",
      "[9/10][11/100][1948] Loss_D: 0.003369 Loss_G: 0.004379 \n",
      "[9/10][11/100][1949] Loss_D: 0.004764 Loss_G: 0.004855 \n",
      "[9/10][11/100][1950] Loss_D: 0.005138 Loss_G: 0.003122 \n",
      "[9/10][11/100][1951] Loss_D: 0.003802 Loss_G: 0.004340 \n",
      "[9/10][11/100][1952] Loss_D: 0.004383 Loss_G: 0.003358 \n",
      "[9/10][11/100][1953] Loss_D: 0.004174 Loss_G: 0.003943 \n",
      "[9/10][11/100][1954] Loss_D: 0.003775 Loss_G: 0.003609 \n",
      "[9/10][11/100][1955] Loss_D: 0.003044 Loss_G: 0.004071 \n",
      "[9/10][11/100][1956] Loss_D: 0.003868 Loss_G: 0.003038 \n",
      "[9/10][11/100][1957] Loss_D: 0.003503 Loss_G: 0.004040 \n",
      "[9/10][11/100][1958] Loss_D: 0.004109 Loss_G: 0.003483 \n",
      "[9/10][11/100][1959] Loss_D: 0.004502 Loss_G: 0.004143 \n",
      "[9/10][11/100][1960] Loss_D: 0.002942 Loss_G: 0.001280 \n",
      "[9/10][11/100][1961] Loss_D: 0.002799 Loss_G: 0.003412 \n",
      "[9/10][11/100][1962] Loss_D: 0.002027 Loss_G: 0.001382 \n",
      "[9/10][11/100][1963] Loss_D: 0.002897 Loss_G: 0.003120 \n",
      "[9/10][11/100][1964] Loss_D: 0.001718 Loss_G: 0.003483 \n",
      "[9/10][11/100][1965] Loss_D: 0.004823 Loss_G: 0.003345 \n",
      "[9/10][11/100][1966] Loss_D: 0.001505 Loss_G: 0.002473 \n",
      "[9/10][11/100][1967] Loss_D: 0.002750 Loss_G: 0.002268 \n",
      "[9/10][11/100][1968] Loss_D: 0.003002 Loss_G: 0.002434 \n",
      "[9/10][11/100][1969] Loss_D: 0.001866 Loss_G: 0.002499 \n",
      "[9/10][11/100][1970] Loss_D: 0.005919 Loss_G: 0.002773 \n",
      "[9/10][11/100][1971] Loss_D: 0.002761 Loss_G: 0.000969 \n",
      "[9/10][11/100][1972] Loss_D: 0.001997 Loss_G: 0.004211 \n",
      "[9/10][11/100][1973] Loss_D: 0.003631 Loss_G: 0.001303 \n",
      "[9/10][11/100][1974] Loss_D: 0.003028 Loss_G: 0.004434 \n",
      "[9/10][11/100][1975] Loss_D: 0.003219 Loss_G: 0.003324 \n",
      "[9/10][11/100][1976] Loss_D: 0.001625 Loss_G: 0.003020 \n",
      "[9/10][11/100][1977] Loss_D: 0.002564 Loss_G: 0.005717 \n",
      "[9/10][11/100][1978] Loss_D: 0.001900 Loss_G: 0.003051 \n",
      "[9/10][11/100][1979] Loss_D: 0.003769 Loss_G: 0.002496 \n",
      "[9/10][11/100][1980] Loss_D: 0.003409 Loss_G: 0.002284 \n",
      "[9/10][11/100][1981] Loss_D: 0.003385 Loss_G: 0.003680 \n",
      "[9/10][11/100][1982] Loss_D: 0.002987 Loss_G: 0.001533 \n",
      "[9/10][11/100][1983] Loss_D: 0.001954 Loss_G: 0.002520 \n",
      "[9/10][11/100][1984] Loss_D: 0.001701 Loss_G: 0.003755 \n",
      "[9/10][11/100][1985] Loss_D: 0.004220 Loss_G: 0.002797 \n",
      "[9/10][11/100][1986] Loss_D: 0.001323 Loss_G: 0.003033 \n",
      "[9/10][11/100][1987] Loss_D: 0.001804 Loss_G: 0.004727 \n",
      "[9/10][11/100][1988] Loss_D: 0.002582 Loss_G: 0.001663 \n",
      "[9/10][11/100][1989] Loss_D: 0.002080 Loss_G: 0.004067 \n",
      "[9/10][11/100][1990] Loss_D: 0.003673 Loss_G: 0.002987 \n",
      "[9/10][11/100][1991] Loss_D: 0.002137 Loss_G: 0.002727 \n",
      "[9/10][11/100][1992] Loss_D: 0.002692 Loss_G: 0.002532 \n",
      "[9/10][11/100][1993] Loss_D: 0.002537 Loss_G: 0.004051 \n",
      "[9/10][11/100][1994] Loss_D: 0.002739 Loss_G: 0.002558 \n",
      "[9/10][11/100][1995] Loss_D: 0.005265 Loss_G: 0.003360 \n",
      "[9/10][11/100][1996] Loss_D: 0.004114 Loss_G: 0.002298 \n",
      "[9/10][11/100][1997] Loss_D: 0.003484 Loss_G: 0.004475 \n",
      "[9/10][11/100][1998] Loss_D: 0.004229 Loss_G: 0.004753 \n",
      "[9/10][11/100][1999] Loss_D: 0.004605 Loss_G: 0.004605 \n",
      "[9/10][11/100][2000] Loss_D: 0.003992 Loss_G: 0.006345 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0005731555866077542,\n",
       "  0.0021373452618718147,\n",
       "  0.006789423059672117,\n",
       "  0.007092694286257029,\n",
       "  0.01594218984246254,\n",
       "  0.012264921329915524,\n",
       "  0.016552090644836426,\n",
       "  0.0137872863560915,\n",
       "  0.011806292459368706,\n",
       "  0.011451112106442451,\n",
       "  0.014815879985690117,\n",
       "  0.01857762597501278,\n",
       "  0.012334849685430527,\n",
       "  0.012398926541209221,\n",
       "  0.007796157151460648,\n",
       "  0.00977933406829834,\n",
       "  0.00782332569360733,\n",
       "  0.0074753728695213795,\n",
       "  0.006973100360482931,\n",
       "  0.004010837059468031,\n",
       "  0.001847119303420186,\n",
       "  0.0038955823983997107,\n",
       "  0.0031814654357731342,\n",
       "  0.005046564619988203,\n",
       "  0.0035724970512092113,\n",
       "  0.004911117721349001,\n",
       "  0.007714424282312393,\n",
       "  0.007426315452903509,\n",
       "  0.0069347647950053215,\n",
       "  0.0039062045980244875,\n",
       "  0.008141652680933475,\n",
       "  0.0060756616294384,\n",
       "  0.00554185826331377,\n",
       "  0.0033212180715054274,\n",
       "  0.0038676427211612463,\n",
       "  0.006880057044327259,\n",
       "  0.005687375087291002,\n",
       "  0.0037887655198574066,\n",
       "  0.006839580833911896,\n",
       "  0.005418080370873213,\n",
       "  0.00584771391004324,\n",
       "  0.008525031618773937,\n",
       "  0.006110264454036951,\n",
       "  0.007565537001937628,\n",
       "  0.0054777744226157665,\n",
       "  0.0056878142058849335,\n",
       "  0.006990185473114252,\n",
       "  0.005123527720570564,\n",
       "  0.005463373847305775,\n",
       "  0.006132902577519417,\n",
       "  0.005128460004925728,\n",
       "  0.005834849085658789,\n",
       "  0.007170773111283779,\n",
       "  0.005679234862327576,\n",
       "  0.0031726083252578974,\n",
       "  0.005754654761403799,\n",
       "  0.006121542304754257,\n",
       "  0.005724895745515823,\n",
       "  0.005674834828823805,\n",
       "  0.004465885926038027,\n",
       "  0.005395025014877319,\n",
       "  0.006304092239588499,\n",
       "  0.006545739248394966,\n",
       "  0.005122542381286621,\n",
       "  0.007279769517481327,\n",
       "  0.006499289534986019,\n",
       "  0.005304415710270405,\n",
       "  0.007490766234695911,\n",
       "  0.005095228552818298,\n",
       "  0.004670069087296724,\n",
       "  0.003676873166114092,\n",
       "  0.004147915635257959,\n",
       "  0.005866463296115398,\n",
       "  0.004415799863636494,\n",
       "  0.0018616088200360537,\n",
       "  0.003731507807970047,\n",
       "  0.0029829884879291058,\n",
       "  0.003312422428280115,\n",
       "  0.00328816962428391,\n",
       "  0.003259249497205019,\n",
       "  0.004162203054875135,\n",
       "  0.003163866465911269,\n",
       "  0.00393997598439455,\n",
       "  0.0032563607674092054,\n",
       "  0.00468752346932888,\n",
       "  0.004399167373776436,\n",
       "  0.0052026305347681046,\n",
       "  0.004916457459330559,\n",
       "  0.006289467215538025,\n",
       "  0.0075957272201776505,\n",
       "  0.004271369893103838,\n",
       "  0.005927460268139839,\n",
       "  0.005956713110208511,\n",
       "  0.003965200390666723,\n",
       "  0.004602398257702589,\n",
       "  0.003529347013682127,\n",
       "  0.005043502431362867,\n",
       "  0.0032830520067363977,\n",
       "  0.0027106276247650385,\n",
       "  0.0037899708840996027,\n",
       "  0.0031720614060759544,\n",
       "  0.004122327547520399,\n",
       "  0.003136728424578905,\n",
       "  0.002824984258040786,\n",
       "  0.0032749271485954523,\n",
       "  0.0017678063595667481,\n",
       "  0.003690836951136589,\n",
       "  0.002658449811860919,\n",
       "  0.0022704554721713066,\n",
       "  0.0026342931669205427,\n",
       "  0.0032543994020670652,\n",
       "  0.0019315856043249369,\n",
       "  0.002646791748702526,\n",
       "  0.0032794673461467028,\n",
       "  0.0025590171571820974,\n",
       "  0.0034732019994407892,\n",
       "  0.0029786285012960434,\n",
       "  0.005357448011636734,\n",
       "  0.002947192871943116,\n",
       "  0.004589577205479145,\n",
       "  0.0029592211358249187,\n",
       "  0.004024312365800142,\n",
       "  0.002209309721365571,\n",
       "  0.0028827604837715626,\n",
       "  0.003491286188364029,\n",
       "  0.002135606948286295,\n",
       "  0.003269404172897339,\n",
       "  0.002346665831282735,\n",
       "  0.003020848846063018,\n",
       "  0.0034396208357065916,\n",
       "  0.002059451537206769,\n",
       "  0.0024316315539181232,\n",
       "  0.0032185502350330353,\n",
       "  0.004258113447576761,\n",
       "  0.0031433941330760717,\n",
       "  0.0035638166591525078,\n",
       "  0.0029290420934557915,\n",
       "  0.0033887620083987713,\n",
       "  0.0030718990601599216,\n",
       "  0.0032334288116544485,\n",
       "  0.0017455881461501122,\n",
       "  0.0023063269909471273,\n",
       "  0.0049773105420172215,\n",
       "  0.0017380969366058707,\n",
       "  0.002028815448284149,\n",
       "  0.001888881204649806,\n",
       "  0.0029536671936511993,\n",
       "  0.003634607419371605,\n",
       "  0.00388574181124568,\n",
       "  0.0025460016913712025,\n",
       "  0.0013112799497321248,\n",
       "  0.00236990163102746,\n",
       "  0.0014307573437690735,\n",
       "  0.0035509902518242598,\n",
       "  0.002481654053553939,\n",
       "  0.003015467431396246,\n",
       "  0.0027101761661469936,\n",
       "  0.0023192944936454296,\n",
       "  0.0020359354093670845,\n",
       "  0.0012755162315443158,\n",
       "  0.004309820011258125,\n",
       "  0.0033131882082670927,\n",
       "  0.002590832067653537,\n",
       "  0.004245830699801445,\n",
       "  0.003824504092335701,\n",
       "  0.004081035032868385,\n",
       "  0.0036986058112233877,\n",
       "  0.0026986044831573963,\n",
       "  0.003941602539271116,\n",
       "  0.0027989165391772985,\n",
       "  0.003363116644322872,\n",
       "  0.002511983970180154,\n",
       "  0.0022463658824563026,\n",
       "  0.00452670082449913,\n",
       "  0.002258685417473316,\n",
       "  0.002241393318399787,\n",
       "  0.0019081971840932965,\n",
       "  0.0016009683022275567,\n",
       "  0.0022011215332895517,\n",
       "  0.0025569975841790438,\n",
       "  0.001975004095584154,\n",
       "  0.003952122759073973,\n",
       "  0.0019257330568507314,\n",
       "  0.004727859050035477,\n",
       "  0.003189841052517295,\n",
       "  0.003633572021499276,\n",
       "  0.003415475133806467,\n",
       "  0.004864835180342197,\n",
       "  0.005269492045044899,\n",
       "  0.0044912314042449,\n",
       "  0.007441297173500061,\n",
       "  0.004303411580622196,\n",
       "  0.005106721539050341,\n",
       "  0.004938287660479546,\n",
       "  0.005562018137425184,\n",
       "  0.004865907598286867,\n",
       "  0.004898165352642536,\n",
       "  0.0031953761354088783,\n",
       "  0.006320344749838114,\n",
       "  0.002453641965985298,\n",
       "  0.0030877774115651846,\n",
       "  0.0008176025585271418,\n",
       "  0.0040193237364292145,\n",
       "  0.0019284129375591874,\n",
       "  0.0014672670513391495,\n",
       "  0.004190539475530386,\n",
       "  0.0017835330218076706,\n",
       "  0.0026312307454645634,\n",
       "  0.003440859727561474,\n",
       "  0.003927942831069231,\n",
       "  0.002081028651446104,\n",
       "  0.0023373039439320564,\n",
       "  0.001466206507757306,\n",
       "  0.0032752875704318285,\n",
       "  0.0017773821018636227,\n",
       "  0.0022189589217305183,\n",
       "  0.006892916280776262,\n",
       "  0.00583722023293376,\n",
       "  0.0052426098845899105,\n",
       "  0.004093566909432411,\n",
       "  0.0031897362787276506,\n",
       "  0.0033738608472049236,\n",
       "  0.0029033063910901546,\n",
       "  0.003083978546783328,\n",
       "  0.002215777290984988,\n",
       "  0.0030933162197470665,\n",
       "  0.0023899688385427,\n",
       "  0.0042605046182870865,\n",
       "  0.0031833925750106573,\n",
       "  0.002826840151101351,\n",
       "  0.0016923397779464722,\n",
       "  0.002347269793972373,\n",
       "  0.0013520073844119906,\n",
       "  0.0024327486753463745,\n",
       "  0.002550677629187703,\n",
       "  0.0021377832163125277,\n",
       "  0.0014608582714572549,\n",
       "  0.0033394633792340755,\n",
       "  0.003453353652730584,\n",
       "  0.0032115066424012184,\n",
       "  0.003803210100159049,\n",
       "  0.0032412928994745016,\n",
       "  0.006588793359696865,\n",
       "  0.005599216092377901,\n",
       "  0.004224706906825304,\n",
       "  0.0048561799339950085,\n",
       "  0.0037968922406435013,\n",
       "  0.005209702532738447,\n",
       "  0.003512136172503233,\n",
       "  0.003765824483707547,\n",
       "  0.004239936359226704,\n",
       "  0.004386883229017258,\n",
       "  0.0034200726076960564,\n",
       "  0.00323691638186574,\n",
       "  0.004133225884288549,\n",
       "  0.0032690551597625017,\n",
       "  0.0026709658559411764,\n",
       "  0.0035202670842409134,\n",
       "  0.002119320910423994,\n",
       "  0.003502806881442666,\n",
       "  0.0005282564088702202,\n",
       "  0.002135123824700713,\n",
       "  0.003933271858841181,\n",
       "  0.0026260565500706434,\n",
       "  0.00017891406605485827,\n",
       "  0.00012074032565578818,\n",
       "  0.002230706624686718,\n",
       "  0.003250716021284461,\n",
       "  0.0033009788021445274,\n",
       "  0.0010548916179686785,\n",
       "  0.00039756993646733463,\n",
       "  0.0012969522504135966,\n",
       "  0.0008512646891176701,\n",
       "  0.0006284264381974936,\n",
       "  0.001245927414856851,\n",
       "  0.0009187414543703198,\n",
       "  0.0028682281263172626,\n",
       "  0.0036521086003631353,\n",
       "  0.004495603963732719,\n",
       "  0.001714489422738552,\n",
       "  0.004085206892341375,\n",
       "  0.005062327720224857,\n",
       "  0.004547543358057737,\n",
       "  0.004477080889046192,\n",
       "  0.002499046502634883,\n",
       "  0.004713812842965126,\n",
       "  0.001473623444326222,\n",
       "  0.004232301376760006,\n",
       "  0.0038322657346725464,\n",
       "  0.002865568967536092,\n",
       "  0.0028364614117890596,\n",
       "  0.003624442033469677,\n",
       "  0.003905479097738862,\n",
       "  0.0029545389115810394,\n",
       "  0.0015912327216938138,\n",
       "  0.002259050263091922,\n",
       "  0.00385946873575449,\n",
       "  0.0024112886749207973,\n",
       "  0.0029285966884344816,\n",
       "  0.002609209157526493,\n",
       "  0.0034986368846148252,\n",
       "  0.002354281721636653,\n",
       "  0.004008827265352011,\n",
       "  0.004860370885580778,\n",
       "  0.0035438525956124067,\n",
       "  0.004668867215514183,\n",
       "  0.004293345380574465,\n",
       "  0.004777735099196434,\n",
       "  0.005762637127190828,\n",
       "  0.00410058256238699,\n",
       "  0.0045562307350337505,\n",
       "  0.004381359554827213,\n",
       "  0.0028671359177678823,\n",
       "  0.0031923071946948767,\n",
       "  0.00379187217913568,\n",
       "  0.002768915146589279,\n",
       "  0.003207255620509386,\n",
       "  0.001686764881014824,\n",
       "  0.002513691084459424,\n",
       "  0.002334464807063341,\n",
       "  0.00209926743991673,\n",
       "  0.002204392571002245,\n",
       "  0.0017022264655679464,\n",
       "  0.0030465349555015564,\n",
       "  0.0019834497943520546,\n",
       "  0.0016445254441350698,\n",
       "  0.0030472255311906338,\n",
       "  0.002405654639005661,\n",
       "  0.004109885077923536,\n",
       "  0.0020483413245528936,\n",
       "  0.0026948428712785244,\n",
       "  0.0021900360006839037,\n",
       "  0.0015135153662413359,\n",
       "  0.0034334389492869377,\n",
       "  0.0016921948408707976,\n",
       "  0.001279668533243239,\n",
       "  0.0013778825523331761,\n",
       "  0.002166598569601774,\n",
       "  0.0023132525384426117,\n",
       "  0.0012941154418513179,\n",
       "  0.0030799931846559048,\n",
       "  0.0051712291315197945,\n",
       "  0.0013951254077255726,\n",
       "  0.002478210721164942,\n",
       "  0.00237697409465909,\n",
       "  0.0031824209727346897,\n",
       "  0.00354908243753016,\n",
       "  0.0038132101763039827,\n",
       "  0.0035794584546238184,\n",
       "  0.0020013158209621906,\n",
       "  0.002807240467518568,\n",
       "  0.004847981967031956,\n",
       "  0.003953748848289251,\n",
       "  0.0036695196758955717,\n",
       "  0.0029182678554207087,\n",
       "  0.004961990751326084,\n",
       "  0.0034177028574049473,\n",
       "  0.003313529770821333,\n",
       "  0.0044689420610666275,\n",
       "  0.004908701870590448,\n",
       "  0.004745838697999716,\n",
       "  0.004896953236311674,\n",
       "  0.00318930740468204,\n",
       "  0.0049821375869214535,\n",
       "  0.004410544876009226,\n",
       "  0.006562561262398958,\n",
       "  0.00385293853469193,\n",
       "  0.00308025605045259,\n",
       "  0.006096030119806528,\n",
       "  0.003361862851306796,\n",
       "  0.003197557758539915,\n",
       "  0.005249161273241043,\n",
       "  0.00417651841416955,\n",
       "  0.0030030482448637486,\n",
       "  0.002706885803490877,\n",
       "  0.0016410457901656628,\n",
       "  0.0031812647357583046,\n",
       "  0.0026819028425961733,\n",
       "  0.0024778845254331827,\n",
       "  0.0020630971994251013,\n",
       "  0.002973764669150114,\n",
       "  0.0017507224110886455,\n",
       "  0.0026167528703808784,\n",
       "  0.0020123592112213373,\n",
       "  0.004104339051991701,\n",
       "  0.0024488235358148813,\n",
       "  0.002144688507542014,\n",
       "  0.00282247643917799,\n",
       "  0.0033836886286735535,\n",
       "  0.0035747969523072243,\n",
       "  0.002224241616204381,\n",
       "  0.0033868905156850815,\n",
       "  0.0016553341411054134,\n",
       "  0.004077705089002848,\n",
       "  0.0031316771637648344,\n",
       "  0.0050107440911233425,\n",
       "  0.002317206235602498,\n",
       "  0.002795873209834099,\n",
       "  0.0031114229932427406,\n",
       "  0.0022404331248253584,\n",
       "  0.004385844338685274,\n",
       "  0.004415588919073343,\n",
       "  0.0013541876105591655,\n",
       "  0.0028583938255906105,\n",
       "  0.0030778769869357347,\n",
       "  0.002277888124808669,\n",
       "  0.001536607276648283,\n",
       "  0.0064163813367486,\n",
       "  0.0026667877100408077,\n",
       "  0.0029785986989736557,\n",
       "  0.003546481253579259,\n",
       "  0.004647391848266125,\n",
       "  0.003651234321296215,\n",
       "  0.0032089503947645426,\n",
       "  0.002639507409185171,\n",
       "  0.002881038933992386,\n",
       "  0.005979551002383232,\n",
       "  0.0039101243019104,\n",
       "  0.007508337963372469,\n",
       "  0.004286194685846567,\n",
       "  0.005727577488869429,\n",
       "  0.006327738054096699,\n",
       "  0.007034830283373594,\n",
       "  0.006576634477823973,\n",
       "  0.003964458592236042,\n",
       "  0.005381984170526266,\n",
       "  0.006206855643540621,\n",
       "  0.00439770333468914,\n",
       "  0.004916652571409941,\n",
       "  0.004565822891891003,\n",
       "  0.002780250972136855,\n",
       "  0.0039631761610507965,\n",
       "  0.0032905174884945154,\n",
       "  0.002862578956410289,\n",
       "  0.002365528605878353,\n",
       "  0.0007009002147242427,\n",
       "  0.002395770512521267,\n",
       "  0.0040462929755449295,\n",
       "  0.0021056223195046186,\n",
       "  0.0014744630316272378,\n",
       "  0.002384107792750001,\n",
       "  0.0028766037430614233,\n",
       "  0.002655262127518654,\n",
       "  0.0033198236487805843,\n",
       "  0.005598279181867838,\n",
       "  0.0032501311507076025,\n",
       "  0.0022520797792822123,\n",
       "  0.0034661239478737116,\n",
       "  0.004465304780751467,\n",
       "  0.0049664173275232315,\n",
       "  0.00310455821454525,\n",
       "  0.001290412386879325,\n",
       "  0.0031034580897539854,\n",
       "  0.00431629316881299,\n",
       "  0.0022297282703220844,\n",
       "  0.0020355540327727795,\n",
       "  0.0038087524008005857,\n",
       "  0.0018219429766759276,\n",
       "  0.0012537527363747358,\n",
       "  0.003178491722792387,\n",
       "  0.0032956202048808336,\n",
       "  0.003042833646759391,\n",
       "  0.00296725332736969,\n",
       "  0.0037782895378768444,\n",
       "  0.001930988160893321,\n",
       "  0.001653631217777729,\n",
       "  0.003249137895181775,\n",
       "  0.0030889157205820084,\n",
       "  0.0038106529973447323,\n",
       "  0.0036808056756854057,\n",
       "  0.004858382977545261,\n",
       "  0.0027589646633714437,\n",
       "  0.0034314519725739956,\n",
       "  0.006688674911856651,\n",
       "  0.00665816105902195,\n",
       "  0.005262864753603935,\n",
       "  0.008855638094246387,\n",
       "  0.0062578641809523106,\n",
       "  0.005471376236528158,\n",
       "  0.005514886695891619,\n",
       "  0.004369099624454975,\n",
       "  0.004311948083341122,\n",
       "  0.002832158003002405,\n",
       "  0.0016181430546566844,\n",
       "  0.002846871502697468,\n",
       "  0.002172901527956128,\n",
       "  0.002753399545326829,\n",
       "  0.002012519631534815,\n",
       "  0.0032272799871861935,\n",
       "  0.002593086799606681,\n",
       "  0.003263111924752593,\n",
       "  0.002477784641087055,\n",
       "  0.002182251075282693,\n",
       "  0.001483751810155809,\n",
       "  0.0018908214988186955,\n",
       "  0.0008344178786501288,\n",
       "  0.002655566204339266,\n",
       "  0.0026005753315985203,\n",
       "  0.0016916841268539429,\n",
       "  0.002340418752282858,\n",
       "  0.003390917321667075,\n",
       "  0.003091566264629364,\n",
       "  0.0030540782026946545,\n",
       "  0.005063030403107405,\n",
       "  0.0034317034296691418,\n",
       "  0.003508550813421607,\n",
       "  0.003991624340415001,\n",
       "  0.003980978392064571,\n",
       "  0.003323514712974429,\n",
       "  0.003009446430951357,\n",
       "  0.0013831168180331588,\n",
       "  0.002694837749004364,\n",
       "  0.0033306728582829237,\n",
       "  0.004623588174581528,\n",
       "  0.0029432803858071566,\n",
       "  0.0035182556603103876,\n",
       "  0.0025450470857322216,\n",
       "  0.004214960150420666,\n",
       "  0.0029368512332439423,\n",
       "  0.0023000873625278473,\n",
       "  0.0025281626731157303,\n",
       "  0.0029861018992960453,\n",
       "  0.0032460426446050406,\n",
       "  0.0024561306927353144,\n",
       "  0.0031499892938882113,\n",
       "  0.002968595130369067,\n",
       "  0.003163113258779049,\n",
       "  0.0036430598702281713,\n",
       "  0.004424376413226128,\n",
       "  0.0031952590215951204,\n",
       "  0.0024752335157245398,\n",
       "  0.0032795011065900326,\n",
       "  0.0035175117664039135,\n",
       "  0.005895551759749651,\n",
       "  0.005237461533397436,\n",
       "  0.00546315498650074,\n",
       "  0.0058989571407437325,\n",
       "  0.006377341691404581,\n",
       "  0.00540406396612525,\n",
       "  0.00608722073957324,\n",
       "  0.0035498912911862135,\n",
       "  0.003303856821730733,\n",
       "  0.005037447437644005,\n",
       "  0.0029436498880386353,\n",
       "  0.004742949269711971,\n",
       "  0.0027695191092789173,\n",
       "  0.0030714881140738726,\n",
       "  0.0034895201679319143,\n",
       "  0.002534228377044201,\n",
       "  0.0030326424166560173,\n",
       "  0.0027619563043117523,\n",
       "  0.0024183636996895075,\n",
       "  0.004770458675920963,\n",
       "  0.002989601343870163,\n",
       "  0.0019197715446352959,\n",
       "  0.002972033806145191,\n",
       "  0.004191695246845484,\n",
       "  0.0014810188440605998,\n",
       "  0.004536315333098173,\n",
       "  0.004585358314216137,\n",
       "  0.0027054958045482635,\n",
       "  0.0033876497764140368,\n",
       "  0.0038664082530885935,\n",
       "  0.0028462312184274197,\n",
       "  0.0033048978075385094,\n",
       "  0.004236441105604172,\n",
       "  0.004734769929200411,\n",
       "  0.002329531591385603,\n",
       "  0.0010607973672449589,\n",
       "  0.004304409492760897,\n",
       "  0.0028602939564734697,\n",
       "  0.002392553724348545,\n",
       "  0.0027982972096651793,\n",
       "  0.003508044872432947,\n",
       "  0.0028752346988767385,\n",
       "  0.003988552838563919,\n",
       "  0.00259994575753808,\n",
       "  0.00420176750048995,\n",
       "  0.0034038270823657513,\n",
       "  0.002984285354614258,\n",
       "  0.0030153291299939156,\n",
       "  0.004125781822949648,\n",
       "  0.0035568156745284796,\n",
       "  0.0036195425782352686,\n",
       "  0.0038547792937606573,\n",
       "  0.0032083457335829735,\n",
       "  0.006258687935769558,\n",
       "  0.00359032372944057,\n",
       "  0.005943131633102894,\n",
       "  0.004882847424596548,\n",
       "  0.007051223888993263,\n",
       "  0.005354631692171097,\n",
       "  0.005417316686362028,\n",
       "  0.004346322268247604,\n",
       "  0.004398297984153032,\n",
       "  0.0035387526731938124,\n",
       "  0.0030117337591946125,\n",
       "  0.0038199364207684994,\n",
       "  0.003595113754272461,\n",
       "  0.0031597267370671034,\n",
       "  0.003598047187551856,\n",
       "  0.0034088457468897104,\n",
       "  0.002769197802990675,\n",
       "  0.0025275342632085085,\n",
       "  0.0006031669327057898,\n",
       "  0.002473659347742796,\n",
       "  0.0025277934037148952,\n",
       "  0.0014189841458573937,\n",
       "  0.001794401672668755,\n",
       "  0.002914267126470804,\n",
       "  0.0020838433410972357,\n",
       "  0.0016386291244998574,\n",
       "  0.00231719296425581,\n",
       "  0.003909106831997633,\n",
       "  0.004259306471794844,\n",
       "  0.0034438923466950655,\n",
       "  0.0017450916348025203,\n",
       "  0.0036138014402240515,\n",
       "  0.0025272993370890617,\n",
       "  0.002937599550932646,\n",
       "  0.0022250062320381403,\n",
       "  0.0030386897269636393,\n",
       "  0.0031572715379297733,\n",
       "  0.0033035785891115665,\n",
       "  0.00531739229336381,\n",
       "  0.002234778366982937,\n",
       "  0.0025518913753330708,\n",
       "  0.0025003706105053425,\n",
       "  0.0027666818350553513,\n",
       "  0.001868584775365889,\n",
       "  0.0036098528653383255,\n",
       "  0.0031332545913755894,\n",
       "  0.0017823231173679233,\n",
       "  0.0029150231275707483,\n",
       "  0.002619751961901784,\n",
       "  0.0030686059035360813,\n",
       "  0.0024334872141480446,\n",
       "  0.0026522388216108084,\n",
       "  0.002621673047542572,\n",
       "  0.0021352884359657764,\n",
       "  0.003417608793824911,\n",
       "  0.0039183213375508785,\n",
       "  0.0036879342515021563,\n",
       "  0.006250306498259306,\n",
       "  0.00619217474013567,\n",
       "  0.006948813330382109,\n",
       "  0.005344306584447622,\n",
       "  0.005817754659801722,\n",
       "  0.005412694066762924,\n",
       "  0.004431271925568581,\n",
       "  0.007852903567254543,\n",
       "  0.006010770797729492,\n",
       "  0.0032617731485515833,\n",
       "  0.001573079265654087,\n",
       "  0.00423161406069994,\n",
       "  0.002367998007684946,\n",
       "  0.0031024098861962557,\n",
       "  0.0023640310391783714,\n",
       "  0.002877805382013321,\n",
       "  0.0035643011797219515,\n",
       "  0.0017389198765158653,\n",
       "  -0.0002503636642359197,\n",
       "  0.0016409142408519983,\n",
       "  0.0015449682250618935,\n",
       "  0.002072396455332637,\n",
       "  0.004086526110768318,\n",
       "  0.003809344256296754,\n",
       "  0.001695632585324347,\n",
       "  0.004653028678148985,\n",
       "  0.003896488342434168,\n",
       "  0.0021343466360121965,\n",
       "  0.0025913205463439226,\n",
       "  0.005883566103875637,\n",
       "  0.0031475925352424383,\n",
       "  0.00458930991590023,\n",
       "  0.005497366655617952,\n",
       "  0.0029789176769554615,\n",
       "  0.002585012000054121,\n",
       "  0.0038129668682813644,\n",
       "  0.002883713925257325,\n",
       "  0.0016564037650823593,\n",
       "  0.0037388233467936516,\n",
       "  0.00302548473700881,\n",
       "  0.0018384730210527778,\n",
       "  0.0017677597934380174,\n",
       "  0.0018452255753800273,\n",
       "  0.0032214056700468063,\n",
       "  0.0017724353820085526,\n",
       "  0.0019705959130078554,\n",
       "  0.0029373306315392256,\n",
       "  0.0043149683624506,\n",
       "  0.004174229688942432,\n",
       "  0.004280983004719019,\n",
       "  0.005125829949975014,\n",
       "  0.00461815670132637,\n",
       "  0.007633501663804054,\n",
       "  0.0071115801110863686,\n",
       "  0.00645693764090538,\n",
       "  0.0050439974293112755,\n",
       "  0.0064121875911951065,\n",
       "  0.006298408377915621,\n",
       "  0.0061244298703968525,\n",
       "  0.006067880894988775,\n",
       "  0.0025982381775975227,\n",
       "  0.005493091885000467,\n",
       "  0.0033668577671051025,\n",
       "  0.0053190868347883224,\n",
       "  0.0006815487868152559,\n",
       "  0.0024123387411236763,\n",
       "  0.0024774917401373386,\n",
       "  0.0021660728380084038,\n",
       "  0.003686456009745598,\n",
       "  0.002419820288196206,\n",
       "  0.004225417505949736,\n",
       "  0.0017673951806500554,\n",
       "  0.0020517136435955763,\n",
       "  0.00040430319495499134,\n",
       "  0.0034175997134298086,\n",
       "  0.003065234748646617,\n",
       "  0.0003020624862983823,\n",
       "  0.004070000723004341,\n",
       "  0.0019925073720514774,\n",
       "  0.0036772473249584436,\n",
       "  0.006063149776309729,\n",
       "  0.0030998883303254843,\n",
       "  0.006825966294854879,\n",
       "  0.00448673777282238,\n",
       "  0.004385580774396658,\n",
       "  0.002776746405288577,\n",
       "  0.002037342404946685,\n",
       "  0.0030475857201963663,\n",
       "  0.004065562970936298,\n",
       "  0.0024842761922627687,\n",
       "  0.003324479330331087,\n",
       "  0.006926769390702248,\n",
       "  0.004459026735275984,\n",
       "  0.001747008296661079,\n",
       "  0.0027388576418161392,\n",
       "  0.002189642284065485,\n",
       "  0.0008268824312835932,\n",
       "  0.002889217110350728,\n",
       "  0.0027778358198702335,\n",
       "  0.003556264331564307,\n",
       "  0.0033702331129461527,\n",
       "  0.004112021066248417,\n",
       "  0.001173502649180591,\n",
       "  0.001365663600154221,\n",
       "  0.0026411099825054407,\n",
       "  0.001160403829999268,\n",
       "  0.002665561391040683,\n",
       "  0.00337356049567461,\n",
       "  0.002507444005459547,\n",
       "  0.002850846154615283,\n",
       "  0.004112050402909517,\n",
       "  0.005436369217932224,\n",
       "  0.004844650626182556,\n",
       "  0.004442456644028425,\n",
       "  0.004364407155662775,\n",
       "  0.00579025037586689,\n",
       "  0.0053666685707867146,\n",
       "  0.006606233771890402,\n",
       "  0.004970671143382788,\n",
       "  0.006158333271741867,\n",
       "  0.0031851837411522865,\n",
       "  0.004363701678812504,\n",
       "  0.004245422314852476,\n",
       "  0.004109950270503759,\n",
       "  0.005935578607022762,\n",
       "  0.0022187517024576664,\n",
       "  0.0019313860684633255,\n",
       "  0.0037453994154930115,\n",
       "  0.003248424269258976,\n",
       "  0.0005874433554708958,\n",
       "  0.0015632235445082188,\n",
       "  0.0033430212642997503,\n",
       "  0.0010529453866183758,\n",
       "  0.0006457107956521213,\n",
       "  0.001663758186623454,\n",
       "  0.002231373218819499,\n",
       "  0.0026939662639051676,\n",
       "  -0.0010457803728058934,\n",
       "  0.0036589480005204678,\n",
       "  0.004808621946722269,\n",
       "  0.003753904951736331,\n",
       "  0.007488766685128212,\n",
       "  0.006227792706340551,\n",
       "  0.004542509559541941,\n",
       "  0.006553841754794121,\n",
       "  0.003396810730919242,\n",
       "  0.006218773778527975,\n",
       "  0.003658519359305501,\n",
       "  0.001792417373508215,\n",
       "  0.002406172454357147,\n",
       "  0.002688758308067918,\n",
       "  0.004730180371552706,\n",
       "  0.0024232238065451384,\n",
       "  0.0031077160965651274,\n",
       "  0.003722710534930229,\n",
       "  0.0033785575069487095,\n",
       "  0.00136163632851094,\n",
       "  0.0015762693947181106,\n",
       "  0.002082074759528041,\n",
       "  0.003104796167463064,\n",
       "  0.003367065917700529,\n",
       "  0.0038394087459892035,\n",
       "  0.0033057781402021646,\n",
       "  0.004924065433442593,\n",
       "  0.003946762997657061,\n",
       "  0.005012079607695341,\n",
       "  0.00406000716611743,\n",
       "  0.00637413002550602,\n",
       "  0.004664160776883364,\n",
       "  0.004742358811199665,\n",
       "  0.003867505118250847,\n",
       "  0.0059824720956385136,\n",
       "  0.0040081157349050045,\n",
       "  0.006581012159585953,\n",
       "  0.005640452262014151,\n",
       "  0.004687990061938763,\n",
       "  0.004872834775596857,\n",
       "  0.004082128405570984,\n",
       "  0.0030578216537833214,\n",
       "  0.003342584241181612,\n",
       "  0.0037094494327902794,\n",
       "  0.0028382688760757446,\n",
       "  0.0036220820620656013,\n",
       "  0.003694332204759121,\n",
       "  0.002217038068920374,\n",
       "  0.002069631824269891,\n",
       "  0.002885331865400076,\n",
       "  0.0039314208552241325,\n",
       "  0.0016032103449106216,\n",
       "  0.002837528008967638,\n",
       "  0.002512957202270627,\n",
       "  0.0027815098874270916,\n",
       "  0.0015742926625534892,\n",
       "  0.0025882949121296406,\n",
       "  0.0017243520123884082,\n",
       "  0.0030538381543010473,\n",
       "  0.002141977194696665,\n",
       "  0.0036635834258049726,\n",
       "  0.0029588863253593445,\n",
       "  0.0021311617456376553,\n",
       "  0.0029010861180722713,\n",
       "  0.006173979956656694,\n",
       "  0.0022198809310793877,\n",
       "  0.004692960064858198,\n",
       "  0.003904978046193719,\n",
       "  0.004074036609381437,\n",
       "  0.005395389162003994,\n",
       "  0.0025901878252625465,\n",
       "  0.003592641558498144,\n",
       "  0.0023671339731663465,\n",
       "  0.0019419731106609106,\n",
       "  0.003119349479675293,\n",
       "  0.0024711089208722115,\n",
       "  0.0009882233571261168,\n",
       "  0.0026796641759574413,\n",
       "  0.0023444765247404575,\n",
       "  0.0025105560198426247,\n",
       "  0.003854665905237198,\n",
       "  0.00522204115986824,\n",
       "  0.004183226265013218,\n",
       "  0.0030605874489992857,\n",
       "  0.006233712658286095,\n",
       "  0.005114715080708265,\n",
       "  0.0055399020202457905,\n",
       "  0.006039624568074942,\n",
       "  0.006995563395321369,\n",
       "  0.006197439040988684,\n",
       "  0.006411104463040829,\n",
       "  0.004473862238228321,\n",
       "  0.004767764825373888,\n",
       "  0.0050334506668150425,\n",
       "  0.002786060329526663,\n",
       "  0.003418950829654932,\n",
       "  0.0032423476222902536,\n",
       "  0.00506700249388814,\n",
       "  0.004861090332269669,\n",
       "  0.004375511314719915,\n",
       "  0.0019498268375173211,\n",
       "  0.0023148534819483757,\n",
       "  0.004923446103930473,\n",
       "  0.0033577086869627237,\n",
       "  0.00036826805444434285,\n",
       "  0.0016002156771719456,\n",
       "  0.0005276479641906917,\n",
       "  0.003586812876164913,\n",
       "  0.0018522882601246238,\n",
       "  0.0047891163267195225,\n",
       "  0.004357473459094763,\n",
       "  0.004153545945882797,\n",
       "  0.0013726209290325642,\n",
       "  0.0021463504526764154,\n",
       "  0.002200392307713628,\n",
       "  0.0020253697875887156,\n",
       "  0.003258524462580681,\n",
       "  0.005744850262999535,\n",
       "  0.004184510093182325,\n",
       "  0.0023790663108229637,\n",
       "  0.004379380494356155,\n",
       "  0.005913573782891035,\n",
       "  0.00408595148473978,\n",
       "  0.0023405058309435844,\n",
       "  0.0032725753262639046,\n",
       "  0.0019121738150715828,\n",
       "  0.0034208318684250116,\n",
       "  0.003887021215632558,\n",
       "  0.0026045499835163355,\n",
       "  0.0030560146551579237,\n",
       "  0.003407626412808895,\n",
       "  0.0030673707369714975,\n",
       "  0.00230099237523973,\n",
       "  0.0029510571621358395,\n",
       "  0.0018687843112275004,\n",
       "  0.0036574476398527622,\n",
       "  0.003242267295718193,\n",
       "  0.002441598568111658,\n",
       "  0.0033490145578980446,\n",
       "  0.003196143778041005,\n",
       "  0.0022927692625671625,\n",
       "  0.00403627147898078,\n",
       "  0.004540964495390654,\n",
       "  0.004369176458567381,\n",
       "  0.005132185760885477,\n",
       "  0.005048173014074564,\n",
       "  0.003978417254984379,\n",
       "  0.0040184007957577705,\n",
       "  0.0034774858504533768,\n",
       "  0.006401375401765108,\n",
       "  0.005706332623958588,\n",
       "  0.005289828404784203,\n",
       "  0.0018620879855006933,\n",
       "  0.004299660678952932,\n",
       "  0.004069795366376638,\n",
       "  0.0028558471240103245,\n",
       "  0.0031259648967534304,\n",
       "  0.004686607047915459,\n",
       "  0.00415671244263649,\n",
       "  0.00238255737349391,\n",
       "  0.0027196602895855904,\n",
       "  0.002041621133685112,\n",
       "  0.00023901736130937934,\n",
       "  0.003046223893761635,\n",
       "  0.0025946495588868856,\n",
       "  0.002311672316864133,\n",
       "  0.0008073130156844854,\n",
       "  0.004116129130125046,\n",
       "  0.0020023451652377844,\n",
       "  0.0021896366961300373,\n",
       "  0.0030621362384408712,\n",
       "  0.0022478243336081505,\n",
       "  0.0010209071915596724,\n",
       "  0.003043766599148512,\n",
       "  0.004168992396444082,\n",
       "  0.004614569246768951,\n",
       "  0.003161120694130659,\n",
       "  0.00356674543581903,\n",
       "  0.0033404650166630745,\n",
       "  0.004204219207167625,\n",
       "  0.003889656625688076,\n",
       "  0.0057199252769351006,\n",
       "  0.004729302600026131,\n",
       "  0.003234167816117406,\n",
       "  0.003557984484359622,\n",
       "  0.0012619945919141173,\n",
       "  0.0016097917687147856,\n",
       "  0.0026054722256958485,\n",
       "  0.002344041597098112,\n",
       "  0.002355707110837102,\n",
       "  0.003140151035040617,\n",
       "  0.003846468636766076,\n",
       "  0.003166308393701911,\n",
       "  0.002136185532435775,\n",
       "  0.002434131922200322,\n",
       "  0.0027434825897216797,\n",
       "  0.0030569799710065126,\n",
       "  0.002444613492116332,\n",
       "  0.0034874214325100183,\n",
       "  0.003933371044695377,\n",
       "  0.004615450277924538,\n",
       "  0.004091733600944281,\n",
       "  0.005060275550931692,\n",
       "  0.0050723557360470295,\n",
       "  0.005281370133161545,\n",
       "  0.0051171244122087955,\n",
       "  0.005980253219604492,\n",
       "  0.006515043787658215,\n",
       "  0.0055758836679160595,\n",
       "  0.003327476093545556,\n",
       "  0.005283909384161234,\n",
       "  0.003916768357157707,\n",
       "  0.004340954124927521,\n",
       "  0.0036959387362003326,\n",
       "  0.002896188758313656,\n",
       "  0.004606955219060183,\n",
       "  0.002969097113236785,\n",
       "  0.0024405340664088726,\n",
       "  0.002604710403829813,\n",
       "  0.0006163851357996464,\n",
       "  ...],\n",
       " [0.0004682897124439478,\n",
       "  0.0021682451479136944,\n",
       "  0.007352807559072971,\n",
       "  0.012054343707859516,\n",
       "  0.015959320589900017,\n",
       "  0.013804583810269833,\n",
       "  0.014235074631869793,\n",
       "  0.013734778389334679,\n",
       "  0.011920932680368423,\n",
       "  0.015257309190928936,\n",
       "  0.013687126338481903,\n",
       "  0.010875674895942211,\n",
       "  0.010555633343756199,\n",
       "  0.011225746013224125,\n",
       "  0.012440863996744156,\n",
       "  0.01132606714963913,\n",
       "  0.010427881963551044,\n",
       "  0.0077652959153056145,\n",
       "  0.005756549071520567,\n",
       "  0.005146542564034462,\n",
       "  0.004817175678908825,\n",
       "  0.003903880249708891,\n",
       "  0.005460758227854967,\n",
       "  0.003913460299372673,\n",
       "  0.005086048040539026,\n",
       "  0.006048756185919046,\n",
       "  0.010494763031601906,\n",
       "  0.004439147189259529,\n",
       "  0.005867915693670511,\n",
       "  0.006986739579588175,\n",
       "  0.011153104715049267,\n",
       "  0.004510970786213875,\n",
       "  0.0051941960118710995,\n",
       "  0.0048101963475346565,\n",
       "  0.006226237863302231,\n",
       "  0.006960748694837093,\n",
       "  0.005341874901205301,\n",
       "  0.006227503530681133,\n",
       "  0.0025435129646211863,\n",
       "  0.0036281829234212637,\n",
       "  0.005090515129268169,\n",
       "  0.005253382958471775,\n",
       "  0.005070207640528679,\n",
       "  0.006569379009306431,\n",
       "  0.004295018967241049,\n",
       "  0.004912198521196842,\n",
       "  0.005482015665620565,\n",
       "  0.005082590505480766,\n",
       "  0.004726069513708353,\n",
       "  0.006574266590178013,\n",
       "  0.006421885918825865,\n",
       "  0.004693557042628527,\n",
       "  0.005626203026622534,\n",
       "  0.004668483044952154,\n",
       "  0.007846648804843426,\n",
       "  0.005827890243381262,\n",
       "  0.004084737505763769,\n",
       "  0.0036624756176024675,\n",
       "  0.0040611750446259975,\n",
       "  0.003984145354479551,\n",
       "  0.006380112841725349,\n",
       "  0.006744029000401497,\n",
       "  0.0036998731084167957,\n",
       "  0.005982985720038414,\n",
       "  0.006046402733772993,\n",
       "  0.005914435256272554,\n",
       "  0.007603053003549576,\n",
       "  0.004390248097479343,\n",
       "  0.005201292224228382,\n",
       "  0.00633403193205595,\n",
       "  0.006549431011080742,\n",
       "  0.0035057393833994865,\n",
       "  0.005996920168399811,\n",
       "  0.0046752579510211945,\n",
       "  0.005011213012039661,\n",
       "  0.0019957718905061483,\n",
       "  0.0035347763914614916,\n",
       "  0.003966113086789846,\n",
       "  0.0037157367914915085,\n",
       "  0.0024753313045948744,\n",
       "  0.0028843008913099766,\n",
       "  0.0022897468879818916,\n",
       "  0.002262272872030735,\n",
       "  0.002554809907451272,\n",
       "  0.0055835372768342495,\n",
       "  0.00558426883071661,\n",
       "  0.00638633081689477,\n",
       "  0.004988275934010744,\n",
       "  0.005582673940807581,\n",
       "  0.003195991739630699,\n",
       "  0.005678889341652393,\n",
       "  0.0057238140143454075,\n",
       "  0.00610117893666029,\n",
       "  0.00549693126231432,\n",
       "  0.0029798238538205624,\n",
       "  0.005648275837302208,\n",
       "  0.003957025706768036,\n",
       "  0.0028989745769649744,\n",
       "  0.0034412976820021868,\n",
       "  0.0025386640336364508,\n",
       "  0.0027844742871820927,\n",
       "  0.00412095058709383,\n",
       "  0.0034340624697506428,\n",
       "  0.0032781928312033415,\n",
       "  0.001565355691127479,\n",
       "  0.002586657414212823,\n",
       "  0.0009841774590313435,\n",
       "  0.004013713914901018,\n",
       "  0.003271569265052676,\n",
       "  0.001806852174922824,\n",
       "  0.004906586837023497,\n",
       "  0.002360799815505743,\n",
       "  0.002162911230698228,\n",
       "  0.0021878716070204973,\n",
       "  0.0028005363419651985,\n",
       "  0.003613966517150402,\n",
       "  0.0033320493530482054,\n",
       "  0.003622565185651183,\n",
       "  0.003312245709821582,\n",
       "  0.00381479412317276,\n",
       "  0.0026854886673390865,\n",
       "  0.005247952416539192,\n",
       "  0.0030533275566995144,\n",
       "  0.00400035222992301,\n",
       "  0.001885701552964747,\n",
       "  0.003116811392828822,\n",
       "  0.002794261323288083,\n",
       "  0.0021911109797656536,\n",
       "  0.0028387673664838076,\n",
       "  0.0034607851412147284,\n",
       "  0.003603923600167036,\n",
       "  0.00261403014883399,\n",
       "  0.0031373491510748863,\n",
       "  0.0024662804789841175,\n",
       "  0.0030371968168765306,\n",
       "  0.0043253665789961815,\n",
       "  0.0033898227848112583,\n",
       "  0.0016973582096397877,\n",
       "  0.0030230639968067408,\n",
       "  0.0026892204768955708,\n",
       "  0.0036461129784584045,\n",
       "  0.006045211106538773,\n",
       "  0.003770798444747925,\n",
       "  0.0012683682143688202,\n",
       "  0.004463219549506903,\n",
       "  0.0033383534755557775,\n",
       "  0.006133104674518108,\n",
       "  0.0033065134193748236,\n",
       "  0.0020040860399603844,\n",
       "  0.004347872920334339,\n",
       "  0.003410336561501026,\n",
       "  0.0030606426298618317,\n",
       "  0.003157456638291478,\n",
       "  0.0027719016652554274,\n",
       "  0.0030131435487419367,\n",
       "  0.0037702047266066074,\n",
       "  0.0027652450371533632,\n",
       "  0.0032437543850392103,\n",
       "  0.0017314510187134147,\n",
       "  0.0016613577026873827,\n",
       "  0.004413091577589512,\n",
       "  0.002821711590513587,\n",
       "  0.0021158596500754356,\n",
       "  0.0011439707595854998,\n",
       "  0.0029670502990484238,\n",
       "  0.0019584880210459232,\n",
       "  0.002730552339926362,\n",
       "  0.003706537652760744,\n",
       "  0.002571288263425231,\n",
       "  0.002539128065109253,\n",
       "  0.0033200683537870646,\n",
       "  0.003307976061478257,\n",
       "  0.002823856193572283,\n",
       "  0.003768594702705741,\n",
       "  0.0019261760171502829,\n",
       "  0.004187106154859066,\n",
       "  0.0032146568410098553,\n",
       "  0.002581457607448101,\n",
       "  0.003594573587179184,\n",
       "  0.0004884228692390025,\n",
       "  0.003096171887591481,\n",
       "  0.003664071671664715,\n",
       "  0.003215561620891094,\n",
       "  0.004755114670842886,\n",
       "  0.004653031472116709,\n",
       "  0.004556368570774794,\n",
       "  0.004114382900297642,\n",
       "  0.004764792043715715,\n",
       "  0.004174147732555866,\n",
       "  0.003731367876753211,\n",
       "  0.006642203778028488,\n",
       "  0.0056378100998699665,\n",
       "  0.006054909434169531,\n",
       "  0.003972435370087624,\n",
       "  0.00461193872615695,\n",
       "  0.005108082201331854,\n",
       "  0.003673644969239831,\n",
       "  0.003664475167170167,\n",
       "  0.004432814661413431,\n",
       "  0.0021904578898102045,\n",
       "  0.0030865040607750416,\n",
       "  0.000714968831744045,\n",
       "  0.003125235205516219,\n",
       "  0.00256189308129251,\n",
       "  0.002011575736105442,\n",
       "  0.0025880488101392984,\n",
       "  0.002720741555094719,\n",
       "  0.0014681878965348005,\n",
       "  0.0010120102670043707,\n",
       "  0.0028371524531394243,\n",
       "  0.0023385488893836737,\n",
       "  0.001584388897754252,\n",
       "  0.0016741841100156307,\n",
       "  0.0017745229415595531,\n",
       "  0.0031063726637512445,\n",
       "  0.0032163735013455153,\n",
       "  0.0020073747728019953,\n",
       "  0.003184974892064929,\n",
       "  0.005039909854531288,\n",
       "  0.00373117346316576,\n",
       "  0.003438185900449753,\n",
       "  0.004145240876823664,\n",
       "  0.0053769187070429325,\n",
       "  0.004803848918527365,\n",
       "  0.003639986738562584,\n",
       "  0.0036228385288268328,\n",
       "  0.0030152692925184965,\n",
       "  0.001733467448502779,\n",
       "  0.0030300684738904238,\n",
       "  0.0024153003469109535,\n",
       "  0.002261683577671647,\n",
       "  0.003322884440422058,\n",
       "  0.002100386656820774,\n",
       "  0.0023975581862032413,\n",
       "  0.002246418735012412,\n",
       "  0.0016337994020432234,\n",
       "  0.002485095988959074,\n",
       "  0.0021161732729524374,\n",
       "  0.0026638449635356665,\n",
       "  0.0031202915124595165,\n",
       "  0.004427146632224321,\n",
       "  0.0044707199558615685,\n",
       "  0.005879755597561598,\n",
       "  0.005507482681423426,\n",
       "  0.006304208654910326,\n",
       "  0.00499248132109642,\n",
       "  0.006973999086767435,\n",
       "  0.004269130527973175,\n",
       "  0.004222685005515814,\n",
       "  0.005266259890049696,\n",
       "  0.00559433875605464,\n",
       "  0.005055943038314581,\n",
       "  0.0035934720654040575,\n",
       "  0.003113210666924715,\n",
       "  0.004361799918115139,\n",
       "  0.003080317284911871,\n",
       "  0.005389216355979443,\n",
       "  0.003795335302129388,\n",
       "  0.001112666679546237,\n",
       "  0.0023819096386432648,\n",
       "  0.003441608278080821,\n",
       "  0.002334628952667117,\n",
       "  0.0018888766644522548,\n",
       "  0.001736271078698337,\n",
       "  -0.0002925076405517757,\n",
       "  0.001688837306573987,\n",
       "  0.0033506930340081453,\n",
       "  0.001902119256556034,\n",
       "  0.003520331345498562,\n",
       "  0.001358583802357316,\n",
       "  0.0020232319366186857,\n",
       "  0.003791957627981901,\n",
       "  0.002708438551053405,\n",
       "  0.0032374016009271145,\n",
       "  0.0027716169133782387,\n",
       "  0.0049778311513364315,\n",
       "  0.005166169255971909,\n",
       "  0.003657594555988908,\n",
       "  0.004377557896077633,\n",
       "  0.0054910508915781975,\n",
       "  0.002706710947677493,\n",
       "  0.002525684889405966,\n",
       "  0.002667429391294718,\n",
       "  0.003891562344506383,\n",
       "  0.003615737659856677,\n",
       "  0.0037636749912053347,\n",
       "  0.003245964180678129,\n",
       "  0.0034716487862169743,\n",
       "  0.0031293632928282022,\n",
       "  0.003086304059252143,\n",
       "  0.0027708210982382298,\n",
       "  0.0026502057444304228,\n",
       "  0.0037728261668235064,\n",
       "  0.003300686366856098,\n",
       "  0.0036541258450597525,\n",
       "  0.002793758176267147,\n",
       "  0.001998373307287693,\n",
       "  0.0035341493785381317,\n",
       "  0.004617166239768267,\n",
       "  0.0042243944481015205,\n",
       "  0.003130638040602207,\n",
       "  0.003437791718170047,\n",
       "  0.005064012948423624,\n",
       "  0.004304655361920595,\n",
       "  0.003863800782710314,\n",
       "  0.003702387446537614,\n",
       "  0.006224493496119976,\n",
       "  0.0074462974444031715,\n",
       "  0.003366551361978054,\n",
       "  0.005773583892732859,\n",
       "  0.006977396551519632,\n",
       "  0.005622669123113155,\n",
       "  0.002851241733878851,\n",
       "  0.003943588584661484,\n",
       "  0.0032261312007904053,\n",
       "  0.00247037410736084,\n",
       "  0.003538232995197177,\n",
       "  0.0022343499585986137,\n",
       "  0.0010754831600934267,\n",
       "  0.002461113268509507,\n",
       "  0.0011683611664921045,\n",
       "  0.0036312215961515903,\n",
       "  0.0027623288333415985,\n",
       "  0.0036597454454749823,\n",
       "  0.0012510414235293865,\n",
       "  0.0001846719824243337,\n",
       "  0.0017046664142981172,\n",
       "  0.003944711294025183,\n",
       "  0.0032839947380125523,\n",
       "  0.0030131780076771975,\n",
       "  0.003394756466150284,\n",
       "  0.001806918764486909,\n",
       "  0.002245883923023939,\n",
       "  0.004260678309947252,\n",
       "  0.003201909828931093,\n",
       "  0.0015526812057942152,\n",
       "  0.003313428023830056,\n",
       "  0.0035769003443419933,\n",
       "  0.003020977834239602,\n",
       "  0.004060381092131138,\n",
       "  0.0013660617405548692,\n",
       "  0.0048501561395823956,\n",
       "  0.0038161282427608967,\n",
       "  0.002726981183513999,\n",
       "  0.003869066946208477,\n",
       "  0.005727076902985573,\n",
       "  0.0038161484990268946,\n",
       "  0.0044259196147322655,\n",
       "  0.0031513613648712635,\n",
       "  0.004959915764629841,\n",
       "  0.0035849495325237513,\n",
       "  0.00297481007874012,\n",
       "  0.0031662904657423496,\n",
       "  0.0032686355989426374,\n",
       "  0.004204117693006992,\n",
       "  0.003982063382863998,\n",
       "  0.003938463982194662,\n",
       "  0.0028797988779842854,\n",
       "  0.0048209140077233315,\n",
       "  0.005192643031477928,\n",
       "  0.004552919417619705,\n",
       "  0.004102726932615042,\n",
       "  0.004736209753900766,\n",
       "  0.003708376782014966,\n",
       "  0.004907386843115091,\n",
       "  0.005720191169530153,\n",
       "  0.006452261470258236,\n",
       "  0.0025112160947173834,\n",
       "  0.00485345721244812,\n",
       "  0.004657293204218149,\n",
       "  0.004172022454440594,\n",
       "  0.003365920390933752,\n",
       "  0.0034601835068315268,\n",
       "  0.003007335588335991,\n",
       "  0.0032913093455135822,\n",
       "  0.0020837283227592707,\n",
       "  0.0036819023080170155,\n",
       "  0.0024693021550774574,\n",
       "  0.004516211804002523,\n",
       "  0.0025207363069057465,\n",
       "  0.0026817149482667446,\n",
       "  0.002934523392468691,\n",
       "  0.004485054407268763,\n",
       "  0.002363495994359255,\n",
       "  0.0025949389673769474,\n",
       "  0.002805058378726244,\n",
       "  0.0025084661319851875,\n",
       "  0.003061402589082718,\n",
       "  0.003977440297603607,\n",
       "  0.0018876828253269196,\n",
       "  0.0028287942986935377,\n",
       "  0.004455516580492258,\n",
       "  0.004259685520082712,\n",
       "  0.003608174156397581,\n",
       "  0.002317083301022649,\n",
       "  0.0024990299716591835,\n",
       "  0.0026236309204250574,\n",
       "  0.003248888533562422,\n",
       "  0.0041589075699448586,\n",
       "  0.004232770763337612,\n",
       "  0.0029122887644916773,\n",
       "  0.0017331454437226057,\n",
       "  0.002445982536301017,\n",
       "  0.0019954664167016745,\n",
       "  0.002264741575345397,\n",
       "  0.003676449414342642,\n",
       "  0.0021304760593920946,\n",
       "  0.0026039183139801025,\n",
       "  0.0026705830823630095,\n",
       "  0.0021517262794077396,\n",
       "  0.0032912481110543013,\n",
       "  0.004399902652949095,\n",
       "  0.00391580443829298,\n",
       "  0.002515845699235797,\n",
       "  0.004644294269382954,\n",
       "  0.0031266063451766968,\n",
       "  0.006110517308115959,\n",
       "  0.0036536853294819593,\n",
       "  0.003226314205676317,\n",
       "  0.0052512213587760925,\n",
       "  0.005399263929575682,\n",
       "  0.007203623652458191,\n",
       "  0.006633131764829159,\n",
       "  0.006981679238379002,\n",
       "  0.007193082943558693,\n",
       "  0.005391028709709644,\n",
       "  0.003979201894253492,\n",
       "  0.005437340587377548,\n",
       "  0.005692473147064447,\n",
       "  0.004717056639492512,\n",
       "  0.004184873774647713,\n",
       "  0.00602635508403182,\n",
       "  0.002623853273689747,\n",
       "  0.00285340822301805,\n",
       "  0.0023312720004469156,\n",
       "  0.0008846673299558461,\n",
       "  0.003861563978716731,\n",
       "  0.0034383349120616913,\n",
       "  0.004105585161596537,\n",
       "  0.0019004044588655233,\n",
       "  0.0029883235692977905,\n",
       "  0.002810074482113123,\n",
       "  0.004886936862021685,\n",
       "  0.004209761042147875,\n",
       "  0.004481703508645296,\n",
       "  0.0038481191731989384,\n",
       "  0.004180630668997765,\n",
       "  0.004093531519174576,\n",
       "  0.0042208475060760975,\n",
       "  0.005196081008762121,\n",
       "  0.0031726022716611624,\n",
       "  0.003326083067804575,\n",
       "  0.002796802669763565,\n",
       "  0.002765008481219411,\n",
       "  0.0028634360060095787,\n",
       "  0.002942032413557172,\n",
       "  0.0023670108057558537,\n",
       "  0.0024912760127335787,\n",
       "  0.00251725222915411,\n",
       "  0.0018072567181661725,\n",
       "  0.0016040741465985775,\n",
       "  0.002710002474486828,\n",
       "  0.002134998794645071,\n",
       "  0.004074947442859411,\n",
       "  0.0027020182460546494,\n",
       "  0.0022303715813905,\n",
       "  0.003286644583567977,\n",
       "  0.0024767275899648666,\n",
       "  0.004229066893458366,\n",
       "  0.0050772810354828835,\n",
       "  0.0036021440755575895,\n",
       "  0.0052088359370827675,\n",
       "  0.005378097295761108,\n",
       "  0.0049197375774383545,\n",
       "  0.005795187782496214,\n",
       "  0.007993181236088276,\n",
       "  0.004712208174169064,\n",
       "  0.005186180584132671,\n",
       "  0.004131559282541275,\n",
       "  0.005208215210586786,\n",
       "  0.0037549021653831005,\n",
       "  0.004955165553838015,\n",
       "  0.004696530289947987,\n",
       "  0.0032292173709720373,\n",
       "  0.0035153625067323446,\n",
       "  0.002341444371268153,\n",
       "  0.0021850059274584055,\n",
       "  0.002600058913230896,\n",
       "  0.003802285995334387,\n",
       "  0.0015800902619957924,\n",
       "  0.0028969186823815107,\n",
       "  0.0022340433206409216,\n",
       "  0.0017753468127921224,\n",
       "  0.002717565046623349,\n",
       "  0.0031682660337537527,\n",
       "  0.0032557216472923756,\n",
       "  0.003411024110391736,\n",
       "  0.0020765550434589386,\n",
       "  0.0033976284321397543,\n",
       "  0.0015500168083235621,\n",
       "  0.005808514077216387,\n",
       "  0.004146321676671505,\n",
       "  0.003937175963073969,\n",
       "  0.0023405198007822037,\n",
       "  0.0032277500722557306,\n",
       "  0.0033588213846087456,\n",
       "  0.0021721443627029657,\n",
       "  0.00338873453438282,\n",
       "  0.002284954534843564,\n",
       "  0.0038080622907727957,\n",
       "  0.002817499451339245,\n",
       "  0.0036547956988215446,\n",
       "  0.003472649957984686,\n",
       "  0.005108885932713747,\n",
       "  0.0032872152514755726,\n",
       "  0.003684690920636058,\n",
       "  0.004774566274136305,\n",
       "  0.004805340897291899,\n",
       "  0.0015593798598274589,\n",
       "  0.0014164824970066547,\n",
       "  0.0027303691022098064,\n",
       "  0.0032165369484573603,\n",
       "  0.0011797191109508276,\n",
       "  0.0031143524684011936,\n",
       "  0.0038119007367640734,\n",
       "  0.003204232780262828,\n",
       "  0.00234601809643209,\n",
       "  0.004785582888871431,\n",
       "  0.00312853348441422,\n",
       "  0.004308620002120733,\n",
       "  0.0031099319458007812,\n",
       "  0.0051150270737707615,\n",
       "  0.005826911423355341,\n",
       "  0.005508107598870993,\n",
       "  0.008292121812701225,\n",
       "  0.006174107547849417,\n",
       "  0.006089744158089161,\n",
       "  0.005497664213180542,\n",
       "  0.007562295068055391,\n",
       "  0.005522402469068766,\n",
       "  0.004739030264317989,\n",
       "  0.003217545570805669,\n",
       "  0.0033974810503423214,\n",
       "  0.0035404684022068977,\n",
       "  0.0031041214242577553,\n",
       "  0.0017450477462261915,\n",
       "  0.004016289487481117,\n",
       "  0.002685398329049349,\n",
       "  0.0034116485621780157,\n",
       "  0.003194682765752077,\n",
       "  0.002603304572403431,\n",
       "  0.0025672076735645533,\n",
       "  0.0020197562407702208,\n",
       "  0.002633273834362626,\n",
       "  0.002551788929849863,\n",
       "  0.0009694154723547399,\n",
       "  0.0020244477782398462,\n",
       "  0.0037942458875477314,\n",
       "  0.003768590511754155,\n",
       "  0.002294484293088317,\n",
       "  0.001457581645809114,\n",
       "  0.002115693874657154,\n",
       "  0.004122345708310604,\n",
       "  0.004243115894496441,\n",
       "  0.0026203857269138098,\n",
       "  0.0034445286728441715,\n",
       "  0.0021302541717886925,\n",
       "  0.006872219033539295,\n",
       "  0.004959677811712027,\n",
       "  0.002194053726270795,\n",
       "  0.005218907259404659,\n",
       "  0.0019884370267391205,\n",
       "  0.004083006642758846,\n",
       "  0.001938068657182157,\n",
       "  0.002836070256307721,\n",
       "  0.0024389594327658415,\n",
       "  0.0035305703058838844,\n",
       "  0.003369351616129279,\n",
       "  0.0035435764584690332,\n",
       "  0.004701137077063322,\n",
       "  0.004561486653983593,\n",
       "  0.003475477220490575,\n",
       "  0.0038275469560176134,\n",
       "  0.0045011951588094234,\n",
       "  0.0044915759935975075,\n",
       "  0.00428564241155982,\n",
       "  0.004717805422842503,\n",
       "  0.004387276712805033,\n",
       "  0.006079291924834251,\n",
       "  0.004857351537793875,\n",
       "  0.0050601172260940075,\n",
       "  0.006034400314092636,\n",
       "  0.005276304669678211,\n",
       "  0.0023969518952071667,\n",
       "  0.005695456173270941,\n",
       "  0.00360371102578938,\n",
       "  0.004171959590166807,\n",
       "  0.002669410314410925,\n",
       "  0.003143743611872196,\n",
       "  0.002355106407776475,\n",
       "  0.003334520850330591,\n",
       "  0.001321745803579688,\n",
       "  0.0017962133279070258,\n",
       "  0.002779551548883319,\n",
       "  0.001638379180803895,\n",
       "  0.0037585198879241943,\n",
       "  0.0018847222672775388,\n",
       "  0.0014270623214542866,\n",
       "  0.0009862938895821571,\n",
       "  0.0019757235422730446,\n",
       "  0.0031584620010107756,\n",
       "  0.0020934785716235638,\n",
       "  0.0026873124297708273,\n",
       "  0.00367937539704144,\n",
       "  0.0014139292761683464,\n",
       "  0.0025008295197039843,\n",
       "  0.004900984466075897,\n",
       "  0.002654507989063859,\n",
       "  0.002580192405730486,\n",
       "  0.002811310114338994,\n",
       "  0.0025862064212560654,\n",
       "  0.004029544070363045,\n",
       "  0.0042703598737716675,\n",
       "  0.0026817431207746267,\n",
       "  0.002700145123526454,\n",
       "  0.00464993342757225,\n",
       "  0.0032620567362755537,\n",
       "  0.0028308751061558723,\n",
       "  0.00461663817986846,\n",
       "  0.0027440229896456003,\n",
       "  0.0019303661538287997,\n",
       "  0.001908842008560896,\n",
       "  0.0019564651884138584,\n",
       "  0.0020209040958434343,\n",
       "  0.0028644350823014975,\n",
       "  0.0019327627960592508,\n",
       "  0.003350474638864398,\n",
       "  0.003425620961934328,\n",
       "  0.003682308364659548,\n",
       "  0.0032905300613492727,\n",
       "  0.004495029337704182,\n",
       "  0.0023395780008286238,\n",
       "  0.004925821907818317,\n",
       "  0.0051958137191832066,\n",
       "  0.006254215724766254,\n",
       "  0.006024293601512909,\n",
       "  0.005760320462286472,\n",
       "  0.0046280259266495705,\n",
       "  0.007323811296373606,\n",
       "  0.0069196526892483234,\n",
       "  0.005142740905284882,\n",
       "  0.006545088719576597,\n",
       "  0.003593346569687128,\n",
       "  0.0041591571643948555,\n",
       "  0.004049072507768869,\n",
       "  0.0029114102944731712,\n",
       "  0.003520722035318613,\n",
       "  0.004251618403941393,\n",
       "  0.0025233039632439613,\n",
       "  0.002768824575468898,\n",
       "  2.884977584471926e-05,\n",
       "  0.000662792706862092,\n",
       "  0.0026578442193567753,\n",
       "  0.0017496466170996428,\n",
       "  0.0006082887994125485,\n",
       "  0.004180311691015959,\n",
       "  0.0011844704858958721,\n",
       "  0.001664513605646789,\n",
       "  0.003658076748251915,\n",
       "  0.0017777391476556659,\n",
       "  0.0036962723825126886,\n",
       "  0.003046816447749734,\n",
       "  0.0043487995862960815,\n",
       "  0.00562665332108736,\n",
       "  0.0026846183463931084,\n",
       "  0.003853077534586191,\n",
       "  0.002440420910716057,\n",
       "  0.0024209232069551945,\n",
       "  0.0027720327489078045,\n",
       "  0.0026452457532286644,\n",
       "  0.0033189004752784967,\n",
       "  0.003202894004061818,\n",
       "  0.003009960288181901,\n",
       "  0.002770675579085946,\n",
       "  0.0023003544192761183,\n",
       "  0.0018991141114383936,\n",
       "  0.0017311241244897246,\n",
       "  0.002240611705929041,\n",
       "  0.003192594973370433,\n",
       "  0.003694571554660797,\n",
       "  0.0020448327995836735,\n",
       "  0.004155212547630072,\n",
       "  0.0049283867701888084,\n",
       "  0.004838442429900169,\n",
       "  0.005461915396153927,\n",
       "  0.005331837572157383,\n",
       "  0.004411704838275909,\n",
       "  0.006434177979826927,\n",
       "  0.006941711530089378,\n",
       "  0.004328287206590176,\n",
       "  0.006226284429430962,\n",
       "  0.004907896742224693,\n",
       "  0.004961101803928614,\n",
       "  0.005189693532884121,\n",
       "  0.003996527753770351,\n",
       "  0.005998751148581505,\n",
       "  0.003349203849211335,\n",
       "  0.0031392823439091444,\n",
       "  0.004012300167232752,\n",
       "  0.003925321623682976,\n",
       "  0.0028347482439130545,\n",
       "  0.002780448878183961,\n",
       "  0.003128151874989271,\n",
       "  0.0042759752832353115,\n",
       "  0.0020482647232711315,\n",
       "  0.0016068958211690187,\n",
       "  0.0049565378576517105,\n",
       "  5.3426032536663115e-05,\n",
       "  0.0022533871233463287,\n",
       "  0.0026392987929284573,\n",
       "  0.00044360943138599396,\n",
       "  0.0025857454165816307,\n",
       "  0.0008897192310541868,\n",
       "  0.0038916384801268578,\n",
       "  0.008151155896484852,\n",
       "  0.001554696704261005,\n",
       "  0.0028593039605766535,\n",
       "  0.0013864069478586316,\n",
       "  0.0026262893807142973,\n",
       "  0.004114452283829451,\n",
       "  0.00274241017177701,\n",
       "  0.004571207333356142,\n",
       "  0.004681570455431938,\n",
       "  0.0030727488920092583,\n",
       "  0.006174057722091675,\n",
       "  0.004188490100204945,\n",
       "  0.0038675302639603615,\n",
       "  0.0018983049085363746,\n",
       "  0.0015763399424031377,\n",
       "  0.002954211551696062,\n",
       "  0.0018744278931990266,\n",
       "  0.0011869603767991066,\n",
       "  0.0017537346575409174,\n",
       "  0.003255788004025817,\n",
       "  0.003960778471082449,\n",
       "  0.0036110025830566883,\n",
       "  0.0020763070788234472,\n",
       "  0.001812248257920146,\n",
       "  0.004777070600539446,\n",
       "  0.002898668171837926,\n",
       "  0.002993111265823245,\n",
       "  0.0035603102296590805,\n",
       "  0.003550460794940591,\n",
       "  0.003441185923293233,\n",
       "  0.00660797068849206,\n",
       "  0.004462553188204765,\n",
       "  0.00590974185615778,\n",
       "  0.0053101698867976665,\n",
       "  0.005856535397469997,\n",
       "  0.005460512824356556,\n",
       "  0.0068878415040671825,\n",
       "  0.006858990993350744,\n",
       "  0.005899928044527769,\n",
       "  0.0043735867366194725,\n",
       "  0.005602996796369553,\n",
       "  0.0022819628939032555,\n",
       "  0.005403987597674131,\n",
       "  0.0046720849350094795,\n",
       "  0.0010597090004011989,\n",
       "  0.003169998759403825,\n",
       "  0.003754378529265523,\n",
       "  0.002690809778869152,\n",
       "  0.0008093794458545744,\n",
       "  -0.00012676379992626607,\n",
       "  0.0028383522294461727,\n",
       "  0.001732747070491314,\n",
       "  3.670144360512495e-05,\n",
       "  -0.00019485475786495954,\n",
       "  0.002393430331721902,\n",
       "  0.001836331095546484,\n",
       "  0.0034785361494868994,\n",
       "  0.0027849918697029352,\n",
       "  0.002595180179923773,\n",
       "  0.006645317655056715,\n",
       "  0.005642878822982311,\n",
       "  0.004386434331536293,\n",
       "  0.0036688896361738443,\n",
       "  0.0033673788420856,\n",
       "  0.002866811817511916,\n",
       "  0.004181135445833206,\n",
       "  0.0023967656306922436,\n",
       "  0.0030297839548438787,\n",
       "  0.0038667195476591587,\n",
       "  0.0035625870805233717,\n",
       "  0.0022059190087020397,\n",
       "  0.00204574060626328,\n",
       "  0.0034148250706493855,\n",
       "  0.002184587065130472,\n",
       "  0.0016344813629984856,\n",
       "  0.002438624855130911,\n",
       "  0.0026693223044276237,\n",
       "  0.0018718810752034187,\n",
       "  0.0028617018833756447,\n",
       "  0.0037279098760336637,\n",
       "  0.0036438684910535812,\n",
       "  0.004726596642285585,\n",
       "  0.00447317399084568,\n",
       "  0.0030103111639618874,\n",
       "  0.004940551705658436,\n",
       "  0.005905053112655878,\n",
       "  0.005481136031448841,\n",
       "  0.004367182496935129,\n",
       "  0.0038418693002313375,\n",
       "  0.006258076056838036,\n",
       "  0.004270690958946943,\n",
       "  0.004014011472463608,\n",
       "  0.004080628976225853,\n",
       "  0.005955381318926811,\n",
       "  0.004050370771437883,\n",
       "  0.0037546067032963037,\n",
       "  0.0034228255972266197,\n",
       "  0.0036154473200440407,\n",
       "  0.004267185926437378,\n",
       "  0.004581034183502197,\n",
       "  0.003146898467093706,\n",
       "  0.003542488906532526,\n",
       "  0.0018245166866108775,\n",
       "  0.0027536116540431976,\n",
       "  0.002662487793713808,\n",
       "  0.0026337774470448494,\n",
       "  0.001752452808432281,\n",
       "  0.003127587493509054,\n",
       "  0.0024178861640393734,\n",
       "  0.0014719385653734207,\n",
       "  0.0040750219486653805,\n",
       "  0.002473612781614065,\n",
       "  0.0019076768076047301,\n",
       "  0.0018678446067497134,\n",
       "  0.0010221698321402073,\n",
       "  0.002677061827853322,\n",
       "  0.0035550747998058796,\n",
       "  0.005050490144640207,\n",
       "  0.0028378332499414682,\n",
       "  0.003858362790197134,\n",
       "  0.003313260618597269,\n",
       "  0.003632482374086976,\n",
       "  0.00300271506421268,\n",
       "  0.004000946879386902,\n",
       "  0.0015823168214410543,\n",
       "  0.003122438443824649,\n",
       "  0.0017514970386400819,\n",
       "  0.002502766903489828,\n",
       "  0.0016150400042533875,\n",
       "  0.0017245605122298002,\n",
       "  0.002765640150755644,\n",
       "  0.0018883747979998589,\n",
       "  0.0030618044547736645,\n",
       "  0.004790457896888256,\n",
       "  0.0033301811199635267,\n",
       "  0.003723729634657502,\n",
       "  0.001093827886506915,\n",
       "  0.0017651356756687164,\n",
       "  0.0032479772344231606,\n",
       "  0.0031867411453276873,\n",
       "  0.004350714385509491,\n",
       "  0.0051332195289433,\n",
       "  0.006729284301400185,\n",
       "  0.0050881593488156796,\n",
       "  0.005662134382873774,\n",
       "  0.00406799279153347,\n",
       "  0.004462779499590397,\n",
       "  0.005650199484080076,\n",
       "  0.00690314918756485,\n",
       "  0.0048820567317306995,\n",
       "  0.0044620526023209095,\n",
       "  0.0054589551873505116,\n",
       "  0.005736254621297121,\n",
       "  0.00447879871353507,\n",
       "  0.0039977929554879665,\n",
       "  0.004353964701294899,\n",
       "  0.0035882533993571997,\n",
       "  0.002785223303362727,\n",
       "  0.0026581590063869953,\n",
       "  0.002961772494018078,\n",
       "  0.001577940071001649,\n",
       "  0.0021352143958210945,\n",
       "  0.003117311978712678,\n",
       "  0.0020778027828782797,\n",
       "  0.001274589914828539,\n",
       "  0.0026795067824423313,\n",
       "  0.0002880601678043604,\n",
       "  0.00031372124794870615,\n",
       "  0.0006267569260671735,\n",
       "  0.0026369760744273663,\n",
       "  0.002648397581651807,\n",
       "  0.0028573041781783104,\n",
       "  0.002737801754847169,\n",
       "  0.00532520841807127,\n",
       "  0.005616253241896629,\n",
       "  0.0031207860447466373,\n",
       "  0.005701818969100714,\n",
       "  0.00321966758929193,\n",
       "  0.0032383438665419817,\n",
       "  0.004432317800819874,\n",
       "  0.0032871896401047707,\n",
       "  0.0031252240296453238,\n",
       "  0.00446214247494936,\n",
       "  0.0025799369905143976,\n",
       "  0.0015547771472483873,\n",
       "  0.004326433874666691,\n",
       "  0.001918973634019494,\n",
       "  0.0022395895794034004,\n",
       "  0.0027729959692806005,\n",
       "  0.0031375812832266092,\n",
       "  0.0027691717259585857,\n",
       "  0.0031059342436492443,\n",
       "  0.0024777567014098167,\n",
       "  0.0028473094571381807,\n",
       "  0.002386218635365367,\n",
       "  0.002923292340710759,\n",
       "  0.003294178517535329,\n",
       "  0.0032332257833331823,\n",
       "  0.002683514729142189,\n",
       "  0.0039595854468643665,\n",
       "  0.00430819159373641,\n",
       "  0.005923610646277666,\n",
       "  0.005854011978954077,\n",
       "  0.006436590570956469,\n",
       "  0.00646237563341856,\n",
       "  0.004470727872103453,\n",
       "  0.0055313026532530785,\n",
       "  0.005288914777338505,\n",
       "  0.005886164028197527,\n",
       "  0.005268429405987263,\n",
       "  0.004593344405293465,\n",
       "  0.003411112353205681,\n",
       "  0.004636775236576796,\n",
       "  0.003936787135899067,\n",
       "  0.0028771848883479834,\n",
       "  0.0032134847715497017,\n",
       "  0.0034965770319104195,\n",
       "  0.0032286085188388824,\n",
       "  0.002875964855775237,\n",
       "  0.00041627665632404387,\n",
       "  0.0035800558980554342,\n",
       "  0.001859521958976984,\n",
       "  0.0031594280153512955,\n",
       "  0.0007483534864149988,\n",
       "  0.001401072251610458,\n",
       "  0.00021149791427887976,\n",
       "  0.002266914350911975,\n",
       "  0.00316704367287457,\n",
       "  0.0005780123174190521,\n",
       "  0.0034427414648234844,\n",
       "  0.004427604377269745,\n",
       "  0.0023944214917719364,\n",
       "  0.003736269660294056,\n",
       "  0.0031972392462193966,\n",
       "  0.002976370509713888,\n",
       "  0.0026564563158899546,\n",
       "  0.0028868126682937145,\n",
       "  0.00354773853905499,\n",
       "  0.0021398658864200115,\n",
       "  0.004570331424474716,\n",
       "  0.0035488950088620186,\n",
       "  0.003334944136440754,\n",
       "  0.0022181333042681217,\n",
       "  0.003218760248273611,\n",
       "  0.0033770392183214426,\n",
       "  0.0017835868056863546,\n",
       "  0.0021973433904349804,\n",
       "  0.003474742639809847,\n",
       "  0.002564233262091875,\n",
       "  0.0014411048032343388,\n",
       "  0.0014585538301616907,\n",
       "  0.003315160982310772,\n",
       "  0.001054861000739038,\n",
       "  0.0026071227621287107,\n",
       "  0.0026016526389867067,\n",
       "  0.002382790669798851,\n",
       "  0.003376794047653675,\n",
       "  0.003337795613333583,\n",
       "  0.0023907464928925037,\n",
       "  0.0040192678570747375,\n",
       "  0.00470307981595397,\n",
       "  0.005272794049233198,\n",
       "  0.004512385930866003,\n",
       "  0.0043393392115831375,\n",
       "  0.0052864281460642815,\n",
       "  0.004257704131305218,\n",
       "  0.005494413897395134,\n",
       "  0.004515180829912424,\n",
       "  0.006077095866203308,\n",
       "  0.0041527827270329,\n",
       "  0.0027857369277626276,\n",
       "  0.0022739889100193977,\n",
       "  0.003972521983087063,\n",
       "  0.0037172113079577684,\n",
       "  0.0017770456615835428,\n",
       "  0.00365011440590024,\n",
       "  ...])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.train()\n",
    "netG_neg.train()\n",
    "# gen_losses, disc_losses = train_GAN(netD_neg, netG_neg, negative=True)\n",
    "train_GAN(netD_neg, netG_neg, tr=train_100k, epochs=10, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gen_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(disc_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test without train\n",
    "netD_neg_test = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg_test = NetG(train_100k.shape[1]).cuda()\n",
    "\n",
    "netD_neg_test.eval()\n",
    "netG_neg_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.eval()\n",
    "netG_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuraccy\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda() \n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k > 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake_accur_check = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without train\n",
    "fake_test_accur_check = netG_neg_test(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_accur_check_ = (fake_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_accur_check = (fake_test_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25897, 14847)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_accur_check_ * negative_feedback).sum(), (fake_accur_check_ * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20261, 24993)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_test_accur_check * negative_feedback).sum(), (fake_test_accur_check * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6367749391428361"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items\n",
    "(fake_accur_check_ * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49819272664683173"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items - WITHOUT TRAIN \n",
    "(fake_test_accur_check * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7024708923668864"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items\n",
    "((1-fake_accur_check_) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4991483136610489"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4991483136610489"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5799"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(netD_neg.state_dict(), './netD_neg-100k')\n",
    "# torch.save(netG_neg.state_dict(), './netG_neg-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fake_accur_check\n",
    "del fake_test_accur_check \n",
    "del e_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# predicting on ngetavie feedbacks\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k == 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = (fake >0.9).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_augment_negative = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13777078364356143, 0.2542968846049817, 0.6079323317514569]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_probs = [(train_100k == 1).sum()/((train_100k > 0) & (train_100k < 4)).sum(), (train_100k == 2).sum()/(((train_100k > 0) & (train_100k < 4))).sum(), (train_100k == 3).sum()/((train_100k > 0) & (train_100k < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_100k = train_100k + to_augment_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 5.711273883663719)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(augmented_train_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "# MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-da219d16cefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMF_SGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExplicitMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_train_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mMF_SGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[1;34m(self, iter_array, test, learning_rate)\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_iter, learning_rate)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpartial_train\u001b[1;34m(self, n_iter)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mctr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, u, i)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sgd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142822581632546\n",
      "Test mse: 1.179626365384201\n",
      "Iteration: 2\n",
      "Train mse: 1.0730309645422471\n",
      "Test mse: 1.1283647354414428\n",
      "Iteration: 5\n",
      "Train mse: 0.976704786728581\n",
      "Test mse: 1.0498895036356708\n",
      "Iteration: 10\n",
      "Train mse: 0.9191190735106731\n",
      "Test mse: 0.99882388507181\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8670975465312848\n",
      "Test mse: 0.9525039875303193\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8395297477072872\n",
      "Test mse: 0.9342662870213886\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7572607163519403\n",
      "Test mse: 0.917558302954112\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40646564364302945\n",
      "Test mse: 0.9047525011076776\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1583192976603522\n",
      "Test mse: 1.1778921160446154\n",
      "Iteration: 2\n",
      "Train mse: 1.0827823219708648\n",
      "Test mse: 1.124826924658217\n",
      "Iteration: 5\n",
      "Train mse: 0.9809962544391533\n",
      "Test mse: 1.0456674228179015\n",
      "Iteration: 10\n",
      "Train mse: 0.9217915122656181\n",
      "Test mse: 0.9960992361685521\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8695835693024129\n",
      "Test mse: 0.9521000726072203\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8421152097706776\n",
      "Test mse: 0.9345373660525688\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.755397379978269\n",
      "Test mse: 0.916590796822304\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40575111332308983\n",
      "Test mse: 0.906762464167835\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.35978032575701\n",
      "Test mse: 8.193157746225948\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359732338000212\n",
      "Test mse: 8.193099816595986\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359687859300425\n",
      "Test mse: 8.193046053932775\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359646525578646\n",
      "Test mse: 8.192996043819882\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3596080230681835\n",
      "Test mse: 8.192949426343956\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393714646752196\n",
      "Test mse: 8.194362304310943\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393666122560651\n",
      "Test mse: 8.194304259396569\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39362115513897\n",
      "Test mse: 8.194250397772203\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393579374306474\n",
      "Test mse: 8.19420030178969\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393540461520644\n",
      "Test mse: 8.19415360957236\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1583192976603522\n",
      "Test mse: 1.1778921160446154\n",
      "Iteration: 2\n",
      "Train mse: 1.0827823219708648\n",
      "Test mse: 1.124826924658217\n",
      "Iteration: 5\n",
      "Train mse: 0.9809962544391533\n",
      "Test mse: 1.0456674228179015\n",
      "Iteration: 10\n",
      "Train mse: 0.9217915122656181\n",
      "Test mse: 0.9960992361685521\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8695835693024129\n",
      "Test mse: 0.9521000726072203\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8421152097706776\n",
      "Test mse: 0.9345373660525688\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.755397379978269\n",
      "Test mse: 0.916590796822304\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6846501387489577\n",
      "Test mse: 0.9047955049119271\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6088625770108296\n",
      "Test mse: 0.8969742625646109\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5326639192133193\n",
      "Test mse: 0.8944780567537973\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4636710714200561\n",
      "Test mse: 0.8979252620915619\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.40575111332308983\n",
      "Test mse: 0.906762464167835\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.35905711689460085\n",
      "Test mse: 0.9198637997234861\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "\n",
    "best_sgd_model = ExplicitMF(train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1545119024961061\n",
      "Test mse: 1.178567399719999\n",
      "Iteration: 2\n",
      "Train mse: 1.0808310124661051\n",
      "Test mse: 1.1261795339578151\n",
      "Iteration: 5\n",
      "Train mse: 0.9808797891168647\n",
      "Test mse: 1.0474046524170364\n",
      "Iteration: 10\n",
      "Train mse: 0.9222637203091965\n",
      "Test mse: 0.997286882756945\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8708585227221491\n",
      "Test mse: 0.9523215310455245\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8464129951238863\n",
      "Test mse: 0.9346230958725521\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7999830140159173\n",
      "Test mse: 0.9226304802126559\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7510374101643228\n",
      "Test mse: 0.9117502158632841\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6909018150472902\n",
      "Test mse: 0.9010086763640088\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6230042394517625\n",
      "Test mse: 0.8922974818153959\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5493002333842844\n",
      "Test mse: 0.8866310890559449\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.47529433983651304\n",
      "Test mse: 0.8848626444854254\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4062577209484476\n",
      "Test mse: 0.8867158452927663\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(augmented_train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.158486007439644\n",
      "Test mse: 1.1779978613000384\n",
      "Iteration: 2\n",
      "Train mse: 1.083171581612149\n",
      "Test mse: 1.125028404345431\n",
      "Iteration: 5\n",
      "Train mse: 0.9817048627613273\n",
      "Test mse: 1.0459868491969369\n",
      "Iteration: 10\n",
      "Train mse: 0.9227319825521801\n",
      "Test mse: 0.9963652377560377\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8713373835757132\n",
      "Test mse: 0.9521701303030687\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.846936197066563\n",
      "Test mse: 0.934619043527751\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.8000609230973601\n",
      "Test mse: 0.9225265969757358\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7503523923902362\n",
      "Test mse: 0.9115515826764651\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.689505366026202\n",
      "Test mse: 0.9008884227444379\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6213034484620569\n",
      "Test mse: 0.8924318463805855\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5475532983376317\n",
      "Test mse: 0.8870274449200776\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4737341552445482\n",
      "Test mse: 0.8854985087833768\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4050262870207811\n",
      "Test mse: 0.8875516903462294\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(augmented_train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142972933039822\n",
      "Test mse: 1.17972828223402\n",
      "Iteration: 2\n",
      "Train mse: 1.0733920132206576\n",
      "Test mse: 1.1285610630076768\n",
      "Iteration: 5\n",
      "Train mse: 0.9773881833559704\n",
      "Test mse: 1.0502187304294268\n",
      "Iteration: 10\n",
      "Train mse: 0.9200456767969902\n",
      "Test mse: 0.999134022874239\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8688346580213652\n",
      "Test mse: 0.9526617108778075\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8442347461991091\n",
      "Test mse: 0.934442842680263\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7999403795934972\n",
      "Test mse: 0.9232297270102171\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7525020754101615\n",
      "Test mse: 0.9130561640976463\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6925429566514093\n",
      "Test mse: 0.902326297805037\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6242821764329887\n",
      "Test mse: 0.8932252838608207\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5502016125007768\n",
      "Test mse: 0.8871597136187851\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4759825330856412\n",
      "Test mse: 0.8851292592272328\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.40684917561304956\n",
      "Test mse: 0.8868481394223161\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393714646752196\n",
      "Test mse: 8.194362304310943\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393666122560651\n",
      "Test mse: 8.194304259396569\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39362115513897\n",
      "Test mse: 8.194250397772203\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393579374306474\n",
      "Test mse: 8.19420030178969\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393540461520644\n",
      "Test mse: 8.19415360957236\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39350414020554\n",
      "Test mse: 8.194110004824077\n"
     ]
    }
   ],
   "source": [
    "best_als_model = ExplicitMF(train_100k, n_factors=10, learning='als', \\\n",
    "                            item_fact_reg=0.1, user_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_als_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.35978032575701\n",
      "Test mse: 8.193157746225948\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359732338000212\n",
      "Test mse: 8.193099816595986\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359687859300425\n",
      "Test mse: 8.193046053932775\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359646525578646\n",
      "Test mse: 8.192996043819882\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3596080230681835\n",
      "Test mse: 8.192949426343956\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359572079254732\n",
      "Test mse: 8.192905886804215\n"
     ]
    }
   ],
   "source": [
    "best_als_model = ExplicitMF(augmented_train_100k, n_factors=10, learning='als', \\\n",
    "                            item_fact_reg=0.1, user_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_als_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "# from sklearn.decomposition import ProjectedGradientNMF\n",
    "from sklearn.decomposition import SparsePCA\n",
    "# model = NMF(n_components=10, max_iter=400, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "# W = model.fit_transform(train_100k)\n",
    "# H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((943, 10), (10, 1682))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4260560791980288"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NMF(n_components=20, max_iter=800, init='nndsvdar', beta_loss='frobenius', alpha=0.1, random_state=seed)\n",
    "model = NMF(n_components=10, max_iter=400, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "W = model.fit_transform(augmented_train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4260496894481052"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NMF(n_components=20, max_iter=800, init='nndsvdar', beta_loss='frobenius', alpha=0.1, random_state=seed)\n",
    "model = NMF(n_components=10, max_iter=400, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "W = model.fit_transform(train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4260560791980288"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py:170: DeprecationWarning: normalize_components=False is a backward-compatible setting that implements a non-standard definition of sparse PCA. This compatibility mode will be removed in 0.22.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-360-3f29c98d23de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparsePCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_train_100k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    185\u001b[0m                                                \u001b[0mcode_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcode_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                                                \u001b[0mdict_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                                                \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                                                )\n\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\dict_learning.py\u001b[0m in \u001b[0;36mdict_learning\u001b[1;34m(X, n_components, alpha, max_iter, tol, method, n_jobs, dict_init, code_init, callback, verbose, random_state, return_n_iter, positive_dict, positive_code)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Update code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha,\n\u001b[1;32m--> 574\u001b[1;33m                              init=code, n_jobs=n_jobs, positive=positive_code)\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Update dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         dictionary, residuals = _update_dict(dictionary.T, X.T, code.T,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\dict_learning.py\u001b[0m in \u001b[0;36msparse_encode\u001b[1;34m(X, dictionary, gram, cov, algorithm, n_nonzero_coefs, alpha, copy_cov, init, max_iter, n_jobs, check_input, verbose, positive)\u001b[0m\n\u001b[0;32m    311\u001b[0m                               \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m                               positive=positive)\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\dict_learning.py\u001b[0m in \u001b[0;36m_sparse_encode\u001b[1;34m(X, dictionary, gram, cov, algorithm, regularization, copy_cov, init, max_iter, check_input, verbose, positive)\u001b[0m\n\u001b[0;32m    121\u001b[0m                                    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                                    positive=positive)\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0mlasso_lars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[0mnew_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlasso_lars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, Xy)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path,\n\u001b[1;32m--> 708\u001b[1;33m                   Xy=Xy)\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_iter, alpha, fit_path, Xy)\u001b[0m\n\u001b[0;32m    665\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                     \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m                     positive=self.positive)\n\u001b[0m\u001b[0;32m    668\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36mlars_path\u001b[1;34m(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001b[0m\n\u001b[0;32m    288\u001b[0m                                         **solve_triangular_args)\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m             \u001b[0mdiag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_active\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SparsePCA(n_components=10, max_iter=40, alpha=0.1,  random_state=seed)\n",
    "W = model.fit_transform(augmented_train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38982933425042454"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py:170: DeprecationWarning: normalize_components=False is a backward-compatible setting that implements a non-standard definition of sparse PCA. This compatibility mode will be removed in 0.22.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = SparsePCA(n_components=10, max_iter=40, alpha=0.1,  random_state=seed)\n",
    "W = model.fit_transform(train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38935982618491954"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
