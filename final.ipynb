{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')\n",
    "    \n",
    "# # !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('./')\n",
    "    \n",
    "# # !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following blogpost's independent code to benchmark our experiments https://blog.insightdatascience.com/explicit-matrix-factorization-als-sgd-and-all-that-jazz-b00e4d9b21ea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent MF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1     1193       5\n",
       "1        1      661       3\n",
       "2        1      914       3\n",
       "3        1     3408       4\n",
       "4        1     2355       5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating']\n",
    "df = pd.read_csv('./ml-1m/ratings.dat', sep='::', usecols = [0, 1, 2], names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, (3706,), 3952)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.item_id.unique().max(), df.item_id.unique().shape, df.item_id.unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().max()\n",
    "n_items = df.item_id.unique().max()\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "#     print(row)\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.190220560634904"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    np.random.seed(seed)\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1m, test_1m = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.93718412338794, 0.25303643724696356)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_1m), get_sparsity(test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movielens-100k dataset\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_100k, test_100k = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 0.5945303210463734)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((943, 1682), (6040, 3952))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_100k.shape, train_1m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        np.random.seed(seed)\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * \\\n",
    "                                (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.8411868244782985\n",
      "Test mse: 10.956557787912471\n",
      "Iteration: 2\n",
      "Train mse: 5.731156257503645\n",
      "Test mse: 8.66466855942882\n",
      "Iteration: 5\n",
      "Train mse: 5.41223275733234\n",
      "Test mse: 8.225124472876551\n",
      "Iteration: 10\n",
      "Train mse: 5.394598612851955\n",
      "Test mse: 8.198143635604692\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.3944670279169324\n",
      "Test mse: 8.195114378325407\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394455951420811\n",
      "Test mse: 8.194896519679713\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.394255047026079\n",
      "Test mse: 8.194688553798981\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def plot_learning_curve(iter_array, model):\n",
    "    plt.plot(iter_array, model.train_mse, \\\n",
    "             label='Training', linewidth=5)\n",
    "    plt.plot(iter_array, model.test_mse, \\\n",
    "             label='Test', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('MSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(iter_array, MF_ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 8.133017313467287\n",
      "Test mse: 11.385478952329686\n",
      "Iteration: 2\n",
      "Train mse: 6.181422331570321\n",
      "Test mse: 9.510432053967698\n",
      "Iteration: 5\n",
      "Train mse: 5.945868319505223\n",
      "Test mse: 9.181499199125527\n",
      "Iteration: 10\n",
      "Train mse: 5.929921005207816\n",
      "Test mse: 9.163591900058499\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.922879579194776\n",
      "Test mse: 9.155637570112932\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.921741164281515\n",
      "Test mse: 9.152673442345948\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.921434636456923\n",
      "Test mse: 9.152593151189302\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_1m, n_factors=20, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142807793060997\n",
      "Test mse: 1.1796269767247503\n",
      "Iteration: 2\n",
      "Train mse: 1.0730325987217215\n",
      "Test mse: 1.1283621352778417\n",
      "Iteration: 5\n",
      "Train mse: 0.9767157588922124\n",
      "Test mse: 1.0499014864558258\n",
      "Iteration: 10\n",
      "Train mse: 0.9190727366615857\n",
      "Test mse: 0.9988387536431501\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8669457204379917\n",
      "Test mse: 0.9525818812444498\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8387251669562131\n",
      "Test mse: 0.9343161812537658\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7485848531412769\n",
      "Test mse: 0.9170005429005551\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40410579696377263\n",
      "Test mse: 0.9148070379768369\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Proposed GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train_100k == 0)\n",
    "positive_feedback = (train_100k > 3)\n",
    "negative_feedback = ((train_100k < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49901, 40669)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_feedback.sum(), negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback + negative_feedback != zero_mask).all()\n",
    "assert (positive_feedback + negative_feedback == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94.28986095682184, 3.146093059441684, 2.5640459837364746)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback), get_sparsity(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, mat, p=0.25, batch_size=128):\n",
    "        '''\n",
    "        mat is a binary matrix (e.g. positive feedback, or negative feedback)\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.mat = mat\n",
    "        self.p = p\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.mat.shape[0] / self.batch_size))\n",
    "    \n",
    "    def gen_item_GAN(self):\n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y, indexes\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_negative = DataGenerator(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _ = generator_negative.gen_item_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7064878121284186, 2.283832491082045)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(x), get_sparsity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, feat_size):\n",
    "        super(NetD, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "#         self.use_cuda = True\n",
    "#         self.feat_size = feat_size\n",
    "        # top\n",
    "#         print(self.feat_size*2)\n",
    "        self.t1 = torch.nn.Linear(self.feat_size, 1024)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(self.feat_size, 1024)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 1024, self.feat_size)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        \n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "#         if self.use_cuda: \n",
    "        idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "        x = filt * x\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_size):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz + self.feat_size, 1024), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "#                                 torch.nn.ReLU(), \n",
    "# #                                 nn.Dropout(0.5),\n",
    "#                                 torch.nn.Linear(2048, 2048),\n",
    "                                torch.nn.ReLU(), \n",
    "#                                 torch.nn.BatchNorm1d(512),\n",
    "#                                 nn.Dropout(0.6),\n",
    "                                torch.nn.Linear(1024, self.feat_size), \n",
    "                                torch.nn.Sigmoid()\n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "        \n",
    "    def forward(self, e_mask, x):\n",
    "        x = self.netGen(x)\n",
    "#         print(x.shape, )\n",
    "        x = x * e_mask\n",
    "        return x\n",
    "#         return F.dropout(x, 0.7)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_losses = []\n",
    "disc_losses = []\n",
    "def train_GAN(netD, netG, negative, tr, steps_per_epoch = 200, epochs = 10):\n",
    "    d_iter = 10\n",
    "    g_iter = 1\n",
    "    gen_iterations = 0\n",
    "#     gen_losses = []\n",
    "#     disc_losses = []\n",
    "#     train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for c in range(steps_per_epoch):\n",
    "#             data_iter = 100\n",
    "            i = 0\n",
    "#             while i < 100:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in netD.parameters(): # reset requires_grad\n",
    "                p.requires_grad = True # they are set to False below in netG update\n",
    "#             d_iter = d_iter\n",
    "            j = 0\n",
    "            while j < d_iter:\n",
    "                j += 1\n",
    "                # load real data\n",
    "                i += 1\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "#                         condition, X, idxs = batch_generator(X_neg, y_neg)\n",
    "#                 X, _ = data_iter.next()\n",
    "#                 X = X.view(X.size(0), -1)\n",
    "#                 X = (X >= 0.5).float()\n",
    "#                     if cuda: \n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "#                     X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "#     + torch.randn(X.size()).cuda() * 0.2\n",
    "#                 print(condition.shape, X_neg.shape, y_neg.shape)\n",
    "                real = Variable(X)\n",
    "\n",
    "                # generate fake data\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "#                     if cuda: \n",
    "                noise = noise.cuda()\n",
    "#                     noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "                with torch.no_grad():\n",
    "                    noisev = Variable(noise) # totally freeze netG\n",
    "                concated = torch.cat((noisev, condition), 1)\n",
    "#                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\n",
    "                e_mask = torch.Tensor(tr[idxs]>0).cuda()\n",
    "#                     print(e_mask.shape, concated.shape, condition.shape)\n",
    "                fake = Variable(netG(e_mask, concated).data)\n",
    "\n",
    "                # compute gradient, take step\n",
    "                netD.zero_grad()\n",
    "#                 concated_real = torch.cat((real, condition), 1)\n",
    "#                 print(concated_real)\n",
    "                out = netD(real, fake)\n",
    "                outputD = torch.mean(out) + lamba * out.norm()\n",
    "                stdD = torch.std(out)\n",
    "                outputD.backward(mone)\n",
    "                optimizerD.step()\n",
    "#                     print('AAAAAAAAA mse:=WWWWWWWWWWWWWWWWWWWWWW')\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "\n",
    "#         g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "                netG.zero_grad()\n",
    "                # load real data\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "\n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "    #                 X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "                real = Variable(X)\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "                noise = noise.cuda()\n",
    "                noisev = Variable(noise)\n",
    "                concated_ = torch.cat((noisev, condition), 1)\n",
    "                e_mask_ = torch.Tensor(tr[idxs]>0).cuda()\n",
    "\n",
    "                fake = netG(e_mask_, concated_)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "                gen_iterations += 1\n",
    "    #             print('AAAAAA')\n",
    "#                 eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "    #             eval_losses.append(eval_loss)\n",
    "    #             print('mse:', eval_loss)\n",
    "    #             print(outputG.item(), outputD.item())\n",
    "                gen_losses.append(outputG.item())\n",
    "                disc_losses.append(outputD.item())\n",
    "                print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, 100, gen_iterations, outputD.item(), outputG.item()))\n",
    "    return gen_losses, disc_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 128\n",
    "cuda = True\n",
    "epochs = 12\n",
    "# device = 5\n",
    "# seed = 1\n",
    "nz = 8\n",
    "lamba = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=1682, out_features=1024, bias=True)\n",
      "  (b1): Linear(in_features=1682, out_features=1024, bias=True)\n",
      "  (fc): Linear(in_features=2048, out_features=1682, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=1690, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1682, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_neg = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg = NetG(train_100k.shape[1]).cuda()\n",
    "print(netD_neg)\n",
    "print(netG_neg)\n",
    "optimizerG = optim.Adam(netG_neg.parameters(), lr=lrG, weight_decay=1e-4)\n",
    "optimizerD = optim.Adam(netD_neg.parameters(), lr=lrD, weight_decay=1e-4)\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = (-1 * one).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][11/100][1] Loss_D: 0.005009 Loss_G: 0.004957 \n",
      "[0/10][11/100][2] Loss_D: 0.004496 Loss_G: 0.004891 \n",
      "[0/10][11/100][3] Loss_D: 0.004807 Loss_G: 0.004153 \n",
      "[0/10][11/100][4] Loss_D: 0.003266 Loss_G: 0.004216 \n",
      "[0/10][11/100][5] Loss_D: 0.003872 Loss_G: 0.004134 \n",
      "[0/10][11/100][6] Loss_D: 0.004039 Loss_G: 0.004755 \n",
      "[0/10][11/100][7] Loss_D: 0.003965 Loss_G: 0.004231 \n",
      "[0/10][11/100][8] Loss_D: 0.004520 Loss_G: 0.003642 \n",
      "[0/10][11/100][9] Loss_D: 0.002767 Loss_G: 0.003363 \n",
      "[0/10][11/100][10] Loss_D: 0.003489 Loss_G: 0.002500 \n",
      "[0/10][11/100][11] Loss_D: 0.002434 Loss_G: 0.002661 \n",
      "[0/10][11/100][12] Loss_D: 0.003679 Loss_G: 0.004276 \n",
      "[0/10][11/100][13] Loss_D: 0.003276 Loss_G: 0.002073 \n",
      "[0/10][11/100][14] Loss_D: 0.002787 Loss_G: 0.002228 \n",
      "[0/10][11/100][15] Loss_D: 0.001644 Loss_G: 0.002510 \n",
      "[0/10][11/100][16] Loss_D: 0.003962 Loss_G: 0.002729 \n",
      "[0/10][11/100][17] Loss_D: 0.003268 Loss_G: 0.002235 \n",
      "[0/10][11/100][18] Loss_D: 0.003026 Loss_G: 0.003084 \n",
      "[0/10][11/100][19] Loss_D: 0.002298 Loss_G: 0.003611 \n",
      "[0/10][11/100][20] Loss_D: 0.002478 Loss_G: 0.001718 \n",
      "[0/10][11/100][21] Loss_D: 0.001144 Loss_G: 0.000815 \n",
      "[0/10][11/100][22] Loss_D: 0.002880 Loss_G: 0.003662 \n",
      "[0/10][11/100][23] Loss_D: 0.002834 Loss_G: 0.003650 \n",
      "[0/10][11/100][24] Loss_D: 0.003378 Loss_G: 0.003553 \n",
      "[0/10][11/100][25] Loss_D: 0.002427 Loss_G: 0.002609 \n",
      "[0/10][11/100][26] Loss_D: 0.004972 Loss_G: 0.001686 \n",
      "[0/10][11/100][27] Loss_D: 0.002120 Loss_G: 0.000999 \n",
      "[0/10][11/100][28] Loss_D: 0.003568 Loss_G: 0.004665 \n",
      "[0/10][11/100][29] Loss_D: 0.003353 Loss_G: 0.003206 \n",
      "[0/10][11/100][30] Loss_D: 0.004051 Loss_G: 0.003559 \n",
      "[0/10][11/100][31] Loss_D: 0.004164 Loss_G: 0.002014 \n",
      "[0/10][11/100][32] Loss_D: 0.002765 Loss_G: 0.003269 \n",
      "[0/10][11/100][33] Loss_D: 0.002636 Loss_G: 0.002056 \n",
      "[0/10][11/100][34] Loss_D: 0.003306 Loss_G: 0.003295 \n",
      "[0/10][11/100][35] Loss_D: 0.002171 Loss_G: 0.002345 \n",
      "[0/10][11/100][36] Loss_D: 0.002357 Loss_G: 0.003525 \n",
      "[0/10][11/100][37] Loss_D: 0.002678 Loss_G: 0.004216 \n",
      "[0/10][11/100][38] Loss_D: 0.002123 Loss_G: 0.004087 \n",
      "[0/10][11/100][39] Loss_D: 0.002831 Loss_G: 0.002475 \n",
      "[0/10][11/100][40] Loss_D: 0.003529 Loss_G: 0.003467 \n",
      "[0/10][11/100][41] Loss_D: 0.002376 Loss_G: 0.001168 \n",
      "[0/10][11/100][42] Loss_D: 0.002569 Loss_G: 0.003169 \n",
      "[0/10][11/100][43] Loss_D: 0.003015 Loss_G: 0.002278 \n",
      "[0/10][11/100][44] Loss_D: 0.004294 Loss_G: 0.004379 \n",
      "[0/10][11/100][45] Loss_D: 0.003076 Loss_G: 0.003957 \n",
      "[0/10][11/100][46] Loss_D: 0.004490 Loss_G: 0.002697 \n",
      "[0/10][11/100][47] Loss_D: 0.002370 Loss_G: 0.002996 \n",
      "[0/10][11/100][48] Loss_D: 0.003126 Loss_G: 0.003227 \n",
      "[0/10][11/100][49] Loss_D: 0.003410 Loss_G: 0.002333 \n",
      "[0/10][11/100][50] Loss_D: 0.003325 Loss_G: 0.004380 \n",
      "[0/10][11/100][51] Loss_D: 0.004054 Loss_G: 0.003857 \n",
      "[0/10][11/100][52] Loss_D: 0.002739 Loss_G: 0.003190 \n",
      "[0/10][11/100][53] Loss_D: 0.004341 Loss_G: 0.003157 \n",
      "[0/10][11/100][54] Loss_D: 0.004166 Loss_G: 0.003493 \n",
      "[0/10][11/100][55] Loss_D: 0.003136 Loss_G: 0.005550 \n",
      "[0/10][11/100][56] Loss_D: 0.004748 Loss_G: 0.004427 \n",
      "[0/10][11/100][57] Loss_D: 0.004049 Loss_G: 0.003815 \n",
      "[0/10][11/100][58] Loss_D: 0.003717 Loss_G: 0.003174 \n",
      "[0/10][11/100][59] Loss_D: 0.003831 Loss_G: 0.003926 \n",
      "[0/10][11/100][60] Loss_D: 0.002659 Loss_G: 0.002079 \n",
      "[0/10][11/100][61] Loss_D: 0.005942 Loss_G: 0.003654 \n",
      "[0/10][11/100][62] Loss_D: 0.003102 Loss_G: 0.004409 \n",
      "[0/10][11/100][63] Loss_D: 0.003303 Loss_G: 0.003068 \n",
      "[0/10][11/100][64] Loss_D: 0.003238 Loss_G: 0.003306 \n",
      "[0/10][11/100][65] Loss_D: 0.005217 Loss_G: 0.002424 \n",
      "[0/10][11/100][66] Loss_D: 0.003096 Loss_G: 0.001174 \n",
      "[0/10][11/100][67] Loss_D: 0.002067 Loss_G: 0.001202 \n",
      "[0/10][11/100][68] Loss_D: 0.002848 Loss_G: 0.001603 \n",
      "[0/10][11/100][69] Loss_D: 0.000824 Loss_G: 0.002812 \n",
      "[0/10][11/100][70] Loss_D: 0.002145 Loss_G: 0.002056 \n",
      "[0/10][11/100][71] Loss_D: 0.002404 Loss_G: 0.004412 \n",
      "[0/10][11/100][72] Loss_D: 0.002619 Loss_G: 0.002093 \n",
      "[0/10][11/100][73] Loss_D: 0.002761 Loss_G: 0.003129 \n",
      "[0/10][11/100][74] Loss_D: 0.002779 Loss_G: 0.002917 \n",
      "[0/10][11/100][75] Loss_D: 0.002416 Loss_G: 0.002228 \n",
      "[0/10][11/100][76] Loss_D: 0.002598 Loss_G: 0.002043 \n",
      "[0/10][11/100][77] Loss_D: 0.003073 Loss_G: 0.003077 \n",
      "[0/10][11/100][78] Loss_D: 0.004174 Loss_G: 0.001885 \n",
      "[0/10][11/100][79] Loss_D: 0.003861 Loss_G: 0.002825 \n",
      "[0/10][11/100][80] Loss_D: 0.003462 Loss_G: 0.003807 \n",
      "[0/10][11/100][81] Loss_D: 0.002210 Loss_G: 0.002672 \n",
      "[0/10][11/100][82] Loss_D: 0.003422 Loss_G: 0.003866 \n",
      "[0/10][11/100][83] Loss_D: 0.003077 Loss_G: 0.004009 \n",
      "[0/10][11/100][84] Loss_D: 0.002002 Loss_G: 0.002923 \n",
      "[0/10][11/100][85] Loss_D: 0.002305 Loss_G: 0.002132 \n",
      "[0/10][11/100][86] Loss_D: 0.003265 Loss_G: 0.003955 \n",
      "[0/10][11/100][87] Loss_D: 0.002866 Loss_G: 0.002067 \n",
      "[0/10][11/100][88] Loss_D: 0.002429 Loss_G: 0.001728 \n",
      "[0/10][11/100][89] Loss_D: 0.004388 Loss_G: 0.003500 \n",
      "[0/10][11/100][90] Loss_D: 0.003330 Loss_G: 0.002601 \n",
      "[0/10][11/100][91] Loss_D: 0.002362 Loss_G: 0.002749 \n",
      "[0/10][11/100][92] Loss_D: 0.002117 Loss_G: 0.001794 \n",
      "[0/10][11/100][93] Loss_D: 0.003482 Loss_G: 0.003366 \n",
      "[0/10][11/100][94] Loss_D: 0.003072 Loss_G: 0.002502 \n",
      "[0/10][11/100][95] Loss_D: 0.002838 Loss_G: 0.002569 \n",
      "[0/10][11/100][96] Loss_D: 0.002822 Loss_G: 0.003784 \n",
      "[0/10][11/100][97] Loss_D: 0.003998 Loss_G: 0.003684 \n",
      "[0/10][11/100][98] Loss_D: 0.003374 Loss_G: 0.002664 \n",
      "[0/10][11/100][99] Loss_D: 0.003053 Loss_G: 0.003859 \n",
      "[0/10][11/100][100] Loss_D: 0.002029 Loss_G: 0.003770 \n",
      "[0/10][11/100][101] Loss_D: 0.002833 Loss_G: 0.002068 \n",
      "[0/10][11/100][102] Loss_D: 0.002203 Loss_G: 0.000799 \n",
      "[0/10][11/100][103] Loss_D: 0.003145 Loss_G: 0.003740 \n",
      "[0/10][11/100][104] Loss_D: 0.001906 Loss_G: 0.002155 \n",
      "[0/10][11/100][105] Loss_D: 0.003271 Loss_G: 0.001978 \n",
      "[0/10][11/100][106] Loss_D: 0.003889 Loss_G: 0.004154 \n",
      "[0/10][11/100][107] Loss_D: 0.003002 Loss_G: 0.003873 \n",
      "[0/10][11/100][108] Loss_D: 0.002943 Loss_G: 0.002546 \n",
      "[0/10][11/100][109] Loss_D: 0.001199 Loss_G: 0.002310 \n",
      "[0/10][11/100][110] Loss_D: 0.003494 Loss_G: 0.003417 \n",
      "[0/10][11/100][111] Loss_D: 0.002588 Loss_G: 0.001693 \n",
      "[0/10][11/100][112] Loss_D: 0.001885 Loss_G: 0.003110 \n",
      "[0/10][11/100][113] Loss_D: 0.003758 Loss_G: 0.001340 \n",
      "[0/10][11/100][114] Loss_D: 0.003204 Loss_G: 0.002031 \n",
      "[0/10][11/100][115] Loss_D: 0.002853 Loss_G: 0.002449 \n",
      "[0/10][11/100][116] Loss_D: 0.002965 Loss_G: 0.003936 \n",
      "[0/10][11/100][117] Loss_D: 0.002066 Loss_G: 0.001318 \n",
      "[0/10][11/100][118] Loss_D: 0.003049 Loss_G: 0.001547 \n",
      "[0/10][11/100][119] Loss_D: 0.003899 Loss_G: 0.001844 \n",
      "[0/10][11/100][120] Loss_D: 0.001638 Loss_G: 0.001766 \n",
      "[0/10][11/100][121] Loss_D: 0.003045 Loss_G: 0.002869 \n",
      "[0/10][11/100][122] Loss_D: 0.001736 Loss_G: 0.000627 \n",
      "[0/10][11/100][123] Loss_D: 0.001339 Loss_G: 0.002987 \n",
      "[0/10][11/100][124] Loss_D: 0.003805 Loss_G: 0.002314 \n",
      "[0/10][11/100][125] Loss_D: 0.003532 Loss_G: 0.003620 \n",
      "[0/10][11/100][126] Loss_D: 0.003235 Loss_G: 0.001402 \n",
      "[0/10][11/100][127] Loss_D: 0.004321 Loss_G: 0.003108 \n",
      "[0/10][11/100][128] Loss_D: 0.003129 Loss_G: 0.003059 \n",
      "[0/10][11/100][129] Loss_D: 0.001940 Loss_G: 0.003062 \n",
      "[0/10][11/100][130] Loss_D: 0.003647 Loss_G: 0.002598 \n",
      "[0/10][11/100][131] Loss_D: 0.003882 Loss_G: 0.002769 \n",
      "[0/10][11/100][132] Loss_D: 0.002900 Loss_G: 0.002107 \n",
      "[0/10][11/100][133] Loss_D: 0.002681 Loss_G: 0.003941 \n",
      "[0/10][11/100][134] Loss_D: 0.003346 Loss_G: 0.002114 \n",
      "[0/10][11/100][135] Loss_D: 0.002643 Loss_G: 0.003524 \n",
      "[0/10][11/100][136] Loss_D: 0.002175 Loss_G: 0.003083 \n",
      "[0/10][11/100][137] Loss_D: 0.002778 Loss_G: 0.002490 \n",
      "[0/10][11/100][138] Loss_D: 0.002240 Loss_G: 0.004104 \n",
      "[0/10][11/100][139] Loss_D: 0.002964 Loss_G: 0.002317 \n",
      "[0/10][11/100][140] Loss_D: 0.002416 Loss_G: 0.001888 \n",
      "[0/10][11/100][141] Loss_D: 0.003465 Loss_G: 0.003498 \n",
      "[0/10][11/100][142] Loss_D: 0.002856 Loss_G: 0.003536 \n",
      "[0/10][11/100][143] Loss_D: 0.003297 Loss_G: 0.003106 \n",
      "[0/10][11/100][144] Loss_D: 0.002286 Loss_G: 0.001868 \n",
      "[0/10][11/100][145] Loss_D: 0.002258 Loss_G: 0.003307 \n",
      "[0/10][11/100][146] Loss_D: 0.002693 Loss_G: 0.004109 \n",
      "[0/10][11/100][147] Loss_D: 0.004968 Loss_G: 0.006053 \n",
      "[0/10][11/100][148] Loss_D: 0.002691 Loss_G: 0.003027 \n",
      "[0/10][11/100][149] Loss_D: 0.002696 Loss_G: 0.004013 \n",
      "[0/10][11/100][150] Loss_D: 0.003074 Loss_G: 0.002975 \n",
      "[0/10][11/100][151] Loss_D: 0.004231 Loss_G: 0.002138 \n",
      "[0/10][11/100][152] Loss_D: 0.003896 Loss_G: 0.001323 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][11/100][153] Loss_D: 0.002806 Loss_G: 0.003491 \n",
      "[0/10][11/100][154] Loss_D: 0.003123 Loss_G: 0.002971 \n",
      "[0/10][11/100][155] Loss_D: 0.001143 Loss_G: 0.004335 \n",
      "[0/10][11/100][156] Loss_D: 0.005601 Loss_G: 0.001798 \n",
      "[0/10][11/100][157] Loss_D: 0.004456 Loss_G: 0.002579 \n",
      "[0/10][11/100][158] Loss_D: 0.004287 Loss_G: 0.002962 \n",
      "[0/10][11/100][159] Loss_D: 0.002873 Loss_G: 0.003205 \n",
      "[0/10][11/100][160] Loss_D: 0.002956 Loss_G: 0.002735 \n",
      "[0/10][11/100][161] Loss_D: 0.001380 Loss_G: 0.002992 \n",
      "[0/10][11/100][162] Loss_D: 0.004555 Loss_G: 0.002654 \n",
      "[0/10][11/100][163] Loss_D: 0.001914 Loss_G: 0.002990 \n",
      "[0/10][11/100][164] Loss_D: 0.004162 Loss_G: 0.002616 \n",
      "[0/10][11/100][165] Loss_D: 0.001424 Loss_G: 0.003702 \n",
      "[0/10][11/100][166] Loss_D: 0.003184 Loss_G: 0.003515 \n",
      "[0/10][11/100][167] Loss_D: 0.003206 Loss_G: 0.003099 \n",
      "[0/10][11/100][168] Loss_D: 0.002510 Loss_G: 0.002057 \n",
      "[0/10][11/100][169] Loss_D: 0.001998 Loss_G: 0.002945 \n",
      "[0/10][11/100][170] Loss_D: 0.004867 Loss_G: 0.003124 \n",
      "[0/10][11/100][171] Loss_D: 0.002942 Loss_G: 0.004457 \n",
      "[0/10][11/100][172] Loss_D: 0.003081 Loss_G: 0.003954 \n",
      "[0/10][11/100][173] Loss_D: 0.002136 Loss_G: 0.002596 \n",
      "[0/10][11/100][174] Loss_D: 0.002414 Loss_G: 0.001982 \n",
      "[0/10][11/100][175] Loss_D: 0.001928 Loss_G: 0.001713 \n",
      "[0/10][11/100][176] Loss_D: 0.002815 Loss_G: 0.001248 \n",
      "[0/10][11/100][177] Loss_D: 0.002980 Loss_G: 0.002231 \n",
      "[0/10][11/100][178] Loss_D: 0.003564 Loss_G: 0.002634 \n",
      "[0/10][11/100][179] Loss_D: 0.001951 Loss_G: 0.003813 \n",
      "[0/10][11/100][180] Loss_D: 0.003967 Loss_G: 0.002525 \n",
      "[0/10][11/100][181] Loss_D: 0.002136 Loss_G: 0.004798 \n",
      "[0/10][11/100][182] Loss_D: 0.002325 Loss_G: 0.003382 \n",
      "[0/10][11/100][183] Loss_D: 0.003170 Loss_G: 0.002200 \n",
      "[0/10][11/100][184] Loss_D: 0.001555 Loss_G: 0.003131 \n",
      "[0/10][11/100][185] Loss_D: 0.002853 Loss_G: 0.003665 \n",
      "[0/10][11/100][186] Loss_D: 0.003142 Loss_G: 0.003261 \n",
      "[0/10][11/100][187] Loss_D: 0.002001 Loss_G: 0.002491 \n",
      "[0/10][11/100][188] Loss_D: 0.002762 Loss_G: 0.001840 \n",
      "[0/10][11/100][189] Loss_D: 0.002759 Loss_G: 0.002926 \n",
      "[0/10][11/100][190] Loss_D: 0.005077 Loss_G: 0.003390 \n",
      "[0/10][11/100][191] Loss_D: 0.002988 Loss_G: 0.003189 \n",
      "[0/10][11/100][192] Loss_D: 0.004012 Loss_G: 0.002814 \n",
      "[0/10][11/100][193] Loss_D: 0.004052 Loss_G: 0.002384 \n",
      "[0/10][11/100][194] Loss_D: 0.004394 Loss_G: 0.004375 \n",
      "[0/10][11/100][195] Loss_D: 0.002325 Loss_G: 0.003463 \n",
      "[0/10][11/100][196] Loss_D: 0.002232 Loss_G: 0.002507 \n",
      "[0/10][11/100][197] Loss_D: 0.000553 Loss_G: 0.002879 \n",
      "[0/10][11/100][198] Loss_D: 0.003326 Loss_G: 0.003952 \n",
      "[0/10][11/100][199] Loss_D: 0.003118 Loss_G: 0.003304 \n",
      "[0/10][11/100][200] Loss_D: 0.002883 Loss_G: 0.003944 \n",
      "[1/10][11/100][201] Loss_D: 0.003040 Loss_G: 0.002469 \n",
      "[1/10][11/100][202] Loss_D: 0.003901 Loss_G: 0.002356 \n",
      "[1/10][11/100][203] Loss_D: 0.000752 Loss_G: 0.003333 \n",
      "[1/10][11/100][204] Loss_D: 0.003883 Loss_G: 0.002979 \n",
      "[1/10][11/100][205] Loss_D: 0.001552 Loss_G: 0.000165 \n",
      "[1/10][11/100][206] Loss_D: 0.002488 Loss_G: 0.003331 \n",
      "[1/10][11/100][207] Loss_D: 0.003431 Loss_G: 0.002329 \n",
      "[1/10][11/100][208] Loss_D: 0.002782 Loss_G: 0.002417 \n",
      "[1/10][11/100][209] Loss_D: 0.002952 Loss_G: 0.002749 \n",
      "[1/10][11/100][210] Loss_D: 0.001197 Loss_G: 0.001997 \n",
      "[1/10][11/100][211] Loss_D: 0.002726 Loss_G: 0.001583 \n",
      "[1/10][11/100][212] Loss_D: 0.002089 Loss_G: 0.002833 \n",
      "[1/10][11/100][213] Loss_D: 0.004139 Loss_G: 0.003849 \n",
      "[1/10][11/100][214] Loss_D: 0.003517 Loss_G: 0.002616 \n",
      "[1/10][11/100][215] Loss_D: 0.002177 Loss_G: 0.002847 \n",
      "[1/10][11/100][216] Loss_D: 0.001530 Loss_G: 0.001627 \n",
      "[1/10][11/100][217] Loss_D: 0.002557 Loss_G: 0.002737 \n",
      "[1/10][11/100][218] Loss_D: 0.002621 Loss_G: 0.002791 \n",
      "[1/10][11/100][219] Loss_D: 0.002086 Loss_G: 0.002096 \n",
      "[1/10][11/100][220] Loss_D: 0.003298 Loss_G: 0.001626 \n",
      "[1/10][11/100][221] Loss_D: 0.001772 Loss_G: 0.002440 \n",
      "[1/10][11/100][222] Loss_D: 0.001666 Loss_G: 0.003377 \n",
      "[1/10][11/100][223] Loss_D: 0.001982 Loss_G: 0.002656 \n",
      "[1/10][11/100][224] Loss_D: 0.001631 Loss_G: 0.001740 \n",
      "[1/10][11/100][225] Loss_D: 0.001382 Loss_G: 0.002463 \n",
      "[1/10][11/100][226] Loss_D: 0.001983 Loss_G: 0.002898 \n",
      "[1/10][11/100][227] Loss_D: 0.002612 Loss_G: 0.002932 \n",
      "[1/10][11/100][228] Loss_D: 0.003717 Loss_G: 0.003148 \n",
      "[1/10][11/100][229] Loss_D: 0.002844 Loss_G: 0.001635 \n",
      "[1/10][11/100][230] Loss_D: 0.003467 Loss_G: 0.003323 \n",
      "[1/10][11/100][231] Loss_D: 0.003793 Loss_G: 0.004803 \n",
      "[1/10][11/100][232] Loss_D: 0.005539 Loss_G: 0.003862 \n",
      "[1/10][11/100][233] Loss_D: 0.004258 Loss_G: 0.003748 \n",
      "[1/10][11/100][234] Loss_D: 0.004671 Loss_G: 0.002595 \n",
      "[1/10][11/100][235] Loss_D: 0.003193 Loss_G: 0.002124 \n",
      "[1/10][11/100][236] Loss_D: 0.002734 Loss_G: 0.002515 \n",
      "[1/10][11/100][237] Loss_D: 0.002005 Loss_G: 0.002504 \n",
      "[1/10][11/100][238] Loss_D: 0.002588 Loss_G: 0.002746 \n",
      "[1/10][11/100][239] Loss_D: 0.002631 Loss_G: 0.001962 \n",
      "[1/10][11/100][240] Loss_D: 0.002650 Loss_G: 0.004282 \n",
      "[1/10][11/100][241] Loss_D: 0.002264 Loss_G: 0.002558 \n",
      "[1/10][11/100][242] Loss_D: 0.003321 Loss_G: 0.001892 \n",
      "[1/10][11/100][243] Loss_D: 0.001986 Loss_G: 0.002947 \n",
      "[1/10][11/100][244] Loss_D: 0.002571 Loss_G: 0.002578 \n",
      "[1/10][11/100][245] Loss_D: 0.002507 Loss_G: 0.002304 \n",
      "[1/10][11/100][246] Loss_D: 0.003040 Loss_G: 0.002463 \n",
      "[1/10][11/100][247] Loss_D: 0.002470 Loss_G: 0.001343 \n",
      "[1/10][11/100][248] Loss_D: 0.002510 Loss_G: 0.002562 \n",
      "[1/10][11/100][249] Loss_D: 0.002974 Loss_G: 0.002415 \n",
      "[1/10][11/100][250] Loss_D: 0.002675 Loss_G: 0.001880 \n",
      "[1/10][11/100][251] Loss_D: 0.003006 Loss_G: 0.001686 \n",
      "[1/10][11/100][252] Loss_D: 0.002093 Loss_G: 0.002503 \n",
      "[1/10][11/100][253] Loss_D: 0.004210 Loss_G: 0.003212 \n",
      "[1/10][11/100][254] Loss_D: 0.002826 Loss_G: 0.003146 \n",
      "[1/10][11/100][255] Loss_D: 0.003384 Loss_G: 0.001660 \n",
      "[1/10][11/100][256] Loss_D: 0.004726 Loss_G: 0.002327 \n",
      "[1/10][11/100][257] Loss_D: 0.004294 Loss_G: 0.002173 \n",
      "[1/10][11/100][258] Loss_D: 0.001834 Loss_G: 0.002866 \n",
      "[1/10][11/100][259] Loss_D: 0.004961 Loss_G: 0.002966 \n",
      "[1/10][11/100][260] Loss_D: 0.001054 Loss_G: 0.002917 \n",
      "[1/10][11/100][261] Loss_D: 0.003616 Loss_G: 0.002299 \n",
      "[1/10][11/100][262] Loss_D: 0.004031 Loss_G: 0.003507 \n",
      "[1/10][11/100][263] Loss_D: 0.002082 Loss_G: 0.003071 \n",
      "[1/10][11/100][264] Loss_D: 0.003070 Loss_G: 0.003221 \n",
      "[1/10][11/100][265] Loss_D: 0.003392 Loss_G: 0.002313 \n",
      "[1/10][11/100][266] Loss_D: 0.002242 Loss_G: 0.003614 \n",
      "[1/10][11/100][267] Loss_D: 0.001704 Loss_G: 0.002787 \n",
      "[1/10][11/100][268] Loss_D: 0.002829 Loss_G: 0.002198 \n",
      "[1/10][11/100][269] Loss_D: 0.002341 Loss_G: 0.002885 \n",
      "[1/10][11/100][270] Loss_D: 0.003874 Loss_G: 0.004157 \n",
      "[1/10][11/100][271] Loss_D: 0.004045 Loss_G: 0.003791 \n",
      "[1/10][11/100][272] Loss_D: 0.005588 Loss_G: 0.004874 \n",
      "[1/10][11/100][273] Loss_D: 0.003708 Loss_G: 0.004951 \n",
      "[1/10][11/100][274] Loss_D: 0.005777 Loss_G: 0.003352 \n",
      "[1/10][11/100][275] Loss_D: 0.004220 Loss_G: 0.005479 \n",
      "[1/10][11/100][276] Loss_D: 0.005821 Loss_G: 0.007205 \n",
      "[1/10][11/100][277] Loss_D: 0.002877 Loss_G: 0.003897 \n",
      "[1/10][11/100][278] Loss_D: 0.004076 Loss_G: 0.002524 \n",
      "[1/10][11/100][279] Loss_D: 0.005675 Loss_G: 0.005180 \n",
      "[1/10][11/100][280] Loss_D: 0.004847 Loss_G: 0.004949 \n",
      "[1/10][11/100][281] Loss_D: 0.003778 Loss_G: 0.004274 \n",
      "[1/10][11/100][282] Loss_D: 0.002247 Loss_G: 0.003279 \n",
      "[1/10][11/100][283] Loss_D: 0.002209 Loss_G: 0.003079 \n",
      "[1/10][11/100][284] Loss_D: 0.002947 Loss_G: 0.002143 \n",
      "[1/10][11/100][285] Loss_D: 0.002107 Loss_G: 0.002631 \n",
      "[1/10][11/100][286] Loss_D: 0.001937 Loss_G: 0.002064 \n",
      "[1/10][11/100][287] Loss_D: 0.001769 Loss_G: -0.000032 \n",
      "[1/10][11/100][288] Loss_D: 0.002741 Loss_G: 0.001391 \n",
      "[1/10][11/100][289] Loss_D: 0.003503 Loss_G: 0.002121 \n",
      "[1/10][11/100][290] Loss_D: 0.002818 Loss_G: 0.003494 \n",
      "[1/10][11/100][291] Loss_D: 0.002075 Loss_G: 0.002944 \n",
      "[1/10][11/100][292] Loss_D: 0.001472 Loss_G: 0.003016 \n",
      "[1/10][11/100][293] Loss_D: 0.004112 Loss_G: 0.002368 \n",
      "[1/10][11/100][294] Loss_D: 0.002647 Loss_G: 0.003944 \n",
      "[1/10][11/100][295] Loss_D: 0.004246 Loss_G: 0.004445 \n",
      "[1/10][11/100][296] Loss_D: 0.002894 Loss_G: 0.004083 \n",
      "[1/10][11/100][297] Loss_D: 0.002133 Loss_G: 0.003794 \n",
      "[1/10][11/100][298] Loss_D: 0.002292 Loss_G: 0.003962 \n",
      "[1/10][11/100][299] Loss_D: 0.002770 Loss_G: 0.002444 \n",
      "[1/10][11/100][300] Loss_D: 0.002804 Loss_G: 0.002194 \n",
      "[1/10][11/100][301] Loss_D: 0.003648 Loss_G: 0.002784 \n",
      "[1/10][11/100][302] Loss_D: 0.003204 Loss_G: 0.002356 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][11/100][303] Loss_D: 0.001557 Loss_G: 0.002756 \n",
      "[1/10][11/100][304] Loss_D: 0.002071 Loss_G: 0.003203 \n",
      "[1/10][11/100][305] Loss_D: 0.003962 Loss_G: 0.003422 \n",
      "[1/10][11/100][306] Loss_D: 0.004222 Loss_G: 0.004217 \n",
      "[1/10][11/100][307] Loss_D: 0.002758 Loss_G: 0.003276 \n",
      "[1/10][11/100][308] Loss_D: 0.003256 Loss_G: 0.001758 \n",
      "[1/10][11/100][309] Loss_D: 0.002034 Loss_G: 0.003780 \n",
      "[1/10][11/100][310] Loss_D: 0.003672 Loss_G: 0.002198 \n",
      "[1/10][11/100][311] Loss_D: 0.003556 Loss_G: 0.004231 \n",
      "[1/10][11/100][312] Loss_D: 0.001982 Loss_G: 0.004044 \n",
      "[1/10][11/100][313] Loss_D: 0.003570 Loss_G: 0.002280 \n",
      "[1/10][11/100][314] Loss_D: 0.003120 Loss_G: 0.002481 \n",
      "[1/10][11/100][315] Loss_D: 0.003068 Loss_G: 0.002453 \n",
      "[1/10][11/100][316] Loss_D: 0.003351 Loss_G: 0.003179 \n",
      "[1/10][11/100][317] Loss_D: 0.004504 Loss_G: 0.002699 \n",
      "[1/10][11/100][318] Loss_D: 0.005358 Loss_G: 0.004194 \n",
      "[1/10][11/100][319] Loss_D: 0.004221 Loss_G: 0.004841 \n",
      "[1/10][11/100][320] Loss_D: 0.003940 Loss_G: 0.004979 \n",
      "[1/10][11/100][321] Loss_D: 0.002721 Loss_G: 0.004297 \n",
      "[1/10][11/100][322] Loss_D: 0.004497 Loss_G: 0.004188 \n",
      "[1/10][11/100][323] Loss_D: 0.003480 Loss_G: 0.004577 \n",
      "[1/10][11/100][324] Loss_D: 0.003297 Loss_G: 0.004463 \n",
      "[1/10][11/100][325] Loss_D: 0.003634 Loss_G: 0.003187 \n",
      "[1/10][11/100][326] Loss_D: 0.002684 Loss_G: 0.004005 \n",
      "[1/10][11/100][327] Loss_D: 0.004175 Loss_G: 0.004522 \n",
      "[1/10][11/100][328] Loss_D: 0.003427 Loss_G: 0.004261 \n",
      "[1/10][11/100][329] Loss_D: 0.004110 Loss_G: 0.004458 \n",
      "[1/10][11/100][330] Loss_D: 0.004171 Loss_G: 0.004788 \n",
      "[1/10][11/100][331] Loss_D: 0.002376 Loss_G: 0.003074 \n",
      "[1/10][11/100][332] Loss_D: 0.001498 Loss_G: 0.005213 \n",
      "[1/10][11/100][333] Loss_D: 0.004042 Loss_G: 0.002112 \n",
      "[1/10][11/100][334] Loss_D: 0.002118 Loss_G: 0.001241 \n",
      "[1/10][11/100][335] Loss_D: 0.002344 Loss_G: 0.002722 \n",
      "[1/10][11/100][336] Loss_D: 0.002520 Loss_G: 0.001738 \n",
      "[1/10][11/100][337] Loss_D: 0.001904 Loss_G: 0.002523 \n",
      "[1/10][11/100][338] Loss_D: 0.003911 Loss_G: 0.001965 \n",
      "[1/10][11/100][339] Loss_D: 0.004149 Loss_G: 0.001685 \n",
      "[1/10][11/100][340] Loss_D: 0.003494 Loss_G: 0.002923 \n",
      "[1/10][11/100][341] Loss_D: 0.002519 Loss_G: 0.003189 \n",
      "[1/10][11/100][342] Loss_D: 0.001542 Loss_G: 0.003475 \n",
      "[1/10][11/100][343] Loss_D: 0.004266 Loss_G: 0.001434 \n",
      "[1/10][11/100][344] Loss_D: 0.003403 Loss_G: 0.000526 \n",
      "[1/10][11/100][345] Loss_D: 0.003517 Loss_G: 0.003107 \n",
      "[1/10][11/100][346] Loss_D: 0.005218 Loss_G: 0.003376 \n",
      "[1/10][11/100][347] Loss_D: 0.001190 Loss_G: 0.002906 \n",
      "[1/10][11/100][348] Loss_D: 0.003283 Loss_G: 0.002764 \n",
      "[1/10][11/100][349] Loss_D: 0.003607 Loss_G: 0.003525 \n",
      "[1/10][11/100][350] Loss_D: 0.003456 Loss_G: 0.003304 \n",
      "[1/10][11/100][351] Loss_D: 0.002591 Loss_G: 0.005613 \n",
      "[1/10][11/100][352] Loss_D: 0.003540 Loss_G: 0.003365 \n",
      "[1/10][11/100][353] Loss_D: 0.002578 Loss_G: 0.002449 \n",
      "[1/10][11/100][354] Loss_D: 0.001724 Loss_G: 0.002526 \n",
      "[1/10][11/100][355] Loss_D: 0.002503 Loss_G: 0.002284 \n",
      "[1/10][11/100][356] Loss_D: 0.002867 Loss_G: 0.001510 \n",
      "[1/10][11/100][357] Loss_D: 0.002674 Loss_G: 0.002075 \n",
      "[1/10][11/100][358] Loss_D: 0.001756 Loss_G: 0.003307 \n",
      "[1/10][11/100][359] Loss_D: 0.002476 Loss_G: 0.003671 \n",
      "[1/10][11/100][360] Loss_D: 0.003820 Loss_G: 0.002913 \n",
      "[1/10][11/100][361] Loss_D: 0.002919 Loss_G: 0.003290 \n",
      "[1/10][11/100][362] Loss_D: 0.003662 Loss_G: 0.002880 \n",
      "[1/10][11/100][363] Loss_D: 0.001515 Loss_G: 0.003583 \n",
      "[1/10][11/100][364] Loss_D: 0.001933 Loss_G: 0.003143 \n",
      "[1/10][11/100][365] Loss_D: 0.002369 Loss_G: 0.003221 \n",
      "[1/10][11/100][366] Loss_D: 0.003185 Loss_G: 0.003497 \n",
      "[1/10][11/100][367] Loss_D: 0.002309 Loss_G: 0.003247 \n",
      "[1/10][11/100][368] Loss_D: 0.002878 Loss_G: 0.003254 \n",
      "[1/10][11/100][369] Loss_D: 0.002983 Loss_G: 0.002584 \n",
      "[1/10][11/100][370] Loss_D: 0.002561 Loss_G: 0.002802 \n",
      "[1/10][11/100][371] Loss_D: 0.003123 Loss_G: 0.003773 \n",
      "[1/10][11/100][372] Loss_D: 0.003657 Loss_G: 0.004105 \n",
      "[1/10][11/100][373] Loss_D: 0.004145 Loss_G: 0.003798 \n",
      "[1/10][11/100][374] Loss_D: 0.005272 Loss_G: 0.003807 \n",
      "[1/10][11/100][375] Loss_D: 0.003366 Loss_G: 0.004664 \n",
      "[1/10][11/100][376] Loss_D: 0.004760 Loss_G: 0.004447 \n",
      "[1/10][11/100][377] Loss_D: 0.007197 Loss_G: 0.004322 \n",
      "[1/10][11/100][378] Loss_D: 0.004661 Loss_G: 0.005542 \n",
      "[1/10][11/100][379] Loss_D: 0.004899 Loss_G: 0.004039 \n",
      "[1/10][11/100][380] Loss_D: 0.006145 Loss_G: 0.003652 \n",
      "[1/10][11/100][381] Loss_D: 0.004258 Loss_G: 0.002998 \n",
      "[1/10][11/100][382] Loss_D: 0.002827 Loss_G: 0.002981 \n",
      "[1/10][11/100][383] Loss_D: 0.003970 Loss_G: 0.002347 \n",
      "[1/10][11/100][384] Loss_D: 0.003471 Loss_G: 0.003268 \n",
      "[1/10][11/100][385] Loss_D: 0.001469 Loss_G: 0.005325 \n",
      "[1/10][11/100][386] Loss_D: 0.003606 Loss_G: 0.002638 \n",
      "[1/10][11/100][387] Loss_D: 0.001694 Loss_G: 0.002281 \n",
      "[1/10][11/100][388] Loss_D: 0.001821 Loss_G: 0.001360 \n",
      "[1/10][11/100][389] Loss_D: 0.002307 Loss_G: 0.002483 \n",
      "[1/10][11/100][390] Loss_D: 0.002176 Loss_G: 0.002644 \n",
      "[1/10][11/100][391] Loss_D: 0.004772 Loss_G: 0.004825 \n",
      "[1/10][11/100][392] Loss_D: 0.003197 Loss_G: 0.004282 \n",
      "[1/10][11/100][393] Loss_D: 0.005738 Loss_G: 0.002999 \n",
      "[1/10][11/100][394] Loss_D: 0.002940 Loss_G: 0.003735 \n",
      "[1/10][11/100][395] Loss_D: 0.003522 Loss_G: 0.002785 \n",
      "[1/10][11/100][396] Loss_D: 0.001956 Loss_G: 0.002704 \n",
      "[1/10][11/100][397] Loss_D: 0.003211 Loss_G: 0.003454 \n",
      "[1/10][11/100][398] Loss_D: 0.003501 Loss_G: 0.002177 \n",
      "[1/10][11/100][399] Loss_D: 0.003023 Loss_G: 0.002199 \n",
      "[1/10][11/100][400] Loss_D: 0.003041 Loss_G: 0.003341 \n",
      "[2/10][11/100][401] Loss_D: 0.004021 Loss_G: 0.000798 \n",
      "[2/10][11/100][402] Loss_D: 0.002601 Loss_G: 0.002881 \n",
      "[2/10][11/100][403] Loss_D: 0.002449 Loss_G: 0.002700 \n",
      "[2/10][11/100][404] Loss_D: 0.003488 Loss_G: 0.002989 \n",
      "[2/10][11/100][405] Loss_D: 0.002837 Loss_G: 0.002323 \n",
      "[2/10][11/100][406] Loss_D: 0.003179 Loss_G: 0.002543 \n",
      "[2/10][11/100][407] Loss_D: 0.003594 Loss_G: 0.003748 \n",
      "[2/10][11/100][408] Loss_D: 0.002939 Loss_G: 0.002718 \n",
      "[2/10][11/100][409] Loss_D: 0.003570 Loss_G: 0.003489 \n",
      "[2/10][11/100][410] Loss_D: 0.002370 Loss_G: 0.002576 \n",
      "[2/10][11/100][411] Loss_D: 0.003278 Loss_G: 0.004109 \n",
      "[2/10][11/100][412] Loss_D: 0.004177 Loss_G: 0.002965 \n",
      "[2/10][11/100][413] Loss_D: 0.002007 Loss_G: 0.003818 \n",
      "[2/10][11/100][414] Loss_D: 0.002759 Loss_G: 0.005252 \n",
      "[2/10][11/100][415] Loss_D: 0.003142 Loss_G: 0.005164 \n",
      "[2/10][11/100][416] Loss_D: 0.003280 Loss_G: 0.004113 \n",
      "[2/10][11/100][417] Loss_D: 0.005398 Loss_G: 0.004228 \n",
      "[2/10][11/100][418] Loss_D: 0.004345 Loss_G: 0.006811 \n",
      "[2/10][11/100][419] Loss_D: 0.004956 Loss_G: 0.004204 \n",
      "[2/10][11/100][420] Loss_D: 0.003634 Loss_G: 0.003703 \n",
      "[2/10][11/100][421] Loss_D: 0.005925 Loss_G: 0.002624 \n",
      "[2/10][11/100][422] Loss_D: 0.003264 Loss_G: 0.002834 \n",
      "[2/10][11/100][423] Loss_D: 0.004125 Loss_G: 0.001901 \n",
      "[2/10][11/100][424] Loss_D: 0.003118 Loss_G: 0.002209 \n",
      "[2/10][11/100][425] Loss_D: 0.003241 Loss_G: 0.002394 \n",
      "[2/10][11/100][426] Loss_D: 0.002400 Loss_G: 0.001633 \n",
      "[2/10][11/100][427] Loss_D: 0.002853 Loss_G: 0.002141 \n",
      "[2/10][11/100][428] Loss_D: 0.001039 Loss_G: 0.002440 \n",
      "[2/10][11/100][429] Loss_D: 0.003151 Loss_G: 0.001578 \n",
      "[2/10][11/100][430] Loss_D: 0.001360 Loss_G: 0.002837 \n",
      "[2/10][11/100][431] Loss_D: 0.003723 Loss_G: 0.002893 \n",
      "[2/10][11/100][432] Loss_D: 0.002026 Loss_G: 0.002323 \n",
      "[2/10][11/100][433] Loss_D: 0.006055 Loss_G: 0.001696 \n",
      "[2/10][11/100][434] Loss_D: 0.002554 Loss_G: 0.002219 \n",
      "[2/10][11/100][435] Loss_D: 0.000551 Loss_G: 0.001185 \n",
      "[2/10][11/100][436] Loss_D: 0.004431 Loss_G: 0.001377 \n",
      "[2/10][11/100][437] Loss_D: 0.002421 Loss_G: 0.003483 \n",
      "[2/10][11/100][438] Loss_D: 0.001735 Loss_G: 0.001685 \n",
      "[2/10][11/100][439] Loss_D: 0.001583 Loss_G: 0.002580 \n",
      "[2/10][11/100][440] Loss_D: 0.003452 Loss_G: 0.002123 \n",
      "[2/10][11/100][441] Loss_D: 0.002975 Loss_G: 0.001492 \n",
      "[2/10][11/100][442] Loss_D: 0.002193 Loss_G: 0.002984 \n",
      "[2/10][11/100][443] Loss_D: 0.001626 Loss_G: 0.003768 \n",
      "[2/10][11/100][444] Loss_D: 0.000934 Loss_G: 0.002569 \n",
      "[2/10][11/100][445] Loss_D: 0.002869 Loss_G: 0.003054 \n",
      "[2/10][11/100][446] Loss_D: 0.002816 Loss_G: 0.002543 \n",
      "[2/10][11/100][447] Loss_D: 0.003427 Loss_G: 0.003654 \n",
      "[2/10][11/100][448] Loss_D: 0.003717 Loss_G: 0.003037 \n",
      "[2/10][11/100][449] Loss_D: 0.004655 Loss_G: 0.003349 \n",
      "[2/10][11/100][450] Loss_D: 0.003409 Loss_G: 0.003335 \n",
      "[2/10][11/100][451] Loss_D: 0.003993 Loss_G: 0.005060 \n",
      "[2/10][11/100][452] Loss_D: 0.002913 Loss_G: 0.003436 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][11/100][453] Loss_D: 0.005490 Loss_G: 0.004759 \n",
      "[2/10][11/100][454] Loss_D: 0.004481 Loss_G: 0.002858 \n",
      "[2/10][11/100][455] Loss_D: 0.004011 Loss_G: 0.004032 \n",
      "[2/10][11/100][456] Loss_D: 0.004991 Loss_G: 0.005875 \n",
      "[2/10][11/100][457] Loss_D: 0.003714 Loss_G: 0.005718 \n",
      "[2/10][11/100][458] Loss_D: 0.004713 Loss_G: 0.003746 \n",
      "[2/10][11/100][459] Loss_D: 0.004897 Loss_G: 0.003533 \n",
      "[2/10][11/100][460] Loss_D: 0.005263 Loss_G: 0.004765 \n",
      "[2/10][11/100][461] Loss_D: 0.003818 Loss_G: 0.003366 \n",
      "[2/10][11/100][462] Loss_D: 0.005657 Loss_G: 0.002901 \n",
      "[2/10][11/100][463] Loss_D: 0.002155 Loss_G: 0.002315 \n",
      "[2/10][11/100][464] Loss_D: 0.003250 Loss_G: 0.003325 \n",
      "[2/10][11/100][465] Loss_D: 0.003704 Loss_G: 0.003310 \n",
      "[2/10][11/100][466] Loss_D: 0.003791 Loss_G: 0.003036 \n",
      "[2/10][11/100][467] Loss_D: 0.003231 Loss_G: 0.003395 \n",
      "[2/10][11/100][468] Loss_D: 0.003700 Loss_G: 0.001725 \n",
      "[2/10][11/100][469] Loss_D: 0.002853 Loss_G: 0.005001 \n",
      "[2/10][11/100][470] Loss_D: 0.002387 Loss_G: 0.002465 \n",
      "[2/10][11/100][471] Loss_D: 0.001586 Loss_G: 0.002399 \n",
      "[2/10][11/100][472] Loss_D: 0.003057 Loss_G: 0.001033 \n",
      "[2/10][11/100][473] Loss_D: 0.001786 Loss_G: 0.001164 \n",
      "[2/10][11/100][474] Loss_D: 0.001332 Loss_G: 0.002684 \n",
      "[2/10][11/100][475] Loss_D: 0.004518 Loss_G: 0.003060 \n",
      "[2/10][11/100][476] Loss_D: 0.002188 Loss_G: 0.001044 \n",
      "[2/10][11/100][477] Loss_D: 0.002355 Loss_G: 0.003963 \n",
      "[2/10][11/100][478] Loss_D: 0.002118 Loss_G: 0.003358 \n",
      "[2/10][11/100][479] Loss_D: 0.001436 Loss_G: 0.002918 \n",
      "[2/10][11/100][480] Loss_D: 0.002079 Loss_G: 0.002597 \n",
      "[2/10][11/100][481] Loss_D: 0.001812 Loss_G: 0.002131 \n",
      "[2/10][11/100][482] Loss_D: 0.002104 Loss_G: 0.002134 \n",
      "[2/10][11/100][483] Loss_D: 0.002156 Loss_G: 0.002781 \n",
      "[2/10][11/100][484] Loss_D: 0.003358 Loss_G: 0.002063 \n",
      "[2/10][11/100][485] Loss_D: 0.002596 Loss_G: 0.003127 \n",
      "[2/10][11/100][486] Loss_D: 0.002032 Loss_G: 0.002251 \n",
      "[2/10][11/100][487] Loss_D: 0.002069 Loss_G: 0.002851 \n",
      "[2/10][11/100][488] Loss_D: 0.004055 Loss_G: 0.004238 \n",
      "[2/10][11/100][489] Loss_D: 0.003401 Loss_G: 0.001678 \n",
      "[2/10][11/100][490] Loss_D: 0.003608 Loss_G: 0.003748 \n",
      "[2/10][11/100][491] Loss_D: 0.003291 Loss_G: 0.003767 \n",
      "[2/10][11/100][492] Loss_D: 0.001399 Loss_G: 0.002141 \n",
      "[2/10][11/100][493] Loss_D: 0.002411 Loss_G: 0.003316 \n",
      "[2/10][11/100][494] Loss_D: 0.001565 Loss_G: 0.002587 \n",
      "[2/10][11/100][495] Loss_D: 0.002192 Loss_G: 0.003347 \n",
      "[2/10][11/100][496] Loss_D: 0.003010 Loss_G: 0.003326 \n",
      "[2/10][11/100][497] Loss_D: 0.002831 Loss_G: 0.001947 \n",
      "[2/10][11/100][498] Loss_D: 0.002876 Loss_G: 0.002766 \n",
      "[2/10][11/100][499] Loss_D: 0.002420 Loss_G: 0.004716 \n",
      "[2/10][11/100][500] Loss_D: 0.004662 Loss_G: 0.003827 \n",
      "[2/10][11/100][501] Loss_D: 0.003785 Loss_G: 0.003013 \n",
      "[2/10][11/100][502] Loss_D: 0.005527 Loss_G: 0.005263 \n",
      "[2/10][11/100][503] Loss_D: 0.006136 Loss_G: 0.006034 \n",
      "[2/10][11/100][504] Loss_D: 0.005721 Loss_G: 0.005393 \n",
      "[2/10][11/100][505] Loss_D: 0.004904 Loss_G: 0.004597 \n",
      "[2/10][11/100][506] Loss_D: 0.003527 Loss_G: 0.007044 \n",
      "[2/10][11/100][507] Loss_D: 0.002667 Loss_G: 0.004721 \n",
      "[2/10][11/100][508] Loss_D: 0.002903 Loss_G: 0.004019 \n",
      "[2/10][11/100][509] Loss_D: 0.003327 Loss_G: 0.003432 \n",
      "[2/10][11/100][510] Loss_D: 0.004505 Loss_G: 0.002861 \n",
      "[2/10][11/100][511] Loss_D: 0.005670 Loss_G: 0.004523 \n",
      "[2/10][11/100][512] Loss_D: 0.002734 Loss_G: 0.002647 \n",
      "[2/10][11/100][513] Loss_D: 0.002562 Loss_G: 0.002741 \n",
      "[2/10][11/100][514] Loss_D: 0.003376 Loss_G: 0.001468 \n",
      "[2/10][11/100][515] Loss_D: 0.001655 Loss_G: 0.002853 \n",
      "[2/10][11/100][516] Loss_D: 0.001717 Loss_G: 0.002386 \n",
      "[2/10][11/100][517] Loss_D: 0.001665 Loss_G: 0.000480 \n",
      "[2/10][11/100][518] Loss_D: 0.001971 Loss_G: 0.002947 \n",
      "[2/10][11/100][519] Loss_D: 0.002575 Loss_G: 0.001704 \n",
      "[2/10][11/100][520] Loss_D: 0.001852 Loss_G: 0.003302 \n",
      "[2/10][11/100][521] Loss_D: 0.000845 Loss_G: 0.002703 \n",
      "[2/10][11/100][522] Loss_D: 0.001459 Loss_G: 0.002276 \n",
      "[2/10][11/100][523] Loss_D: 0.000937 Loss_G: 0.002021 \n",
      "[2/10][11/100][524] Loss_D: 0.003187 Loss_G: 0.002914 \n",
      "[2/10][11/100][525] Loss_D: 0.002248 Loss_G: 0.002391 \n",
      "[2/10][11/100][526] Loss_D: 0.002432 Loss_G: 0.003934 \n",
      "[2/10][11/100][527] Loss_D: 0.001338 Loss_G: 0.004281 \n",
      "[2/10][11/100][528] Loss_D: 0.001717 Loss_G: 0.003539 \n",
      "[2/10][11/100][529] Loss_D: 0.000906 Loss_G: 0.002687 \n",
      "[2/10][11/100][530] Loss_D: 0.002637 Loss_G: 0.004127 \n",
      "[2/10][11/100][531] Loss_D: 0.003683 Loss_G: 0.002490 \n",
      "[2/10][11/100][532] Loss_D: 0.002796 Loss_G: 0.001762 \n",
      "[2/10][11/100][533] Loss_D: 0.001819 Loss_G: 0.002825 \n",
      "[2/10][11/100][534] Loss_D: 0.001481 Loss_G: 0.005189 \n",
      "[2/10][11/100][535] Loss_D: 0.003191 Loss_G: 0.003223 \n",
      "[2/10][11/100][536] Loss_D: 0.001490 Loss_G: 0.002165 \n",
      "[2/10][11/100][537] Loss_D: 0.002598 Loss_G: 0.002227 \n",
      "[2/10][11/100][538] Loss_D: 0.002719 Loss_G: 0.003290 \n",
      "[2/10][11/100][539] Loss_D: 0.002623 Loss_G: 0.003215 \n",
      "[2/10][11/100][540] Loss_D: 0.002121 Loss_G: 0.003003 \n",
      "[2/10][11/100][541] Loss_D: 0.004028 Loss_G: 0.003116 \n",
      "[2/10][11/100][542] Loss_D: 0.001805 Loss_G: 0.005066 \n",
      "[2/10][11/100][543] Loss_D: 0.003056 Loss_G: 0.003857 \n",
      "[2/10][11/100][544] Loss_D: 0.002980 Loss_G: 0.002734 \n",
      "[2/10][11/100][545] Loss_D: 0.002773 Loss_G: 0.002058 \n",
      "[2/10][11/100][546] Loss_D: 0.002952 Loss_G: 0.006537 \n",
      "[2/10][11/100][547] Loss_D: 0.005741 Loss_G: 0.002412 \n",
      "[2/10][11/100][548] Loss_D: 0.004763 Loss_G: 0.001785 \n",
      "[2/10][11/100][549] Loss_D: 0.003052 Loss_G: 0.005549 \n",
      "[2/10][11/100][550] Loss_D: 0.003591 Loss_G: 0.004586 \n",
      "[2/10][11/100][551] Loss_D: 0.004781 Loss_G: 0.004321 \n",
      "[2/10][11/100][552] Loss_D: 0.005188 Loss_G: 0.006299 \n",
      "[2/10][11/100][553] Loss_D: 0.005003 Loss_G: 0.003719 \n",
      "[2/10][11/100][554] Loss_D: 0.003033 Loss_G: 0.005062 \n",
      "[2/10][11/100][555] Loss_D: 0.003206 Loss_G: 0.002934 \n",
      "[2/10][11/100][556] Loss_D: 0.005422 Loss_G: 0.002549 \n",
      "[2/10][11/100][557] Loss_D: 0.004802 Loss_G: 0.003972 \n",
      "[2/10][11/100][558] Loss_D: 0.005238 Loss_G: 0.004078 \n",
      "[2/10][11/100][559] Loss_D: 0.003084 Loss_G: 0.005664 \n",
      "[2/10][11/100][560] Loss_D: 0.004638 Loss_G: 0.004259 \n",
      "[2/10][11/100][561] Loss_D: 0.005879 Loss_G: 0.001988 \n",
      "[2/10][11/100][562] Loss_D: 0.002744 Loss_G: 0.003664 \n",
      "[2/10][11/100][563] Loss_D: 0.002953 Loss_G: 0.002694 \n",
      "[2/10][11/100][564] Loss_D: 0.002376 Loss_G: 0.002585 \n",
      "[2/10][11/100][565] Loss_D: 0.003547 Loss_G: 0.002466 \n",
      "[2/10][11/100][566] Loss_D: 0.004223 Loss_G: 0.003035 \n",
      "[2/10][11/100][567] Loss_D: 0.003819 Loss_G: 0.002690 \n",
      "[2/10][11/100][568] Loss_D: 0.001955 Loss_G: 0.003016 \n",
      "[2/10][11/100][569] Loss_D: 0.002533 Loss_G: 0.002454 \n",
      "[2/10][11/100][570] Loss_D: 0.002644 Loss_G: 0.003236 \n",
      "[2/10][11/100][571] Loss_D: 0.000666 Loss_G: 0.001988 \n",
      "[2/10][11/100][572] Loss_D: 0.001129 Loss_G: 0.001177 \n",
      "[2/10][11/100][573] Loss_D: 0.002441 Loss_G: 0.001434 \n",
      "[2/10][11/100][574] Loss_D: 0.003928 Loss_G: 0.002488 \n",
      "[2/10][11/100][575] Loss_D: 0.003935 Loss_G: 0.002616 \n",
      "[2/10][11/100][576] Loss_D: 0.003333 Loss_G: 0.002355 \n",
      "[2/10][11/100][577] Loss_D: 0.001961 Loss_G: 0.002266 \n",
      "[2/10][11/100][578] Loss_D: 0.001833 Loss_G: 0.002040 \n",
      "[2/10][11/100][579] Loss_D: 0.002968 Loss_G: 0.002766 \n",
      "[2/10][11/100][580] Loss_D: 0.002526 Loss_G: 0.004899 \n",
      "[2/10][11/100][581] Loss_D: 0.003907 Loss_G: 0.002205 \n",
      "[2/10][11/100][582] Loss_D: 0.002445 Loss_G: 0.001756 \n",
      "[2/10][11/100][583] Loss_D: 0.001949 Loss_G: 0.003346 \n",
      "[2/10][11/100][584] Loss_D: 0.004523 Loss_G: 0.002283 \n",
      "[2/10][11/100][585] Loss_D: 0.002535 Loss_G: 0.003719 \n",
      "[2/10][11/100][586] Loss_D: 0.001711 Loss_G: 0.002032 \n",
      "[2/10][11/100][587] Loss_D: 0.002548 Loss_G: 0.002755 \n",
      "[2/10][11/100][588] Loss_D: 0.003347 Loss_G: 0.003391 \n",
      "[2/10][11/100][589] Loss_D: 0.003869 Loss_G: 0.002883 \n",
      "[2/10][11/100][590] Loss_D: 0.001610 Loss_G: 0.002003 \n",
      "[2/10][11/100][591] Loss_D: 0.002476 Loss_G: 0.003092 \n",
      "[2/10][11/100][592] Loss_D: 0.002107 Loss_G: 0.002943 \n",
      "[2/10][11/100][593] Loss_D: 0.001836 Loss_G: 0.002270 \n",
      "[2/10][11/100][594] Loss_D: 0.002892 Loss_G: 0.002473 \n",
      "[2/10][11/100][595] Loss_D: 0.002481 Loss_G: 0.001566 \n",
      "[2/10][11/100][596] Loss_D: 0.002040 Loss_G: 0.003442 \n",
      "[2/10][11/100][597] Loss_D: 0.003090 Loss_G: 0.006290 \n",
      "[2/10][11/100][598] Loss_D: 0.003868 Loss_G: 0.002312 \n",
      "[2/10][11/100][599] Loss_D: 0.006622 Loss_G: 0.005725 \n",
      "[2/10][11/100][600] Loss_D: 0.003435 Loss_G: 0.006753 \n",
      "[3/10][11/100][601] Loss_D: 0.005596 Loss_G: 0.003682 \n",
      "[3/10][11/100][602] Loss_D: 0.005781 Loss_G: 0.005060 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][11/100][603] Loss_D: 0.006789 Loss_G: 0.005846 \n",
      "[3/10][11/100][604] Loss_D: 0.005116 Loss_G: 0.005928 \n",
      "[3/10][11/100][605] Loss_D: 0.005491 Loss_G: 0.004209 \n",
      "[3/10][11/100][606] Loss_D: 0.005211 Loss_G: 0.003942 \n",
      "[3/10][11/100][607] Loss_D: 0.004571 Loss_G: 0.004827 \n",
      "[3/10][11/100][608] Loss_D: 0.004392 Loss_G: 0.003526 \n",
      "[3/10][11/100][609] Loss_D: 0.003696 Loss_G: 0.004231 \n",
      "[3/10][11/100][610] Loss_D: 0.002566 Loss_G: 0.004475 \n",
      "[3/10][11/100][611] Loss_D: 0.002826 Loss_G: 0.001593 \n",
      "[3/10][11/100][612] Loss_D: -0.000137 Loss_G: 0.002676 \n",
      "[3/10][11/100][613] Loss_D: 0.000975 Loss_G: 0.001891 \n",
      "[3/10][11/100][614] Loss_D: 0.002404 Loss_G: 0.003436 \n",
      "[3/10][11/100][615] Loss_D: 0.003797 Loss_G: 0.001619 \n",
      "[3/10][11/100][616] Loss_D: 0.002207 Loss_G: 0.004435 \n",
      "[3/10][11/100][617] Loss_D: 0.003419 Loss_G: 0.002139 \n",
      "[3/10][11/100][618] Loss_D: 0.000064 Loss_G: 0.000481 \n",
      "[3/10][11/100][619] Loss_D: 0.000168 Loss_G: 0.001337 \n",
      "[3/10][11/100][620] Loss_D: 0.002314 Loss_G: 0.002017 \n",
      "[3/10][11/100][621] Loss_D: 0.001370 Loss_G: 0.001913 \n",
      "[3/10][11/100][622] Loss_D: 0.001309 Loss_G: 0.003859 \n",
      "[3/10][11/100][623] Loss_D: 0.002926 Loss_G: 0.002911 \n",
      "[3/10][11/100][624] Loss_D: 0.005424 Loss_G: 0.004048 \n",
      "[3/10][11/100][625] Loss_D: 0.001970 Loss_G: 0.001778 \n",
      "[3/10][11/100][626] Loss_D: 0.002457 Loss_G: 0.003853 \n",
      "[3/10][11/100][627] Loss_D: 0.001263 Loss_G: 0.003208 \n",
      "[3/10][11/100][628] Loss_D: 0.003334 Loss_G: 0.002326 \n",
      "[3/10][11/100][629] Loss_D: 0.005381 Loss_G: 0.002945 \n",
      "[3/10][11/100][630] Loss_D: 0.003303 Loss_G: 0.003516 \n",
      "[3/10][11/100][631] Loss_D: 0.002402 Loss_G: 0.003463 \n",
      "[3/10][11/100][632] Loss_D: 0.003777 Loss_G: 0.002221 \n",
      "[3/10][11/100][633] Loss_D: 0.003412 Loss_G: 0.004794 \n",
      "[3/10][11/100][634] Loss_D: 0.002628 Loss_G: 0.002965 \n",
      "[3/10][11/100][635] Loss_D: 0.001854 Loss_G: 0.002648 \n",
      "[3/10][11/100][636] Loss_D: 0.003060 Loss_G: 0.005850 \n",
      "[3/10][11/100][637] Loss_D: 0.001883 Loss_G: 0.002801 \n",
      "[3/10][11/100][638] Loss_D: 0.002297 Loss_G: 0.001958 \n",
      "[3/10][11/100][639] Loss_D: 0.000930 Loss_G: 0.001976 \n",
      "[3/10][11/100][640] Loss_D: 0.002662 Loss_G: 0.001743 \n",
      "[3/10][11/100][641] Loss_D: 0.002530 Loss_G: 0.002516 \n",
      "[3/10][11/100][642] Loss_D: 0.003627 Loss_G: 0.004200 \n",
      "[3/10][11/100][643] Loss_D: 0.001226 Loss_G: 0.003353 \n",
      "[3/10][11/100][644] Loss_D: 0.002749 Loss_G: 0.002026 \n",
      "[3/10][11/100][645] Loss_D: 0.005452 Loss_G: 0.003294 \n",
      "[3/10][11/100][646] Loss_D: 0.002532 Loss_G: 0.003080 \n",
      "[3/10][11/100][647] Loss_D: 0.002720 Loss_G: 0.004930 \n",
      "[3/10][11/100][648] Loss_D: 0.003678 Loss_G: 0.003317 \n",
      "[3/10][11/100][649] Loss_D: 0.004114 Loss_G: 0.003454 \n",
      "[3/10][11/100][650] Loss_D: 0.005027 Loss_G: 0.002158 \n",
      "[3/10][11/100][651] Loss_D: 0.004995 Loss_G: 0.005137 \n",
      "[3/10][11/100][652] Loss_D: 0.005420 Loss_G: 0.007390 \n",
      "[3/10][11/100][653] Loss_D: 0.004596 Loss_G: 0.003312 \n",
      "[3/10][11/100][654] Loss_D: 0.004326 Loss_G: 0.003800 \n",
      "[3/10][11/100][655] Loss_D: 0.004699 Loss_G: 0.004429 \n",
      "[3/10][11/100][656] Loss_D: 0.004570 Loss_G: 0.003058 \n",
      "[3/10][11/100][657] Loss_D: 0.003836 Loss_G: 0.003203 \n",
      "[3/10][11/100][658] Loss_D: 0.004117 Loss_G: 0.002134 \n",
      "[3/10][11/100][659] Loss_D: 0.003144 Loss_G: 0.003187 \n",
      "[3/10][11/100][660] Loss_D: 0.002942 Loss_G: 0.003874 \n",
      "[3/10][11/100][661] Loss_D: 0.002194 Loss_G: 0.002947 \n",
      "[3/10][11/100][662] Loss_D: 0.001361 Loss_G: 0.001564 \n",
      "[3/10][11/100][663] Loss_D: 0.001892 Loss_G: 0.001524 \n",
      "[3/10][11/100][664] Loss_D: 0.002159 Loss_G: 0.003108 \n",
      "[3/10][11/100][665] Loss_D: 0.003290 Loss_G: 0.003469 \n",
      "[3/10][11/100][666] Loss_D: 0.004475 Loss_G: 0.003823 \n",
      "[3/10][11/100][667] Loss_D: 0.002025 Loss_G: 0.001419 \n",
      "[3/10][11/100][668] Loss_D: 0.002727 Loss_G: 0.003421 \n",
      "[3/10][11/100][669] Loss_D: 0.002024 Loss_G: 0.001897 \n",
      "[3/10][11/100][670] Loss_D: 0.003654 Loss_G: 0.003645 \n",
      "[3/10][11/100][671] Loss_D: 0.001127 Loss_G: 0.003053 \n",
      "[3/10][11/100][672] Loss_D: 0.002066 Loss_G: 0.002762 \n",
      "[3/10][11/100][673] Loss_D: 0.001923 Loss_G: 0.003025 \n",
      "[3/10][11/100][674] Loss_D: 0.003451 Loss_G: 0.002218 \n",
      "[3/10][11/100][675] Loss_D: 0.002509 Loss_G: 0.000974 \n",
      "[3/10][11/100][676] Loss_D: 0.004126 Loss_G: 0.004311 \n",
      "[3/10][11/100][677] Loss_D: 0.000935 Loss_G: 0.004014 \n",
      "[3/10][11/100][678] Loss_D: 0.003070 Loss_G: 0.003812 \n",
      "[3/10][11/100][679] Loss_D: 0.004719 Loss_G: 0.002700 \n",
      "[3/10][11/100][680] Loss_D: 0.004010 Loss_G: 0.002480 \n",
      "[3/10][11/100][681] Loss_D: 0.003921 Loss_G: 0.003526 \n",
      "[3/10][11/100][682] Loss_D: 0.002465 Loss_G: 0.005138 \n",
      "[3/10][11/100][683] Loss_D: 0.003339 Loss_G: 0.001544 \n",
      "[3/10][11/100][684] Loss_D: 0.001933 Loss_G: 0.002626 \n",
      "[3/10][11/100][685] Loss_D: 0.002153 Loss_G: 0.002160 \n",
      "[3/10][11/100][686] Loss_D: 0.002869 Loss_G: 0.003128 \n",
      "[3/10][11/100][687] Loss_D: 0.003071 Loss_G: 0.002901 \n",
      "[3/10][11/100][688] Loss_D: 0.002210 Loss_G: 0.002688 \n",
      "[3/10][11/100][689] Loss_D: 0.002803 Loss_G: 0.002176 \n",
      "[3/10][11/100][690] Loss_D: 0.002078 Loss_G: 0.002527 \n",
      "[3/10][11/100][691] Loss_D: 0.004154 Loss_G: 0.003128 \n",
      "[3/10][11/100][692] Loss_D: 0.001682 Loss_G: 0.004406 \n",
      "[3/10][11/100][693] Loss_D: 0.003694 Loss_G: 0.001492 \n",
      "[3/10][11/100][694] Loss_D: 0.006167 Loss_G: 0.003069 \n",
      "[3/10][11/100][695] Loss_D: 0.004974 Loss_G: 0.002578 \n",
      "[3/10][11/100][696] Loss_D: 0.003541 Loss_G: 0.003678 \n",
      "[3/10][11/100][697] Loss_D: 0.003533 Loss_G: 0.004162 \n",
      "[3/10][11/100][698] Loss_D: 0.003682 Loss_G: 0.004998 \n",
      "[3/10][11/100][699] Loss_D: 0.005624 Loss_G: 0.001802 \n",
      "[3/10][11/100][700] Loss_D: 0.003149 Loss_G: 0.003513 \n",
      "[3/10][11/100][701] Loss_D: 0.005189 Loss_G: 0.004883 \n",
      "[3/10][11/100][702] Loss_D: 0.006271 Loss_G: 0.003995 \n",
      "[3/10][11/100][703] Loss_D: 0.003259 Loss_G: 0.004722 \n",
      "[3/10][11/100][704] Loss_D: 0.005205 Loss_G: 0.004480 \n",
      "[3/10][11/100][705] Loss_D: 0.004050 Loss_G: 0.004592 \n",
      "[3/10][11/100][706] Loss_D: 0.003395 Loss_G: 0.004440 \n",
      "[3/10][11/100][707] Loss_D: 0.001679 Loss_G: 0.002565 \n",
      "[3/10][11/100][708] Loss_D: 0.002117 Loss_G: 0.001837 \n",
      "[3/10][11/100][709] Loss_D: 0.001943 Loss_G: 0.000699 \n",
      "[3/10][11/100][710] Loss_D: 0.003431 Loss_G: 0.002679 \n",
      "[3/10][11/100][711] Loss_D: 0.002149 Loss_G: 0.003620 \n",
      "[3/10][11/100][712] Loss_D: 0.000723 Loss_G: 0.004246 \n",
      "[3/10][11/100][713] Loss_D: 0.002577 Loss_G: 0.001018 \n",
      "[3/10][11/100][714] Loss_D: 0.001672 Loss_G: 0.004041 \n",
      "[3/10][11/100][715] Loss_D: 0.002690 Loss_G: 0.003688 \n",
      "[3/10][11/100][716] Loss_D: 0.001755 Loss_G: 0.003183 \n",
      "[3/10][11/100][717] Loss_D: 0.002280 Loss_G: 0.001982 \n",
      "[3/10][11/100][718] Loss_D: 0.001912 Loss_G: 0.000621 \n",
      "[3/10][11/100][719] Loss_D: 0.003349 Loss_G: 0.002736 \n",
      "[3/10][11/100][720] Loss_D: 0.000680 Loss_G: 0.002950 \n",
      "[3/10][11/100][721] Loss_D: 0.002312 Loss_G: 0.003366 \n",
      "[3/10][11/100][722] Loss_D: 0.003953 Loss_G: 0.001565 \n",
      "[3/10][11/100][723] Loss_D: 0.002947 Loss_G: 0.004585 \n",
      "[3/10][11/100][724] Loss_D: 0.002184 Loss_G: 0.001615 \n",
      "[3/10][11/100][725] Loss_D: 0.002509 Loss_G: 0.002668 \n",
      "[3/10][11/100][726] Loss_D: 0.001651 Loss_G: 0.002269 \n",
      "[3/10][11/100][727] Loss_D: 0.001894 Loss_G: 0.003570 \n",
      "[3/10][11/100][728] Loss_D: 0.001923 Loss_G: 0.002132 \n",
      "[3/10][11/100][729] Loss_D: 0.002491 Loss_G: 0.002636 \n",
      "[3/10][11/100][730] Loss_D: 0.001224 Loss_G: 0.003898 \n",
      "[3/10][11/100][731] Loss_D: 0.001501 Loss_G: 0.002900 \n",
      "[3/10][11/100][732] Loss_D: 0.003613 Loss_G: 0.002208 \n",
      "[3/10][11/100][733] Loss_D: 0.003171 Loss_G: 0.002769 \n",
      "[3/10][11/100][734] Loss_D: 0.001808 Loss_G: 0.003036 \n",
      "[3/10][11/100][735] Loss_D: 0.004588 Loss_G: 0.001805 \n",
      "[3/10][11/100][736] Loss_D: 0.003372 Loss_G: 0.003197 \n",
      "[3/10][11/100][737] Loss_D: 0.002859 Loss_G: 0.001416 \n",
      "[3/10][11/100][738] Loss_D: 0.003337 Loss_G: 0.003125 \n",
      "[3/10][11/100][739] Loss_D: 0.002506 Loss_G: 0.002705 \n",
      "[3/10][11/100][740] Loss_D: 0.004545 Loss_G: 0.002203 \n",
      "[3/10][11/100][741] Loss_D: 0.003017 Loss_G: 0.002538 \n",
      "[3/10][11/100][742] Loss_D: 0.004242 Loss_G: 0.003656 \n",
      "[3/10][11/100][743] Loss_D: 0.005904 Loss_G: 0.003457 \n",
      "[3/10][11/100][744] Loss_D: 0.004004 Loss_G: 0.002923 \n",
      "[3/10][11/100][745] Loss_D: 0.004264 Loss_G: 0.003923 \n",
      "[3/10][11/100][746] Loss_D: 0.004529 Loss_G: 0.002996 \n",
      "[3/10][11/100][747] Loss_D: 0.004973 Loss_G: 0.001323 \n",
      "[3/10][11/100][748] Loss_D: 0.004169 Loss_G: 0.004008 \n",
      "[3/10][11/100][749] Loss_D: 0.005060 Loss_G: 0.003781 \n",
      "[3/10][11/100][750] Loss_D: 0.005172 Loss_G: 0.006059 \n",
      "[3/10][11/100][751] Loss_D: 0.004279 Loss_G: 0.003444 \n",
      "[3/10][11/100][752] Loss_D: 0.003982 Loss_G: 0.003442 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][11/100][753] Loss_D: 0.004464 Loss_G: 0.004103 \n",
      "[3/10][11/100][754] Loss_D: 0.004708 Loss_G: 0.003853 \n",
      "[3/10][11/100][755] Loss_D: 0.006278 Loss_G: 0.003526 \n",
      "[3/10][11/100][756] Loss_D: 0.003275 Loss_G: 0.004473 \n",
      "[3/10][11/100][757] Loss_D: 0.003868 Loss_G: 0.003526 \n",
      "[3/10][11/100][758] Loss_D: 0.004112 Loss_G: 0.003645 \n",
      "[3/10][11/100][759] Loss_D: 0.003635 Loss_G: 0.004661 \n",
      "[3/10][11/100][760] Loss_D: 0.003235 Loss_G: 0.003537 \n",
      "[3/10][11/100][761] Loss_D: 0.004167 Loss_G: 0.001826 \n",
      "[3/10][11/100][762] Loss_D: 0.003664 Loss_G: 0.000663 \n",
      "[3/10][11/100][763] Loss_D: 0.002303 Loss_G: 0.002711 \n",
      "[3/10][11/100][764] Loss_D: 0.004151 Loss_G: 0.001235 \n",
      "[3/10][11/100][765] Loss_D: 0.002174 Loss_G: 0.004030 \n",
      "[3/10][11/100][766] Loss_D: 0.001752 Loss_G: 0.002172 \n",
      "[3/10][11/100][767] Loss_D: 0.001652 Loss_G: 0.001711 \n",
      "[3/10][11/100][768] Loss_D: 0.001960 Loss_G: 0.001693 \n",
      "[3/10][11/100][769] Loss_D: 0.001690 Loss_G: 0.001477 \n",
      "[3/10][11/100][770] Loss_D: 0.002487 Loss_G: 0.001725 \n",
      "[3/10][11/100][771] Loss_D: 0.002980 Loss_G: 0.002132 \n",
      "[3/10][11/100][772] Loss_D: 0.001146 Loss_G: 0.001867 \n",
      "[3/10][11/100][773] Loss_D: 0.003053 Loss_G: 0.002784 \n",
      "[3/10][11/100][774] Loss_D: 0.002392 Loss_G: 0.002390 \n",
      "[3/10][11/100][775] Loss_D: 0.003124 Loss_G: 0.003226 \n",
      "[3/10][11/100][776] Loss_D: 0.005413 Loss_G: 0.004278 \n",
      "[3/10][11/100][777] Loss_D: 0.002579 Loss_G: 0.001913 \n",
      "[3/10][11/100][778] Loss_D: 0.003278 Loss_G: 0.002595 \n",
      "[3/10][11/100][779] Loss_D: 0.003999 Loss_G: 0.003674 \n",
      "[3/10][11/100][780] Loss_D: 0.001843 Loss_G: 0.001667 \n",
      "[3/10][11/100][781] Loss_D: 0.002663 Loss_G: 0.002463 \n",
      "[3/10][11/100][782] Loss_D: 0.002796 Loss_G: 0.002011 \n",
      "[3/10][11/100][783] Loss_D: 0.002206 Loss_G: 0.002617 \n",
      "[3/10][11/100][784] Loss_D: 0.003360 Loss_G: 0.002892 \n",
      "[3/10][11/100][785] Loss_D: 0.002081 Loss_G: 0.003295 \n",
      "[3/10][11/100][786] Loss_D: 0.001899 Loss_G: 0.003162 \n",
      "[3/10][11/100][787] Loss_D: 0.003406 Loss_G: 0.002178 \n",
      "[3/10][11/100][788] Loss_D: 0.003718 Loss_G: 0.002469 \n",
      "[3/10][11/100][789] Loss_D: 0.003050 Loss_G: 0.003073 \n",
      "[3/10][11/100][790] Loss_D: 0.003087 Loss_G: 0.003587 \n",
      "[3/10][11/100][791] Loss_D: 0.004110 Loss_G: 0.002105 \n",
      "[3/10][11/100][792] Loss_D: 0.002462 Loss_G: 0.003170 \n",
      "[3/10][11/100][793] Loss_D: 0.003486 Loss_G: 0.005066 \n",
      "[3/10][11/100][794] Loss_D: 0.003684 Loss_G: 0.003005 \n",
      "[3/10][11/100][795] Loss_D: 0.004780 Loss_G: 0.003912 \n",
      "[3/10][11/100][796] Loss_D: 0.004166 Loss_G: 0.003302 \n",
      "[3/10][11/100][797] Loss_D: 0.003381 Loss_G: 0.002549 \n",
      "[3/10][11/100][798] Loss_D: 0.004301 Loss_G: 0.003709 \n",
      "[3/10][11/100][799] Loss_D: 0.003701 Loss_G: 0.003726 \n",
      "[3/10][11/100][800] Loss_D: 0.004152 Loss_G: 0.004646 \n",
      "[4/10][11/100][801] Loss_D: 0.004715 Loss_G: 0.004216 \n",
      "[4/10][11/100][802] Loss_D: 0.002910 Loss_G: 0.002888 \n",
      "[4/10][11/100][803] Loss_D: 0.003563 Loss_G: 0.003615 \n",
      "[4/10][11/100][804] Loss_D: 0.004906 Loss_G: 0.004303 \n",
      "[4/10][11/100][805] Loss_D: 0.005527 Loss_G: 0.004369 \n",
      "[4/10][11/100][806] Loss_D: 0.003147 Loss_G: 0.003998 \n",
      "[4/10][11/100][807] Loss_D: 0.003417 Loss_G: 0.003729 \n",
      "[4/10][11/100][808] Loss_D: 0.003762 Loss_G: 0.003062 \n",
      "[4/10][11/100][809] Loss_D: 0.003231 Loss_G: 0.004225 \n",
      "[4/10][11/100][810] Loss_D: 0.003666 Loss_G: 0.003996 \n",
      "[4/10][11/100][811] Loss_D: 0.002990 Loss_G: 0.002878 \n",
      "[4/10][11/100][812] Loss_D: 0.004220 Loss_G: 0.002168 \n",
      "[4/10][11/100][813] Loss_D: 0.003514 Loss_G: 0.004107 \n",
      "[4/10][11/100][814] Loss_D: 0.002534 Loss_G: 0.002749 \n",
      "[4/10][11/100][815] Loss_D: 0.004010 Loss_G: 0.003734 \n",
      "[4/10][11/100][816] Loss_D: 0.003670 Loss_G: 0.003012 \n",
      "[4/10][11/100][817] Loss_D: 0.001358 Loss_G: 0.001768 \n",
      "[4/10][11/100][818] Loss_D: 0.002268 Loss_G: 0.002521 \n",
      "[4/10][11/100][819] Loss_D: 0.002067 Loss_G: 0.001768 \n",
      "[4/10][11/100][820] Loss_D: 0.002893 Loss_G: 0.002836 \n",
      "[4/10][11/100][821] Loss_D: 0.002610 Loss_G: 0.001411 \n",
      "[4/10][11/100][822] Loss_D: 0.001629 Loss_G: 0.001562 \n",
      "[4/10][11/100][823] Loss_D: 0.005645 Loss_G: 0.002382 \n",
      "[4/10][11/100][824] Loss_D: 0.001939 Loss_G: 0.003573 \n",
      "[4/10][11/100][825] Loss_D: 0.001491 Loss_G: 0.004005 \n",
      "[4/10][11/100][826] Loss_D: 0.002415 Loss_G: 0.002360 \n",
      "[4/10][11/100][827] Loss_D: 0.003135 Loss_G: 0.003560 \n",
      "[4/10][11/100][828] Loss_D: 0.002176 Loss_G: 0.004402 \n",
      "[4/10][11/100][829] Loss_D: 0.003015 Loss_G: 0.001748 \n",
      "[4/10][11/100][830] Loss_D: 0.002102 Loss_G: 0.002833 \n",
      "[4/10][11/100][831] Loss_D: 0.003296 Loss_G: 0.001420 \n",
      "[4/10][11/100][832] Loss_D: 0.003352 Loss_G: 0.002901 \n",
      "[4/10][11/100][833] Loss_D: 0.002070 Loss_G: 0.003406 \n",
      "[4/10][11/100][834] Loss_D: 0.001457 Loss_G: 0.002462 \n",
      "[4/10][11/100][835] Loss_D: 0.002845 Loss_G: 0.003609 \n",
      "[4/10][11/100][836] Loss_D: 0.001799 Loss_G: 0.003376 \n",
      "[4/10][11/100][837] Loss_D: 0.002598 Loss_G: 0.002968 \n",
      "[4/10][11/100][838] Loss_D: 0.001709 Loss_G: 0.001771 \n",
      "[4/10][11/100][839] Loss_D: 0.002462 Loss_G: 0.003390 \n",
      "[4/10][11/100][840] Loss_D: 0.003796 Loss_G: 0.003263 \n",
      "[4/10][11/100][841] Loss_D: 0.003865 Loss_G: 0.003237 \n",
      "[4/10][11/100][842] Loss_D: 0.003447 Loss_G: 0.002555 \n",
      "[4/10][11/100][843] Loss_D: 0.002819 Loss_G: 0.003366 \n",
      "[4/10][11/100][844] Loss_D: 0.004388 Loss_G: 0.002989 \n",
      "[4/10][11/100][845] Loss_D: 0.003913 Loss_G: 0.002346 \n",
      "[4/10][11/100][846] Loss_D: 0.003873 Loss_G: 0.003677 \n",
      "[4/10][11/100][847] Loss_D: 0.003128 Loss_G: 0.003575 \n",
      "[4/10][11/100][848] Loss_D: 0.002487 Loss_G: 0.003892 \n",
      "[4/10][11/100][849] Loss_D: 0.003428 Loss_G: 0.002148 \n",
      "[4/10][11/100][850] Loss_D: 0.004110 Loss_G: 0.003763 \n",
      "[4/10][11/100][851] Loss_D: 0.003645 Loss_G: 0.001387 \n",
      "[4/10][11/100][852] Loss_D: 0.003322 Loss_G: 0.003710 \n",
      "[4/10][11/100][853] Loss_D: 0.003651 Loss_G: 0.003095 \n",
      "[4/10][11/100][854] Loss_D: 0.003721 Loss_G: 0.005910 \n",
      "[4/10][11/100][855] Loss_D: 0.004479 Loss_G: 0.003616 \n",
      "[4/10][11/100][856] Loss_D: 0.002635 Loss_G: 0.003862 \n",
      "[4/10][11/100][857] Loss_D: 0.004860 Loss_G: 0.004489 \n",
      "[4/10][11/100][858] Loss_D: 0.004703 Loss_G: 0.002517 \n",
      "[4/10][11/100][859] Loss_D: 0.005587 Loss_G: 0.005576 \n",
      "[4/10][11/100][860] Loss_D: 0.004525 Loss_G: 0.004038 \n",
      "[4/10][11/100][861] Loss_D: 0.004508 Loss_G: 0.005025 \n",
      "[4/10][11/100][862] Loss_D: 0.003960 Loss_G: 0.002319 \n",
      "[4/10][11/100][863] Loss_D: 0.002740 Loss_G: 0.003995 \n",
      "[4/10][11/100][864] Loss_D: 0.003465 Loss_G: 0.003608 \n",
      "[4/10][11/100][865] Loss_D: 0.003988 Loss_G: 0.002883 \n",
      "[4/10][11/100][866] Loss_D: 0.001961 Loss_G: 0.004448 \n",
      "[4/10][11/100][867] Loss_D: 0.003793 Loss_G: 0.003899 \n",
      "[4/10][11/100][868] Loss_D: 0.003214 Loss_G: 0.003083 \n",
      "[4/10][11/100][869] Loss_D: 0.002136 Loss_G: 0.003032 \n",
      "[4/10][11/100][870] Loss_D: 0.002531 Loss_G: 0.003034 \n",
      "[4/10][11/100][871] Loss_D: 0.002683 Loss_G: 0.002600 \n",
      "[4/10][11/100][872] Loss_D: 0.002635 Loss_G: 0.002129 \n",
      "[4/10][11/100][873] Loss_D: 0.002258 Loss_G: 0.002151 \n",
      "[4/10][11/100][874] Loss_D: 0.001868 Loss_G: 0.003353 \n",
      "[4/10][11/100][875] Loss_D: 0.001876 Loss_G: 0.002926 \n",
      "[4/10][11/100][876] Loss_D: 0.002006 Loss_G: 0.002643 \n",
      "[4/10][11/100][877] Loss_D: 0.001537 Loss_G: 0.002364 \n",
      "[4/10][11/100][878] Loss_D: 0.001968 Loss_G: 0.001602 \n",
      "[4/10][11/100][879] Loss_D: 0.003502 Loss_G: 0.002926 \n",
      "[4/10][11/100][880] Loss_D: 0.001683 Loss_G: 0.004182 \n",
      "[4/10][11/100][881] Loss_D: 0.003185 Loss_G: 0.002159 \n",
      "[4/10][11/100][882] Loss_D: 0.002742 Loss_G: 0.001639 \n",
      "[4/10][11/100][883] Loss_D: 0.003159 Loss_G: 0.003746 \n",
      "[4/10][11/100][884] Loss_D: 0.003538 Loss_G: 0.002197 \n",
      "[4/10][11/100][885] Loss_D: 0.002269 Loss_G: 0.002558 \n",
      "[4/10][11/100][886] Loss_D: 0.002312 Loss_G: 0.002292 \n",
      "[4/10][11/100][887] Loss_D: 0.003735 Loss_G: 0.000320 \n",
      "[4/10][11/100][888] Loss_D: 0.001436 Loss_G: 0.002505 \n",
      "[4/10][11/100][889] Loss_D: 0.000974 Loss_G: 0.003708 \n",
      "[4/10][11/100][890] Loss_D: 0.003381 Loss_G: 0.001951 \n",
      "[4/10][11/100][891] Loss_D: 0.003151 Loss_G: 0.002146 \n",
      "[4/10][11/100][892] Loss_D: 0.003556 Loss_G: 0.003058 \n",
      "[4/10][11/100][893] Loss_D: 0.001833 Loss_G: 0.002324 \n",
      "[4/10][11/100][894] Loss_D: 0.003773 Loss_G: 0.002987 \n",
      "[4/10][11/100][895] Loss_D: 0.001897 Loss_G: 0.001407 \n",
      "[4/10][11/100][896] Loss_D: 0.003132 Loss_G: 0.003316 \n",
      "[4/10][11/100][897] Loss_D: 0.002851 Loss_G: 0.003311 \n",
      "[4/10][11/100][898] Loss_D: 0.001808 Loss_G: 0.004869 \n",
      "[4/10][11/100][899] Loss_D: 0.002378 Loss_G: 0.002352 \n",
      "[4/10][11/100][900] Loss_D: 0.003195 Loss_G: 0.003693 \n",
      "[4/10][11/100][901] Loss_D: 0.003466 Loss_G: 0.002336 \n",
      "[4/10][11/100][902] Loss_D: 0.003287 Loss_G: 0.001759 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10][11/100][903] Loss_D: 0.003218 Loss_G: 0.002720 \n",
      "[4/10][11/100][904] Loss_D: 0.002464 Loss_G: 0.004421 \n",
      "[4/10][11/100][905] Loss_D: 0.002945 Loss_G: 0.002863 \n",
      "[4/10][11/100][906] Loss_D: 0.003998 Loss_G: 0.004579 \n",
      "[4/10][11/100][907] Loss_D: 0.003385 Loss_G: 0.004666 \n",
      "[4/10][11/100][908] Loss_D: 0.002787 Loss_G: 0.002249 \n",
      "[4/10][11/100][909] Loss_D: 0.003740 Loss_G: 0.003740 \n",
      "[4/10][11/100][910] Loss_D: 0.003073 Loss_G: 0.003619 \n",
      "[4/10][11/100][911] Loss_D: 0.005072 Loss_G: 0.003742 \n",
      "[4/10][11/100][912] Loss_D: 0.005452 Loss_G: 0.004831 \n",
      "[4/10][11/100][913] Loss_D: 0.002908 Loss_G: 0.003853 \n",
      "[4/10][11/100][914] Loss_D: 0.005200 Loss_G: 0.002530 \n",
      "[4/10][11/100][915] Loss_D: 0.003136 Loss_G: 0.002696 \n",
      "[4/10][11/100][916] Loss_D: 0.005146 Loss_G: 0.004174 \n",
      "[4/10][11/100][917] Loss_D: 0.005029 Loss_G: 0.004705 \n",
      "[4/10][11/100][918] Loss_D: 0.004313 Loss_G: 0.004655 \n",
      "[4/10][11/100][919] Loss_D: 0.003771 Loss_G: 0.005587 \n",
      "[4/10][11/100][920] Loss_D: 0.004515 Loss_G: 0.004060 \n",
      "[4/10][11/100][921] Loss_D: 0.003473 Loss_G: 0.004719 \n",
      "[4/10][11/100][922] Loss_D: 0.004672 Loss_G: 0.005888 \n",
      "[4/10][11/100][923] Loss_D: 0.004877 Loss_G: 0.004960 \n",
      "[4/10][11/100][924] Loss_D: 0.004608 Loss_G: 0.002986 \n",
      "[4/10][11/100][925] Loss_D: 0.004582 Loss_G: 0.003104 \n",
      "[4/10][11/100][926] Loss_D: 0.002896 Loss_G: 0.002280 \n",
      "[4/10][11/100][927] Loss_D: 0.003874 Loss_G: 0.003386 \n",
      "[4/10][11/100][928] Loss_D: 0.003495 Loss_G: 0.003691 \n",
      "[4/10][11/100][929] Loss_D: 0.003434 Loss_G: 0.002622 \n",
      "[4/10][11/100][930] Loss_D: 0.003499 Loss_G: 0.003318 \n",
      "[4/10][11/100][931] Loss_D: 0.002628 Loss_G: 0.003056 \n",
      "[4/10][11/100][932] Loss_D: 0.003079 Loss_G: 0.003568 \n",
      "[4/10][11/100][933] Loss_D: 0.002578 Loss_G: 0.003698 \n",
      "[4/10][11/100][934] Loss_D: 0.003060 Loss_G: 0.002886 \n",
      "[4/10][11/100][935] Loss_D: 0.002567 Loss_G: 0.002265 \n",
      "[4/10][11/100][936] Loss_D: 0.002131 Loss_G: 0.002678 \n",
      "[4/10][11/100][937] Loss_D: 0.002739 Loss_G: 0.001400 \n",
      "[4/10][11/100][938] Loss_D: 0.002031 Loss_G: 0.001993 \n",
      "[4/10][11/100][939] Loss_D: 0.002807 Loss_G: 0.001448 \n",
      "[4/10][11/100][940] Loss_D: 0.002414 Loss_G: 0.003571 \n",
      "[4/10][11/100][941] Loss_D: 0.002705 Loss_G: 0.002473 \n",
      "[4/10][11/100][942] Loss_D: 0.001157 Loss_G: 0.001461 \n",
      "[4/10][11/100][943] Loss_D: 0.003456 Loss_G: 0.002418 \n",
      "[4/10][11/100][944] Loss_D: 0.003020 Loss_G: 0.003455 \n",
      "[4/10][11/100][945] Loss_D: 0.002855 Loss_G: 0.003163 \n",
      "[4/10][11/100][946] Loss_D: 0.002708 Loss_G: 0.002579 \n",
      "[4/10][11/100][947] Loss_D: 0.002490 Loss_G: 0.003295 \n",
      "[4/10][11/100][948] Loss_D: 0.002601 Loss_G: 0.002533 \n",
      "[4/10][11/100][949] Loss_D: 0.006204 Loss_G: 0.002215 \n",
      "[4/10][11/100][950] Loss_D: 0.004132 Loss_G: 0.004489 \n",
      "[4/10][11/100][951] Loss_D: 0.003476 Loss_G: 0.003215 \n",
      "[4/10][11/100][952] Loss_D: 0.003072 Loss_G: 0.003158 \n",
      "[4/10][11/100][953] Loss_D: 0.003655 Loss_G: 0.001332 \n",
      "[4/10][11/100][954] Loss_D: 0.002817 Loss_G: 0.003647 \n",
      "[4/10][11/100][955] Loss_D: 0.004152 Loss_G: 0.003033 \n",
      "[4/10][11/100][956] Loss_D: 0.003358 Loss_G: 0.003977 \n",
      "[4/10][11/100][957] Loss_D: 0.002496 Loss_G: 0.002038 \n",
      "[4/10][11/100][958] Loss_D: 0.003700 Loss_G: 0.004509 \n",
      "[4/10][11/100][959] Loss_D: 0.003288 Loss_G: 0.002024 \n",
      "[4/10][11/100][960] Loss_D: 0.003851 Loss_G: 0.002518 \n",
      "[4/10][11/100][961] Loss_D: 0.003823 Loss_G: 0.003442 \n",
      "[4/10][11/100][962] Loss_D: 0.003639 Loss_G: 0.006013 \n",
      "[4/10][11/100][963] Loss_D: 0.003275 Loss_G: 0.003437 \n",
      "[4/10][11/100][964] Loss_D: 0.005619 Loss_G: 0.003362 \n",
      "[4/10][11/100][965] Loss_D: 0.004368 Loss_G: 0.005137 \n",
      "[4/10][11/100][966] Loss_D: 0.003015 Loss_G: 0.003063 \n",
      "[4/10][11/100][967] Loss_D: 0.003066 Loss_G: 0.004857 \n",
      "[4/10][11/100][968] Loss_D: 0.005636 Loss_G: 0.005442 \n",
      "[4/10][11/100][969] Loss_D: 0.003914 Loss_G: 0.004041 \n",
      "[4/10][11/100][970] Loss_D: 0.003771 Loss_G: 0.004500 \n",
      "[4/10][11/100][971] Loss_D: 0.003760 Loss_G: 0.004146 \n",
      "[4/10][11/100][972] Loss_D: 0.006261 Loss_G: 0.003355 \n",
      "[4/10][11/100][973] Loss_D: 0.003143 Loss_G: 0.004479 \n",
      "[4/10][11/100][974] Loss_D: 0.003111 Loss_G: 0.002795 \n",
      "[4/10][11/100][975] Loss_D: 0.002311 Loss_G: 0.003523 \n",
      "[4/10][11/100][976] Loss_D: 0.002743 Loss_G: 0.003050 \n",
      "[4/10][11/100][977] Loss_D: 0.001623 Loss_G: 0.002985 \n",
      "[4/10][11/100][978] Loss_D: 0.002692 Loss_G: 0.002394 \n",
      "[4/10][11/100][979] Loss_D: 0.002391 Loss_G: 0.002598 \n",
      "[4/10][11/100][980] Loss_D: 0.001810 Loss_G: 0.002063 \n",
      "[4/10][11/100][981] Loss_D: 0.003154 Loss_G: 0.002017 \n",
      "[4/10][11/100][982] Loss_D: 0.003031 Loss_G: 0.001615 \n",
      "[4/10][11/100][983] Loss_D: 0.002029 Loss_G: 0.003311 \n",
      "[4/10][11/100][984] Loss_D: 0.002395 Loss_G: 0.004545 \n",
      "[4/10][11/100][985] Loss_D: 0.003295 Loss_G: 0.002995 \n",
      "[4/10][11/100][986] Loss_D: 0.003605 Loss_G: 0.003317 \n",
      "[4/10][11/100][987] Loss_D: 0.004021 Loss_G: 0.003302 \n",
      "[4/10][11/100][988] Loss_D: 0.001911 Loss_G: 0.003437 \n",
      "[4/10][11/100][989] Loss_D: 0.002606 Loss_G: 0.004561 \n",
      "[4/10][11/100][990] Loss_D: 0.002047 Loss_G: 0.003343 \n",
      "[4/10][11/100][991] Loss_D: 0.002391 Loss_G: 0.003923 \n",
      "[4/10][11/100][992] Loss_D: 0.003544 Loss_G: 0.002035 \n",
      "[4/10][11/100][993] Loss_D: 0.003087 Loss_G: 0.004194 \n",
      "[4/10][11/100][994] Loss_D: 0.002692 Loss_G: 0.003775 \n",
      "[4/10][11/100][995] Loss_D: 0.001645 Loss_G: 0.001486 \n",
      "[4/10][11/100][996] Loss_D: 0.003043 Loss_G: 0.003468 \n",
      "[4/10][11/100][997] Loss_D: 0.002821 Loss_G: 0.004112 \n",
      "[4/10][11/100][998] Loss_D: 0.004080 Loss_G: 0.003406 \n",
      "[4/10][11/100][999] Loss_D: 0.002495 Loss_G: 0.003779 \n",
      "[4/10][11/100][1000] Loss_D: 0.002674 Loss_G: 0.003320 \n",
      "[5/10][11/100][1001] Loss_D: 0.004489 Loss_G: 0.002775 \n",
      "[5/10][11/100][1002] Loss_D: 0.003988 Loss_G: 0.003919 \n",
      "[5/10][11/100][1003] Loss_D: 0.003989 Loss_G: 0.002553 \n",
      "[5/10][11/100][1004] Loss_D: 0.004233 Loss_G: 0.002930 \n",
      "[5/10][11/100][1005] Loss_D: 0.004259 Loss_G: 0.003189 \n",
      "[5/10][11/100][1006] Loss_D: 0.004332 Loss_G: 0.002951 \n",
      "[5/10][11/100][1007] Loss_D: 0.003202 Loss_G: 0.002425 \n",
      "[5/10][11/100][1008] Loss_D: 0.003728 Loss_G: 0.003639 \n",
      "[5/10][11/100][1009] Loss_D: 0.003384 Loss_G: 0.002402 \n",
      "[5/10][11/100][1010] Loss_D: 0.002958 Loss_G: 0.002359 \n",
      "[5/10][11/100][1011] Loss_D: 0.004234 Loss_G: 0.002353 \n",
      "[5/10][11/100][1012] Loss_D: 0.003347 Loss_G: 0.003338 \n",
      "[5/10][11/100][1013] Loss_D: 0.002774 Loss_G: 0.003205 \n",
      "[5/10][11/100][1014] Loss_D: 0.002631 Loss_G: 0.004398 \n",
      "[5/10][11/100][1015] Loss_D: 0.002625 Loss_G: 0.003332 \n",
      "[5/10][11/100][1016] Loss_D: 0.003591 Loss_G: 0.003532 \n",
      "[5/10][11/100][1017] Loss_D: 0.003367 Loss_G: 0.002080 \n",
      "[5/10][11/100][1018] Loss_D: 0.002184 Loss_G: 0.003064 \n",
      "[5/10][11/100][1019] Loss_D: 0.003776 Loss_G: 0.003869 \n",
      "[5/10][11/100][1020] Loss_D: 0.002488 Loss_G: 0.003075 \n",
      "[5/10][11/100][1021] Loss_D: 0.005039 Loss_G: 0.003942 \n",
      "[5/10][11/100][1022] Loss_D: 0.004793 Loss_G: 0.001795 \n",
      "[5/10][11/100][1023] Loss_D: 0.003097 Loss_G: 0.006116 \n",
      "[5/10][11/100][1024] Loss_D: 0.003930 Loss_G: 0.003731 \n",
      "[5/10][11/100][1025] Loss_D: 0.003584 Loss_G: 0.002691 \n",
      "[5/10][11/100][1026] Loss_D: 0.004559 Loss_G: 0.002766 \n",
      "[5/10][11/100][1027] Loss_D: 0.003958 Loss_G: 0.003211 \n",
      "[5/10][11/100][1028] Loss_D: 0.004223 Loss_G: 0.002826 \n",
      "[5/10][11/100][1029] Loss_D: 0.002298 Loss_G: 0.004123 \n",
      "[5/10][11/100][1030] Loss_D: 0.003819 Loss_G: 0.004120 \n",
      "[5/10][11/100][1031] Loss_D: 0.003761 Loss_G: 0.004088 \n",
      "[5/10][11/100][1032] Loss_D: 0.001664 Loss_G: 0.003832 \n",
      "[5/10][11/100][1033] Loss_D: 0.003428 Loss_G: 0.004916 \n",
      "[5/10][11/100][1034] Loss_D: 0.004979 Loss_G: 0.003866 \n",
      "[5/10][11/100][1035] Loss_D: 0.003957 Loss_G: 0.003838 \n",
      "[5/10][11/100][1036] Loss_D: 0.003961 Loss_G: 0.001777 \n",
      "[5/10][11/100][1037] Loss_D: 0.002759 Loss_G: 0.002110 \n",
      "[5/10][11/100][1038] Loss_D: 0.002549 Loss_G: 0.002586 \n",
      "[5/10][11/100][1039] Loss_D: 0.003432 Loss_G: 0.002503 \n",
      "[5/10][11/100][1040] Loss_D: 0.001852 Loss_G: 0.003463 \n",
      "[5/10][11/100][1041] Loss_D: 0.002889 Loss_G: 0.003828 \n",
      "[5/10][11/100][1042] Loss_D: 0.004214 Loss_G: 0.003610 \n",
      "[5/10][11/100][1043] Loss_D: 0.002434 Loss_G: 0.001801 \n",
      "[5/10][11/100][1044] Loss_D: 0.003631 Loss_G: 0.003583 \n",
      "[5/10][11/100][1045] Loss_D: 0.004050 Loss_G: 0.002154 \n",
      "[5/10][11/100][1046] Loss_D: 0.004073 Loss_G: 0.004634 \n",
      "[5/10][11/100][1047] Loss_D: 0.003030 Loss_G: 0.002836 \n",
      "[5/10][11/100][1048] Loss_D: 0.005116 Loss_G: 0.002208 \n",
      "[5/10][11/100][1049] Loss_D: 0.003394 Loss_G: 0.003349 \n",
      "[5/10][11/100][1050] Loss_D: 0.003361 Loss_G: 0.004236 \n",
      "[5/10][11/100][1051] Loss_D: 0.002781 Loss_G: 0.003547 \n",
      "[5/10][11/100][1052] Loss_D: 0.002562 Loss_G: 0.004063 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10][11/100][1053] Loss_D: 0.002526 Loss_G: 0.003413 \n",
      "[5/10][11/100][1054] Loss_D: 0.002069 Loss_G: 0.003803 \n",
      "[5/10][11/100][1055] Loss_D: 0.003197 Loss_G: 0.003382 \n",
      "[5/10][11/100][1056] Loss_D: 0.002822 Loss_G: 0.002731 \n",
      "[5/10][11/100][1057] Loss_D: 0.002930 Loss_G: 0.002298 \n",
      "[5/10][11/100][1058] Loss_D: 0.004652 Loss_G: 0.002264 \n",
      "[5/10][11/100][1059] Loss_D: 0.004324 Loss_G: 0.002782 \n",
      "[5/10][11/100][1060] Loss_D: 0.004422 Loss_G: 0.003600 \n",
      "[5/10][11/100][1061] Loss_D: 0.003878 Loss_G: 0.002411 \n",
      "[5/10][11/100][1062] Loss_D: 0.003896 Loss_G: 0.003381 \n",
      "[5/10][11/100][1063] Loss_D: 0.003662 Loss_G: 0.003048 \n",
      "[5/10][11/100][1064] Loss_D: 0.002763 Loss_G: 0.003090 \n",
      "[5/10][11/100][1065] Loss_D: 0.004629 Loss_G: 0.002590 \n",
      "[5/10][11/100][1066] Loss_D: 0.005567 Loss_G: 0.002673 \n",
      "[5/10][11/100][1067] Loss_D: 0.001475 Loss_G: 0.001770 \n",
      "[5/10][11/100][1068] Loss_D: 0.003251 Loss_G: 0.003742 \n",
      "[5/10][11/100][1069] Loss_D: 0.003208 Loss_G: 0.002676 \n",
      "[5/10][11/100][1070] Loss_D: 0.001994 Loss_G: 0.002687 \n",
      "[5/10][11/100][1071] Loss_D: 0.005180 Loss_G: 0.002789 \n",
      "[5/10][11/100][1072] Loss_D: 0.002122 Loss_G: 0.003293 \n",
      "[5/10][11/100][1073] Loss_D: 0.003063 Loss_G: 0.002593 \n",
      "[5/10][11/100][1074] Loss_D: 0.003253 Loss_G: 0.004793 \n",
      "[5/10][11/100][1075] Loss_D: 0.004198 Loss_G: 0.003138 \n",
      "[5/10][11/100][1076] Loss_D: 0.005283 Loss_G: 0.004527 \n",
      "[5/10][11/100][1077] Loss_D: 0.002518 Loss_G: 0.002353 \n",
      "[5/10][11/100][1078] Loss_D: 0.003977 Loss_G: 0.003732 \n",
      "[5/10][11/100][1079] Loss_D: 0.002876 Loss_G: 0.002860 \n",
      "[5/10][11/100][1080] Loss_D: 0.003468 Loss_G: 0.001032 \n",
      "[5/10][11/100][1081] Loss_D: 0.002101 Loss_G: 0.003022 \n",
      "[5/10][11/100][1082] Loss_D: 0.002891 Loss_G: 0.002617 \n",
      "[5/10][11/100][1083] Loss_D: 0.002658 Loss_G: 0.002980 \n",
      "[5/10][11/100][1084] Loss_D: 0.002280 Loss_G: 0.002368 \n",
      "[5/10][11/100][1085] Loss_D: 0.002829 Loss_G: 0.004168 \n",
      "[5/10][11/100][1086] Loss_D: 0.002843 Loss_G: 0.003091 \n",
      "[5/10][11/100][1087] Loss_D: 0.003487 Loss_G: 0.003540 \n",
      "[5/10][11/100][1088] Loss_D: 0.001711 Loss_G: 0.004066 \n",
      "[5/10][11/100][1089] Loss_D: 0.003483 Loss_G: 0.002173 \n",
      "[5/10][11/100][1090] Loss_D: 0.002555 Loss_G: 0.002781 \n",
      "[5/10][11/100][1091] Loss_D: 0.003321 Loss_G: 0.002615 \n",
      "[5/10][11/100][1092] Loss_D: 0.004528 Loss_G: 0.002225 \n",
      "[5/10][11/100][1093] Loss_D: 0.003477 Loss_G: 0.002347 \n",
      "[5/10][11/100][1094] Loss_D: 0.003239 Loss_G: 0.002874 \n",
      "[5/10][11/100][1095] Loss_D: 0.001910 Loss_G: 0.003372 \n",
      "[5/10][11/100][1096] Loss_D: 0.002853 Loss_G: 0.003243 \n",
      "[5/10][11/100][1097] Loss_D: 0.004062 Loss_G: 0.003232 \n",
      "[5/10][11/100][1098] Loss_D: 0.002083 Loss_G: 0.004313 \n",
      "[5/10][11/100][1099] Loss_D: 0.006012 Loss_G: 0.001820 \n",
      "[5/10][11/100][1100] Loss_D: 0.001691 Loss_G: 0.002757 \n",
      "[5/10][11/100][1101] Loss_D: 0.003192 Loss_G: 0.002667 \n",
      "[5/10][11/100][1102] Loss_D: 0.002758 Loss_G: 0.003136 \n",
      "[5/10][11/100][1103] Loss_D: 0.002758 Loss_G: 0.002368 \n",
      "[5/10][11/100][1104] Loss_D: 0.003143 Loss_G: 0.004567 \n",
      "[5/10][11/100][1105] Loss_D: 0.003424 Loss_G: 0.003918 \n",
      "[5/10][11/100][1106] Loss_D: 0.003558 Loss_G: 0.003482 \n",
      "[5/10][11/100][1107] Loss_D: 0.001778 Loss_G: 0.001172 \n",
      "[5/10][11/100][1108] Loss_D: 0.003327 Loss_G: 0.003942 \n",
      "[5/10][11/100][1109] Loss_D: 0.003436 Loss_G: 0.003966 \n",
      "[5/10][11/100][1110] Loss_D: 0.003111 Loss_G: 0.003429 \n",
      "[5/10][11/100][1111] Loss_D: 0.004319 Loss_G: 0.003041 \n",
      "[5/10][11/100][1112] Loss_D: 0.002081 Loss_G: 0.003043 \n",
      "[5/10][11/100][1113] Loss_D: 0.002021 Loss_G: 0.002015 \n",
      "[5/10][11/100][1114] Loss_D: 0.002358 Loss_G: 0.003743 \n",
      "[5/10][11/100][1115] Loss_D: 0.003474 Loss_G: 0.003593 \n",
      "[5/10][11/100][1116] Loss_D: 0.002619 Loss_G: 0.002968 \n",
      "[5/10][11/100][1117] Loss_D: 0.004110 Loss_G: 0.002629 \n",
      "[5/10][11/100][1118] Loss_D: 0.003528 Loss_G: 0.002785 \n",
      "[5/10][11/100][1119] Loss_D: 0.003264 Loss_G: 0.003466 \n",
      "[5/10][11/100][1120] Loss_D: 0.003335 Loss_G: 0.003025 \n",
      "[5/10][11/100][1121] Loss_D: 0.004380 Loss_G: 0.002639 \n",
      "[5/10][11/100][1122] Loss_D: 0.003821 Loss_G: 0.001793 \n",
      "[5/10][11/100][1123] Loss_D: 0.001820 Loss_G: 0.002138 \n",
      "[5/10][11/100][1124] Loss_D: 0.001829 Loss_G: 0.002706 \n",
      "[5/10][11/100][1125] Loss_D: 0.003195 Loss_G: 0.002460 \n",
      "[5/10][11/100][1126] Loss_D: 0.001780 Loss_G: 0.002545 \n",
      "[5/10][11/100][1127] Loss_D: 0.003537 Loss_G: 0.002081 \n",
      "[5/10][11/100][1128] Loss_D: 0.001970 Loss_G: 0.002849 \n",
      "[5/10][11/100][1129] Loss_D: 0.002548 Loss_G: 0.003834 \n",
      "[5/10][11/100][1130] Loss_D: 0.002382 Loss_G: 0.003439 \n",
      "[5/10][11/100][1131] Loss_D: 0.002965 Loss_G: 0.002076 \n",
      "[5/10][11/100][1132] Loss_D: 0.002292 Loss_G: 0.003197 \n",
      "[5/10][11/100][1133] Loss_D: 0.001397 Loss_G: 0.002528 \n",
      "[5/10][11/100][1134] Loss_D: 0.004347 Loss_G: 0.002856 \n",
      "[5/10][11/100][1135] Loss_D: 0.001730 Loss_G: 0.001777 \n",
      "[5/10][11/100][1136] Loss_D: 0.002103 Loss_G: 0.001134 \n",
      "[5/10][11/100][1137] Loss_D: 0.001190 Loss_G: 0.003508 \n",
      "[5/10][11/100][1138] Loss_D: 0.002925 Loss_G: 0.005522 \n",
      "[5/10][11/100][1139] Loss_D: 0.002946 Loss_G: 0.002078 \n",
      "[5/10][11/100][1140] Loss_D: 0.002784 Loss_G: 0.003102 \n",
      "[5/10][11/100][1141] Loss_D: 0.002485 Loss_G: 0.000945 \n",
      "[5/10][11/100][1142] Loss_D: 0.002573 Loss_G: 0.002541 \n",
      "[5/10][11/100][1143] Loss_D: 0.001640 Loss_G: 0.002647 \n",
      "[5/10][11/100][1144] Loss_D: 0.004250 Loss_G: 0.002280 \n",
      "[5/10][11/100][1145] Loss_D: 0.002292 Loss_G: 0.002389 \n",
      "[5/10][11/100][1146] Loss_D: 0.002596 Loss_G: 0.002177 \n",
      "[5/10][11/100][1147] Loss_D: 0.002916 Loss_G: 0.003109 \n",
      "[5/10][11/100][1148] Loss_D: 0.002197 Loss_G: 0.002587 \n",
      "[5/10][11/100][1149] Loss_D: 0.002810 Loss_G: 0.002977 \n",
      "[5/10][11/100][1150] Loss_D: 0.003855 Loss_G: 0.002876 \n",
      "[5/10][11/100][1151] Loss_D: 0.004626 Loss_G: 0.003090 \n",
      "[5/10][11/100][1152] Loss_D: 0.002489 Loss_G: 0.002304 \n",
      "[5/10][11/100][1153] Loss_D: 0.002292 Loss_G: 0.002883 \n",
      "[5/10][11/100][1154] Loss_D: 0.004416 Loss_G: 0.003251 \n",
      "[5/10][11/100][1155] Loss_D: 0.002325 Loss_G: 0.002588 \n",
      "[5/10][11/100][1156] Loss_D: 0.003657 Loss_G: 0.003621 \n",
      "[5/10][11/100][1157] Loss_D: 0.002735 Loss_G: 0.002758 \n",
      "[5/10][11/100][1158] Loss_D: 0.002604 Loss_G: 0.001760 \n",
      "[5/10][11/100][1159] Loss_D: 0.002917 Loss_G: 0.002295 \n",
      "[5/10][11/100][1160] Loss_D: 0.003415 Loss_G: 0.002506 \n",
      "[5/10][11/100][1161] Loss_D: 0.003401 Loss_G: 0.003546 \n",
      "[5/10][11/100][1162] Loss_D: 0.001553 Loss_G: 0.003917 \n",
      "[5/10][11/100][1163] Loss_D: 0.001849 Loss_G: 0.002872 \n",
      "[5/10][11/100][1164] Loss_D: 0.002933 Loss_G: 0.002486 \n",
      "[5/10][11/100][1165] Loss_D: 0.004062 Loss_G: 0.001535 \n",
      "[5/10][11/100][1166] Loss_D: 0.003449 Loss_G: 0.003694 \n",
      "[5/10][11/100][1167] Loss_D: 0.003761 Loss_G: 0.003152 \n",
      "[5/10][11/100][1168] Loss_D: 0.001788 Loss_G: 0.003886 \n",
      "[5/10][11/100][1169] Loss_D: 0.002452 Loss_G: 0.002629 \n",
      "[5/10][11/100][1170] Loss_D: 0.002446 Loss_G: 0.005049 \n",
      "[5/10][11/100][1171] Loss_D: 0.002567 Loss_G: 0.002713 \n",
      "[5/10][11/100][1172] Loss_D: 0.001159 Loss_G: 0.003137 \n",
      "[5/10][11/100][1173] Loss_D: 0.000998 Loss_G: 0.002631 \n",
      "[5/10][11/100][1174] Loss_D: 0.003757 Loss_G: 0.004335 \n",
      "[5/10][11/100][1175] Loss_D: 0.002519 Loss_G: 0.001709 \n",
      "[5/10][11/100][1176] Loss_D: 0.002258 Loss_G: 0.004108 \n",
      "[5/10][11/100][1177] Loss_D: 0.002930 Loss_G: 0.002970 \n",
      "[5/10][11/100][1178] Loss_D: 0.002223 Loss_G: 0.003518 \n",
      "[5/10][11/100][1179] Loss_D: 0.001825 Loss_G: 0.003189 \n",
      "[5/10][11/100][1180] Loss_D: 0.002795 Loss_G: 0.002550 \n",
      "[5/10][11/100][1181] Loss_D: 0.003809 Loss_G: 0.003735 \n",
      "[5/10][11/100][1182] Loss_D: 0.002412 Loss_G: 0.004259 \n",
      "[5/10][11/100][1183] Loss_D: 0.004170 Loss_G: 0.001342 \n",
      "[5/10][11/100][1184] Loss_D: 0.002850 Loss_G: 0.001377 \n",
      "[5/10][11/100][1185] Loss_D: 0.003023 Loss_G: 0.002629 \n",
      "[5/10][11/100][1186] Loss_D: 0.002770 Loss_G: 0.001984 \n",
      "[5/10][11/100][1187] Loss_D: 0.003522 Loss_G: 0.003418 \n",
      "[5/10][11/100][1188] Loss_D: 0.001894 Loss_G: 0.002965 \n",
      "[5/10][11/100][1189] Loss_D: 0.004512 Loss_G: 0.003032 \n",
      "[5/10][11/100][1190] Loss_D: 0.004147 Loss_G: 0.002078 \n",
      "[5/10][11/100][1191] Loss_D: 0.003215 Loss_G: 0.003180 \n",
      "[5/10][11/100][1192] Loss_D: 0.003713 Loss_G: 0.002510 \n",
      "[5/10][11/100][1193] Loss_D: 0.002810 Loss_G: 0.002378 \n",
      "[5/10][11/100][1194] Loss_D: 0.003195 Loss_G: 0.002459 \n",
      "[5/10][11/100][1195] Loss_D: 0.003487 Loss_G: 0.004079 \n",
      "[5/10][11/100][1196] Loss_D: 0.002816 Loss_G: 0.004367 \n",
      "[5/10][11/100][1197] Loss_D: 0.003289 Loss_G: 0.003044 \n",
      "[5/10][11/100][1198] Loss_D: 0.004579 Loss_G: 0.003433 \n",
      "[5/10][11/100][1199] Loss_D: 0.003731 Loss_G: 0.003026 \n",
      "[5/10][11/100][1200] Loss_D: 0.003980 Loss_G: 0.002602 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][11/100][1201] Loss_D: 0.002937 Loss_G: 0.003203 \n",
      "[6/10][11/100][1202] Loss_D: 0.003433 Loss_G: 0.003760 \n",
      "[6/10][11/100][1203] Loss_D: 0.005197 Loss_G: 0.003641 \n",
      "[6/10][11/100][1204] Loss_D: 0.006259 Loss_G: 0.003862 \n",
      "[6/10][11/100][1205] Loss_D: 0.003705 Loss_G: 0.003980 \n",
      "[6/10][11/100][1206] Loss_D: 0.004526 Loss_G: 0.004573 \n",
      "[6/10][11/100][1207] Loss_D: 0.005658 Loss_G: 0.004049 \n",
      "[6/10][11/100][1208] Loss_D: 0.003661 Loss_G: 0.005915 \n",
      "[6/10][11/100][1209] Loss_D: 0.004322 Loss_G: 0.003091 \n",
      "[6/10][11/100][1210] Loss_D: 0.001961 Loss_G: 0.002541 \n",
      "[6/10][11/100][1211] Loss_D: 0.002892 Loss_G: 0.004398 \n",
      "[6/10][11/100][1212] Loss_D: 0.003096 Loss_G: 0.001108 \n",
      "[6/10][11/100][1213] Loss_D: 0.003404 Loss_G: 0.002036 \n",
      "[6/10][11/100][1214] Loss_D: 0.002297 Loss_G: 0.002031 \n",
      "[6/10][11/100][1215] Loss_D: 0.001599 Loss_G: 0.001950 \n",
      "[6/10][11/100][1216] Loss_D: 0.003480 Loss_G: 0.001730 \n",
      "[6/10][11/100][1217] Loss_D: 0.002944 Loss_G: 0.002699 \n",
      "[6/10][11/100][1218] Loss_D: 0.001671 Loss_G: 0.001771 \n",
      "[6/10][11/100][1219] Loss_D: 0.002734 Loss_G: 0.002947 \n",
      "[6/10][11/100][1220] Loss_D: 0.001266 Loss_G: 0.003174 \n",
      "[6/10][11/100][1221] Loss_D: 0.002781 Loss_G: 0.003557 \n",
      "[6/10][11/100][1222] Loss_D: 0.003014 Loss_G: 0.002669 \n",
      "[6/10][11/100][1223] Loss_D: 0.004344 Loss_G: 0.002884 \n",
      "[6/10][11/100][1224] Loss_D: 0.002117 Loss_G: 0.004204 \n",
      "[6/10][11/100][1225] Loss_D: 0.002705 Loss_G: 0.002925 \n",
      "[6/10][11/100][1226] Loss_D: 0.002858 Loss_G: 0.003500 \n",
      "[6/10][11/100][1227] Loss_D: 0.002700 Loss_G: 0.003737 \n",
      "[6/10][11/100][1228] Loss_D: 0.003069 Loss_G: 0.004238 \n",
      "[6/10][11/100][1229] Loss_D: 0.004075 Loss_G: 0.003500 \n",
      "[6/10][11/100][1230] Loss_D: 0.005005 Loss_G: 0.002586 \n",
      "[6/10][11/100][1231] Loss_D: 0.002694 Loss_G: 0.001767 \n",
      "[6/10][11/100][1232] Loss_D: 0.002998 Loss_G: 0.005261 \n",
      "[6/10][11/100][1233] Loss_D: 0.004560 Loss_G: 0.002568 \n",
      "[6/10][11/100][1234] Loss_D: 0.003018 Loss_G: 0.003156 \n",
      "[6/10][11/100][1235] Loss_D: 0.003235 Loss_G: 0.003214 \n",
      "[6/10][11/100][1236] Loss_D: 0.002686 Loss_G: 0.002579 \n",
      "[6/10][11/100][1237] Loss_D: 0.004347 Loss_G: 0.007125 \n",
      "[6/10][11/100][1238] Loss_D: 0.002166 Loss_G: 0.003730 \n",
      "[6/10][11/100][1239] Loss_D: 0.002959 Loss_G: 0.002618 \n",
      "[6/10][11/100][1240] Loss_D: 0.003875 Loss_G: 0.003146 \n",
      "[6/10][11/100][1241] Loss_D: 0.003159 Loss_G: 0.003819 \n",
      "[6/10][11/100][1242] Loss_D: 0.004119 Loss_G: 0.003518 \n",
      "[6/10][11/100][1243] Loss_D: 0.001655 Loss_G: 0.003467 \n",
      "[6/10][11/100][1244] Loss_D: 0.003367 Loss_G: 0.002490 \n",
      "[6/10][11/100][1245] Loss_D: 0.003706 Loss_G: 0.003536 \n",
      "[6/10][11/100][1246] Loss_D: 0.003081 Loss_G: 0.004885 \n",
      "[6/10][11/100][1247] Loss_D: 0.002157 Loss_G: 0.002523 \n",
      "[6/10][11/100][1248] Loss_D: 0.002819 Loss_G: 0.003149 \n",
      "[6/10][11/100][1249] Loss_D: 0.002617 Loss_G: 0.001802 \n",
      "[6/10][11/100][1250] Loss_D: 0.003696 Loss_G: 0.002809 \n",
      "[6/10][11/100][1251] Loss_D: 0.000681 Loss_G: 0.003813 \n",
      "[6/10][11/100][1252] Loss_D: 0.002965 Loss_G: 0.003713 \n",
      "[6/10][11/100][1253] Loss_D: 0.003331 Loss_G: 0.002370 \n",
      "[6/10][11/100][1254] Loss_D: 0.003701 Loss_G: 0.003500 \n",
      "[6/10][11/100][1255] Loss_D: 0.004269 Loss_G: 0.004561 \n",
      "[6/10][11/100][1256] Loss_D: 0.005259 Loss_G: 0.002549 \n",
      "[6/10][11/100][1257] Loss_D: 0.004447 Loss_G: 0.006126 \n",
      "[6/10][11/100][1258] Loss_D: 0.003825 Loss_G: 0.005207 \n",
      "[6/10][11/100][1259] Loss_D: 0.003389 Loss_G: 0.002698 \n",
      "[6/10][11/100][1260] Loss_D: 0.002401 Loss_G: 0.003347 \n",
      "[6/10][11/100][1261] Loss_D: 0.002698 Loss_G: 0.003395 \n",
      "[6/10][11/100][1262] Loss_D: 0.004098 Loss_G: 0.003596 \n",
      "[6/10][11/100][1263] Loss_D: 0.003403 Loss_G: 0.002819 \n",
      "[6/10][11/100][1264] Loss_D: 0.002449 Loss_G: 0.002846 \n",
      "[6/10][11/100][1265] Loss_D: 0.002788 Loss_G: 0.003223 \n",
      "[6/10][11/100][1266] Loss_D: 0.002669 Loss_G: 0.002227 \n",
      "[6/10][11/100][1267] Loss_D: 0.003009 Loss_G: 0.004093 \n",
      "[6/10][11/100][1268] Loss_D: 0.002871 Loss_G: 0.003323 \n",
      "[6/10][11/100][1269] Loss_D: 0.002835 Loss_G: 0.003347 \n",
      "[6/10][11/100][1270] Loss_D: 0.003294 Loss_G: 0.000556 \n",
      "[6/10][11/100][1271] Loss_D: 0.001843 Loss_G: 0.000923 \n",
      "[6/10][11/100][1272] Loss_D: 0.001721 Loss_G: 0.002785 \n",
      "[6/10][11/100][1273] Loss_D: 0.002824 Loss_G: 0.002117 \n",
      "[6/10][11/100][1274] Loss_D: 0.003186 Loss_G: 0.002502 \n",
      "[6/10][11/100][1275] Loss_D: 0.003095 Loss_G: 0.003946 \n",
      "[6/10][11/100][1276] Loss_D: 0.003337 Loss_G: 0.003924 \n",
      "[6/10][11/100][1277] Loss_D: 0.003894 Loss_G: 0.001801 \n",
      "[6/10][11/100][1278] Loss_D: 0.001796 Loss_G: 0.001049 \n",
      "[6/10][11/100][1279] Loss_D: 0.003391 Loss_G: 0.003798 \n",
      "[6/10][11/100][1280] Loss_D: 0.001916 Loss_G: 0.001668 \n",
      "[6/10][11/100][1281] Loss_D: 0.002684 Loss_G: 0.002672 \n",
      "[6/10][11/100][1282] Loss_D: 0.003438 Loss_G: 0.001591 \n",
      "[6/10][11/100][1283] Loss_D: 0.003687 Loss_G: 0.003278 \n",
      "[6/10][11/100][1284] Loss_D: 0.002539 Loss_G: 0.001636 \n",
      "[6/10][11/100][1285] Loss_D: 0.002182 Loss_G: 0.002125 \n",
      "[6/10][11/100][1286] Loss_D: 0.002584 Loss_G: 0.002855 \n",
      "[6/10][11/100][1287] Loss_D: 0.001568 Loss_G: 0.001354 \n",
      "[6/10][11/100][1288] Loss_D: 0.001581 Loss_G: 0.002791 \n",
      "[6/10][11/100][1289] Loss_D: 0.001365 Loss_G: 0.002756 \n",
      "[6/10][11/100][1290] Loss_D: 0.001791 Loss_G: 0.003892 \n",
      "[6/10][11/100][1291] Loss_D: 0.005248 Loss_G: 0.003718 \n",
      "[6/10][11/100][1292] Loss_D: 0.002369 Loss_G: 0.004513 \n",
      "[6/10][11/100][1293] Loss_D: 0.003020 Loss_G: 0.003877 \n",
      "[6/10][11/100][1294] Loss_D: 0.002944 Loss_G: 0.002723 \n",
      "[6/10][11/100][1295] Loss_D: 0.002873 Loss_G: 0.002956 \n",
      "[6/10][11/100][1296] Loss_D: 0.003503 Loss_G: 0.005041 \n",
      "[6/10][11/100][1297] Loss_D: 0.002623 Loss_G: 0.004799 \n",
      "[6/10][11/100][1298] Loss_D: 0.002562 Loss_G: 0.003040 \n",
      "[6/10][11/100][1299] Loss_D: 0.002364 Loss_G: 0.004811 \n",
      "[6/10][11/100][1300] Loss_D: 0.003733 Loss_G: 0.004742 \n",
      "[6/10][11/100][1301] Loss_D: 0.001951 Loss_G: 0.003133 \n",
      "[6/10][11/100][1302] Loss_D: 0.004244 Loss_G: 0.003060 \n",
      "[6/10][11/100][1303] Loss_D: 0.004736 Loss_G: 0.003600 \n",
      "[6/10][11/100][1304] Loss_D: 0.004236 Loss_G: 0.003601 \n",
      "[6/10][11/100][1305] Loss_D: 0.003710 Loss_G: 0.004307 \n",
      "[6/10][11/100][1306] Loss_D: 0.005214 Loss_G: 0.003399 \n",
      "[6/10][11/100][1307] Loss_D: 0.002860 Loss_G: 0.005232 \n",
      "[6/10][11/100][1308] Loss_D: 0.003477 Loss_G: 0.005685 \n",
      "[6/10][11/100][1309] Loss_D: 0.004411 Loss_G: 0.003858 \n",
      "[6/10][11/100][1310] Loss_D: 0.004158 Loss_G: 0.005334 \n",
      "[6/10][11/100][1311] Loss_D: 0.004660 Loss_G: 0.005813 \n",
      "[6/10][11/100][1312] Loss_D: 0.003951 Loss_G: 0.004544 \n",
      "[6/10][11/100][1313] Loss_D: 0.003729 Loss_G: 0.004507 \n",
      "[6/10][11/100][1314] Loss_D: 0.003886 Loss_G: 0.003933 \n",
      "[6/10][11/100][1315] Loss_D: 0.004136 Loss_G: 0.003175 \n",
      "[6/10][11/100][1316] Loss_D: 0.003076 Loss_G: 0.004541 \n",
      "[6/10][11/100][1317] Loss_D: 0.000911 Loss_G: 0.002546 \n",
      "[6/10][11/100][1318] Loss_D: 0.002193 Loss_G: 0.002011 \n",
      "[6/10][11/100][1319] Loss_D: 0.003571 Loss_G: 0.003824 \n",
      "[6/10][11/100][1320] Loss_D: 0.003517 Loss_G: 0.001731 \n",
      "[6/10][11/100][1321] Loss_D: 0.002936 Loss_G: 0.002968 \n",
      "[6/10][11/100][1322] Loss_D: 0.001936 Loss_G: 0.002793 \n",
      "[6/10][11/100][1323] Loss_D: 0.003789 Loss_G: 0.002294 \n",
      "[6/10][11/100][1324] Loss_D: 0.003196 Loss_G: 0.002970 \n",
      "[6/10][11/100][1325] Loss_D: 0.005199 Loss_G: 0.003414 \n",
      "[6/10][11/100][1326] Loss_D: 0.002357 Loss_G: 0.002878 \n",
      "[6/10][11/100][1327] Loss_D: 0.004773 Loss_G: 0.002251 \n",
      "[6/10][11/100][1328] Loss_D: 0.003126 Loss_G: 0.002503 \n",
      "[6/10][11/100][1329] Loss_D: 0.001825 Loss_G: 0.004247 \n",
      "[6/10][11/100][1330] Loss_D: 0.003191 Loss_G: 0.003583 \n",
      "[6/10][11/100][1331] Loss_D: 0.002455 Loss_G: 0.001455 \n",
      "[6/10][11/100][1332] Loss_D: 0.003856 Loss_G: 0.001189 \n",
      "[6/10][11/100][1333] Loss_D: 0.003191 Loss_G: 0.004552 \n",
      "[6/10][11/100][1334] Loss_D: 0.003264 Loss_G: 0.003222 \n",
      "[6/10][11/100][1335] Loss_D: 0.002738 Loss_G: 0.003555 \n",
      "[6/10][11/100][1336] Loss_D: 0.004085 Loss_G: 0.004873 \n",
      "[6/10][11/100][1337] Loss_D: 0.002383 Loss_G: 0.002808 \n",
      "[6/10][11/100][1338] Loss_D: 0.002495 Loss_G: 0.002862 \n",
      "[6/10][11/100][1339] Loss_D: 0.002502 Loss_G: 0.005291 \n",
      "[6/10][11/100][1340] Loss_D: 0.002648 Loss_G: 0.003546 \n",
      "[6/10][11/100][1341] Loss_D: 0.002566 Loss_G: 0.002684 \n",
      "[6/10][11/100][1342] Loss_D: 0.003060 Loss_G: 0.005422 \n",
      "[6/10][11/100][1343] Loss_D: 0.004132 Loss_G: 0.004748 \n",
      "[6/10][11/100][1344] Loss_D: 0.002813 Loss_G: 0.003467 \n",
      "[6/10][11/100][1345] Loss_D: 0.004978 Loss_G: 0.002789 \n",
      "[6/10][11/100][1346] Loss_D: 0.005085 Loss_G: 0.003749 \n",
      "[6/10][11/100][1347] Loss_D: 0.003381 Loss_G: 0.003999 \n",
      "[6/10][11/100][1348] Loss_D: 0.003388 Loss_G: 0.004857 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][11/100][1349] Loss_D: 0.003922 Loss_G: 0.003786 \n",
      "[6/10][11/100][1350] Loss_D: 0.003868 Loss_G: 0.004049 \n",
      "[6/10][11/100][1351] Loss_D: 0.004879 Loss_G: 0.003693 \n",
      "[6/10][11/100][1352] Loss_D: 0.004660 Loss_G: 0.003464 \n",
      "[6/10][11/100][1353] Loss_D: 0.003672 Loss_G: 0.004905 \n",
      "[6/10][11/100][1354] Loss_D: 0.003672 Loss_G: 0.003340 \n",
      "[6/10][11/100][1355] Loss_D: 0.003219 Loss_G: 0.002471 \n",
      "[6/10][11/100][1356] Loss_D: 0.003655 Loss_G: 0.004033 \n",
      "[6/10][11/100][1357] Loss_D: 0.003052 Loss_G: 0.005459 \n",
      "[6/10][11/100][1358] Loss_D: 0.002479 Loss_G: 0.003111 \n",
      "[6/10][11/100][1359] Loss_D: 0.003761 Loss_G: 0.003147 \n",
      "[6/10][11/100][1360] Loss_D: 0.003038 Loss_G: 0.003951 \n",
      "[6/10][11/100][1361] Loss_D: 0.002816 Loss_G: 0.003054 \n",
      "[6/10][11/100][1362] Loss_D: 0.002589 Loss_G: 0.003302 \n",
      "[6/10][11/100][1363] Loss_D: 0.002491 Loss_G: 0.001809 \n",
      "[6/10][11/100][1364] Loss_D: 0.001251 Loss_G: 0.001947 \n",
      "[6/10][11/100][1365] Loss_D: 0.003091 Loss_G: 0.004444 \n",
      "[6/10][11/100][1366] Loss_D: 0.002591 Loss_G: 0.002843 \n",
      "[6/10][11/100][1367] Loss_D: 0.002160 Loss_G: 0.003593 \n",
      "[6/10][11/100][1368] Loss_D: 0.001834 Loss_G: 0.003096 \n",
      "[6/10][11/100][1369] Loss_D: 0.002113 Loss_G: 0.003453 \n",
      "[6/10][11/100][1370] Loss_D: 0.003315 Loss_G: 0.002310 \n",
      "[6/10][11/100][1371] Loss_D: 0.002814 Loss_G: 0.003139 \n",
      "[6/10][11/100][1372] Loss_D: 0.005053 Loss_G: 0.001923 \n",
      "[6/10][11/100][1373] Loss_D: 0.002730 Loss_G: 0.002709 \n",
      "[6/10][11/100][1374] Loss_D: 0.003168 Loss_G: 0.002850 \n",
      "[6/10][11/100][1375] Loss_D: 0.002936 Loss_G: 0.003131 \n",
      "[6/10][11/100][1376] Loss_D: 0.002751 Loss_G: 0.003096 \n",
      "[6/10][11/100][1377] Loss_D: 0.002415 Loss_G: 0.003694 \n",
      "[6/10][11/100][1378] Loss_D: 0.003088 Loss_G: 0.002984 \n",
      "[6/10][11/100][1379] Loss_D: 0.001692 Loss_G: 0.001754 \n",
      "[6/10][11/100][1380] Loss_D: 0.003873 Loss_G: 0.002145 \n",
      "[6/10][11/100][1381] Loss_D: 0.005274 Loss_G: 0.003779 \n",
      "[6/10][11/100][1382] Loss_D: 0.003091 Loss_G: 0.002882 \n",
      "[6/10][11/100][1383] Loss_D: 0.002855 Loss_G: 0.002484 \n",
      "[6/10][11/100][1384] Loss_D: 0.002210 Loss_G: 0.003466 \n",
      "[6/10][11/100][1385] Loss_D: 0.002456 Loss_G: 0.003415 \n",
      "[6/10][11/100][1386] Loss_D: 0.002720 Loss_G: 0.002775 \n",
      "[6/10][11/100][1387] Loss_D: 0.002923 Loss_G: 0.004417 \n",
      "[6/10][11/100][1388] Loss_D: 0.003279 Loss_G: 0.004654 \n",
      "[6/10][11/100][1389] Loss_D: 0.002709 Loss_G: 0.001912 \n",
      "[6/10][11/100][1390] Loss_D: 0.002757 Loss_G: 0.002683 \n",
      "[6/10][11/100][1391] Loss_D: 0.002270 Loss_G: 0.002912 \n",
      "[6/10][11/100][1392] Loss_D: 0.002657 Loss_G: 0.001957 \n",
      "[6/10][11/100][1393] Loss_D: 0.004880 Loss_G: 0.003905 \n",
      "[6/10][11/100][1394] Loss_D: 0.003041 Loss_G: 0.002819 \n",
      "[6/10][11/100][1395] Loss_D: 0.002913 Loss_G: 0.003445 \n",
      "[6/10][11/100][1396] Loss_D: 0.003607 Loss_G: 0.002421 \n",
      "[6/10][11/100][1397] Loss_D: 0.003946 Loss_G: 0.003141 \n",
      "[6/10][11/100][1398] Loss_D: 0.003210 Loss_G: 0.003614 \n",
      "[6/10][11/100][1399] Loss_D: 0.002709 Loss_G: 0.003531 \n",
      "[6/10][11/100][1400] Loss_D: 0.003792 Loss_G: 0.003386 \n",
      "[7/10][11/100][1401] Loss_D: 0.003794 Loss_G: 0.004037 \n",
      "[7/10][11/100][1402] Loss_D: 0.004186 Loss_G: 0.002986 \n",
      "[7/10][11/100][1403] Loss_D: 0.003513 Loss_G: 0.003719 \n",
      "[7/10][11/100][1404] Loss_D: 0.003989 Loss_G: 0.003670 \n",
      "[7/10][11/100][1405] Loss_D: 0.002798 Loss_G: 0.005364 \n",
      "[7/10][11/100][1406] Loss_D: 0.002567 Loss_G: 0.003169 \n",
      "[7/10][11/100][1407] Loss_D: 0.003720 Loss_G: 0.002804 \n",
      "[7/10][11/100][1408] Loss_D: 0.003132 Loss_G: 0.002957 \n",
      "[7/10][11/100][1409] Loss_D: 0.002579 Loss_G: 0.004146 \n",
      "[7/10][11/100][1410] Loss_D: 0.004017 Loss_G: 0.004480 \n",
      "[7/10][11/100][1411] Loss_D: 0.002556 Loss_G: 0.003084 \n",
      "[7/10][11/100][1412] Loss_D: 0.002735 Loss_G: 0.002421 \n",
      "[7/10][11/100][1413] Loss_D: 0.005403 Loss_G: 0.001499 \n",
      "[7/10][11/100][1414] Loss_D: 0.001495 Loss_G: 0.001929 \n",
      "[7/10][11/100][1415] Loss_D: 0.003083 Loss_G: 0.002437 \n",
      "[7/10][11/100][1416] Loss_D: 0.002754 Loss_G: 0.004205 \n",
      "[7/10][11/100][1417] Loss_D: 0.003196 Loss_G: 0.003867 \n",
      "[7/10][11/100][1418] Loss_D: 0.002578 Loss_G: 0.002536 \n",
      "[7/10][11/100][1419] Loss_D: 0.003932 Loss_G: 0.001500 \n",
      "[7/10][11/100][1420] Loss_D: 0.003233 Loss_G: 0.002629 \n",
      "[7/10][11/100][1421] Loss_D: 0.002851 Loss_G: 0.002752 \n",
      "[7/10][11/100][1422] Loss_D: 0.002947 Loss_G: 0.002518 \n",
      "[7/10][11/100][1423] Loss_D: 0.002302 Loss_G: 0.003970 \n",
      "[7/10][11/100][1424] Loss_D: 0.002987 Loss_G: 0.002623 \n",
      "[7/10][11/100][1425] Loss_D: 0.002929 Loss_G: 0.003030 \n",
      "[7/10][11/100][1426] Loss_D: 0.002812 Loss_G: 0.003564 \n",
      "[7/10][11/100][1427] Loss_D: 0.002301 Loss_G: 0.002949 \n",
      "[7/10][11/100][1428] Loss_D: 0.002459 Loss_G: 0.001396 \n",
      "[7/10][11/100][1429] Loss_D: 0.003449 Loss_G: 0.001394 \n",
      "[7/10][11/100][1430] Loss_D: 0.003417 Loss_G: 0.003856 \n",
      "[7/10][11/100][1431] Loss_D: 0.002778 Loss_G: 0.002228 \n",
      "[7/10][11/100][1432] Loss_D: 0.002816 Loss_G: 0.004326 \n",
      "[7/10][11/100][1433] Loss_D: 0.003156 Loss_G: 0.003270 \n",
      "[7/10][11/100][1434] Loss_D: 0.004225 Loss_G: 0.001962 \n",
      "[7/10][11/100][1435] Loss_D: 0.002941 Loss_G: 0.001883 \n",
      "[7/10][11/100][1436] Loss_D: 0.002340 Loss_G: 0.004156 \n",
      "[7/10][11/100][1437] Loss_D: 0.002903 Loss_G: 0.003137 \n",
      "[7/10][11/100][1438] Loss_D: 0.002941 Loss_G: 0.003966 \n",
      "[7/10][11/100][1439] Loss_D: 0.004503 Loss_G: 0.004508 \n",
      "[7/10][11/100][1440] Loss_D: 0.003418 Loss_G: 0.004313 \n",
      "[7/10][11/100][1441] Loss_D: 0.003098 Loss_G: 0.003514 \n",
      "[7/10][11/100][1442] Loss_D: 0.003352 Loss_G: 0.003164 \n",
      "[7/10][11/100][1443] Loss_D: 0.003227 Loss_G: 0.003782 \n",
      "[7/10][11/100][1444] Loss_D: 0.003156 Loss_G: 0.003737 \n",
      "[7/10][11/100][1445] Loss_D: 0.002176 Loss_G: 0.002550 \n",
      "[7/10][11/100][1446] Loss_D: 0.003788 Loss_G: 0.004680 \n",
      "[7/10][11/100][1447] Loss_D: 0.002589 Loss_G: 0.002849 \n",
      "[7/10][11/100][1448] Loss_D: 0.002576 Loss_G: 0.002873 \n",
      "[7/10][11/100][1449] Loss_D: 0.004206 Loss_G: 0.002398 \n",
      "[7/10][11/100][1450] Loss_D: 0.002759 Loss_G: 0.003339 \n",
      "[7/10][11/100][1451] Loss_D: 0.002201 Loss_G: 0.003071 \n",
      "[7/10][11/100][1452] Loss_D: 0.005401 Loss_G: 0.003798 \n",
      "[7/10][11/100][1453] Loss_D: 0.002612 Loss_G: 0.002289 \n",
      "[7/10][11/100][1454] Loss_D: 0.005664 Loss_G: 0.001719 \n",
      "[7/10][11/100][1455] Loss_D: 0.002528 Loss_G: 0.003393 \n",
      "[7/10][11/100][1456] Loss_D: 0.002182 Loss_G: 0.001664 \n",
      "[7/10][11/100][1457] Loss_D: 0.002652 Loss_G: 0.003440 \n",
      "[7/10][11/100][1458] Loss_D: 0.003026 Loss_G: 0.003400 \n",
      "[7/10][11/100][1459] Loss_D: 0.001879 Loss_G: 0.001827 \n",
      "[7/10][11/100][1460] Loss_D: 0.002754 Loss_G: 0.002870 \n",
      "[7/10][11/100][1461] Loss_D: 0.003982 Loss_G: 0.003130 \n",
      "[7/10][11/100][1462] Loss_D: 0.003788 Loss_G: 0.003104 \n",
      "[7/10][11/100][1463] Loss_D: 0.004443 Loss_G: 0.003169 \n",
      "[7/10][11/100][1464] Loss_D: 0.002598 Loss_G: 0.004089 \n",
      "[7/10][11/100][1465] Loss_D: 0.003598 Loss_G: 0.003886 \n",
      "[7/10][11/100][1466] Loss_D: 0.003660 Loss_G: 0.002933 \n",
      "[7/10][11/100][1467] Loss_D: 0.003000 Loss_G: 0.002126 \n",
      "[7/10][11/100][1468] Loss_D: 0.003221 Loss_G: 0.002280 \n",
      "[7/10][11/100][1469] Loss_D: 0.002987 Loss_G: 0.003126 \n",
      "[7/10][11/100][1470] Loss_D: 0.003654 Loss_G: 0.001235 \n",
      "[7/10][11/100][1471] Loss_D: 0.001663 Loss_G: 0.002356 \n",
      "[7/10][11/100][1472] Loss_D: 0.003452 Loss_G: 0.003970 \n",
      "[7/10][11/100][1473] Loss_D: 0.002431 Loss_G: 0.003503 \n",
      "[7/10][11/100][1474] Loss_D: 0.003335 Loss_G: 0.001894 \n",
      "[7/10][11/100][1475] Loss_D: 0.002827 Loss_G: 0.004447 \n",
      "[7/10][11/100][1476] Loss_D: 0.003324 Loss_G: 0.003429 \n",
      "[7/10][11/100][1477] Loss_D: 0.003363 Loss_G: 0.003452 \n",
      "[7/10][11/100][1478] Loss_D: 0.004069 Loss_G: 0.003389 \n",
      "[7/10][11/100][1479] Loss_D: 0.003853 Loss_G: 0.002336 \n",
      "[7/10][11/100][1480] Loss_D: 0.004911 Loss_G: 0.001987 \n",
      "[7/10][11/100][1481] Loss_D: 0.003582 Loss_G: 0.002824 \n",
      "[7/10][11/100][1482] Loss_D: 0.003488 Loss_G: 0.003768 \n",
      "[7/10][11/100][1483] Loss_D: 0.002559 Loss_G: 0.002913 \n",
      "[7/10][11/100][1484] Loss_D: 0.004884 Loss_G: 0.002940 \n",
      "[7/10][11/100][1485] Loss_D: 0.003109 Loss_G: 0.002545 \n",
      "[7/10][11/100][1486] Loss_D: 0.003359 Loss_G: 0.003206 \n",
      "[7/10][11/100][1487] Loss_D: 0.003698 Loss_G: 0.004315 \n",
      "[7/10][11/100][1488] Loss_D: 0.002565 Loss_G: 0.002924 \n",
      "[7/10][11/100][1489] Loss_D: 0.003811 Loss_G: 0.003124 \n",
      "[7/10][11/100][1490] Loss_D: 0.003724 Loss_G: 0.001775 \n",
      "[7/10][11/100][1491] Loss_D: 0.002034 Loss_G: 0.003231 \n",
      "[7/10][11/100][1492] Loss_D: 0.002144 Loss_G: 0.001621 \n",
      "[7/10][11/100][1493] Loss_D: 0.001809 Loss_G: 0.002176 \n",
      "[7/10][11/100][1494] Loss_D: 0.002457 Loss_G: 0.002624 \n",
      "[7/10][11/100][1495] Loss_D: 0.004114 Loss_G: 0.002751 \n",
      "[7/10][11/100][1496] Loss_D: 0.002339 Loss_G: 0.004094 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10][11/100][1497] Loss_D: 0.002022 Loss_G: 0.003133 \n",
      "[7/10][11/100][1498] Loss_D: 0.003548 Loss_G: 0.002295 \n",
      "[7/10][11/100][1499] Loss_D: 0.001598 Loss_G: 0.003488 \n",
      "[7/10][11/100][1500] Loss_D: 0.002967 Loss_G: 0.001534 \n",
      "[7/10][11/100][1501] Loss_D: 0.003208 Loss_G: 0.001129 \n",
      "[7/10][11/100][1502] Loss_D: 0.002113 Loss_G: 0.003087 \n",
      "[7/10][11/100][1503] Loss_D: 0.002268 Loss_G: 0.003081 \n",
      "[7/10][11/100][1504] Loss_D: 0.003118 Loss_G: 0.002821 \n",
      "[7/10][11/100][1505] Loss_D: 0.001423 Loss_G: 0.002226 \n",
      "[7/10][11/100][1506] Loss_D: 0.001893 Loss_G: 0.002984 \n",
      "[7/10][11/100][1507] Loss_D: 0.002830 Loss_G: 0.002197 \n",
      "[7/10][11/100][1508] Loss_D: 0.002405 Loss_G: 0.002671 \n",
      "[7/10][11/100][1509] Loss_D: 0.002610 Loss_G: 0.002697 \n",
      "[7/10][11/100][1510] Loss_D: 0.002525 Loss_G: 0.002674 \n",
      "[7/10][11/100][1511] Loss_D: 0.002810 Loss_G: 0.003301 \n",
      "[7/10][11/100][1512] Loss_D: 0.002630 Loss_G: 0.002647 \n",
      "[7/10][11/100][1513] Loss_D: 0.001927 Loss_G: 0.003266 \n",
      "[7/10][11/100][1514] Loss_D: 0.002436 Loss_G: 0.003876 \n",
      "[7/10][11/100][1515] Loss_D: 0.002209 Loss_G: 0.001934 \n",
      "[7/10][11/100][1516] Loss_D: 0.003075 Loss_G: 0.003364 \n",
      "[7/10][11/100][1517] Loss_D: 0.002225 Loss_G: 0.002056 \n",
      "[7/10][11/100][1518] Loss_D: 0.002874 Loss_G: 0.001632 \n",
      "[7/10][11/100][1519] Loss_D: 0.003013 Loss_G: 0.001801 \n",
      "[7/10][11/100][1520] Loss_D: 0.002820 Loss_G: 0.003164 \n",
      "[7/10][11/100][1521] Loss_D: 0.003523 Loss_G: 0.003247 \n",
      "[7/10][11/100][1522] Loss_D: 0.002296 Loss_G: 0.001359 \n",
      "[7/10][11/100][1523] Loss_D: 0.001656 Loss_G: 0.002229 \n",
      "[7/10][11/100][1524] Loss_D: 0.002213 Loss_G: 0.003592 \n",
      "[7/10][11/100][1525] Loss_D: 0.001522 Loss_G: 0.002539 \n",
      "[7/10][11/100][1526] Loss_D: 0.003339 Loss_G: 0.001650 \n",
      "[7/10][11/100][1527] Loss_D: 0.003344 Loss_G: 0.003291 \n",
      "[7/10][11/100][1528] Loss_D: 0.002883 Loss_G: 0.002166 \n",
      "[7/10][11/100][1529] Loss_D: 0.003104 Loss_G: 0.001036 \n",
      "[7/10][11/100][1530] Loss_D: 0.002520 Loss_G: 0.002839 \n",
      "[7/10][11/100][1531] Loss_D: 0.003234 Loss_G: 0.001442 \n",
      "[7/10][11/100][1532] Loss_D: 0.001101 Loss_G: 0.003134 \n",
      "[7/10][11/100][1533] Loss_D: 0.000936 Loss_G: 0.003000 \n",
      "[7/10][11/100][1534] Loss_D: 0.002728 Loss_G: 0.002143 \n",
      "[7/10][11/100][1535] Loss_D: 0.002373 Loss_G: 0.002778 \n",
      "[7/10][11/100][1536] Loss_D: 0.002447 Loss_G: 0.000779 \n",
      "[7/10][11/100][1537] Loss_D: 0.002981 Loss_G: 0.001652 \n",
      "[7/10][11/100][1538] Loss_D: 0.002172 Loss_G: 0.001766 \n",
      "[7/10][11/100][1539] Loss_D: 0.002642 Loss_G: 0.000920 \n",
      "[7/10][11/100][1540] Loss_D: 0.003380 Loss_G: 0.003018 \n",
      "[7/10][11/100][1541] Loss_D: 0.003890 Loss_G: 0.002973 \n",
      "[7/10][11/100][1542] Loss_D: 0.004672 Loss_G: 0.003612 \n",
      "[7/10][11/100][1543] Loss_D: 0.002029 Loss_G: 0.002181 \n",
      "[7/10][11/100][1544] Loss_D: 0.003164 Loss_G: 0.002909 \n",
      "[7/10][11/100][1545] Loss_D: 0.000943 Loss_G: 0.001696 \n",
      "[7/10][11/100][1546] Loss_D: 0.001940 Loss_G: 0.001411 \n",
      "[7/10][11/100][1547] Loss_D: 0.001781 Loss_G: 0.001498 \n",
      "[7/10][11/100][1548] Loss_D: 0.001176 Loss_G: 0.002856 \n",
      "[7/10][11/100][1549] Loss_D: 0.002107 Loss_G: 0.001652 \n",
      "[7/10][11/100][1550] Loss_D: 0.001539 Loss_G: 0.002671 \n",
      "[7/10][11/100][1551] Loss_D: 0.002684 Loss_G: 0.002734 \n",
      "[7/10][11/100][1552] Loss_D: 0.001408 Loss_G: 0.002475 \n",
      "[7/10][11/100][1553] Loss_D: 0.002175 Loss_G: 0.001140 \n",
      "[7/10][11/100][1554] Loss_D: -0.000010 Loss_G: 0.003066 \n",
      "[7/10][11/100][1555] Loss_D: 0.002489 Loss_G: 0.002665 \n",
      "[7/10][11/100][1556] Loss_D: 0.002306 Loss_G: 0.002864 \n",
      "[7/10][11/100][1557] Loss_D: 0.003709 Loss_G: 0.003478 \n",
      "[7/10][11/100][1558] Loss_D: 0.004133 Loss_G: 0.003255 \n",
      "[7/10][11/100][1559] Loss_D: 0.001170 Loss_G: 0.003338 \n",
      "[7/10][11/100][1560] Loss_D: 0.004707 Loss_G: 0.002710 \n",
      "[7/10][11/100][1561] Loss_D: 0.003330 Loss_G: 0.002606 \n",
      "[7/10][11/100][1562] Loss_D: 0.003074 Loss_G: 0.002038 \n",
      "[7/10][11/100][1563] Loss_D: 0.003357 Loss_G: 0.004014 \n",
      "[7/10][11/100][1564] Loss_D: 0.003658 Loss_G: 0.003738 \n",
      "[7/10][11/100][1565] Loss_D: 0.004213 Loss_G: 0.003631 \n",
      "[7/10][11/100][1566] Loss_D: 0.004349 Loss_G: 0.002497 \n",
      "[7/10][11/100][1567] Loss_D: 0.003064 Loss_G: 0.003269 \n",
      "[7/10][11/100][1568] Loss_D: 0.003628 Loss_G: 0.003023 \n",
      "[7/10][11/100][1569] Loss_D: 0.002780 Loss_G: 0.002559 \n",
      "[7/10][11/100][1570] Loss_D: 0.003804 Loss_G: 0.003641 \n",
      "[7/10][11/100][1571] Loss_D: 0.001149 Loss_G: 0.002047 \n",
      "[7/10][11/100][1572] Loss_D: 0.001689 Loss_G: 0.002230 \n",
      "[7/10][11/100][1573] Loss_D: 0.001928 Loss_G: 0.001949 \n",
      "[7/10][11/100][1574] Loss_D: 0.002587 Loss_G: 0.001790 \n",
      "[7/10][11/100][1575] Loss_D: 0.001778 Loss_G: 0.002105 \n",
      "[7/10][11/100][1576] Loss_D: 0.002592 Loss_G: 0.001701 \n",
      "[7/10][11/100][1577] Loss_D: 0.003993 Loss_G: 0.006111 \n",
      "[7/10][11/100][1578] Loss_D: 0.005873 Loss_G: 0.003784 \n",
      "[7/10][11/100][1579] Loss_D: 0.005538 Loss_G: 0.005604 \n",
      "[7/10][11/100][1580] Loss_D: 0.006462 Loss_G: 0.005500 \n",
      "[7/10][11/100][1581] Loss_D: 0.004543 Loss_G: 0.002828 \n",
      "[7/10][11/100][1582] Loss_D: 0.003861 Loss_G: 0.005173 \n",
      "[7/10][11/100][1583] Loss_D: 0.004386 Loss_G: 0.004514 \n",
      "[7/10][11/100][1584] Loss_D: 0.003411 Loss_G: 0.003542 \n",
      "[7/10][11/100][1585] Loss_D: 0.002426 Loss_G: 0.003353 \n",
      "[7/10][11/100][1586] Loss_D: 0.001115 Loss_G: 0.002669 \n",
      "[7/10][11/100][1587] Loss_D: 0.002355 Loss_G: 0.003313 \n",
      "[7/10][11/100][1588] Loss_D: 0.002147 Loss_G: 0.002249 \n",
      "[7/10][11/100][1589] Loss_D: 0.002527 Loss_G: 0.001353 \n",
      "[7/10][11/100][1590] Loss_D: 0.001489 Loss_G: 0.005603 \n",
      "[7/10][11/100][1591] Loss_D: 0.001598 Loss_G: 0.002239 \n",
      "[7/10][11/100][1592] Loss_D: 0.002183 Loss_G: 0.002337 \n",
      "[7/10][11/100][1593] Loss_D: 0.000653 Loss_G: 0.001773 \n",
      "[7/10][11/100][1594] Loss_D: 0.002062 Loss_G: 0.001713 \n",
      "[7/10][11/100][1595] Loss_D: 0.003520 Loss_G: 0.002406 \n",
      "[7/10][11/100][1596] Loss_D: 0.002904 Loss_G: 0.002417 \n",
      "[7/10][11/100][1597] Loss_D: 0.003269 Loss_G: 0.002296 \n",
      "[7/10][11/100][1598] Loss_D: 0.004022 Loss_G: 0.003804 \n",
      "[7/10][11/100][1599] Loss_D: 0.003377 Loss_G: 0.004562 \n",
      "[7/10][11/100][1600] Loss_D: 0.001248 Loss_G: 0.002436 \n",
      "[8/10][11/100][1601] Loss_D: 0.001750 Loss_G: 0.002243 \n",
      "[8/10][11/100][1602] Loss_D: 0.004832 Loss_G: 0.003470 \n",
      "[8/10][11/100][1603] Loss_D: 0.003059 Loss_G: 0.003555 \n",
      "[8/10][11/100][1604] Loss_D: 0.002540 Loss_G: 0.002530 \n",
      "[8/10][11/100][1605] Loss_D: 0.004306 Loss_G: 0.001866 \n",
      "[8/10][11/100][1606] Loss_D: 0.002799 Loss_G: 0.002630 \n",
      "[8/10][11/100][1607] Loss_D: 0.003790 Loss_G: 0.002395 \n",
      "[8/10][11/100][1608] Loss_D: 0.005043 Loss_G: 0.003240 \n",
      "[8/10][11/100][1609] Loss_D: 0.005753 Loss_G: 0.003878 \n",
      "[8/10][11/100][1610] Loss_D: 0.003722 Loss_G: 0.003927 \n",
      "[8/10][11/100][1611] Loss_D: 0.005110 Loss_G: 0.003796 \n",
      "[8/10][11/100][1612] Loss_D: 0.002560 Loss_G: 0.002517 \n",
      "[8/10][11/100][1613] Loss_D: 0.003000 Loss_G: 0.003818 \n",
      "[8/10][11/100][1614] Loss_D: 0.001977 Loss_G: 0.002845 \n",
      "[8/10][11/100][1615] Loss_D: 0.003825 Loss_G: 0.002854 \n",
      "[8/10][11/100][1616] Loss_D: 0.001828 Loss_G: 0.001972 \n",
      "[8/10][11/100][1617] Loss_D: 0.002728 Loss_G: 0.003185 \n",
      "[8/10][11/100][1618] Loss_D: 0.002737 Loss_G: 0.002453 \n",
      "[8/10][11/100][1619] Loss_D: 0.001961 Loss_G: 0.003135 \n",
      "[8/10][11/100][1620] Loss_D: 0.001871 Loss_G: 0.001171 \n",
      "[8/10][11/100][1621] Loss_D: 0.003168 Loss_G: 0.002352 \n",
      "[8/10][11/100][1622] Loss_D: 0.001587 Loss_G: 0.002870 \n",
      "[8/10][11/100][1623] Loss_D: 0.002970 Loss_G: 0.004085 \n",
      "[8/10][11/100][1624] Loss_D: 0.002934 Loss_G: 0.002966 \n",
      "[8/10][11/100][1625] Loss_D: 0.003808 Loss_G: 0.003740 \n",
      "[8/10][11/100][1626] Loss_D: 0.003336 Loss_G: 0.002379 \n",
      "[8/10][11/100][1627] Loss_D: 0.004002 Loss_G: 0.004919 \n",
      "[8/10][11/100][1628] Loss_D: 0.004460 Loss_G: 0.003013 \n",
      "[8/10][11/100][1629] Loss_D: 0.003745 Loss_G: 0.005146 \n",
      "[8/10][11/100][1630] Loss_D: 0.004926 Loss_G: 0.003899 \n",
      "[8/10][11/100][1631] Loss_D: 0.004400 Loss_G: 0.005428 \n",
      "[8/10][11/100][1632] Loss_D: 0.003033 Loss_G: 0.003793 \n",
      "[8/10][11/100][1633] Loss_D: 0.003258 Loss_G: 0.002107 \n",
      "[8/10][11/100][1634] Loss_D: 0.001253 Loss_G: 0.004322 \n",
      "[8/10][11/100][1635] Loss_D: 0.002627 Loss_G: 0.004237 \n",
      "[8/10][11/100][1636] Loss_D: 0.002852 Loss_G: 0.002609 \n",
      "[8/10][11/100][1637] Loss_D: 0.002623 Loss_G: 0.004448 \n",
      "[8/10][11/100][1638] Loss_D: 0.002321 Loss_G: 0.002358 \n",
      "[8/10][11/100][1639] Loss_D: 0.002666 Loss_G: 0.003859 \n",
      "[8/10][11/100][1640] Loss_D: 0.003176 Loss_G: 0.002641 \n",
      "[8/10][11/100][1641] Loss_D: 0.002480 Loss_G: 0.001617 \n",
      "[8/10][11/100][1642] Loss_D: 0.001537 Loss_G: 0.002513 \n",
      "[8/10][11/100][1643] Loss_D: 0.003877 Loss_G: 0.002045 \n",
      "[8/10][11/100][1644] Loss_D: 0.001752 Loss_G: 0.001850 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][11/100][1645] Loss_D: 0.001902 Loss_G: 0.001817 \n",
      "[8/10][11/100][1646] Loss_D: 0.002258 Loss_G: 0.002042 \n",
      "[8/10][11/100][1647] Loss_D: 0.002945 Loss_G: 0.002262 \n",
      "[8/10][11/100][1648] Loss_D: 0.002560 Loss_G: 0.001017 \n",
      "[8/10][11/100][1649] Loss_D: 0.003062 Loss_G: 0.002428 \n",
      "[8/10][11/100][1650] Loss_D: 0.002572 Loss_G: 0.002142 \n",
      "[8/10][11/100][1651] Loss_D: 0.002481 Loss_G: 0.002252 \n",
      "[8/10][11/100][1652] Loss_D: 0.000510 Loss_G: 0.003076 \n",
      "[8/10][11/100][1653] Loss_D: 0.003790 Loss_G: 0.002527 \n",
      "[8/10][11/100][1654] Loss_D: 0.002426 Loss_G: 0.001838 \n",
      "[8/10][11/100][1655] Loss_D: 0.001685 Loss_G: 0.001998 \n",
      "[8/10][11/100][1656] Loss_D: 0.002175 Loss_G: 0.002138 \n",
      "[8/10][11/100][1657] Loss_D: 0.001872 Loss_G: 0.001328 \n",
      "[8/10][11/100][1658] Loss_D: 0.003037 Loss_G: 0.002413 \n",
      "[8/10][11/100][1659] Loss_D: 0.002018 Loss_G: 0.002746 \n",
      "[8/10][11/100][1660] Loss_D: 0.002303 Loss_G: 0.004054 \n",
      "[8/10][11/100][1661] Loss_D: 0.002276 Loss_G: 0.003615 \n",
      "[8/10][11/100][1662] Loss_D: 0.001786 Loss_G: 0.002649 \n",
      "[8/10][11/100][1663] Loss_D: 0.004013 Loss_G: 0.003659 \n",
      "[8/10][11/100][1664] Loss_D: 0.005288 Loss_G: 0.003903 \n",
      "[8/10][11/100][1665] Loss_D: 0.002670 Loss_G: 0.002331 \n",
      "[8/10][11/100][1666] Loss_D: 0.003137 Loss_G: 0.002683 \n",
      "[8/10][11/100][1667] Loss_D: 0.003168 Loss_G: 0.003067 \n",
      "[8/10][11/100][1668] Loss_D: 0.002236 Loss_G: 0.002443 \n",
      "[8/10][11/100][1669] Loss_D: 0.001745 Loss_G: 0.002607 \n",
      "[8/10][11/100][1670] Loss_D: 0.004225 Loss_G: 0.002674 \n",
      "[8/10][11/100][1671] Loss_D: 0.003279 Loss_G: 0.003844 \n",
      "[8/10][11/100][1672] Loss_D: 0.003306 Loss_G: 0.005016 \n",
      "[8/10][11/100][1673] Loss_D: 0.002447 Loss_G: 0.003349 \n",
      "[8/10][11/100][1674] Loss_D: 0.002837 Loss_G: 0.003691 \n",
      "[8/10][11/100][1675] Loss_D: 0.003120 Loss_G: 0.003534 \n",
      "[8/10][11/100][1676] Loss_D: 0.003169 Loss_G: 0.004520 \n",
      "[8/10][11/100][1677] Loss_D: 0.004784 Loss_G: 0.002218 \n",
      "[8/10][11/100][1678] Loss_D: 0.004422 Loss_G: 0.004296 \n",
      "[8/10][11/100][1679] Loss_D: 0.003212 Loss_G: 0.004984 \n",
      "[8/10][11/100][1680] Loss_D: 0.002948 Loss_G: 0.003505 \n",
      "[8/10][11/100][1681] Loss_D: 0.003977 Loss_G: 0.003022 \n",
      "[8/10][11/100][1682] Loss_D: 0.003353 Loss_G: 0.005765 \n",
      "[8/10][11/100][1683] Loss_D: 0.003146 Loss_G: 0.002885 \n",
      "[8/10][11/100][1684] Loss_D: 0.003469 Loss_G: 0.003503 \n",
      "[8/10][11/100][1685] Loss_D: 0.003070 Loss_G: 0.004357 \n",
      "[8/10][11/100][1686] Loss_D: 0.002325 Loss_G: 0.004312 \n",
      "[8/10][11/100][1687] Loss_D: 0.003841 Loss_G: 0.003260 \n",
      "[8/10][11/100][1688] Loss_D: 0.006058 Loss_G: 0.004072 \n",
      "[8/10][11/100][1689] Loss_D: 0.004389 Loss_G: 0.005411 \n",
      "[8/10][11/100][1690] Loss_D: 0.004790 Loss_G: 0.003906 \n",
      "[8/10][11/100][1691] Loss_D: 0.006298 Loss_G: 0.004141 \n",
      "[8/10][11/100][1692] Loss_D: 0.003837 Loss_G: 0.003713 \n",
      "[8/10][11/100][1693] Loss_D: 0.002556 Loss_G: 0.002798 \n",
      "[8/10][11/100][1694] Loss_D: 0.003205 Loss_G: 0.002280 \n",
      "[8/10][11/100][1695] Loss_D: 0.002758 Loss_G: 0.003155 \n",
      "[8/10][11/100][1696] Loss_D: 0.003492 Loss_G: 0.002197 \n",
      "[8/10][11/100][1697] Loss_D: 0.002249 Loss_G: 0.002434 \n",
      "[8/10][11/100][1698] Loss_D: 0.001492 Loss_G: 0.002696 \n",
      "[8/10][11/100][1699] Loss_D: 0.002543 Loss_G: 0.003299 \n",
      "[8/10][11/100][1700] Loss_D: 0.001652 Loss_G: 0.002431 \n",
      "[8/10][11/100][1701] Loss_D: 0.002544 Loss_G: 0.003493 \n",
      "[8/10][11/100][1702] Loss_D: 0.004408 Loss_G: 0.003803 \n",
      "[8/10][11/100][1703] Loss_D: 0.002201 Loss_G: 0.002941 \n",
      "[8/10][11/100][1704] Loss_D: 0.003027 Loss_G: 0.001714 \n",
      "[8/10][11/100][1705] Loss_D: 0.001710 Loss_G: 0.002895 \n",
      "[8/10][11/100][1706] Loss_D: 0.003836 Loss_G: 0.003171 \n",
      "[8/10][11/100][1707] Loss_D: 0.003451 Loss_G: 0.002607 \n",
      "[8/10][11/100][1708] Loss_D: 0.004178 Loss_G: 0.003628 \n",
      "[8/10][11/100][1709] Loss_D: 0.003602 Loss_G: 0.002421 \n",
      "[8/10][11/100][1710] Loss_D: 0.003543 Loss_G: 0.004202 \n",
      "[8/10][11/100][1711] Loss_D: 0.003198 Loss_G: 0.002904 \n",
      "[8/10][11/100][1712] Loss_D: 0.002699 Loss_G: 0.003918 \n",
      "[8/10][11/100][1713] Loss_D: 0.004372 Loss_G: 0.001816 \n",
      "[8/10][11/100][1714] Loss_D: 0.002593 Loss_G: 0.004334 \n",
      "[8/10][11/100][1715] Loss_D: 0.003282 Loss_G: 0.003654 \n",
      "[8/10][11/100][1716] Loss_D: 0.003004 Loss_G: 0.003696 \n",
      "[8/10][11/100][1717] Loss_D: 0.002784 Loss_G: 0.004603 \n",
      "[8/10][11/100][1718] Loss_D: 0.002302 Loss_G: 0.003324 \n",
      "[8/10][11/100][1719] Loss_D: 0.003428 Loss_G: 0.003144 \n",
      "[8/10][11/100][1720] Loss_D: 0.003327 Loss_G: 0.002719 \n",
      "[8/10][11/100][1721] Loss_D: 0.002815 Loss_G: 0.003387 \n",
      "[8/10][11/100][1722] Loss_D: 0.003525 Loss_G: 0.003154 \n",
      "[8/10][11/100][1723] Loss_D: 0.003994 Loss_G: 0.003609 \n",
      "[8/10][11/100][1724] Loss_D: 0.004616 Loss_G: 0.003509 \n",
      "[8/10][11/100][1725] Loss_D: 0.004003 Loss_G: 0.003579 \n",
      "[8/10][11/100][1726] Loss_D: 0.003151 Loss_G: 0.002638 \n",
      "[8/10][11/100][1727] Loss_D: 0.003452 Loss_G: 0.002327 \n",
      "[8/10][11/100][1728] Loss_D: 0.003577 Loss_G: 0.003047 \n",
      "[8/10][11/100][1729] Loss_D: 0.005253 Loss_G: 0.002518 \n",
      "[8/10][11/100][1730] Loss_D: 0.004432 Loss_G: 0.002590 \n",
      "[8/10][11/100][1731] Loss_D: 0.002250 Loss_G: 0.003369 \n",
      "[8/10][11/100][1732] Loss_D: 0.003217 Loss_G: 0.003440 \n",
      "[8/10][11/100][1733] Loss_D: 0.002474 Loss_G: 0.003293 \n",
      "[8/10][11/100][1734] Loss_D: 0.003238 Loss_G: 0.002744 \n",
      "[8/10][11/100][1735] Loss_D: 0.003170 Loss_G: 0.004381 \n",
      "[8/10][11/100][1736] Loss_D: 0.003180 Loss_G: 0.001965 \n",
      "[8/10][11/100][1737] Loss_D: 0.003680 Loss_G: 0.002025 \n",
      "[8/10][11/100][1738] Loss_D: 0.003184 Loss_G: 0.002819 \n",
      "[8/10][11/100][1739] Loss_D: 0.001978 Loss_G: 0.002418 \n",
      "[8/10][11/100][1740] Loss_D: 0.003614 Loss_G: 0.003252 \n",
      "[8/10][11/100][1741] Loss_D: 0.002080 Loss_G: 0.003627 \n",
      "[8/10][11/100][1742] Loss_D: 0.003468 Loss_G: 0.003676 \n",
      "[8/10][11/100][1743] Loss_D: 0.002666 Loss_G: 0.003082 \n",
      "[8/10][11/100][1744] Loss_D: 0.002254 Loss_G: 0.001773 \n",
      "[8/10][11/100][1745] Loss_D: 0.003773 Loss_G: 0.003356 \n",
      "[8/10][11/100][1746] Loss_D: 0.002308 Loss_G: 0.003472 \n",
      "[8/10][11/100][1747] Loss_D: 0.003348 Loss_G: 0.002869 \n",
      "[8/10][11/100][1748] Loss_D: 0.003040 Loss_G: 0.002768 \n",
      "[8/10][11/100][1749] Loss_D: 0.003216 Loss_G: 0.002260 \n",
      "[8/10][11/100][1750] Loss_D: 0.002619 Loss_G: 0.002697 \n",
      "[8/10][11/100][1751] Loss_D: 0.003598 Loss_G: 0.003021 \n",
      "[8/10][11/100][1752] Loss_D: 0.005811 Loss_G: 0.001932 \n",
      "[8/10][11/100][1753] Loss_D: 0.002194 Loss_G: 0.002248 \n",
      "[8/10][11/100][1754] Loss_D: 0.002370 Loss_G: 0.003553 \n",
      "[8/10][11/100][1755] Loss_D: 0.004389 Loss_G: 0.003004 \n",
      "[8/10][11/100][1756] Loss_D: 0.002477 Loss_G: 0.003304 \n",
      "[8/10][11/100][1757] Loss_D: 0.004675 Loss_G: 0.003544 \n",
      "[8/10][11/100][1758] Loss_D: 0.002460 Loss_G: 0.002642 \n",
      "[8/10][11/100][1759] Loss_D: 0.002707 Loss_G: 0.003190 \n",
      "[8/10][11/100][1760] Loss_D: 0.004140 Loss_G: 0.002005 \n",
      "[8/10][11/100][1761] Loss_D: 0.002308 Loss_G: 0.003767 \n",
      "[8/10][11/100][1762] Loss_D: 0.003882 Loss_G: 0.002842 \n",
      "[8/10][11/100][1763] Loss_D: 0.002948 Loss_G: 0.002344 \n",
      "[8/10][11/100][1764] Loss_D: 0.002972 Loss_G: 0.002948 \n",
      "[8/10][11/100][1765] Loss_D: 0.001915 Loss_G: 0.001886 \n",
      "[8/10][11/100][1766] Loss_D: 0.002623 Loss_G: 0.003302 \n",
      "[8/10][11/100][1767] Loss_D: 0.003123 Loss_G: 0.002849 \n",
      "[8/10][11/100][1768] Loss_D: 0.004495 Loss_G: 0.004387 \n",
      "[8/10][11/100][1769] Loss_D: 0.002118 Loss_G: 0.001538 \n",
      "[8/10][11/100][1770] Loss_D: 0.003120 Loss_G: 0.002803 \n",
      "[8/10][11/100][1771] Loss_D: 0.002402 Loss_G: 0.003962 \n",
      "[8/10][11/100][1772] Loss_D: 0.003121 Loss_G: 0.004098 \n",
      "[8/10][11/100][1773] Loss_D: 0.002055 Loss_G: 0.003103 \n",
      "[8/10][11/100][1774] Loss_D: 0.002518 Loss_G: 0.002421 \n",
      "[8/10][11/100][1775] Loss_D: 0.002844 Loss_G: 0.003828 \n",
      "[8/10][11/100][1776] Loss_D: 0.004539 Loss_G: 0.002851 \n",
      "[8/10][11/100][1777] Loss_D: 0.003044 Loss_G: 0.002496 \n",
      "[8/10][11/100][1778] Loss_D: 0.003228 Loss_G: 0.002199 \n",
      "[8/10][11/100][1779] Loss_D: 0.002592 Loss_G: 0.002183 \n",
      "[8/10][11/100][1780] Loss_D: 0.003315 Loss_G: 0.001588 \n",
      "[8/10][11/100][1781] Loss_D: 0.002185 Loss_G: 0.003494 \n",
      "[8/10][11/100][1782] Loss_D: 0.002913 Loss_G: 0.002619 \n",
      "[8/10][11/100][1783] Loss_D: 0.002032 Loss_G: 0.004675 \n",
      "[8/10][11/100][1784] Loss_D: 0.003406 Loss_G: 0.003546 \n",
      "[8/10][11/100][1785] Loss_D: 0.002826 Loss_G: 0.002235 \n",
      "[8/10][11/100][1786] Loss_D: 0.003800 Loss_G: 0.002183 \n",
      "[8/10][11/100][1787] Loss_D: 0.002044 Loss_G: 0.004284 \n",
      "[8/10][11/100][1788] Loss_D: 0.003135 Loss_G: 0.003312 \n",
      "[8/10][11/100][1789] Loss_D: 0.003307 Loss_G: 0.003039 \n",
      "[8/10][11/100][1790] Loss_D: 0.002032 Loss_G: 0.001310 \n",
      "[8/10][11/100][1791] Loss_D: 0.002193 Loss_G: 0.003166 \n",
      "[8/10][11/100][1792] Loss_D: 0.003193 Loss_G: 0.002288 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][11/100][1793] Loss_D: 0.003555 Loss_G: 0.003304 \n",
      "[8/10][11/100][1794] Loss_D: 0.003337 Loss_G: 0.001663 \n",
      "[8/10][11/100][1795] Loss_D: 0.001198 Loss_G: 0.003174 \n",
      "[8/10][11/100][1796] Loss_D: 0.002995 Loss_G: 0.002838 \n",
      "[8/10][11/100][1797] Loss_D: 0.005410 Loss_G: 0.002530 \n",
      "[8/10][11/100][1798] Loss_D: 0.002499 Loss_G: 0.003018 \n",
      "[8/10][11/100][1799] Loss_D: 0.001649 Loss_G: 0.002529 \n",
      "[8/10][11/100][1800] Loss_D: 0.002775 Loss_G: 0.003123 \n",
      "[9/10][11/100][1801] Loss_D: 0.003512 Loss_G: 0.002023 \n",
      "[9/10][11/100][1802] Loss_D: 0.005170 Loss_G: 0.004091 \n",
      "[9/10][11/100][1803] Loss_D: 0.004000 Loss_G: 0.004246 \n",
      "[9/10][11/100][1804] Loss_D: 0.002453 Loss_G: 0.003010 \n",
      "[9/10][11/100][1805] Loss_D: 0.001837 Loss_G: 0.003041 \n",
      "[9/10][11/100][1806] Loss_D: 0.002501 Loss_G: 0.003812 \n",
      "[9/10][11/100][1807] Loss_D: 0.002089 Loss_G: 0.003231 \n",
      "[9/10][11/100][1808] Loss_D: 0.003683 Loss_G: 0.002872 \n",
      "[9/10][11/100][1809] Loss_D: 0.005315 Loss_G: 0.004899 \n",
      "[9/10][11/100][1810] Loss_D: 0.002240 Loss_G: 0.001354 \n",
      "[9/10][11/100][1811] Loss_D: 0.002600 Loss_G: 0.002734 \n",
      "[9/10][11/100][1812] Loss_D: 0.003250 Loss_G: 0.003289 \n",
      "[9/10][11/100][1813] Loss_D: 0.003343 Loss_G: 0.002585 \n",
      "[9/10][11/100][1814] Loss_D: 0.003142 Loss_G: 0.003663 \n",
      "[9/10][11/100][1815] Loss_D: 0.003996 Loss_G: 0.004521 \n",
      "[9/10][11/100][1816] Loss_D: 0.002816 Loss_G: 0.002677 \n",
      "[9/10][11/100][1817] Loss_D: 0.002259 Loss_G: 0.003987 \n",
      "[9/10][11/100][1818] Loss_D: 0.002133 Loss_G: 0.003025 \n",
      "[9/10][11/100][1819] Loss_D: 0.003487 Loss_G: 0.001791 \n",
      "[9/10][11/100][1820] Loss_D: 0.003583 Loss_G: 0.003380 \n",
      "[9/10][11/100][1821] Loss_D: 0.003715 Loss_G: 0.003499 \n",
      "[9/10][11/100][1822] Loss_D: 0.003042 Loss_G: 0.002725 \n",
      "[9/10][11/100][1823] Loss_D: 0.003735 Loss_G: 0.002768 \n",
      "[9/10][11/100][1824] Loss_D: 0.004132 Loss_G: 0.002302 \n",
      "[9/10][11/100][1825] Loss_D: 0.004266 Loss_G: 0.004852 \n",
      "[9/10][11/100][1826] Loss_D: 0.003003 Loss_G: 0.002975 \n",
      "[9/10][11/100][1827] Loss_D: 0.002024 Loss_G: 0.002894 \n",
      "[9/10][11/100][1828] Loss_D: 0.003680 Loss_G: 0.002264 \n",
      "[9/10][11/100][1829] Loss_D: 0.002291 Loss_G: 0.002546 \n",
      "[9/10][11/100][1830] Loss_D: 0.001824 Loss_G: 0.002863 \n",
      "[9/10][11/100][1831] Loss_D: 0.001796 Loss_G: 0.003366 \n",
      "[9/10][11/100][1832] Loss_D: 0.002965 Loss_G: 0.002546 \n",
      "[9/10][11/100][1833] Loss_D: 0.002720 Loss_G: 0.003156 \n",
      "[9/10][11/100][1834] Loss_D: 0.002684 Loss_G: 0.002863 \n",
      "[9/10][11/100][1835] Loss_D: 0.002654 Loss_G: 0.002255 \n",
      "[9/10][11/100][1836] Loss_D: 0.003222 Loss_G: 0.003164 \n",
      "[9/10][11/100][1837] Loss_D: 0.003032 Loss_G: 0.004204 \n",
      "[9/10][11/100][1838] Loss_D: 0.004049 Loss_G: 0.003655 \n",
      "[9/10][11/100][1839] Loss_D: 0.003481 Loss_G: 0.003481 \n",
      "[9/10][11/100][1840] Loss_D: 0.002951 Loss_G: 0.002655 \n",
      "[9/10][11/100][1841] Loss_D: 0.002316 Loss_G: 0.006404 \n",
      "[9/10][11/100][1842] Loss_D: 0.003247 Loss_G: 0.001996 \n",
      "[9/10][11/100][1843] Loss_D: 0.003296 Loss_G: 0.002872 \n",
      "[9/10][11/100][1844] Loss_D: 0.004086 Loss_G: 0.002834 \n",
      "[9/10][11/100][1845] Loss_D: 0.004391 Loss_G: 0.003660 \n",
      "[9/10][11/100][1846] Loss_D: 0.003145 Loss_G: 0.004505 \n",
      "[9/10][11/100][1847] Loss_D: 0.004055 Loss_G: 0.003231 \n",
      "[9/10][11/100][1848] Loss_D: 0.003638 Loss_G: 0.002544 \n",
      "[9/10][11/100][1849] Loss_D: 0.004961 Loss_G: 0.003239 \n",
      "[9/10][11/100][1850] Loss_D: 0.002446 Loss_G: 0.003846 \n",
      "[9/10][11/100][1851] Loss_D: 0.002810 Loss_G: 0.002297 \n",
      "[9/10][11/100][1852] Loss_D: 0.003152 Loss_G: 0.004003 \n",
      "[9/10][11/100][1853] Loss_D: 0.003769 Loss_G: 0.002895 \n",
      "[9/10][11/100][1854] Loss_D: 0.005201 Loss_G: 0.002834 \n",
      "[9/10][11/100][1855] Loss_D: 0.002528 Loss_G: 0.002433 \n",
      "[9/10][11/100][1856] Loss_D: 0.003710 Loss_G: 0.001721 \n",
      "[9/10][11/100][1857] Loss_D: 0.003334 Loss_G: 0.005692 \n",
      "[9/10][11/100][1858] Loss_D: 0.006065 Loss_G: 0.006962 \n",
      "[9/10][11/100][1859] Loss_D: 0.004456 Loss_G: 0.004773 \n",
      "[9/10][11/100][1860] Loss_D: 0.003661 Loss_G: 0.002479 \n",
      "[9/10][11/100][1861] Loss_D: 0.003307 Loss_G: 0.005457 \n",
      "[9/10][11/100][1862] Loss_D: 0.002885 Loss_G: 0.001816 \n",
      "[9/10][11/100][1863] Loss_D: 0.002903 Loss_G: 0.002442 \n",
      "[9/10][11/100][1864] Loss_D: 0.002669 Loss_G: 0.002747 \n",
      "[9/10][11/100][1865] Loss_D: 0.003631 Loss_G: 0.001506 \n",
      "[9/10][11/100][1866] Loss_D: 0.003537 Loss_G: 0.000503 \n",
      "[9/10][11/100][1867] Loss_D: 0.003814 Loss_G: 0.002351 \n",
      "[9/10][11/100][1868] Loss_D: 0.004014 Loss_G: 0.002301 \n",
      "[9/10][11/100][1869] Loss_D: 0.003070 Loss_G: 0.001596 \n",
      "[9/10][11/100][1870] Loss_D: 0.001858 Loss_G: 0.004069 \n",
      "[9/10][11/100][1871] Loss_D: 0.004679 Loss_G: 0.002452 \n",
      "[9/10][11/100][1872] Loss_D: 0.003246 Loss_G: 0.002691 \n",
      "[9/10][11/100][1873] Loss_D: 0.004356 Loss_G: 0.003123 \n",
      "[9/10][11/100][1874] Loss_D: 0.001878 Loss_G: 0.001926 \n",
      "[9/10][11/100][1875] Loss_D: 0.002762 Loss_G: 0.003324 \n",
      "[9/10][11/100][1876] Loss_D: 0.001152 Loss_G: 0.003072 \n",
      "[9/10][11/100][1877] Loss_D: 0.002763 Loss_G: 0.002641 \n",
      "[9/10][11/100][1878] Loss_D: 0.001433 Loss_G: 0.004861 \n",
      "[9/10][11/100][1879] Loss_D: 0.001682 Loss_G: 0.002899 \n",
      "[9/10][11/100][1880] Loss_D: 0.001408 Loss_G: 0.004072 \n",
      "[9/10][11/100][1881] Loss_D: 0.002865 Loss_G: 0.004106 \n",
      "[9/10][11/100][1882] Loss_D: 0.003423 Loss_G: 0.003226 \n",
      "[9/10][11/100][1883] Loss_D: 0.003116 Loss_G: 0.003196 \n",
      "[9/10][11/100][1884] Loss_D: 0.004157 Loss_G: 0.002496 \n",
      "[9/10][11/100][1885] Loss_D: 0.004199 Loss_G: 0.004091 \n",
      "[9/10][11/100][1886] Loss_D: 0.004542 Loss_G: 0.004133 \n",
      "[9/10][11/100][1887] Loss_D: 0.003324 Loss_G: 0.004243 \n",
      "[9/10][11/100][1888] Loss_D: 0.002193 Loss_G: 0.004007 \n",
      "[9/10][11/100][1889] Loss_D: 0.004003 Loss_G: 0.005633 \n",
      "[9/10][11/100][1890] Loss_D: 0.002935 Loss_G: 0.002383 \n",
      "[9/10][11/100][1891] Loss_D: 0.004426 Loss_G: 0.003613 \n",
      "[9/10][11/100][1892] Loss_D: 0.002751 Loss_G: 0.003251 \n",
      "[9/10][11/100][1893] Loss_D: 0.002584 Loss_G: 0.003048 \n",
      "[9/10][11/100][1894] Loss_D: 0.002202 Loss_G: 0.003798 \n",
      "[9/10][11/100][1895] Loss_D: 0.004860 Loss_G: 0.003040 \n",
      "[9/10][11/100][1896] Loss_D: 0.002167 Loss_G: 0.004331 \n",
      "[9/10][11/100][1897] Loss_D: 0.002894 Loss_G: 0.002536 \n",
      "[9/10][11/100][1898] Loss_D: 0.003096 Loss_G: 0.001939 \n",
      "[9/10][11/100][1899] Loss_D: 0.002958 Loss_G: 0.004088 \n",
      "[9/10][11/100][1900] Loss_D: 0.003720 Loss_G: 0.003041 \n",
      "[9/10][11/100][1901] Loss_D: 0.002275 Loss_G: 0.004459 \n",
      "[9/10][11/100][1902] Loss_D: 0.003114 Loss_G: 0.003163 \n",
      "[9/10][11/100][1903] Loss_D: 0.004406 Loss_G: 0.003738 \n",
      "[9/10][11/100][1904] Loss_D: 0.003583 Loss_G: 0.003740 \n",
      "[9/10][11/100][1905] Loss_D: 0.007040 Loss_G: 0.003937 \n",
      "[9/10][11/100][1906] Loss_D: 0.004600 Loss_G: 0.004118 \n",
      "[9/10][11/100][1907] Loss_D: 0.004342 Loss_G: 0.003896 \n",
      "[9/10][11/100][1908] Loss_D: 0.004351 Loss_G: 0.004152 \n",
      "[9/10][11/100][1909] Loss_D: 0.003961 Loss_G: 0.003149 \n",
      "[9/10][11/100][1910] Loss_D: 0.003893 Loss_G: 0.004149 \n",
      "[9/10][11/100][1911] Loss_D: 0.003688 Loss_G: 0.003555 \n",
      "[9/10][11/100][1912] Loss_D: 0.002774 Loss_G: 0.002009 \n",
      "[9/10][11/100][1913] Loss_D: 0.003705 Loss_G: 0.002983 \n",
      "[9/10][11/100][1914] Loss_D: 0.003633 Loss_G: 0.002680 \n",
      "[9/10][11/100][1915] Loss_D: 0.002632 Loss_G: 0.003347 \n",
      "[9/10][11/100][1916] Loss_D: 0.002212 Loss_G: 0.002187 \n",
      "[9/10][11/100][1917] Loss_D: 0.002531 Loss_G: 0.002725 \n",
      "[9/10][11/100][1918] Loss_D: 0.002827 Loss_G: 0.003112 \n",
      "[9/10][11/100][1919] Loss_D: 0.004012 Loss_G: 0.002431 \n",
      "[9/10][11/100][1920] Loss_D: 0.002964 Loss_G: 0.002965 \n",
      "[9/10][11/100][1921] Loss_D: 0.003733 Loss_G: 0.003942 \n",
      "[9/10][11/100][1922] Loss_D: 0.003433 Loss_G: 0.001837 \n",
      "[9/10][11/100][1923] Loss_D: 0.002980 Loss_G: 0.001703 \n",
      "[9/10][11/100][1924] Loss_D: 0.004790 Loss_G: 0.003985 \n",
      "[9/10][11/100][1925] Loss_D: 0.002376 Loss_G: 0.003205 \n",
      "[9/10][11/100][1926] Loss_D: 0.001437 Loss_G: 0.003147 \n",
      "[9/10][11/100][1927] Loss_D: 0.002557 Loss_G: 0.004180 \n",
      "[9/10][11/100][1928] Loss_D: 0.002638 Loss_G: 0.002803 \n",
      "[9/10][11/100][1929] Loss_D: 0.002337 Loss_G: 0.002384 \n",
      "[9/10][11/100][1930] Loss_D: 0.002647 Loss_G: 0.002386 \n",
      "[9/10][11/100][1931] Loss_D: 0.003946 Loss_G: 0.003563 \n",
      "[9/10][11/100][1932] Loss_D: 0.002323 Loss_G: 0.003066 \n",
      "[9/10][11/100][1933] Loss_D: 0.002676 Loss_G: 0.004421 \n",
      "[9/10][11/100][1934] Loss_D: 0.003093 Loss_G: 0.003263 \n",
      "[9/10][11/100][1935] Loss_D: 0.001352 Loss_G: 0.001867 \n",
      "[9/10][11/100][1936] Loss_D: 0.002582 Loss_G: 0.004376 \n",
      "[9/10][11/100][1937] Loss_D: 0.003059 Loss_G: 0.002983 \n",
      "[9/10][11/100][1938] Loss_D: 0.002659 Loss_G: 0.002907 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10][11/100][1939] Loss_D: 0.002404 Loss_G: 0.002376 \n",
      "[9/10][11/100][1940] Loss_D: 0.003911 Loss_G: 0.002433 \n",
      "[9/10][11/100][1941] Loss_D: 0.002464 Loss_G: 0.003474 \n",
      "[9/10][11/100][1942] Loss_D: 0.003023 Loss_G: 0.002844 \n",
      "[9/10][11/100][1943] Loss_D: 0.002209 Loss_G: 0.004823 \n",
      "[9/10][11/100][1944] Loss_D: 0.003586 Loss_G: 0.002571 \n",
      "[9/10][11/100][1945] Loss_D: 0.002942 Loss_G: 0.002037 \n",
      "[9/10][11/100][1946] Loss_D: 0.003799 Loss_G: 0.004437 \n",
      "[9/10][11/100][1947] Loss_D: 0.002632 Loss_G: 0.002509 \n",
      "[9/10][11/100][1948] Loss_D: 0.003152 Loss_G: 0.002590 \n",
      "[9/10][11/100][1949] Loss_D: 0.004355 Loss_G: 0.003478 \n",
      "[9/10][11/100][1950] Loss_D: 0.002998 Loss_G: 0.003271 \n",
      "[9/10][11/100][1951] Loss_D: 0.003689 Loss_G: 0.005056 \n",
      "[9/10][11/100][1952] Loss_D: 0.002567 Loss_G: 0.005715 \n",
      "[9/10][11/100][1953] Loss_D: 0.003883 Loss_G: 0.004865 \n",
      "[9/10][11/100][1954] Loss_D: 0.004798 Loss_G: 0.003598 \n",
      "[9/10][11/100][1955] Loss_D: 0.004596 Loss_G: 0.003747 \n",
      "[9/10][11/100][1956] Loss_D: 0.002867 Loss_G: 0.003255 \n",
      "[9/10][11/100][1957] Loss_D: 0.004229 Loss_G: 0.004525 \n",
      "[9/10][11/100][1958] Loss_D: 0.003493 Loss_G: 0.005409 \n",
      "[9/10][11/100][1959] Loss_D: 0.003505 Loss_G: 0.004122 \n",
      "[9/10][11/100][1960] Loss_D: 0.004130 Loss_G: 0.004044 \n",
      "[9/10][11/100][1961] Loss_D: 0.003749 Loss_G: 0.004129 \n",
      "[9/10][11/100][1962] Loss_D: 0.005390 Loss_G: 0.003928 \n",
      "[9/10][11/100][1963] Loss_D: 0.004601 Loss_G: 0.004084 \n",
      "[9/10][11/100][1964] Loss_D: 0.003924 Loss_G: 0.002957 \n",
      "[9/10][11/100][1965] Loss_D: 0.001342 Loss_G: 0.002538 \n",
      "[9/10][11/100][1966] Loss_D: 0.003982 Loss_G: 0.003276 \n",
      "[9/10][11/100][1967] Loss_D: 0.003968 Loss_G: 0.000501 \n",
      "[9/10][11/100][1968] Loss_D: 0.002404 Loss_G: 0.002552 \n",
      "[9/10][11/100][1969] Loss_D: 0.001812 Loss_G: 0.000411 \n",
      "[9/10][11/100][1970] Loss_D: 0.002307 Loss_G: 0.003647 \n",
      "[9/10][11/100][1971] Loss_D: 0.000401 Loss_G: 0.001652 \n",
      "[9/10][11/100][1972] Loss_D: 0.002330 Loss_G: 0.002165 \n",
      "[9/10][11/100][1973] Loss_D: 0.002780 Loss_G: 0.002274 \n",
      "[9/10][11/100][1974] Loss_D: 0.001877 Loss_G: 0.002540 \n",
      "[9/10][11/100][1975] Loss_D: 0.004221 Loss_G: 0.001998 \n",
      "[9/10][11/100][1976] Loss_D: 0.002080 Loss_G: 0.002954 \n",
      "[9/10][11/100][1977] Loss_D: 0.002541 Loss_G: 0.001510 \n",
      "[9/10][11/100][1978] Loss_D: 0.000640 Loss_G: 0.001482 \n",
      "[9/10][11/100][1979] Loss_D: 0.001886 Loss_G: 0.001724 \n",
      "[9/10][11/100][1980] Loss_D: 0.002748 Loss_G: 0.003366 \n",
      "[9/10][11/100][1981] Loss_D: 0.001936 Loss_G: 0.004656 \n",
      "[9/10][11/100][1982] Loss_D: 0.002417 Loss_G: 0.003510 \n",
      "[9/10][11/100][1983] Loss_D: 0.002002 Loss_G: 0.002304 \n",
      "[9/10][11/100][1984] Loss_D: 0.003252 Loss_G: 0.002758 \n",
      "[9/10][11/100][1985] Loss_D: 0.003611 Loss_G: 0.002701 \n",
      "[9/10][11/100][1986] Loss_D: 0.002973 Loss_G: 0.004649 \n",
      "[9/10][11/100][1987] Loss_D: 0.003095 Loss_G: 0.002947 \n",
      "[9/10][11/100][1988] Loss_D: 0.003336 Loss_G: 0.003769 \n",
      "[9/10][11/100][1989] Loss_D: 0.002302 Loss_G: 0.002392 \n",
      "[9/10][11/100][1990] Loss_D: 0.002577 Loss_G: 0.002294 \n",
      "[9/10][11/100][1991] Loss_D: 0.003976 Loss_G: 0.003403 \n",
      "[9/10][11/100][1992] Loss_D: 0.003703 Loss_G: 0.004239 \n",
      "[9/10][11/100][1993] Loss_D: 0.002601 Loss_G: 0.003301 \n",
      "[9/10][11/100][1994] Loss_D: 0.003167 Loss_G: 0.003223 \n",
      "[9/10][11/100][1995] Loss_D: 0.003428 Loss_G: 0.002125 \n",
      "[9/10][11/100][1996] Loss_D: 0.003618 Loss_G: 0.004431 \n",
      "[9/10][11/100][1997] Loss_D: 0.004324 Loss_G: 0.004666 \n",
      "[9/10][11/100][1998] Loss_D: 0.003470 Loss_G: 0.003832 \n",
      "[9/10][11/100][1999] Loss_D: 0.002179 Loss_G: 0.004299 \n",
      "[9/10][11/100][2000] Loss_D: 0.003377 Loss_G: 0.004511 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0005731555866077542,\n",
       "  0.0021373452618718147,\n",
       "  0.006789423059672117,\n",
       "  0.007092694286257029,\n",
       "  0.01594218984246254,\n",
       "  0.012264921329915524,\n",
       "  0.016552090644836426,\n",
       "  0.0137872863560915,\n",
       "  0.011806292459368706,\n",
       "  0.011451112106442451,\n",
       "  0.014815879985690117,\n",
       "  0.01857762597501278,\n",
       "  0.012334849685430527,\n",
       "  0.012398926541209221,\n",
       "  0.007796157151460648,\n",
       "  0.00977933406829834,\n",
       "  0.00782332569360733,\n",
       "  0.0074753728695213795,\n",
       "  0.006973100360482931,\n",
       "  0.004010837059468031,\n",
       "  0.001847119303420186,\n",
       "  0.0038955823983997107,\n",
       "  0.0031814654357731342,\n",
       "  0.005046564619988203,\n",
       "  0.0035724970512092113,\n",
       "  0.004911117721349001,\n",
       "  0.007714424282312393,\n",
       "  0.007426315452903509,\n",
       "  0.0069347647950053215,\n",
       "  0.0039062045980244875,\n",
       "  0.008141652680933475,\n",
       "  0.0060756616294384,\n",
       "  0.00554185826331377,\n",
       "  0.0033212180715054274,\n",
       "  0.0038676427211612463,\n",
       "  0.006880057044327259,\n",
       "  0.005687375087291002,\n",
       "  0.0037887655198574066,\n",
       "  0.006839580833911896,\n",
       "  0.005418080370873213,\n",
       "  0.00584771391004324,\n",
       "  0.008525031618773937,\n",
       "  0.006110264454036951,\n",
       "  0.007565537001937628,\n",
       "  0.0054777744226157665,\n",
       "  0.0056878142058849335,\n",
       "  0.006990185473114252,\n",
       "  0.005123527720570564,\n",
       "  0.005463373847305775,\n",
       "  0.006132902577519417,\n",
       "  0.005128460004925728,\n",
       "  0.005834849085658789,\n",
       "  0.007170773111283779,\n",
       "  0.005679234862327576,\n",
       "  0.0031726083252578974,\n",
       "  0.005754654761403799,\n",
       "  0.006121542304754257,\n",
       "  0.005724895745515823,\n",
       "  0.005674834828823805,\n",
       "  0.004465885926038027,\n",
       "  0.005395025014877319,\n",
       "  0.006304092239588499,\n",
       "  0.006545739248394966,\n",
       "  0.005122542381286621,\n",
       "  0.007279769517481327,\n",
       "  0.006499289534986019,\n",
       "  0.005304415710270405,\n",
       "  0.007490766234695911,\n",
       "  0.005095228552818298,\n",
       "  0.004670069087296724,\n",
       "  0.003676873166114092,\n",
       "  0.004147915635257959,\n",
       "  0.005866463296115398,\n",
       "  0.004415799863636494,\n",
       "  0.0018616088200360537,\n",
       "  0.003731507807970047,\n",
       "  0.0029829884879291058,\n",
       "  0.003312422428280115,\n",
       "  0.00328816962428391,\n",
       "  0.003259249497205019,\n",
       "  0.004162203054875135,\n",
       "  0.003163866465911269,\n",
       "  0.00393997598439455,\n",
       "  0.0032563607674092054,\n",
       "  0.00468752346932888,\n",
       "  0.004399167373776436,\n",
       "  0.0052026305347681046,\n",
       "  0.004916457459330559,\n",
       "  0.006289467215538025,\n",
       "  0.0075957272201776505,\n",
       "  0.004271369893103838,\n",
       "  0.005927460268139839,\n",
       "  0.005956713110208511,\n",
       "  0.003965200390666723,\n",
       "  0.004602398257702589,\n",
       "  0.003529347013682127,\n",
       "  0.005043502431362867,\n",
       "  0.0032830520067363977,\n",
       "  0.0027106276247650385,\n",
       "  0.0037899708840996027,\n",
       "  0.0031720614060759544,\n",
       "  0.004122327547520399,\n",
       "  0.003136728424578905,\n",
       "  0.002824984258040786,\n",
       "  0.0032749271485954523,\n",
       "  0.0017678063595667481,\n",
       "  0.003690836951136589,\n",
       "  0.002658449811860919,\n",
       "  0.0022704554721713066,\n",
       "  0.0026342931669205427,\n",
       "  0.0032543994020670652,\n",
       "  0.0019315856043249369,\n",
       "  0.002646791748702526,\n",
       "  0.0032794673461467028,\n",
       "  0.0025590171571820974,\n",
       "  0.0034732019994407892,\n",
       "  0.0029786285012960434,\n",
       "  0.005357448011636734,\n",
       "  0.002947192871943116,\n",
       "  0.004589577205479145,\n",
       "  0.0029592211358249187,\n",
       "  0.004024312365800142,\n",
       "  0.002209309721365571,\n",
       "  0.0028827604837715626,\n",
       "  0.003491286188364029,\n",
       "  0.002135606948286295,\n",
       "  0.003269404172897339,\n",
       "  0.002346665831282735,\n",
       "  0.003020848846063018,\n",
       "  0.0034396208357065916,\n",
       "  0.002059451537206769,\n",
       "  0.0024316315539181232,\n",
       "  0.0032185502350330353,\n",
       "  0.004258113447576761,\n",
       "  0.0031433941330760717,\n",
       "  0.0035638166591525078,\n",
       "  0.0029290420934557915,\n",
       "  0.0033887620083987713,\n",
       "  0.0030718990601599216,\n",
       "  0.0032334288116544485,\n",
       "  0.0017455881461501122,\n",
       "  0.0023063269909471273,\n",
       "  0.0049773105420172215,\n",
       "  0.0017380969366058707,\n",
       "  0.002028815448284149,\n",
       "  0.001888881204649806,\n",
       "  0.0029536671936511993,\n",
       "  0.003634607419371605,\n",
       "  0.00388574181124568,\n",
       "  0.0025460016913712025,\n",
       "  0.0013112799497321248,\n",
       "  0.00236990163102746,\n",
       "  0.0014307573437690735,\n",
       "  0.0035509902518242598,\n",
       "  0.002481654053553939,\n",
       "  0.003015467431396246,\n",
       "  0.0027101761661469936,\n",
       "  0.0023192944936454296,\n",
       "  0.0020359354093670845,\n",
       "  0.0012755162315443158,\n",
       "  0.004309820011258125,\n",
       "  0.0033131882082670927,\n",
       "  0.002590832067653537,\n",
       "  0.004245830699801445,\n",
       "  0.003824504092335701,\n",
       "  0.004081035032868385,\n",
       "  0.0036986058112233877,\n",
       "  0.0026986044831573963,\n",
       "  0.003941602539271116,\n",
       "  0.0027989165391772985,\n",
       "  0.003363116644322872,\n",
       "  0.002511983970180154,\n",
       "  0.0022463658824563026,\n",
       "  0.00452670082449913,\n",
       "  0.002258685417473316,\n",
       "  0.002241393318399787,\n",
       "  0.0019081971840932965,\n",
       "  0.0016009683022275567,\n",
       "  0.0022011215332895517,\n",
       "  0.0025569975841790438,\n",
       "  0.001975004095584154,\n",
       "  0.003952122759073973,\n",
       "  0.0019257330568507314,\n",
       "  0.004727859050035477,\n",
       "  0.003189841052517295,\n",
       "  0.003633572021499276,\n",
       "  0.003415475133806467,\n",
       "  0.004864835180342197,\n",
       "  0.005269492045044899,\n",
       "  0.0044912314042449,\n",
       "  0.007441297173500061,\n",
       "  0.004303411580622196,\n",
       "  0.005106721539050341,\n",
       "  0.004938287660479546,\n",
       "  0.005562018137425184,\n",
       "  0.004865907598286867,\n",
       "  0.004898165352642536,\n",
       "  0.0031953761354088783,\n",
       "  0.006320344749838114,\n",
       "  0.002453641965985298,\n",
       "  0.0030877774115651846,\n",
       "  0.0008176025585271418,\n",
       "  0.0040193237364292145,\n",
       "  0.0019284129375591874,\n",
       "  0.0014672670513391495,\n",
       "  0.004190539475530386,\n",
       "  0.0017835330218076706,\n",
       "  0.0026312307454645634,\n",
       "  0.003440859727561474,\n",
       "  0.003927942831069231,\n",
       "  0.002081028651446104,\n",
       "  0.0023373039439320564,\n",
       "  0.001466206507757306,\n",
       "  0.0032752875704318285,\n",
       "  0.0017773821018636227,\n",
       "  0.0022189589217305183,\n",
       "  0.006892916280776262,\n",
       "  0.00583722023293376,\n",
       "  0.0052426098845899105,\n",
       "  0.004093566909432411,\n",
       "  0.0031897362787276506,\n",
       "  0.0033738608472049236,\n",
       "  0.0029033063910901546,\n",
       "  0.003083978546783328,\n",
       "  0.002215777290984988,\n",
       "  0.0030933162197470665,\n",
       "  0.0023899688385427,\n",
       "  0.0042605046182870865,\n",
       "  0.0031833925750106573,\n",
       "  0.002826840151101351,\n",
       "  0.0016923397779464722,\n",
       "  0.002347269793972373,\n",
       "  0.0013520073844119906,\n",
       "  0.0024327486753463745,\n",
       "  0.002550677629187703,\n",
       "  0.0021377832163125277,\n",
       "  0.0014608582714572549,\n",
       "  0.0033394633792340755,\n",
       "  0.003453353652730584,\n",
       "  0.0032115066424012184,\n",
       "  0.003803210100159049,\n",
       "  0.0032412928994745016,\n",
       "  0.006588793359696865,\n",
       "  0.005599216092377901,\n",
       "  0.004224706906825304,\n",
       "  0.0048561799339950085,\n",
       "  0.0037968922406435013,\n",
       "  0.005209702532738447,\n",
       "  0.003512136172503233,\n",
       "  0.003765824483707547,\n",
       "  0.004239936359226704,\n",
       "  0.004386883229017258,\n",
       "  0.0034200726076960564,\n",
       "  0.00323691638186574,\n",
       "  0.004133225884288549,\n",
       "  0.0032690551597625017,\n",
       "  0.0026709658559411764,\n",
       "  0.0035202670842409134,\n",
       "  0.002119320910423994,\n",
       "  0.003502806881442666,\n",
       "  0.0005282564088702202,\n",
       "  0.002135123824700713,\n",
       "  0.003933271858841181,\n",
       "  0.0026260565500706434,\n",
       "  0.00017891406605485827,\n",
       "  0.00012074032565578818,\n",
       "  0.002230706624686718,\n",
       "  0.003250716021284461,\n",
       "  0.0033009788021445274,\n",
       "  0.0010548916179686785,\n",
       "  0.00039756993646733463,\n",
       "  0.0012969522504135966,\n",
       "  0.0008512646891176701,\n",
       "  0.0006284264381974936,\n",
       "  0.001245927414856851,\n",
       "  0.0009187414543703198,\n",
       "  0.0028682281263172626,\n",
       "  0.0036521086003631353,\n",
       "  0.004495603963732719,\n",
       "  0.001714489422738552,\n",
       "  0.004085206892341375,\n",
       "  0.005062327720224857,\n",
       "  0.004547543358057737,\n",
       "  0.004477080889046192,\n",
       "  0.002499046502634883,\n",
       "  0.004713812842965126,\n",
       "  0.001473623444326222,\n",
       "  0.004232301376760006,\n",
       "  0.0038322657346725464,\n",
       "  0.002865568967536092,\n",
       "  0.0028364614117890596,\n",
       "  0.003624442033469677,\n",
       "  0.003905479097738862,\n",
       "  0.0029545389115810394,\n",
       "  0.0015912327216938138,\n",
       "  0.002259050263091922,\n",
       "  0.00385946873575449,\n",
       "  0.0024112886749207973,\n",
       "  0.0029285966884344816,\n",
       "  0.002609209157526493,\n",
       "  0.0034986368846148252,\n",
       "  0.002354281721636653,\n",
       "  0.004008827265352011,\n",
       "  0.004860370885580778,\n",
       "  0.0035438525956124067,\n",
       "  0.004668867215514183,\n",
       "  0.004293345380574465,\n",
       "  0.004777735099196434,\n",
       "  0.005762637127190828,\n",
       "  0.00410058256238699,\n",
       "  0.0045562307350337505,\n",
       "  0.004381359554827213,\n",
       "  0.0028671359177678823,\n",
       "  0.0031923071946948767,\n",
       "  0.00379187217913568,\n",
       "  0.002768915146589279,\n",
       "  0.003207255620509386,\n",
       "  0.001686764881014824,\n",
       "  0.002513691084459424,\n",
       "  0.002334464807063341,\n",
       "  0.00209926743991673,\n",
       "  0.002204392571002245,\n",
       "  0.0017022264655679464,\n",
       "  0.0030465349555015564,\n",
       "  0.0019834497943520546,\n",
       "  0.0016445254441350698,\n",
       "  0.0030472255311906338,\n",
       "  0.002405654639005661,\n",
       "  0.004109885077923536,\n",
       "  0.0020483413245528936,\n",
       "  0.0026948428712785244,\n",
       "  0.0021900360006839037,\n",
       "  0.0015135153662413359,\n",
       "  0.0034334389492869377,\n",
       "  0.0016921948408707976,\n",
       "  0.001279668533243239,\n",
       "  0.0013778825523331761,\n",
       "  0.002166598569601774,\n",
       "  0.0023132525384426117,\n",
       "  0.0012941154418513179,\n",
       "  0.0030799931846559048,\n",
       "  0.0051712291315197945,\n",
       "  0.0013951254077255726,\n",
       "  0.002478210721164942,\n",
       "  0.00237697409465909,\n",
       "  0.0031824209727346897,\n",
       "  0.00354908243753016,\n",
       "  0.0038132101763039827,\n",
       "  0.0035794584546238184,\n",
       "  0.0020013158209621906,\n",
       "  0.002807240467518568,\n",
       "  0.004847981967031956,\n",
       "  0.003953748848289251,\n",
       "  0.0036695196758955717,\n",
       "  0.0029182678554207087,\n",
       "  0.004961990751326084,\n",
       "  0.0034177028574049473,\n",
       "  0.003313529770821333,\n",
       "  0.0044689420610666275,\n",
       "  0.004908701870590448,\n",
       "  0.004745838697999716,\n",
       "  0.004896953236311674,\n",
       "  0.00318930740468204,\n",
       "  0.0049821375869214535,\n",
       "  0.004410544876009226,\n",
       "  0.006562561262398958,\n",
       "  0.00385293853469193,\n",
       "  0.00308025605045259,\n",
       "  0.006096030119806528,\n",
       "  0.003361862851306796,\n",
       "  0.003197557758539915,\n",
       "  0.005249161273241043,\n",
       "  0.00417651841416955,\n",
       "  0.0030030482448637486,\n",
       "  0.002706885803490877,\n",
       "  0.0016410457901656628,\n",
       "  0.0031812647357583046,\n",
       "  0.0026819028425961733,\n",
       "  0.0024778845254331827,\n",
       "  0.0020630971994251013,\n",
       "  0.002973764669150114,\n",
       "  0.0017507224110886455,\n",
       "  0.0026167528703808784,\n",
       "  0.0020123592112213373,\n",
       "  0.004104339051991701,\n",
       "  0.0024488235358148813,\n",
       "  0.002144688507542014,\n",
       "  0.00282247643917799,\n",
       "  0.0033836886286735535,\n",
       "  0.0035747969523072243,\n",
       "  0.002224241616204381,\n",
       "  0.0033868905156850815,\n",
       "  0.0016553341411054134,\n",
       "  0.004077705089002848,\n",
       "  0.0031316771637648344,\n",
       "  0.0050107440911233425,\n",
       "  0.002317206235602498,\n",
       "  0.002795873209834099,\n",
       "  0.0031114229932427406,\n",
       "  0.0022404331248253584,\n",
       "  0.004385844338685274,\n",
       "  0.004415588919073343,\n",
       "  0.0013541876105591655,\n",
       "  0.0028583938255906105,\n",
       "  0.0030778769869357347,\n",
       "  0.002277888124808669,\n",
       "  0.001536607276648283,\n",
       "  0.0064163813367486,\n",
       "  0.0026667877100408077,\n",
       "  0.0029785986989736557,\n",
       "  0.003546481253579259,\n",
       "  0.004647391848266125,\n",
       "  0.003651234321296215,\n",
       "  0.0032089503947645426,\n",
       "  0.002639507409185171,\n",
       "  0.002881038933992386,\n",
       "  0.005979551002383232,\n",
       "  0.0039101243019104,\n",
       "  0.007508337963372469,\n",
       "  0.004286194685846567,\n",
       "  0.005727577488869429,\n",
       "  0.006327738054096699,\n",
       "  0.007034830283373594,\n",
       "  0.006576634477823973,\n",
       "  0.003964458592236042,\n",
       "  0.005381984170526266,\n",
       "  0.006206855643540621,\n",
       "  0.00439770333468914,\n",
       "  0.004916652571409941,\n",
       "  0.004565822891891003,\n",
       "  0.002780250972136855,\n",
       "  0.0039631761610507965,\n",
       "  0.0032905174884945154,\n",
       "  0.002862578956410289,\n",
       "  0.002365528605878353,\n",
       "  0.0007009002147242427,\n",
       "  0.002395770512521267,\n",
       "  0.0040462929755449295,\n",
       "  0.0021056223195046186,\n",
       "  0.0014744630316272378,\n",
       "  0.002384107792750001,\n",
       "  0.0028766037430614233,\n",
       "  0.002655262127518654,\n",
       "  0.0033198236487805843,\n",
       "  0.005598279181867838,\n",
       "  0.0032501311507076025,\n",
       "  0.0022520797792822123,\n",
       "  0.0034661239478737116,\n",
       "  0.004465304780751467,\n",
       "  0.0049664173275232315,\n",
       "  0.00310455821454525,\n",
       "  0.001290412386879325,\n",
       "  0.0031034580897539854,\n",
       "  0.00431629316881299,\n",
       "  0.0022297282703220844,\n",
       "  0.0020355540327727795,\n",
       "  0.0038087524008005857,\n",
       "  0.0018219429766759276,\n",
       "  0.0012537527363747358,\n",
       "  0.003178491722792387,\n",
       "  0.0032956202048808336,\n",
       "  0.003042833646759391,\n",
       "  0.00296725332736969,\n",
       "  0.0037782895378768444,\n",
       "  0.001930988160893321,\n",
       "  0.001653631217777729,\n",
       "  0.003249137895181775,\n",
       "  0.0030889157205820084,\n",
       "  0.0038106529973447323,\n",
       "  0.0036808056756854057,\n",
       "  0.004858382977545261,\n",
       "  0.0027589646633714437,\n",
       "  0.0034314519725739956,\n",
       "  0.006688674911856651,\n",
       "  0.00665816105902195,\n",
       "  0.005262864753603935,\n",
       "  0.008855638094246387,\n",
       "  0.0062578641809523106,\n",
       "  0.005471376236528158,\n",
       "  0.005514886695891619,\n",
       "  0.004369099624454975,\n",
       "  0.004311948083341122,\n",
       "  0.002832158003002405,\n",
       "  0.0016181430546566844,\n",
       "  0.002846871502697468,\n",
       "  0.002172901527956128,\n",
       "  0.002753399545326829,\n",
       "  0.002012519631534815,\n",
       "  0.0032272799871861935,\n",
       "  0.002593086799606681,\n",
       "  0.003263111924752593,\n",
       "  0.002477784641087055,\n",
       "  0.002182251075282693,\n",
       "  0.001483751810155809,\n",
       "  0.0018908214988186955,\n",
       "  0.0008344178786501288,\n",
       "  0.002655566204339266,\n",
       "  0.0026005753315985203,\n",
       "  0.0016916841268539429,\n",
       "  0.002340418752282858,\n",
       "  0.003390917321667075,\n",
       "  0.003091566264629364,\n",
       "  0.0030540782026946545,\n",
       "  0.005063030403107405,\n",
       "  0.0034317034296691418,\n",
       "  0.003508550813421607,\n",
       "  0.003991624340415001,\n",
       "  0.003980978392064571,\n",
       "  0.003323514712974429,\n",
       "  0.003009446430951357,\n",
       "  0.0013831168180331588,\n",
       "  0.002694837749004364,\n",
       "  0.0033306728582829237,\n",
       "  0.004623588174581528,\n",
       "  0.0029432803858071566,\n",
       "  0.0035182556603103876,\n",
       "  0.0025450470857322216,\n",
       "  0.004214960150420666,\n",
       "  0.0029368512332439423,\n",
       "  0.0023000873625278473,\n",
       "  0.0025281626731157303,\n",
       "  0.0029861018992960453,\n",
       "  0.0032460426446050406,\n",
       "  0.0024561306927353144,\n",
       "  0.0031499892938882113,\n",
       "  0.002968595130369067,\n",
       "  0.003163113258779049,\n",
       "  0.0036430598702281713,\n",
       "  0.004424376413226128,\n",
       "  0.0031952590215951204,\n",
       "  0.0024752335157245398,\n",
       "  0.0032795011065900326,\n",
       "  0.0035175117664039135,\n",
       "  0.005895551759749651,\n",
       "  0.005237461533397436,\n",
       "  0.00546315498650074,\n",
       "  0.0058989571407437325,\n",
       "  0.006377341691404581,\n",
       "  0.00540406396612525,\n",
       "  0.00608722073957324,\n",
       "  0.0035498912911862135,\n",
       "  0.003303856821730733,\n",
       "  0.005037447437644005,\n",
       "  0.0029436498880386353,\n",
       "  0.004742949269711971,\n",
       "  0.0027695191092789173,\n",
       "  0.0030714881140738726,\n",
       "  0.0034895201679319143,\n",
       "  0.002534228377044201,\n",
       "  0.0030326424166560173,\n",
       "  0.0027619563043117523,\n",
       "  0.0024183636996895075,\n",
       "  0.004770458675920963,\n",
       "  0.002989601343870163,\n",
       "  0.0019197715446352959,\n",
       "  0.002972033806145191,\n",
       "  0.004191695246845484,\n",
       "  0.0014810188440605998,\n",
       "  0.004536315333098173,\n",
       "  0.004585358314216137,\n",
       "  0.0027054958045482635,\n",
       "  0.0033876497764140368,\n",
       "  0.0038664082530885935,\n",
       "  0.0028462312184274197,\n",
       "  0.0033048978075385094,\n",
       "  0.004236441105604172,\n",
       "  0.004734769929200411,\n",
       "  0.002329531591385603,\n",
       "  0.0010607973672449589,\n",
       "  0.004304409492760897,\n",
       "  0.0028602939564734697,\n",
       "  0.002392553724348545,\n",
       "  0.0027982972096651793,\n",
       "  0.003508044872432947,\n",
       "  0.0028752346988767385,\n",
       "  0.003988552838563919,\n",
       "  0.00259994575753808,\n",
       "  0.00420176750048995,\n",
       "  0.0034038270823657513,\n",
       "  0.002984285354614258,\n",
       "  0.0030153291299939156,\n",
       "  0.004125781822949648,\n",
       "  0.0035568156745284796,\n",
       "  0.0036195425782352686,\n",
       "  0.0038547792937606573,\n",
       "  0.0032083457335829735,\n",
       "  0.006258687935769558,\n",
       "  0.00359032372944057,\n",
       "  0.005943131633102894,\n",
       "  0.004882847424596548,\n",
       "  0.007051223888993263,\n",
       "  0.005354631692171097,\n",
       "  0.005417316686362028,\n",
       "  0.004346322268247604,\n",
       "  0.004398297984153032,\n",
       "  0.0035387526731938124,\n",
       "  0.0030117337591946125,\n",
       "  0.0038199364207684994,\n",
       "  0.003595113754272461,\n",
       "  0.0031597267370671034,\n",
       "  0.003598047187551856,\n",
       "  0.0034088457468897104,\n",
       "  0.002769197802990675,\n",
       "  0.0025275342632085085,\n",
       "  0.0006031669327057898,\n",
       "  0.002473659347742796,\n",
       "  0.0025277934037148952,\n",
       "  0.0014189841458573937,\n",
       "  0.001794401672668755,\n",
       "  0.002914267126470804,\n",
       "  0.0020838433410972357,\n",
       "  0.0016386291244998574,\n",
       "  0.00231719296425581,\n",
       "  0.003909106831997633,\n",
       "  0.004259306471794844,\n",
       "  0.0034438923466950655,\n",
       "  0.0017450916348025203,\n",
       "  0.0036138014402240515,\n",
       "  0.0025272993370890617,\n",
       "  0.002937599550932646,\n",
       "  0.0022250062320381403,\n",
       "  0.0030386897269636393,\n",
       "  0.0031572715379297733,\n",
       "  0.0033035785891115665,\n",
       "  0.00531739229336381,\n",
       "  0.002234778366982937,\n",
       "  0.0025518913753330708,\n",
       "  0.0025003706105053425,\n",
       "  0.0027666818350553513,\n",
       "  0.001868584775365889,\n",
       "  0.0036098528653383255,\n",
       "  0.0031332545913755894,\n",
       "  0.0017823231173679233,\n",
       "  0.0029150231275707483,\n",
       "  0.002619751961901784,\n",
       "  0.0030686059035360813,\n",
       "  0.0024334872141480446,\n",
       "  0.0026522388216108084,\n",
       "  0.002621673047542572,\n",
       "  0.0021352884359657764,\n",
       "  0.003417608793824911,\n",
       "  0.0039183213375508785,\n",
       "  0.0036879342515021563,\n",
       "  0.006250306498259306,\n",
       "  0.00619217474013567,\n",
       "  0.006948813330382109,\n",
       "  0.005344306584447622,\n",
       "  0.005817754659801722,\n",
       "  0.005412694066762924,\n",
       "  0.004431271925568581,\n",
       "  0.007852903567254543,\n",
       "  0.006010770797729492,\n",
       "  0.0032617731485515833,\n",
       "  0.001573079265654087,\n",
       "  0.00423161406069994,\n",
       "  0.002367998007684946,\n",
       "  0.0031024098861962557,\n",
       "  0.0023640310391783714,\n",
       "  0.002877805382013321,\n",
       "  0.0035643011797219515,\n",
       "  0.0017389198765158653,\n",
       "  -0.0002503636642359197,\n",
       "  0.0016409142408519983,\n",
       "  0.0015449682250618935,\n",
       "  0.002072396455332637,\n",
       "  0.004086526110768318,\n",
       "  0.003809344256296754,\n",
       "  0.001695632585324347,\n",
       "  0.004653028678148985,\n",
       "  0.003896488342434168,\n",
       "  0.0021343466360121965,\n",
       "  0.0025913205463439226,\n",
       "  0.005883566103875637,\n",
       "  0.0031475925352424383,\n",
       "  0.00458930991590023,\n",
       "  0.005497366655617952,\n",
       "  0.0029789176769554615,\n",
       "  0.002585012000054121,\n",
       "  0.0038129668682813644,\n",
       "  0.002883713925257325,\n",
       "  0.0016564037650823593,\n",
       "  0.0037388233467936516,\n",
       "  0.00302548473700881,\n",
       "  0.0018384730210527778,\n",
       "  0.0017677597934380174,\n",
       "  0.0018452255753800273,\n",
       "  0.0032214056700468063,\n",
       "  0.0017724353820085526,\n",
       "  0.0019705959130078554,\n",
       "  0.0029373306315392256,\n",
       "  0.0043149683624506,\n",
       "  0.004174229688942432,\n",
       "  0.004280983004719019,\n",
       "  0.005125829949975014,\n",
       "  0.00461815670132637,\n",
       "  0.007633501663804054,\n",
       "  0.0071115801110863686,\n",
       "  0.00645693764090538,\n",
       "  0.0050439974293112755,\n",
       "  0.0064121875911951065,\n",
       "  0.006298408377915621,\n",
       "  0.0061244298703968525,\n",
       "  0.006067880894988775,\n",
       "  0.0025982381775975227,\n",
       "  0.005493091885000467,\n",
       "  0.0033668577671051025,\n",
       "  0.0053190868347883224,\n",
       "  0.0006815487868152559,\n",
       "  0.0024123387411236763,\n",
       "  0.0024774917401373386,\n",
       "  0.0021660728380084038,\n",
       "  0.003686456009745598,\n",
       "  0.002419820288196206,\n",
       "  0.004225417505949736,\n",
       "  0.0017673951806500554,\n",
       "  0.0020517136435955763,\n",
       "  0.00040430319495499134,\n",
       "  0.0034175997134298086,\n",
       "  0.003065234748646617,\n",
       "  0.0003020624862983823,\n",
       "  0.004070000723004341,\n",
       "  0.0019925073720514774,\n",
       "  0.0036772473249584436,\n",
       "  0.006063149776309729,\n",
       "  0.0030998883303254843,\n",
       "  0.006825966294854879,\n",
       "  0.00448673777282238,\n",
       "  0.004385580774396658,\n",
       "  0.002776746405288577,\n",
       "  0.002037342404946685,\n",
       "  0.0030475857201963663,\n",
       "  0.004065562970936298,\n",
       "  0.0024842761922627687,\n",
       "  0.003324479330331087,\n",
       "  0.006926769390702248,\n",
       "  0.004459026735275984,\n",
       "  0.001747008296661079,\n",
       "  0.0027388576418161392,\n",
       "  0.002189642284065485,\n",
       "  0.0008268824312835932,\n",
       "  0.002889217110350728,\n",
       "  0.0027778358198702335,\n",
       "  0.003556264331564307,\n",
       "  0.0033702331129461527,\n",
       "  0.004112021066248417,\n",
       "  0.001173502649180591,\n",
       "  0.001365663600154221,\n",
       "  0.0026411099825054407,\n",
       "  0.001160403829999268,\n",
       "  0.002665561391040683,\n",
       "  0.00337356049567461,\n",
       "  0.002507444005459547,\n",
       "  0.002850846154615283,\n",
       "  0.004112050402909517,\n",
       "  0.005436369217932224,\n",
       "  0.004844650626182556,\n",
       "  0.004442456644028425,\n",
       "  0.004364407155662775,\n",
       "  0.00579025037586689,\n",
       "  0.0053666685707867146,\n",
       "  0.006606233771890402,\n",
       "  0.004970671143382788,\n",
       "  0.006158333271741867,\n",
       "  0.0031851837411522865,\n",
       "  0.004363701678812504,\n",
       "  0.004245422314852476,\n",
       "  0.004109950270503759,\n",
       "  0.005935578607022762,\n",
       "  0.0022187517024576664,\n",
       "  0.0019313860684633255,\n",
       "  0.0037453994154930115,\n",
       "  0.003248424269258976,\n",
       "  0.0005874433554708958,\n",
       "  0.0015632235445082188,\n",
       "  0.0033430212642997503,\n",
       "  0.0010529453866183758,\n",
       "  0.0006457107956521213,\n",
       "  0.001663758186623454,\n",
       "  0.002231373218819499,\n",
       "  0.0026939662639051676,\n",
       "  -0.0010457803728058934,\n",
       "  0.0036589480005204678,\n",
       "  0.004808621946722269,\n",
       "  0.003753904951736331,\n",
       "  0.007488766685128212,\n",
       "  0.006227792706340551,\n",
       "  0.004542509559541941,\n",
       "  0.006553841754794121,\n",
       "  0.003396810730919242,\n",
       "  0.006218773778527975,\n",
       "  0.003658519359305501,\n",
       "  0.001792417373508215,\n",
       "  0.002406172454357147,\n",
       "  0.002688758308067918,\n",
       "  0.004730180371552706,\n",
       "  0.0024232238065451384,\n",
       "  0.0031077160965651274,\n",
       "  0.003722710534930229,\n",
       "  0.0033785575069487095,\n",
       "  0.00136163632851094,\n",
       "  0.0015762693947181106,\n",
       "  0.002082074759528041,\n",
       "  0.003104796167463064,\n",
       "  0.003367065917700529,\n",
       "  0.0038394087459892035,\n",
       "  0.0033057781402021646,\n",
       "  0.004924065433442593,\n",
       "  0.003946762997657061,\n",
       "  0.005012079607695341,\n",
       "  0.00406000716611743,\n",
       "  0.00637413002550602,\n",
       "  0.004664160776883364,\n",
       "  0.004742358811199665,\n",
       "  0.003867505118250847,\n",
       "  0.0059824720956385136,\n",
       "  0.0040081157349050045,\n",
       "  0.006581012159585953,\n",
       "  0.005640452262014151,\n",
       "  0.004687990061938763,\n",
       "  0.004872834775596857,\n",
       "  0.004082128405570984,\n",
       "  0.0030578216537833214,\n",
       "  0.003342584241181612,\n",
       "  0.0037094494327902794,\n",
       "  0.0028382688760757446,\n",
       "  0.0036220820620656013,\n",
       "  0.003694332204759121,\n",
       "  0.002217038068920374,\n",
       "  0.002069631824269891,\n",
       "  0.002885331865400076,\n",
       "  0.0039314208552241325,\n",
       "  0.0016032103449106216,\n",
       "  0.002837528008967638,\n",
       "  0.002512957202270627,\n",
       "  0.0027815098874270916,\n",
       "  0.0015742926625534892,\n",
       "  0.0025882949121296406,\n",
       "  0.0017243520123884082,\n",
       "  0.0030538381543010473,\n",
       "  0.002141977194696665,\n",
       "  0.0036635834258049726,\n",
       "  0.0029588863253593445,\n",
       "  0.0021311617456376553,\n",
       "  0.0029010861180722713,\n",
       "  0.006173979956656694,\n",
       "  0.0022198809310793877,\n",
       "  0.004692960064858198,\n",
       "  0.003904978046193719,\n",
       "  0.004074036609381437,\n",
       "  0.005395389162003994,\n",
       "  0.0025901878252625465,\n",
       "  0.003592641558498144,\n",
       "  0.0023671339731663465,\n",
       "  0.0019419731106609106,\n",
       "  0.003119349479675293,\n",
       "  0.0024711089208722115,\n",
       "  0.0009882233571261168,\n",
       "  0.0026796641759574413,\n",
       "  0.0023444765247404575,\n",
       "  0.0025105560198426247,\n",
       "  0.003854665905237198,\n",
       "  0.00522204115986824,\n",
       "  0.004183226265013218,\n",
       "  0.0030605874489992857,\n",
       "  0.006233712658286095,\n",
       "  0.005114715080708265,\n",
       "  0.0055399020202457905,\n",
       "  0.006039624568074942,\n",
       "  0.006995563395321369,\n",
       "  0.006197439040988684,\n",
       "  0.006411104463040829,\n",
       "  0.004473862238228321,\n",
       "  0.004767764825373888,\n",
       "  0.0050334506668150425,\n",
       "  0.002786060329526663,\n",
       "  0.003418950829654932,\n",
       "  0.0032423476222902536,\n",
       "  0.00506700249388814,\n",
       "  0.004861090332269669,\n",
       "  0.004375511314719915,\n",
       "  0.0019498268375173211,\n",
       "  0.0023148534819483757,\n",
       "  0.004923446103930473,\n",
       "  0.0033577086869627237,\n",
       "  0.00036826805444434285,\n",
       "  0.0016002156771719456,\n",
       "  0.0005276479641906917,\n",
       "  0.003586812876164913,\n",
       "  0.0018522882601246238,\n",
       "  0.0047891163267195225,\n",
       "  0.004357473459094763,\n",
       "  0.004153545945882797,\n",
       "  0.0013726209290325642,\n",
       "  0.0021463504526764154,\n",
       "  0.002200392307713628,\n",
       "  0.0020253697875887156,\n",
       "  0.003258524462580681,\n",
       "  0.005744850262999535,\n",
       "  0.004184510093182325,\n",
       "  0.0023790663108229637,\n",
       "  0.004379380494356155,\n",
       "  0.005913573782891035,\n",
       "  0.00408595148473978,\n",
       "  0.0023405058309435844,\n",
       "  0.0032725753262639046,\n",
       "  0.0019121738150715828,\n",
       "  0.0034208318684250116,\n",
       "  0.003887021215632558,\n",
       "  0.0026045499835163355,\n",
       "  0.0030560146551579237,\n",
       "  0.003407626412808895,\n",
       "  0.0030673707369714975,\n",
       "  0.00230099237523973,\n",
       "  0.0029510571621358395,\n",
       "  0.0018687843112275004,\n",
       "  0.0036574476398527622,\n",
       "  0.003242267295718193,\n",
       "  0.002441598568111658,\n",
       "  0.0033490145578980446,\n",
       "  0.003196143778041005,\n",
       "  0.0022927692625671625,\n",
       "  0.00403627147898078,\n",
       "  0.004540964495390654,\n",
       "  0.004369176458567381,\n",
       "  0.005132185760885477,\n",
       "  0.005048173014074564,\n",
       "  0.003978417254984379,\n",
       "  0.0040184007957577705,\n",
       "  0.0034774858504533768,\n",
       "  0.006401375401765108,\n",
       "  0.005706332623958588,\n",
       "  0.005289828404784203,\n",
       "  0.0018620879855006933,\n",
       "  0.004299660678952932,\n",
       "  0.004069795366376638,\n",
       "  0.0028558471240103245,\n",
       "  0.0031259648967534304,\n",
       "  0.004686607047915459,\n",
       "  0.00415671244263649,\n",
       "  0.00238255737349391,\n",
       "  0.0027196602895855904,\n",
       "  0.002041621133685112,\n",
       "  0.00023901736130937934,\n",
       "  0.003046223893761635,\n",
       "  0.0025946495588868856,\n",
       "  0.002311672316864133,\n",
       "  0.0008073130156844854,\n",
       "  0.004116129130125046,\n",
       "  0.0020023451652377844,\n",
       "  0.0021896366961300373,\n",
       "  0.0030621362384408712,\n",
       "  0.0022478243336081505,\n",
       "  0.0010209071915596724,\n",
       "  0.003043766599148512,\n",
       "  0.004168992396444082,\n",
       "  0.004614569246768951,\n",
       "  0.003161120694130659,\n",
       "  0.00356674543581903,\n",
       "  0.0033404650166630745,\n",
       "  0.004204219207167625,\n",
       "  0.003889656625688076,\n",
       "  0.0057199252769351006,\n",
       "  0.004729302600026131,\n",
       "  0.003234167816117406,\n",
       "  0.003557984484359622,\n",
       "  0.0012619945919141173,\n",
       "  0.0016097917687147856,\n",
       "  0.0026054722256958485,\n",
       "  0.002344041597098112,\n",
       "  0.002355707110837102,\n",
       "  0.003140151035040617,\n",
       "  0.003846468636766076,\n",
       "  0.003166308393701911,\n",
       "  0.002136185532435775,\n",
       "  0.002434131922200322,\n",
       "  0.0027434825897216797,\n",
       "  0.0030569799710065126,\n",
       "  0.002444613492116332,\n",
       "  0.0034874214325100183,\n",
       "  0.003933371044695377,\n",
       "  0.004615450277924538,\n",
       "  0.004091733600944281,\n",
       "  0.005060275550931692,\n",
       "  0.0050723557360470295,\n",
       "  0.005281370133161545,\n",
       "  0.0051171244122087955,\n",
       "  0.005980253219604492,\n",
       "  0.006515043787658215,\n",
       "  0.0055758836679160595,\n",
       "  0.003327476093545556,\n",
       "  0.005283909384161234,\n",
       "  0.003916768357157707,\n",
       "  0.004340954124927521,\n",
       "  0.0036959387362003326,\n",
       "  0.002896188758313656,\n",
       "  0.004606955219060183,\n",
       "  0.002969097113236785,\n",
       "  0.0024405340664088726,\n",
       "  0.002604710403829813,\n",
       "  0.0006163851357996464,\n",
       "  ...],\n",
       " [0.0004682897124439478,\n",
       "  0.0021682451479136944,\n",
       "  0.007352807559072971,\n",
       "  0.012054343707859516,\n",
       "  0.015959320589900017,\n",
       "  0.013804583810269833,\n",
       "  0.014235074631869793,\n",
       "  0.013734778389334679,\n",
       "  0.011920932680368423,\n",
       "  0.015257309190928936,\n",
       "  0.013687126338481903,\n",
       "  0.010875674895942211,\n",
       "  0.010555633343756199,\n",
       "  0.011225746013224125,\n",
       "  0.012440863996744156,\n",
       "  0.01132606714963913,\n",
       "  0.010427881963551044,\n",
       "  0.0077652959153056145,\n",
       "  0.005756549071520567,\n",
       "  0.005146542564034462,\n",
       "  0.004817175678908825,\n",
       "  0.003903880249708891,\n",
       "  0.005460758227854967,\n",
       "  0.003913460299372673,\n",
       "  0.005086048040539026,\n",
       "  0.006048756185919046,\n",
       "  0.010494763031601906,\n",
       "  0.004439147189259529,\n",
       "  0.005867915693670511,\n",
       "  0.006986739579588175,\n",
       "  0.011153104715049267,\n",
       "  0.004510970786213875,\n",
       "  0.0051941960118710995,\n",
       "  0.0048101963475346565,\n",
       "  0.006226237863302231,\n",
       "  0.006960748694837093,\n",
       "  0.005341874901205301,\n",
       "  0.006227503530681133,\n",
       "  0.0025435129646211863,\n",
       "  0.0036281829234212637,\n",
       "  0.005090515129268169,\n",
       "  0.005253382958471775,\n",
       "  0.005070207640528679,\n",
       "  0.006569379009306431,\n",
       "  0.004295018967241049,\n",
       "  0.004912198521196842,\n",
       "  0.005482015665620565,\n",
       "  0.005082590505480766,\n",
       "  0.004726069513708353,\n",
       "  0.006574266590178013,\n",
       "  0.006421885918825865,\n",
       "  0.004693557042628527,\n",
       "  0.005626203026622534,\n",
       "  0.004668483044952154,\n",
       "  0.007846648804843426,\n",
       "  0.005827890243381262,\n",
       "  0.004084737505763769,\n",
       "  0.0036624756176024675,\n",
       "  0.0040611750446259975,\n",
       "  0.003984145354479551,\n",
       "  0.006380112841725349,\n",
       "  0.006744029000401497,\n",
       "  0.0036998731084167957,\n",
       "  0.005982985720038414,\n",
       "  0.006046402733772993,\n",
       "  0.005914435256272554,\n",
       "  0.007603053003549576,\n",
       "  0.004390248097479343,\n",
       "  0.005201292224228382,\n",
       "  0.00633403193205595,\n",
       "  0.006549431011080742,\n",
       "  0.0035057393833994865,\n",
       "  0.005996920168399811,\n",
       "  0.0046752579510211945,\n",
       "  0.005011213012039661,\n",
       "  0.0019957718905061483,\n",
       "  0.0035347763914614916,\n",
       "  0.003966113086789846,\n",
       "  0.0037157367914915085,\n",
       "  0.0024753313045948744,\n",
       "  0.0028843008913099766,\n",
       "  0.0022897468879818916,\n",
       "  0.002262272872030735,\n",
       "  0.002554809907451272,\n",
       "  0.0055835372768342495,\n",
       "  0.00558426883071661,\n",
       "  0.00638633081689477,\n",
       "  0.004988275934010744,\n",
       "  0.005582673940807581,\n",
       "  0.003195991739630699,\n",
       "  0.005678889341652393,\n",
       "  0.0057238140143454075,\n",
       "  0.00610117893666029,\n",
       "  0.00549693126231432,\n",
       "  0.0029798238538205624,\n",
       "  0.005648275837302208,\n",
       "  0.003957025706768036,\n",
       "  0.0028989745769649744,\n",
       "  0.0034412976820021868,\n",
       "  0.0025386640336364508,\n",
       "  0.0027844742871820927,\n",
       "  0.00412095058709383,\n",
       "  0.0034340624697506428,\n",
       "  0.0032781928312033415,\n",
       "  0.001565355691127479,\n",
       "  0.002586657414212823,\n",
       "  0.0009841774590313435,\n",
       "  0.004013713914901018,\n",
       "  0.003271569265052676,\n",
       "  0.001806852174922824,\n",
       "  0.004906586837023497,\n",
       "  0.002360799815505743,\n",
       "  0.002162911230698228,\n",
       "  0.0021878716070204973,\n",
       "  0.0028005363419651985,\n",
       "  0.003613966517150402,\n",
       "  0.0033320493530482054,\n",
       "  0.003622565185651183,\n",
       "  0.003312245709821582,\n",
       "  0.00381479412317276,\n",
       "  0.0026854886673390865,\n",
       "  0.005247952416539192,\n",
       "  0.0030533275566995144,\n",
       "  0.00400035222992301,\n",
       "  0.001885701552964747,\n",
       "  0.003116811392828822,\n",
       "  0.002794261323288083,\n",
       "  0.0021911109797656536,\n",
       "  0.0028387673664838076,\n",
       "  0.0034607851412147284,\n",
       "  0.003603923600167036,\n",
       "  0.00261403014883399,\n",
       "  0.0031373491510748863,\n",
       "  0.0024662804789841175,\n",
       "  0.0030371968168765306,\n",
       "  0.0043253665789961815,\n",
       "  0.0033898227848112583,\n",
       "  0.0016973582096397877,\n",
       "  0.0030230639968067408,\n",
       "  0.0026892204768955708,\n",
       "  0.0036461129784584045,\n",
       "  0.006045211106538773,\n",
       "  0.003770798444747925,\n",
       "  0.0012683682143688202,\n",
       "  0.004463219549506903,\n",
       "  0.0033383534755557775,\n",
       "  0.006133104674518108,\n",
       "  0.0033065134193748236,\n",
       "  0.0020040860399603844,\n",
       "  0.004347872920334339,\n",
       "  0.003410336561501026,\n",
       "  0.0030606426298618317,\n",
       "  0.003157456638291478,\n",
       "  0.0027719016652554274,\n",
       "  0.0030131435487419367,\n",
       "  0.0037702047266066074,\n",
       "  0.0027652450371533632,\n",
       "  0.0032437543850392103,\n",
       "  0.0017314510187134147,\n",
       "  0.0016613577026873827,\n",
       "  0.004413091577589512,\n",
       "  0.002821711590513587,\n",
       "  0.0021158596500754356,\n",
       "  0.0011439707595854998,\n",
       "  0.0029670502990484238,\n",
       "  0.0019584880210459232,\n",
       "  0.002730552339926362,\n",
       "  0.003706537652760744,\n",
       "  0.002571288263425231,\n",
       "  0.002539128065109253,\n",
       "  0.0033200683537870646,\n",
       "  0.003307976061478257,\n",
       "  0.002823856193572283,\n",
       "  0.003768594702705741,\n",
       "  0.0019261760171502829,\n",
       "  0.004187106154859066,\n",
       "  0.0032146568410098553,\n",
       "  0.002581457607448101,\n",
       "  0.003594573587179184,\n",
       "  0.0004884228692390025,\n",
       "  0.003096171887591481,\n",
       "  0.003664071671664715,\n",
       "  0.003215561620891094,\n",
       "  0.004755114670842886,\n",
       "  0.004653031472116709,\n",
       "  0.004556368570774794,\n",
       "  0.004114382900297642,\n",
       "  0.004764792043715715,\n",
       "  0.004174147732555866,\n",
       "  0.003731367876753211,\n",
       "  0.006642203778028488,\n",
       "  0.0056378100998699665,\n",
       "  0.006054909434169531,\n",
       "  0.003972435370087624,\n",
       "  0.00461193872615695,\n",
       "  0.005108082201331854,\n",
       "  0.003673644969239831,\n",
       "  0.003664475167170167,\n",
       "  0.004432814661413431,\n",
       "  0.0021904578898102045,\n",
       "  0.0030865040607750416,\n",
       "  0.000714968831744045,\n",
       "  0.003125235205516219,\n",
       "  0.00256189308129251,\n",
       "  0.002011575736105442,\n",
       "  0.0025880488101392984,\n",
       "  0.002720741555094719,\n",
       "  0.0014681878965348005,\n",
       "  0.0010120102670043707,\n",
       "  0.0028371524531394243,\n",
       "  0.0023385488893836737,\n",
       "  0.001584388897754252,\n",
       "  0.0016741841100156307,\n",
       "  0.0017745229415595531,\n",
       "  0.0031063726637512445,\n",
       "  0.0032163735013455153,\n",
       "  0.0020073747728019953,\n",
       "  0.003184974892064929,\n",
       "  0.005039909854531288,\n",
       "  0.00373117346316576,\n",
       "  0.003438185900449753,\n",
       "  0.004145240876823664,\n",
       "  0.0053769187070429325,\n",
       "  0.004803848918527365,\n",
       "  0.003639986738562584,\n",
       "  0.0036228385288268328,\n",
       "  0.0030152692925184965,\n",
       "  0.001733467448502779,\n",
       "  0.0030300684738904238,\n",
       "  0.0024153003469109535,\n",
       "  0.002261683577671647,\n",
       "  0.003322884440422058,\n",
       "  0.002100386656820774,\n",
       "  0.0023975581862032413,\n",
       "  0.002246418735012412,\n",
       "  0.0016337994020432234,\n",
       "  0.002485095988959074,\n",
       "  0.0021161732729524374,\n",
       "  0.0026638449635356665,\n",
       "  0.0031202915124595165,\n",
       "  0.004427146632224321,\n",
       "  0.0044707199558615685,\n",
       "  0.005879755597561598,\n",
       "  0.005507482681423426,\n",
       "  0.006304208654910326,\n",
       "  0.00499248132109642,\n",
       "  0.006973999086767435,\n",
       "  0.004269130527973175,\n",
       "  0.004222685005515814,\n",
       "  0.005266259890049696,\n",
       "  0.00559433875605464,\n",
       "  0.005055943038314581,\n",
       "  0.0035934720654040575,\n",
       "  0.003113210666924715,\n",
       "  0.004361799918115139,\n",
       "  0.003080317284911871,\n",
       "  0.005389216355979443,\n",
       "  0.003795335302129388,\n",
       "  0.001112666679546237,\n",
       "  0.0023819096386432648,\n",
       "  0.003441608278080821,\n",
       "  0.002334628952667117,\n",
       "  0.0018888766644522548,\n",
       "  0.001736271078698337,\n",
       "  -0.0002925076405517757,\n",
       "  0.001688837306573987,\n",
       "  0.0033506930340081453,\n",
       "  0.001902119256556034,\n",
       "  0.003520331345498562,\n",
       "  0.001358583802357316,\n",
       "  0.0020232319366186857,\n",
       "  0.003791957627981901,\n",
       "  0.002708438551053405,\n",
       "  0.0032374016009271145,\n",
       "  0.0027716169133782387,\n",
       "  0.0049778311513364315,\n",
       "  0.005166169255971909,\n",
       "  0.003657594555988908,\n",
       "  0.004377557896077633,\n",
       "  0.0054910508915781975,\n",
       "  0.002706710947677493,\n",
       "  0.002525684889405966,\n",
       "  0.002667429391294718,\n",
       "  0.003891562344506383,\n",
       "  0.003615737659856677,\n",
       "  0.0037636749912053347,\n",
       "  0.003245964180678129,\n",
       "  0.0034716487862169743,\n",
       "  0.0031293632928282022,\n",
       "  0.003086304059252143,\n",
       "  0.0027708210982382298,\n",
       "  0.0026502057444304228,\n",
       "  0.0037728261668235064,\n",
       "  0.003300686366856098,\n",
       "  0.0036541258450597525,\n",
       "  0.002793758176267147,\n",
       "  0.001998373307287693,\n",
       "  0.0035341493785381317,\n",
       "  0.004617166239768267,\n",
       "  0.0042243944481015205,\n",
       "  0.003130638040602207,\n",
       "  0.003437791718170047,\n",
       "  0.005064012948423624,\n",
       "  0.004304655361920595,\n",
       "  0.003863800782710314,\n",
       "  0.003702387446537614,\n",
       "  0.006224493496119976,\n",
       "  0.0074462974444031715,\n",
       "  0.003366551361978054,\n",
       "  0.005773583892732859,\n",
       "  0.006977396551519632,\n",
       "  0.005622669123113155,\n",
       "  0.002851241733878851,\n",
       "  0.003943588584661484,\n",
       "  0.0032261312007904053,\n",
       "  0.00247037410736084,\n",
       "  0.003538232995197177,\n",
       "  0.0022343499585986137,\n",
       "  0.0010754831600934267,\n",
       "  0.002461113268509507,\n",
       "  0.0011683611664921045,\n",
       "  0.0036312215961515903,\n",
       "  0.0027623288333415985,\n",
       "  0.0036597454454749823,\n",
       "  0.0012510414235293865,\n",
       "  0.0001846719824243337,\n",
       "  0.0017046664142981172,\n",
       "  0.003944711294025183,\n",
       "  0.0032839947380125523,\n",
       "  0.0030131780076771975,\n",
       "  0.003394756466150284,\n",
       "  0.001806918764486909,\n",
       "  0.002245883923023939,\n",
       "  0.004260678309947252,\n",
       "  0.003201909828931093,\n",
       "  0.0015526812057942152,\n",
       "  0.003313428023830056,\n",
       "  0.0035769003443419933,\n",
       "  0.003020977834239602,\n",
       "  0.004060381092131138,\n",
       "  0.0013660617405548692,\n",
       "  0.0048501561395823956,\n",
       "  0.0038161282427608967,\n",
       "  0.002726981183513999,\n",
       "  0.003869066946208477,\n",
       "  0.005727076902985573,\n",
       "  0.0038161484990268946,\n",
       "  0.0044259196147322655,\n",
       "  0.0031513613648712635,\n",
       "  0.004959915764629841,\n",
       "  0.0035849495325237513,\n",
       "  0.00297481007874012,\n",
       "  0.0031662904657423496,\n",
       "  0.0032686355989426374,\n",
       "  0.004204117693006992,\n",
       "  0.003982063382863998,\n",
       "  0.003938463982194662,\n",
       "  0.0028797988779842854,\n",
       "  0.0048209140077233315,\n",
       "  0.005192643031477928,\n",
       "  0.004552919417619705,\n",
       "  0.004102726932615042,\n",
       "  0.004736209753900766,\n",
       "  0.003708376782014966,\n",
       "  0.004907386843115091,\n",
       "  0.005720191169530153,\n",
       "  0.006452261470258236,\n",
       "  0.0025112160947173834,\n",
       "  0.00485345721244812,\n",
       "  0.004657293204218149,\n",
       "  0.004172022454440594,\n",
       "  0.003365920390933752,\n",
       "  0.0034601835068315268,\n",
       "  0.003007335588335991,\n",
       "  0.0032913093455135822,\n",
       "  0.0020837283227592707,\n",
       "  0.0036819023080170155,\n",
       "  0.0024693021550774574,\n",
       "  0.004516211804002523,\n",
       "  0.0025207363069057465,\n",
       "  0.0026817149482667446,\n",
       "  0.002934523392468691,\n",
       "  0.004485054407268763,\n",
       "  0.002363495994359255,\n",
       "  0.0025949389673769474,\n",
       "  0.002805058378726244,\n",
       "  0.0025084661319851875,\n",
       "  0.003061402589082718,\n",
       "  0.003977440297603607,\n",
       "  0.0018876828253269196,\n",
       "  0.0028287942986935377,\n",
       "  0.004455516580492258,\n",
       "  0.004259685520082712,\n",
       "  0.003608174156397581,\n",
       "  0.002317083301022649,\n",
       "  0.0024990299716591835,\n",
       "  0.0026236309204250574,\n",
       "  0.003248888533562422,\n",
       "  0.0041589075699448586,\n",
       "  0.004232770763337612,\n",
       "  0.0029122887644916773,\n",
       "  0.0017331454437226057,\n",
       "  0.002445982536301017,\n",
       "  0.0019954664167016745,\n",
       "  0.002264741575345397,\n",
       "  0.003676449414342642,\n",
       "  0.0021304760593920946,\n",
       "  0.0026039183139801025,\n",
       "  0.0026705830823630095,\n",
       "  0.0021517262794077396,\n",
       "  0.0032912481110543013,\n",
       "  0.004399902652949095,\n",
       "  0.00391580443829298,\n",
       "  0.002515845699235797,\n",
       "  0.004644294269382954,\n",
       "  0.0031266063451766968,\n",
       "  0.006110517308115959,\n",
       "  0.0036536853294819593,\n",
       "  0.003226314205676317,\n",
       "  0.0052512213587760925,\n",
       "  0.005399263929575682,\n",
       "  0.007203623652458191,\n",
       "  0.006633131764829159,\n",
       "  0.006981679238379002,\n",
       "  0.007193082943558693,\n",
       "  0.005391028709709644,\n",
       "  0.003979201894253492,\n",
       "  0.005437340587377548,\n",
       "  0.005692473147064447,\n",
       "  0.004717056639492512,\n",
       "  0.004184873774647713,\n",
       "  0.00602635508403182,\n",
       "  0.002623853273689747,\n",
       "  0.00285340822301805,\n",
       "  0.0023312720004469156,\n",
       "  0.0008846673299558461,\n",
       "  0.003861563978716731,\n",
       "  0.0034383349120616913,\n",
       "  0.004105585161596537,\n",
       "  0.0019004044588655233,\n",
       "  0.0029883235692977905,\n",
       "  0.002810074482113123,\n",
       "  0.004886936862021685,\n",
       "  0.004209761042147875,\n",
       "  0.004481703508645296,\n",
       "  0.0038481191731989384,\n",
       "  0.004180630668997765,\n",
       "  0.004093531519174576,\n",
       "  0.0042208475060760975,\n",
       "  0.005196081008762121,\n",
       "  0.0031726022716611624,\n",
       "  0.003326083067804575,\n",
       "  0.002796802669763565,\n",
       "  0.002765008481219411,\n",
       "  0.0028634360060095787,\n",
       "  0.002942032413557172,\n",
       "  0.0023670108057558537,\n",
       "  0.0024912760127335787,\n",
       "  0.00251725222915411,\n",
       "  0.0018072567181661725,\n",
       "  0.0016040741465985775,\n",
       "  0.002710002474486828,\n",
       "  0.002134998794645071,\n",
       "  0.004074947442859411,\n",
       "  0.0027020182460546494,\n",
       "  0.0022303715813905,\n",
       "  0.003286644583567977,\n",
       "  0.0024767275899648666,\n",
       "  0.004229066893458366,\n",
       "  0.0050772810354828835,\n",
       "  0.0036021440755575895,\n",
       "  0.0052088359370827675,\n",
       "  0.005378097295761108,\n",
       "  0.0049197375774383545,\n",
       "  0.005795187782496214,\n",
       "  0.007993181236088276,\n",
       "  0.004712208174169064,\n",
       "  0.005186180584132671,\n",
       "  0.004131559282541275,\n",
       "  0.005208215210586786,\n",
       "  0.0037549021653831005,\n",
       "  0.004955165553838015,\n",
       "  0.004696530289947987,\n",
       "  0.0032292173709720373,\n",
       "  0.0035153625067323446,\n",
       "  0.002341444371268153,\n",
       "  0.0021850059274584055,\n",
       "  0.002600058913230896,\n",
       "  0.003802285995334387,\n",
       "  0.0015800902619957924,\n",
       "  0.0028969186823815107,\n",
       "  0.0022340433206409216,\n",
       "  0.0017753468127921224,\n",
       "  0.002717565046623349,\n",
       "  0.0031682660337537527,\n",
       "  0.0032557216472923756,\n",
       "  0.003411024110391736,\n",
       "  0.0020765550434589386,\n",
       "  0.0033976284321397543,\n",
       "  0.0015500168083235621,\n",
       "  0.005808514077216387,\n",
       "  0.004146321676671505,\n",
       "  0.003937175963073969,\n",
       "  0.0023405198007822037,\n",
       "  0.0032277500722557306,\n",
       "  0.0033588213846087456,\n",
       "  0.0021721443627029657,\n",
       "  0.00338873453438282,\n",
       "  0.002284954534843564,\n",
       "  0.0038080622907727957,\n",
       "  0.002817499451339245,\n",
       "  0.0036547956988215446,\n",
       "  0.003472649957984686,\n",
       "  0.005108885932713747,\n",
       "  0.0032872152514755726,\n",
       "  0.003684690920636058,\n",
       "  0.004774566274136305,\n",
       "  0.004805340897291899,\n",
       "  0.0015593798598274589,\n",
       "  0.0014164824970066547,\n",
       "  0.0027303691022098064,\n",
       "  0.0032165369484573603,\n",
       "  0.0011797191109508276,\n",
       "  0.0031143524684011936,\n",
       "  0.0038119007367640734,\n",
       "  0.003204232780262828,\n",
       "  0.00234601809643209,\n",
       "  0.004785582888871431,\n",
       "  0.00312853348441422,\n",
       "  0.004308620002120733,\n",
       "  0.0031099319458007812,\n",
       "  0.0051150270737707615,\n",
       "  0.005826911423355341,\n",
       "  0.005508107598870993,\n",
       "  0.008292121812701225,\n",
       "  0.006174107547849417,\n",
       "  0.006089744158089161,\n",
       "  0.005497664213180542,\n",
       "  0.007562295068055391,\n",
       "  0.005522402469068766,\n",
       "  0.004739030264317989,\n",
       "  0.003217545570805669,\n",
       "  0.0033974810503423214,\n",
       "  0.0035404684022068977,\n",
       "  0.0031041214242577553,\n",
       "  0.0017450477462261915,\n",
       "  0.004016289487481117,\n",
       "  0.002685398329049349,\n",
       "  0.0034116485621780157,\n",
       "  0.003194682765752077,\n",
       "  0.002603304572403431,\n",
       "  0.0025672076735645533,\n",
       "  0.0020197562407702208,\n",
       "  0.002633273834362626,\n",
       "  0.002551788929849863,\n",
       "  0.0009694154723547399,\n",
       "  0.0020244477782398462,\n",
       "  0.0037942458875477314,\n",
       "  0.003768590511754155,\n",
       "  0.002294484293088317,\n",
       "  0.001457581645809114,\n",
       "  0.002115693874657154,\n",
       "  0.004122345708310604,\n",
       "  0.004243115894496441,\n",
       "  0.0026203857269138098,\n",
       "  0.0034445286728441715,\n",
       "  0.0021302541717886925,\n",
       "  0.006872219033539295,\n",
       "  0.004959677811712027,\n",
       "  0.002194053726270795,\n",
       "  0.005218907259404659,\n",
       "  0.0019884370267391205,\n",
       "  0.004083006642758846,\n",
       "  0.001938068657182157,\n",
       "  0.002836070256307721,\n",
       "  0.0024389594327658415,\n",
       "  0.0035305703058838844,\n",
       "  0.003369351616129279,\n",
       "  0.0035435764584690332,\n",
       "  0.004701137077063322,\n",
       "  0.004561486653983593,\n",
       "  0.003475477220490575,\n",
       "  0.0038275469560176134,\n",
       "  0.0045011951588094234,\n",
       "  0.0044915759935975075,\n",
       "  0.00428564241155982,\n",
       "  0.004717805422842503,\n",
       "  0.004387276712805033,\n",
       "  0.006079291924834251,\n",
       "  0.004857351537793875,\n",
       "  0.0050601172260940075,\n",
       "  0.006034400314092636,\n",
       "  0.005276304669678211,\n",
       "  0.0023969518952071667,\n",
       "  0.005695456173270941,\n",
       "  0.00360371102578938,\n",
       "  0.004171959590166807,\n",
       "  0.002669410314410925,\n",
       "  0.003143743611872196,\n",
       "  0.002355106407776475,\n",
       "  0.003334520850330591,\n",
       "  0.001321745803579688,\n",
       "  0.0017962133279070258,\n",
       "  0.002779551548883319,\n",
       "  0.001638379180803895,\n",
       "  0.0037585198879241943,\n",
       "  0.0018847222672775388,\n",
       "  0.0014270623214542866,\n",
       "  0.0009862938895821571,\n",
       "  0.0019757235422730446,\n",
       "  0.0031584620010107756,\n",
       "  0.0020934785716235638,\n",
       "  0.0026873124297708273,\n",
       "  0.00367937539704144,\n",
       "  0.0014139292761683464,\n",
       "  0.0025008295197039843,\n",
       "  0.004900984466075897,\n",
       "  0.002654507989063859,\n",
       "  0.002580192405730486,\n",
       "  0.002811310114338994,\n",
       "  0.0025862064212560654,\n",
       "  0.004029544070363045,\n",
       "  0.0042703598737716675,\n",
       "  0.0026817431207746267,\n",
       "  0.002700145123526454,\n",
       "  0.00464993342757225,\n",
       "  0.0032620567362755537,\n",
       "  0.0028308751061558723,\n",
       "  0.00461663817986846,\n",
       "  0.0027440229896456003,\n",
       "  0.0019303661538287997,\n",
       "  0.001908842008560896,\n",
       "  0.0019564651884138584,\n",
       "  0.0020209040958434343,\n",
       "  0.0028644350823014975,\n",
       "  0.0019327627960592508,\n",
       "  0.003350474638864398,\n",
       "  0.003425620961934328,\n",
       "  0.003682308364659548,\n",
       "  0.0032905300613492727,\n",
       "  0.004495029337704182,\n",
       "  0.0023395780008286238,\n",
       "  0.004925821907818317,\n",
       "  0.0051958137191832066,\n",
       "  0.006254215724766254,\n",
       "  0.006024293601512909,\n",
       "  0.005760320462286472,\n",
       "  0.0046280259266495705,\n",
       "  0.007323811296373606,\n",
       "  0.0069196526892483234,\n",
       "  0.005142740905284882,\n",
       "  0.006545088719576597,\n",
       "  0.003593346569687128,\n",
       "  0.0041591571643948555,\n",
       "  0.004049072507768869,\n",
       "  0.0029114102944731712,\n",
       "  0.003520722035318613,\n",
       "  0.004251618403941393,\n",
       "  0.0025233039632439613,\n",
       "  0.002768824575468898,\n",
       "  2.884977584471926e-05,\n",
       "  0.000662792706862092,\n",
       "  0.0026578442193567753,\n",
       "  0.0017496466170996428,\n",
       "  0.0006082887994125485,\n",
       "  0.004180311691015959,\n",
       "  0.0011844704858958721,\n",
       "  0.001664513605646789,\n",
       "  0.003658076748251915,\n",
       "  0.0017777391476556659,\n",
       "  0.0036962723825126886,\n",
       "  0.003046816447749734,\n",
       "  0.0043487995862960815,\n",
       "  0.00562665332108736,\n",
       "  0.0026846183463931084,\n",
       "  0.003853077534586191,\n",
       "  0.002440420910716057,\n",
       "  0.0024209232069551945,\n",
       "  0.0027720327489078045,\n",
       "  0.0026452457532286644,\n",
       "  0.0033189004752784967,\n",
       "  0.003202894004061818,\n",
       "  0.003009960288181901,\n",
       "  0.002770675579085946,\n",
       "  0.0023003544192761183,\n",
       "  0.0018991141114383936,\n",
       "  0.0017311241244897246,\n",
       "  0.002240611705929041,\n",
       "  0.003192594973370433,\n",
       "  0.003694571554660797,\n",
       "  0.0020448327995836735,\n",
       "  0.004155212547630072,\n",
       "  0.0049283867701888084,\n",
       "  0.004838442429900169,\n",
       "  0.005461915396153927,\n",
       "  0.005331837572157383,\n",
       "  0.004411704838275909,\n",
       "  0.006434177979826927,\n",
       "  0.006941711530089378,\n",
       "  0.004328287206590176,\n",
       "  0.006226284429430962,\n",
       "  0.004907896742224693,\n",
       "  0.004961101803928614,\n",
       "  0.005189693532884121,\n",
       "  0.003996527753770351,\n",
       "  0.005998751148581505,\n",
       "  0.003349203849211335,\n",
       "  0.0031392823439091444,\n",
       "  0.004012300167232752,\n",
       "  0.003925321623682976,\n",
       "  0.0028347482439130545,\n",
       "  0.002780448878183961,\n",
       "  0.003128151874989271,\n",
       "  0.0042759752832353115,\n",
       "  0.0020482647232711315,\n",
       "  0.0016068958211690187,\n",
       "  0.0049565378576517105,\n",
       "  5.3426032536663115e-05,\n",
       "  0.0022533871233463287,\n",
       "  0.0026392987929284573,\n",
       "  0.00044360943138599396,\n",
       "  0.0025857454165816307,\n",
       "  0.0008897192310541868,\n",
       "  0.0038916384801268578,\n",
       "  0.008151155896484852,\n",
       "  0.001554696704261005,\n",
       "  0.0028593039605766535,\n",
       "  0.0013864069478586316,\n",
       "  0.0026262893807142973,\n",
       "  0.004114452283829451,\n",
       "  0.00274241017177701,\n",
       "  0.004571207333356142,\n",
       "  0.004681570455431938,\n",
       "  0.0030727488920092583,\n",
       "  0.006174057722091675,\n",
       "  0.004188490100204945,\n",
       "  0.0038675302639603615,\n",
       "  0.0018983049085363746,\n",
       "  0.0015763399424031377,\n",
       "  0.002954211551696062,\n",
       "  0.0018744278931990266,\n",
       "  0.0011869603767991066,\n",
       "  0.0017537346575409174,\n",
       "  0.003255788004025817,\n",
       "  0.003960778471082449,\n",
       "  0.0036110025830566883,\n",
       "  0.0020763070788234472,\n",
       "  0.001812248257920146,\n",
       "  0.004777070600539446,\n",
       "  0.002898668171837926,\n",
       "  0.002993111265823245,\n",
       "  0.0035603102296590805,\n",
       "  0.003550460794940591,\n",
       "  0.003441185923293233,\n",
       "  0.00660797068849206,\n",
       "  0.004462553188204765,\n",
       "  0.00590974185615778,\n",
       "  0.0053101698867976665,\n",
       "  0.005856535397469997,\n",
       "  0.005460512824356556,\n",
       "  0.0068878415040671825,\n",
       "  0.006858990993350744,\n",
       "  0.005899928044527769,\n",
       "  0.0043735867366194725,\n",
       "  0.005602996796369553,\n",
       "  0.0022819628939032555,\n",
       "  0.005403987597674131,\n",
       "  0.0046720849350094795,\n",
       "  0.0010597090004011989,\n",
       "  0.003169998759403825,\n",
       "  0.003754378529265523,\n",
       "  0.002690809778869152,\n",
       "  0.0008093794458545744,\n",
       "  -0.00012676379992626607,\n",
       "  0.0028383522294461727,\n",
       "  0.001732747070491314,\n",
       "  3.670144360512495e-05,\n",
       "  -0.00019485475786495954,\n",
       "  0.002393430331721902,\n",
       "  0.001836331095546484,\n",
       "  0.0034785361494868994,\n",
       "  0.0027849918697029352,\n",
       "  0.002595180179923773,\n",
       "  0.006645317655056715,\n",
       "  0.005642878822982311,\n",
       "  0.004386434331536293,\n",
       "  0.0036688896361738443,\n",
       "  0.0033673788420856,\n",
       "  0.002866811817511916,\n",
       "  0.004181135445833206,\n",
       "  0.0023967656306922436,\n",
       "  0.0030297839548438787,\n",
       "  0.0038667195476591587,\n",
       "  0.0035625870805233717,\n",
       "  0.0022059190087020397,\n",
       "  0.00204574060626328,\n",
       "  0.0034148250706493855,\n",
       "  0.002184587065130472,\n",
       "  0.0016344813629984856,\n",
       "  0.002438624855130911,\n",
       "  0.0026693223044276237,\n",
       "  0.0018718810752034187,\n",
       "  0.0028617018833756447,\n",
       "  0.0037279098760336637,\n",
       "  0.0036438684910535812,\n",
       "  0.004726596642285585,\n",
       "  0.00447317399084568,\n",
       "  0.0030103111639618874,\n",
       "  0.004940551705658436,\n",
       "  0.005905053112655878,\n",
       "  0.005481136031448841,\n",
       "  0.004367182496935129,\n",
       "  0.0038418693002313375,\n",
       "  0.006258076056838036,\n",
       "  0.004270690958946943,\n",
       "  0.004014011472463608,\n",
       "  0.004080628976225853,\n",
       "  0.005955381318926811,\n",
       "  0.004050370771437883,\n",
       "  0.0037546067032963037,\n",
       "  0.0034228255972266197,\n",
       "  0.0036154473200440407,\n",
       "  0.004267185926437378,\n",
       "  0.004581034183502197,\n",
       "  0.003146898467093706,\n",
       "  0.003542488906532526,\n",
       "  0.0018245166866108775,\n",
       "  0.0027536116540431976,\n",
       "  0.002662487793713808,\n",
       "  0.0026337774470448494,\n",
       "  0.001752452808432281,\n",
       "  0.003127587493509054,\n",
       "  0.0024178861640393734,\n",
       "  0.0014719385653734207,\n",
       "  0.0040750219486653805,\n",
       "  0.002473612781614065,\n",
       "  0.0019076768076047301,\n",
       "  0.0018678446067497134,\n",
       "  0.0010221698321402073,\n",
       "  0.002677061827853322,\n",
       "  0.0035550747998058796,\n",
       "  0.005050490144640207,\n",
       "  0.0028378332499414682,\n",
       "  0.003858362790197134,\n",
       "  0.003313260618597269,\n",
       "  0.003632482374086976,\n",
       "  0.00300271506421268,\n",
       "  0.004000946879386902,\n",
       "  0.0015823168214410543,\n",
       "  0.003122438443824649,\n",
       "  0.0017514970386400819,\n",
       "  0.002502766903489828,\n",
       "  0.0016150400042533875,\n",
       "  0.0017245605122298002,\n",
       "  0.002765640150755644,\n",
       "  0.0018883747979998589,\n",
       "  0.0030618044547736645,\n",
       "  0.004790457896888256,\n",
       "  0.0033301811199635267,\n",
       "  0.003723729634657502,\n",
       "  0.001093827886506915,\n",
       "  0.0017651356756687164,\n",
       "  0.0032479772344231606,\n",
       "  0.0031867411453276873,\n",
       "  0.004350714385509491,\n",
       "  0.0051332195289433,\n",
       "  0.006729284301400185,\n",
       "  0.0050881593488156796,\n",
       "  0.005662134382873774,\n",
       "  0.00406799279153347,\n",
       "  0.004462779499590397,\n",
       "  0.005650199484080076,\n",
       "  0.00690314918756485,\n",
       "  0.0048820567317306995,\n",
       "  0.0044620526023209095,\n",
       "  0.0054589551873505116,\n",
       "  0.005736254621297121,\n",
       "  0.00447879871353507,\n",
       "  0.0039977929554879665,\n",
       "  0.004353964701294899,\n",
       "  0.0035882533993571997,\n",
       "  0.002785223303362727,\n",
       "  0.0026581590063869953,\n",
       "  0.002961772494018078,\n",
       "  0.001577940071001649,\n",
       "  0.0021352143958210945,\n",
       "  0.003117311978712678,\n",
       "  0.0020778027828782797,\n",
       "  0.001274589914828539,\n",
       "  0.0026795067824423313,\n",
       "  0.0002880601678043604,\n",
       "  0.00031372124794870615,\n",
       "  0.0006267569260671735,\n",
       "  0.0026369760744273663,\n",
       "  0.002648397581651807,\n",
       "  0.0028573041781783104,\n",
       "  0.002737801754847169,\n",
       "  0.00532520841807127,\n",
       "  0.005616253241896629,\n",
       "  0.0031207860447466373,\n",
       "  0.005701818969100714,\n",
       "  0.00321966758929193,\n",
       "  0.0032383438665419817,\n",
       "  0.004432317800819874,\n",
       "  0.0032871896401047707,\n",
       "  0.0031252240296453238,\n",
       "  0.00446214247494936,\n",
       "  0.0025799369905143976,\n",
       "  0.0015547771472483873,\n",
       "  0.004326433874666691,\n",
       "  0.001918973634019494,\n",
       "  0.0022395895794034004,\n",
       "  0.0027729959692806005,\n",
       "  0.0031375812832266092,\n",
       "  0.0027691717259585857,\n",
       "  0.0031059342436492443,\n",
       "  0.0024777567014098167,\n",
       "  0.0028473094571381807,\n",
       "  0.002386218635365367,\n",
       "  0.002923292340710759,\n",
       "  0.003294178517535329,\n",
       "  0.0032332257833331823,\n",
       "  0.002683514729142189,\n",
       "  0.0039595854468643665,\n",
       "  0.00430819159373641,\n",
       "  0.005923610646277666,\n",
       "  0.005854011978954077,\n",
       "  0.006436590570956469,\n",
       "  0.00646237563341856,\n",
       "  0.004470727872103453,\n",
       "  0.0055313026532530785,\n",
       "  0.005288914777338505,\n",
       "  0.005886164028197527,\n",
       "  0.005268429405987263,\n",
       "  0.004593344405293465,\n",
       "  0.003411112353205681,\n",
       "  0.004636775236576796,\n",
       "  0.003936787135899067,\n",
       "  0.0028771848883479834,\n",
       "  0.0032134847715497017,\n",
       "  0.0034965770319104195,\n",
       "  0.0032286085188388824,\n",
       "  0.002875964855775237,\n",
       "  0.00041627665632404387,\n",
       "  0.0035800558980554342,\n",
       "  0.001859521958976984,\n",
       "  0.0031594280153512955,\n",
       "  0.0007483534864149988,\n",
       "  0.001401072251610458,\n",
       "  0.00021149791427887976,\n",
       "  0.002266914350911975,\n",
       "  0.00316704367287457,\n",
       "  0.0005780123174190521,\n",
       "  0.0034427414648234844,\n",
       "  0.004427604377269745,\n",
       "  0.0023944214917719364,\n",
       "  0.003736269660294056,\n",
       "  0.0031972392462193966,\n",
       "  0.002976370509713888,\n",
       "  0.0026564563158899546,\n",
       "  0.0028868126682937145,\n",
       "  0.00354773853905499,\n",
       "  0.0021398658864200115,\n",
       "  0.004570331424474716,\n",
       "  0.0035488950088620186,\n",
       "  0.003334944136440754,\n",
       "  0.0022181333042681217,\n",
       "  0.003218760248273611,\n",
       "  0.0033770392183214426,\n",
       "  0.0017835868056863546,\n",
       "  0.0021973433904349804,\n",
       "  0.003474742639809847,\n",
       "  0.002564233262091875,\n",
       "  0.0014411048032343388,\n",
       "  0.0014585538301616907,\n",
       "  0.003315160982310772,\n",
       "  0.001054861000739038,\n",
       "  0.0026071227621287107,\n",
       "  0.0026016526389867067,\n",
       "  0.002382790669798851,\n",
       "  0.003376794047653675,\n",
       "  0.003337795613333583,\n",
       "  0.0023907464928925037,\n",
       "  0.0040192678570747375,\n",
       "  0.00470307981595397,\n",
       "  0.005272794049233198,\n",
       "  0.004512385930866003,\n",
       "  0.0043393392115831375,\n",
       "  0.0052864281460642815,\n",
       "  0.004257704131305218,\n",
       "  0.005494413897395134,\n",
       "  0.004515180829912424,\n",
       "  0.006077095866203308,\n",
       "  0.0041527827270329,\n",
       "  0.0027857369277626276,\n",
       "  0.0022739889100193977,\n",
       "  0.003972521983087063,\n",
       "  0.0037172113079577684,\n",
       "  0.0017770456615835428,\n",
       "  0.00365011440590024,\n",
       "  ...])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.train()\n",
    "netG_neg.train()\n",
    "# gen_losses, disc_losses = train_GAN(netD_neg, netG_neg, negative=True)\n",
    "train_GAN(netD_neg, netG_neg, tr=train_100k, epochs=10, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gen_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEBCAYAAABi/DI2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXlgVNXVwH+TECCQELawC4rCBUQJiooKuKDWpXWpolbqrtSFr7Xar7VVW1tra7UqdcEuaK1Fqp9Wuyi4ILiLiIoKwhEX9iCQAIGE7Pn+eO8lb2bezLxZMjNJzu+fmXeX9859yz33nnvuvYGmpiYURVEUJZScTAugKIqiZCeqIBRFURRPVEEoiqIonqiCUBRFUTxRBaEoiqJ4ogpCURRF8UQVhKIoiuKJKghFURTFE1UQiqIoiieqIBRFURRPVEEoiqIonnTKtABx0gU4DCgFGjIsi6IoSlshFxgIvAfU+M3U1hTEYcAbmRZCURSljTIZeNNv4ramIEoBduyopLExsVVo+/QpoKxsT0qFygTtpRygZclW2ktZ2ks5IPGy5OQE6NWrO9h1qF/amoJoAGhsbEpYQTj52wPtpRygZclW2ktZ2ks5IOmyxGWa10FqRVEUxRNVEIqiKIonqiAURVEUT3yNQRhjLgBuBvKAWSLyYEh8CTAH6AG8DlwlIvWu+NuABhG51T7uATwEjLGTXC4iHyRXFEVRFCWVxOxBGGMGA7cDk4ASYIYxZkxIsrnATBEZCQSAK+28RcaYh4EbQtLfA2wQkfHAT7GUhaIoipJF+DExnQAsEpFyEakEngbOcSKNMcOAfBFZYgc9Ckyz/58BrAHudqUPAGcDdwCIyAvAZckVQ1EURUk1fkxMgwj2nS0FDo8RPwRARB4DMMbc6orvhzWT7xpjzLeAvcAP4xG6T5+CeJI3c9GtL3D4gQOYOa0kofzZRnFxYaZFSBlaluykvZSlvZQD0lsWPwoiB3A73gaAxjjiva7ZH9glIkcaY04EngWG+5IYKCvbk5Av8I7dNby4ZB3nHbt/3HmzjeLiQrZt251pMVKCliU7aS9laS/lgMTLkpMTSKhh7cfEtBFrDQ+HAcDmOOJD2Q7UA/MARORloMAY08+PwIqiKEp68KMgFgJTjTHFxphuWOMHLziRIrIOqDbGHG0HXQgsiHQyEakBXgbOBzDGTAQqsRSHoiiKkiXEVBAisgm4CVgMLAfmichSY8x8Y8wEO9l04F5jzGqgALgvxmkvB04xxqzA8mA6X0SimaUURVGUNONrHoSIzMM2CbnCTnX9/4jggevQ/LeGHJcCp8cjqKIoipJedCa1oiiK4okqCEVRFMUTVRCKoiiKJ6ogFEVRFE9UQSiKoiieqIJQFEVRPFEFoSiKoniiCkJRFEXxRBWEoiiK4okqCEVRFMUTVRCKoiiKJ6ogFEVRFE9UQSiKoiieqIJQFEVRPFEFoSiKonjiaz8IY8wFwM1AHjBLRB4MiS8B5gA9gNeBq0Sk3hV/G9AQui+EMWYI8DFwiIisTbwYiqIoSqqJ2YMwxgwGbgcmASXADGPMmJBkc4GZIjISCABX2nmLjDEPAzd4nDcHS6l0TqoEiqIoSqvgx8R0ArBIRMpFpBJ4GjjHiTTGDAPyRWSJHfQoMM3+fwawBrjb47w/xtrvWveiVhRFyUL8mJgGAaWu41KCtxf1ih8CICKPARhjbnWf0BhzKHA8cDIwM16h+/QpiDdLEMXFhUnlzxbaSzlAy5KttJeytJdyQHrL4kdB5ABNruMA0BhHfBDGmG7AbGCaiDQaY/xLa1NWtofGxqbYCSOwbdvuhPNmC8XFhe2iHKBlyVbaS1naSzkg8bLk5AQSalj7MTFtBAa6jgcAm+OID2Uy0B/4jzFmOVYPZL5JRFMoiqIorYafHsRC4FZjTDFQCZwNzHAiRWSdMabaGHO0iLwFXAgsiHQyEXkR2Nc5NsasBU5VLyZFUZTsImYPQkQ2ATcBi4HlwDwRWWqMmW+MmWAnmw7ca4xZDRQA97WWwIqiKEp68DUPQkTmAfNCwk51/f+I4IHr0Py3Ronb148MiqIoSnrRmdSKoiiKJ6ogFEVRFE9UQSiKoiieqIJQFEVRPFEFoSiKoniiCkJRFEXxRBWEoiiK4okqCEVRFMUTVRCKoiiKJ6ogFEVRFE9UQSiKoiieqIJQFEVRPFEFoSiKonjSIRXE2i0VmRZBURQl6+mQCmJvTUOmRVAURcl6fO0HYYy5ALgZyANmiciDIfElwBygB/A6cJWI1LvibwManH0hjDGjgT/Z6fcCV4vI8qRLoyiKoqSMmD0IY8xg4HZgElACzDDGjAlJNheYKSIjgQBwpZ23yBjzMHBDSPq/AL8TkRKs3er+llQpFEVRlJTjx8R0ArBIRMpFpBJ4GjjHiTTGDAPyRWSJHfQoMM3+fwawBrg75JxzgBfs/x8DQxOSXlEURWk1/JiYBgGlruNSgrcX9YofAiAijwEYY251n1BEHnUd/gr4l1+BFUVRlPTgR0HkAE2u4wDQGEe8J8aYAHAXMBE4zocczfTpUxBP8jCKivIpLi5M6hzZQHsog4OWJTtpL2VpL+WA9JbFj4LYCEx2HQ8ANofED4wSH4YxphPwGDAYOE5EdvmS1qasbA+NjU2xE0Zg184qtm3bnXD+bKC4uLDNl8FBy5KdtJeytJdyQOJlyckJJNSw9jMGsRCYaowpNsZ0A86mZfwAEVkHVBtjjraDLgQWxDjn77E8mE6KVzkoiqIo6SGmghCRTVieRouB5cA8EVlqjJlvjJlgJ5sO3GuMWQ0UAPdFOp8xphiYCRjgXWPMcmOMurgqiqJkGb7mQYjIPGBeSNiprv8fETxwHZr/Vtf/bX6vqyiKomSODjmTWlEURYmNKghFURTFkw6pIBL3f1IURek4dEgFoSiKosSmQyqI3z+hTlOKoiix6JAKQlEURYmNKghFURTFE1UQiqIoiieqIBRFURRPVEEoiqIonqiCUBRFUTxRBaEoiqJ4ogpCURRF8UQVhKIoiuKJKghFURTFE1/7MhhjLgBuBvKAWSLyYEh8CTAHa5e414GrRKTeFX8b0ODsC2GM6Qk8DgwHtgHnisiWpEujKIqipIyYPQhjzGDgdmASUALMMMaMCUk2F5gpIiOBAHClnbfIGPMwcENI+l8Db4jIaOAvwB+SKoWiKIqScvyYmE4AFolIuYhUAk8D5ziRxphhQL6ILLGDHgWm2f/PANYAd4ec8zSsHgTAP4BTjDF5CZVAURRFaRX8KIhBQKnruBQY4ideRB4TkTuAhkjntE1RFUBxXJIriqIorYqfMYgcgvfYCQCNccR7EfA4jpWnmT59CvwmjUhxcWHS58g07aEMDlqW7KS9lKW9lAPSWxY/CmIjMNl1PADYHBI/MEq8F5vsdBuNMZ2AQqDMhywAlJXtobExuX3htm3bnVT+TFNcXNjmy+CgZclO2ktZ2ks5IPGy5OQEEmpY+zExLQSmGmOKjTHdgLOBF5xIEVkHVBtjjraDLgQWxDjnfOAi+/95WAPWdXFJriiKorQqMRWEiGwCbgIWA8uBeSKy1Bgz3xgzwU42HbjXGLMaKADui3HaW4CJxpiVwDXAtYkWQFEURWkdfM2DEJF5wLyQsFNd/z8CDo+S/9aQ43Lg9HgEVRRFUdKLzqRWFEVRPFEFoSiKoniiCkJRFEXxRBWEoiiK4okqCEVRFMUTVRCKoiiKJ6ogFEVRFE9UQSiKoiieqIJQFEVRPFEFoSiKoniiCkJRFEXxRBWEoiiK4okqCEVRFMUTVRCKoiiKJ6ogWomvd1RRUxu6FbeiKErbwdd+EMaYC4CbgTxglog8GBJfAswBegCvA1eJSL0xZigwF+gHCDBdRPYYY3oBjwODgRpghogsT1GZsoKf/mkJo4f14n+/Mz7ToiiKoiREzB6EMWYwcDswCSgBZhhjxoQkmwvMFJGRQAC40g6fDcwWkVHAMqyd5ACuBz4RkXHAbcADyRYkG1m1bkfU+KrqOhYsWUdTU3L7ayuKorQGfkxMJwCLRKRcRCqBp4FznEhjzDAgX0SW2EGPAtOMMXnAFDt9c7j9PxcotP93B/YmUYY2y9yXP+OpV79g5VflmRZFURQlDD8mpkFAqeu4lODtRb3ihwB9gQoRqQ8JB/g9sMQYsxnLLHVi/KJnH/OXrGPsfr0Z2r8wdmJgb7V1a+obtAehZJatO6rYvbeO/QcVZVoUJYvwoyByAHcNFgAafcSHhuPK9wDwgIjcZ4w5EnjSGDNGRPb4EbpPnwI/yaJSXOyvEo+Hp1/9gqdf/YL/3n2Gr+t07mLd/h49uiYkT2uUIVNoWTLLZXcsAgh6d6FtlsWL9lIOSG9Z/CiIjcBk1/EAYHNI/ECP+K1AkTEmV0Qa7DROvjOAGQAi8o4x5mtgNPCeH6HLyvbQ2Jhcq/ucG5/jzquPpLBb56TO48W2bbs9/4dSW2P1IHZV7I2azovi4sK482QrWpbswS17Wy+LQ3spByRelpycQEINaz9jEAuBqcaYYmNMN+Bs4AUnUkTWAdXGmKPtoAuBBSJSB7wBnGeHXwQssP9/BJwJYIwZgWWm+ixu6ZOgpq6BzzfuSucllQ5OQ2MjFZW1mRZDUXwTU0GIyCbgJmAxsByYJyJLjTHzjTET7GTTgXuNMauBAuA+O/waLK+nT7F6ITfb4RcDlxljVgBPABeLSNpr60xb/gOBQIYl6LhcdscinnhlTVqvOe/lNVx3/5tU19bHTqwoWYCveRAiMg+YFxJ2quv/RwQPXDvh64BjPcLXAMfHKWvKeeCZT3jkxtSLoW6rbYOX3tvA+VNHpO1678tWAGrqGumaesumoqQcnUmtKOlGGxBKG0EVhE+eXLSm2dMjFnF//m2kvvhsw86knQMURWk7qILwyYtLN0SN92NWamhs5Pl31lJT1/bWaJL1O7jj8Q947u21YXFrNu7kznkfUN/QGJ5RaUHHnJQ2hiqIJKlvaKS+odFXJ2DJyq/552tf8u83vmp1uVLNjj01AGwuqwyLe/j5Vaxev5OyXdXpFqttoaYlpY2hCiJJZtz1KjPuejU4MEI9UGe3sPe2QS+WALFbv1r9+aSVexKNTU1Z4yhRXVvPZXcs4qX3ovfAlexEFUSq8PE9OtVC6Meb7k/5pfc28Mu/+pqTqLRBrvjdYu79v48yLQYAu6vqAFi4TBVEPDQ0NmbFdgGqIKJQUVnLpu3hJhUvmlzVfFOEKt+Z99CYYVP9E6+sYd3Xic0sXbpqa4ql6YCkoXW/IksWgMyOfkzb44//XsnV97yWaTFUQUTjp39ewi1z3g0K+3JzhWfaSN98XX1LKyDHVhBNTU08/PynLP98O2B1wzd7KCJZv4NN23wtT5VRdOjVJzpIrfjkfdmWaREAVRBR2VsTPlawZ29dXOf43u9fY6NdyTv1Q2MTvPXJluY0c55bxc1z3g1zIf3dvA+55eGlUc9fUVmbFtfT0Lptx+4atu8KXqU9Ebv33U98yANPtau9ohQX8arExsYm9YbLIlRBtAKh9eR6n+achjhtT5XVdVx3/5s8uejzuPKlghsefIsfP/RO0udZuXYHLy5ZlwKJ/JMtA7hKOPc+9VG404eSMVRBhFBT18C6LfHb56PVOX7rI/cOdF9sjr00VZW9n8SHazLXHb3v6Y/5ekfb2O/psw07eXJRetdfUuJDN8/KLlRBhPDwc5/yy0ffi2hKSo0Z2VtjzHrqY74qtcY49lTFZ8oKJV0eEM44Slvgjsc/4MWlGzI+cJrp6yuKX1RBhPCFPQgdfwUb+7N3lEu0lLuraoPSOlRV1/k2QW3ctoer73mNJSu3xE7cijz2wmquvHNxRmXIJlrcnDMqhpIGqqrr28WyNB1eQTgVskNLJe79cJua4KnFn4ety+Tno/c12czjPPUNjcyc9QZPvOJvrGHDVmtQ/OMvynylby1eXb6Zhmz8SDIkknPZjuTMlIVPv9Wpq29k5qzXeXxhWre4aRU6vIL4wX1vBh37mS+84N310VPE+iqijVd4nKO2zuo5vPVJaXiGxC7T5vmqtKLNersk04PYvmsvl92xiGWrdT5KtlJXb72XoT34xqamuL0gM42v/SCMMRdgbfaTB8wSkQdD4kuAOUAP4HXgKhGpN8YMBeYC/QABpovIHmNMD+AhYIx9istF5INUFChlxPkRR0vuVAjlu6tjpn3jo83khGkpK0eklmdHM1lsKa/itr8tY+qhQ5h+4si480fqHbYF1n9t9Q7fXrGFCaP6ZVia2KSis/Tq8k18+Nl2fnjuOH7+8LscNLwP0447IGa+Be+uY1Cf7pzgYw/nxsYmnnhlDScetg/FPfNTIHU4z7+9lmff+Iq7rz2aXoVdWuUaqSZmD8IYMxi4HZgElGDtEDcmJNlcYKaIjMR6J660w2cDs0VkFLAMuMUOvwfYICLjgZ9iKYuMUlvXYM97sCezxXsCHxn++dqXQIsJyIsP12xn1lMfB4W1WGmif263/e09/vzflc3HmbBkLFu91dO09ebHpXzwmeVt9dSrn3PPk8FzH/bWWGv2/GNhbC8jZ9vORLzNUsWXmytYkKB7bjJutk7WZM1UVbaL9Oeb0rORY2V1eMu5urbeV4v6sReET7603qmN2ypj9uDr6hu47I5FPLX4C/7w9MdR0zqs3bKbhe9v5E//WRk7cUy8n+8HayyHjp32wpdtAT8mphOARSJSLiKVwNPAOU6kMWYYkC8iS+ygR4Fpxpg8YIqd3h0ewNrX+g4AEXkBuCz5oiROfUMjP5r9Ntfe+3pzWLyfcHCrNDj3wvc3BJlD/JhGvCoAJ+iLzbv4bP2OsLRfle5mycqvI0hhXdftRvjD+99kVwIv621/WxYxbva/VjDrqfB1gB6Zv4oHnvkEgAVL1octBbHWruxfttfsufvJ5by6fJMveT76fDv/fct7hdxV63Zw2R2L2LqjqjksFT2uXz+2jKde/SKhvBvTODv+5WUbmj3j3Hy+qYKKylr+E+G+pZq9NZbTx9adLS7RP/njO3z/D2+k/FpVNfF78DlKO7W98eCPuNbHMv/rv95NRVX27FvuR0EMAtzG71JgiI/4vkCFiNSHhPcDaoBrjDHvGGMW4dPUlQzRWm1fl1c1t2TKKiwz0IY41yravL0qYtz6r/fw27ktFrR4vRsc2R1FcPtj73PDH1zKzOfpnnn9S+52tdx3VdaydNVW9uyt49ePLWP7Tn/zGbwqnGQJ1YcrvyrnsRfEV94/PP0xz0ZYQv3tFdarKRt2JiNeSpn11MdhzhH+cd4Ff12IfyxcE1Whp9PatuKrMm784zss+dSyze9O0pU7UUrLKpm38DPPOiEVDgSRbmlpWeQ6wuHWv77Hz0OW98kkfirmHILLHAAafcSHhmOHdwL6A7tE5EhjzInAs8Bwv0L36VPgN2kz0RREr17dw8KqG1rSF7tsmEVF3TzDl7gWsevTN9zmGW+l6r5O795WeWvrG4Ou2bu3JXdZRXVQeI/CrgA0NMHjr6xhxpkH0a1rHjv2hFdKO6rq+HTDLr7cXMHij0q55pxx3vJEWIPKjfs+uuXx879nT+/7+vy76ymvqOaiU8fwoWzlxCOGsXW3VY68vNyI53Po2jUPgMKCrs1hfftGzxMPXvmLiwvZXVVLYbfgjadzXINLBYX5FPfuFpo1JoWlVsOlS5dOzdfeU1XLBT9fwK9mHEnJyJZxiWj3pshu0HTu3CkornRXNTW1DRw2ZoB1vaJ8rr1zEd8/dzzjRhbHLW9DTksb1FHiW3fVhMnWENKr9rqvXbp1iRrvkNc1vFfsTn/TX96ltKySaScaBvW1vq2ySktZ5XXKTfqdyLeVf05OwPNcvXp1i3qNCpfijPR+pQs/CmIjMNl1PADYHBI/0CN+K1BkjMkVkQY7zWZgO1APzAMQkZeNMQXGmH4i4ss1o6xsT8KtcC/Ky8MXyqt0mV62bWvpTezaVeUZvre6pfL9cl1091LHyyEa7utsL9vTnM99Tbfc7vAKezB82SrL3NS9cy5nTh5OjcfaUhu3VNC30KrIqqvrgs7z+aZd/Obv73P3tUezuyL2ZkCR5PHzf+fOlvKu29BifvrnYsu1d9VX5Wwpr2LkoB7NaevqGiKez6HGnm2+e3eL/Nu3e+eprK6jtq4xrgHE0GsWFxey4I0vmP2vFfzswkM5YHBRc1yjqxIsK99DoCGyyaG+oZFOueEd/Ap7/avamno2l+7ih/e/yeFj+tPUBPNeWM3gXi0DrNHuzS7nPLX1QXE3PfQ2AI/ceDzFxYWskK1s3bGXPz/7MbdednjkG4H1jS3/fDsHDe/TLHuZq1f6ld3IqKqqDZPthw+8yd7q+qCwUH724JtR4x2cMSo37vT19n3fUV5Jnl0v7HDeqfqGqOf2gzPe0tTY5HmuHTuq2Noll43bKtmnX/TG7tatFUG9xeLiwoTky8kJJNSw9mNiWghMNcYUG2O6YY0fvOBEisg6oNoYc7QddCGwQETqgDeA8+zwi+zwGuBl4HwAY8xEoBJLcWQEL9URqQu/N5J903WSWGsUxTs3INm1g6JdbsVX5RFNVIve3wjA6nU7Wt13333+V5dvDot3Pnr3WE88A6x+7uCPZr/NDQ++5fuckVhljw9FW4Nrl0dvzmHdlt3MuOtVPlyzjSq7sqmrb2TNxp0t5QjArsoaqmrqefVDa6xmx27/40mt8TzfXfU19//zk2Z5/FJVXceuPbXUxmg4+V3TLNM431NVTX1E1/TFH27iF48sZdXa6EuLZNrfLqaCEJFNwE3AYmA5ME9Elhpj5htjJtjJpgP3GmNWAwXAfXb4NVheT59i9UJutsMvB04xxqzA8mA6X0Ra1ak9uhuqR2yEDyiSl0N1HDOv/VX4LQJc/0DsSuvr8sj2Td8KJrTMrkmDfm3eWYeH2O7b8eXmCp5/Zy2QuuVJmj2NoqR59IXVYWE/vP9NXl2+iS/tdbju/+cnzJz1BmW7qnnilTX8du4HlNrLwnude0uUdyASX++IP08ktu20emmfxRjv2by9Mmiflfv/+UlC16tvaKTUYwvcWHh+DnZYPE4be/bW8c6K6KsVPPz8Ks9wxwNva6xxvwxrCF+DwyIyD9sk5Ao71fX/IyCs/2n3Lo71CC8FTo9T1lZjmYdlq6h7i/3YTwX7ZRxjDJXVqd9yNNpAZKKuke6lIfzkTdW7nM7VVn/9mHXfTpk4LKH8TU0eyrPZqSAk3H3sUcRdlbU89oJw4UnBczvKKqpZv9WqUPZ4uIsmg1Opx0NFVS1/f1G49JTRdOvaUoVssQdhl8XYy2DFV+WscA3E+nYgCBB03x57QXjzk1JuOK+Eyuo6Dh/dP+Ypvti8K2oPvqzCv4J46F8rWLVuB/sPKaJfgnMnsr3h1ereQ22BLeUtWnxovwLWb90TNFlm0QfeXWZ3RZbp51zlMb7gEMuFr9lLyhX2y7++F7TrXGu8yG4feL/n/83f3+eCBCbHBZM6BeSlPCNOW3E9gNyQ2ZBBStHjXoQ9uxQ+j3iXmZ//zjrel23sP6iIk48YyhsfbeavC8J7RJDiuTiue7Buy27etM03jmdeLAXx9Y4qbn/s/ZSJ48xnqA8xjaWygWOZVDNXuXQcBRF1Oe6WyJzwacw8/rL3mirBrlvZ0xIIlWTrjr1RJyS1mERacoZuSZqq0tW4fMHf9+eTAFjLFIDlKrhpW/xmBfdErWgmhnhpbGoih9DK3vp1Ql9cup4nF31OQX5ec5pQhRjqBhh8vqYwmRN9Hlt37g1z8Hj+neT244g1cS0RnAbKvTOP9ox/8Flvs9SPZr/FQcP7eMYlu0KyQ1V1Pdt27o3c4PJxDnealV+VM7R/QZjXG1jvUk1dAw0NjXTrmhcW39p0+LWYwPtji+o77uB6ypmYHeln1jHA+59t46d/eidio/Mfr7Sc552VWzwXIkxVD+LNj+NbT8oh0ZUxHamfWtwyqc1bPwSHbt25l9v+tsxzBnBQPjvbM69/2ezK7DQ4HKX8zOvWDPo6lxdT/LfT6eUFmvN7mUqi2eQrKmu58Y/v8LM/Lwl658t9eKi5ibWgJVit+lhjEdFwGigPRFAE23d5y1xeUcNrHk4OEGMcMo4Wwr1PLeeXj77XfBz2LON4VesbGrn7yeX8aPbbEdPcMuddZs5K/YRCP6iCIPEN3t0vVbpWLXXPwo5nLwa/4x7zPVqT67bs9jcGEa93lvu/qznm1TJz39/qWu+yzLhrMf99ey2rXRsveeE1SS30ms+9vZavSiuC9gbetaeGW+a8G7TVqiP3c2+vbW5UOOdyllbxunXxVCqBQMDznjz31tqwsJv+EnmS1V1PfBj5IiE88/oXQc/E/WgDLd4LEVn5VTkP/WuFp8tpPFTuTf14XSy27qjixaXr2bh1D1+4POXq6hu4+4kP+WKT3RCIkN8r/M55LRNld+6u4W17y2HnFtfVN3oq6qamyMowHXQYE1O0FkI8HkhB58yAh0HQuADxWUZiKbEmmjxrs1c+2Mi4Ed7ddjcL398QhzTx3b9GV+J/RZg1Xd/QxLN2a/2RG4+PeK54l0HftL2SwX278/aKLWzaXsmi91vGpLzK4Kc1+lXpbm55+F1uu/wIIHg3QU/Pq5DjQCDA7jhWBl3xZRmlUWb7h/Lc2+v49vGmWZEFLQ/is/ezq7KW2/+enM2/KcL/1uSufyxvXlHB4aJvGPr37sbKtdEbH+A9BrF6fUtv6v5n4usVZZIOoyD8kqiySDWNEWpP9zhBpA/mvQhLQceqGKNV2H5MTOVxeIBA8IQmt/LyNAElWDv4NeVEOv+rH27i0ZAB2MaglrWnhohwkeBD91iKewmUvSG9vaampua87vLkxCjc4g82Nv+/5/8+Cr4Xrv+RlolpInzsw82uytq0LfaXLI1NTfwmirJyl3Ovh8PHYy+K75WDE31Xved5ZNbPVU1MIcTjS96aPYj7IqxC6afC+3BNYnMOG5uaIjYO735ieYQYF24vTtfNcdvx3YsFuu3lqR7ij2Zm8n5u3g9zrceKse5K0UtBuEPWf727uXA1Hou17aqsDRvz2ewxjhDaKwkAsiF6a/bvL4U4V7hO4Z7zEal9Sb2uAAAfyElEQVSSL99V7d3rtINeem8Dv/n7+5l21Y/JuytKueJ3i8MjInxMkTwCw5xVUlABxPB8zvhS/h1GQbTOjU7/02tNd9qGhtTtlOA+z/+4BtjcYyjuusdP1z0e7vyHZW/fsdufDdz9flRU1Xq2Ih2+dK1LtXzN9qBVOs/68X+DzjX7XyuiXnfz9nBlENpbCwQCzTe02eU6EGVWfwTcz+TBZ6PLBfC/97/Bb+eGt7pfWJp6r6WIpODDdZZrSfW5ndyfrt0RpHBDTUx+VyVeuurrsLBMK98OoyBagzjdx1NCa7rTvvlJaUIupM34MBgHBbs+pPnuvRVSpM0bG5ua9xGIhfuKDQ1NQYPT0aiqqeequ19rPrYUYMvZynZVN+8I6MW/3/QeTwmSralFcTsK1r2se2uSlVvGxomX63pKsG/N4y9/xtX3vBYxWbRVid2SOYPfXtfIFKogkiDaxj8dnX++HmGvBFflH2vGbbJccaeHWQHvQUQv848fKj0Git3jWLEqWL+uoJk2NWSKVJQ71lhNokTbtz5V/OC+zLi3OuggdRJEGkhuTTI9YzsqLtkWLPE2Q/jyAkmVPBHw2ujnGdslFeKbCfsfD1fTRMeAHLwfcRZriFb8DmKuVeSDiN+MKyJVs5+XrvqaP/7b/650Xkvwu4m1gGFrowoiCTLR/U7nbmRxk8V1mBuv5dbdYw6lCSx6l0pCKzTZsJNtWegC2eaxPZuKCjoz+eBBKTllPMoBrPkz2YwqiCRw9lhOJ3Oe814dsl2RAUXjvuS/I8yzyBSR5n1kC9neLvg4yoRSx4PL75iTm0QWOmxr6BhEErzy/sbYiToQic5IT4bHXvS3LWk8ZN63PzvtiOlcZVfJDjqMgtB3u+0Qz6OKd3OaSCyLMLkwE2TrONPlv1vsuXTG1h3JjxOkgngV2I1/WtJKkrQO23buTXrpknjpMApCaTvUN2R2YE6JzHX3vxk7UYZo723An/zxHWb8dmFar+lrDMIYcwHWbnB5wCwReTAkvgSYA/QAXgeuEpF6Y8xQYC7QDxBguojsceUbAnwMHCIia5MvjtIecDae6ahEWo1UiUF71xB4u1W3JjF7EMaYwcDtwCSgBGsL0TEhyeYCM0VkJJYB9Uo7fDYwW0RGAcuAW1znzcFSKuGLoLcKHeDtaSe8n4HBf6Xtkwm3czeJzqXJZvyYmE4AFolIuYhUAk8D5ziRxphhQL6IOAa9R4Fpxpg8YIqdvjncdd4fAwuB5JzGFUVRyPw44wqfs/bbEn5MTIMA9y4vpQTvP+0VPwToC1SISH1IOMaYQ4HjgZOBmfEK3adPQbxZgtbLURSl/fGMj2VLWpPCwsT2pY6X4uLCtFwH/CmIHMJ3RGz0ER8aDtBojOmGZXqaJiKNxpi4hS4r2xP3DmOqIBSlfbPs0/SsTxWJXRXp8ebats1rWfDo5OQEEmpY+zExbQQGuo4HAJt9xG8FiowxuXb4QDt8MtAf+I8xZjlWD2S+SURTJEC/3t3ScRlFUdJM6CY/6aY9zhPxoyAWAlONMcV26/9s4AUnUkTWAdXGGGd38QuBBSJSB7wBnGeHX2SHvygi+4pIiYiUYCmNU0Uk9TOePCjumZ5uoKIoSlsnpoIQkU3ATcBiYDkwT0SWGmPmG2Mm2MmmA/caY1YDBcB9dvg1WF5Pn2L1HG5OdQH80v50u6Io2UQ77EAQaGPdon2BrxIZg6ipa+Dqu1/jwOF9WNkOvQ0URekYRNtvPRKuMYj9gLW+88V9JUVRFKVD0HEURJvqKCmKomSejqMgFEVRlLhQBaEoiqJ40uEURBsblFcURckYHUZBRNpgXFEURfGmwygIh0C27saiKIqSZXQ4BaEoiqL4o8MoCB16UBRFiY8OoyAURVGU+FAFoSiKonjS4RREjg5SK4qi+MLPhkHtikNH9WNg73x2VdbyboY3GFEURclmOlwPIjc3wPlTR1DYLS/ToiiKomQ1HUZBhHoxBVBTU6rIzdF7qSjtEV8mJmPMBVib/eQBs0TkwZD4EmAO0AN4HbhKROqNMUOBuUA/QIDpIrLHGDMa+JOdfi9wtYgsT1GZYmBVZjqzOnU0qg+xorRLYvYgjDGDgduBSUAJ1g5xY0KSzQVmishIrBr4Sjt8NjBbREYBy4Bb7PC/AL+ztxy9CfhbsgVRMojqB0VJC/16d0vr9fyYmE4AFolIuYhUAk8D5ziRxphhQL6ILLGDHgWmGWPygCl2+uZw+/8cWva1/hgYmkQZfBJci6mJKXV0BP0wpLgg0yIkxGWnjs60CO2Ws48ZnvZrzvnZCWm9nh8FMQgodR2XAkN8xPcFKkSkPjSfiDwqIg12+K+Af8UvemI4Xq5DirsndZ5h/QtTII3ih9OOHJZpEchpo6N1BwwpSip/z4LOKZKk/dEpt3VfCq+xvXSvJednDCKH4EZiAGj0ER8ajjufMSYA3AVMBI7zLzLO3qpxsWdvXfP/4uJCzpo6knGj+nPdva/5yn/LZUdw2yPvNh/vN7iIdV/vjluOdJGbE6Ahjn27O+UGqG9Ivi8w+8fHc/2s16iubYid2CfFvcOVec+CLuzcU5Oya8QiLy8XgPNPNDzxsqTtusnSq1dyJomcNqgZB/Xtzubtla1+ncLCrgnnvenSw7n9r0ujpomkC4qL09c49aMgNgKTXccDgM0h8QM94rcCRcaYXLu3MNDJZ4zpBDwGDAaOE5Fd8QhdVraHxjgqP4Cq6hYFsW2bVbH36JLrO3/XkKRnT9mPVz/YGJcM6aRnQWfKKvxVoPf9YDL/O/tt6hv8V+oHDe/DJ1+WhYWXl1em3ORUVVUbFva7q47ke79/NcVX8qZfz3zq66y2zcjB2dNzHDW0J6vX74yaprw8uYqysbExdqIsI56GUTKMGJj4u7B//wIeuG4yK74q54//XtkcfujIYt7/bFvUvE79FQ85OYGEGtZ+mgcLganGmGJjTDfgbFrGDxCRdUC1MeZoO+hCYIGI1AFvAOfZ4RcBC+z/v8fyYDopXuWQKMm+MqFdu+5ds30eReyu6OSDB/LIjcdTkJ8X0aur5IC+KbhScozYJ9xMEtq6GrNvL8+8Ew/sn/T1C7sHP+suef4bFsnQp0f0Fur4kcWe4W7TRGG3YBNRa5tFsoHW8Lq+5JRRYWHJ3stuXfPoWdCl+XjywQO52HWdbHAOjFlCEdmE5Wm0GFgOzBORpcaY+caYCXay6cC9xpjVQAFwnx1+DZbX06dYvZCbjTHFwEzAAO8aY5YbY9Lk4pp4ZeaV79ZLD+PG6YckI06r4cdUGZQmwstYkN9SOU47dn/PND++YHys03Db5YfHlKFfr3zPNPsPKuKO702MmA/gR+ePx4v9BvaIIJF/rjnzIDrnWZ9KgAA/v2QCF540MijNhFH9kr5OKImamwtck0AL8vN48IdTEj5nW5vjMrBP63j59CnyVtb94/AqOjBCI8bh0lNH07lTdilwX/MgRGQeMC8k7FTX/4+AsBrA7l0cm+h1M83Y4b1Z8WW5deDxnQztX0hNXWK29qPHDuCtFVvCwi88aSR/f+mzuM51+tH78p+31iYkh8OUkkEsXBZuMjtq7ADe/MTyQRixT0/PvEPdA/ZNTeR3zqXGHoPYf3AP9tY0MDjEC8jpSo8Z1ouVa3cwYVQ/9lTVsnXHXs9r5Lps4ddNGxfUoxvc15/DQY/unamoDDdXhXLkgQMYMaSIx14USg7oS6/CLlx1xlhe/2gzQ/sXEAgEGNine9Bz+tZR+3LKEUNZsHQDy1a1LOEyelgvVq3bEfV63zh8H15cusFXGfwyZdwgPrBNFfldWj63aNX9qROHMX/JuqCw3j26+jZVpoN9BxSydstuuuTlRvz2nPXWTjpsH2TDTmacdRB5NFG5t55fPvoevQq7cPe1R3PZHYt8X9d934q6d2aX/R7dfNGhfLW5gnv+76OY5zh2/BBWrt0R1OhynxMSbxS0FtmlrlqRRLprP5w2rvl/pOfmhOcEAjz8k+M4YcKQCCkjZAzhuEO88x8ZxVRyyhGWl4+7tZwTZ8vvO1NH8K2j9g0LHzUseqsnlCbge6cfCFi2+5sunMCvrzjCksl++/v3yueY8YOa0z/zu29y1ekH0ttlUonUkurdowsH798nrp7gcYcMZvLBA5n1P5PC4rp0DjcX7TugkKKC4A+2V2EXzpi0X0QvkgDW/Xd6Gg5dPc4fynnHj/A+Z4xC5tn3qGvnXI4pGRQUd8kpo7jvB5O9snmSEwhwjkcPMdH6qlNuYjn7R+hFOjTf/5DTH+buwdlxkw4ayC8uOYyDDyimb1E+PexKuFtX7/bplHEDPcNDLzd8kPWddc7LoXvXPMYO7xNVZgfnngzrHz4W0NJ7zi4N0WEURDNx3P+gyiDG1xoIWOkvOGFk1HQtYsT3IkTrynbpnMuvrziCn1wwnl9dZnXk/JkGWtIEAgHOmuLfrzuaD3izXTVEBCfP+BHFQeXP65RLTo73HbnwG8bzGoFAoLmsDn/4/iTuvvbo4HTAhScZLo00H8Cj4RAIWIPwx44fzHdP8r5+eCbr55qzx/HNo/bl4pMN/Xt3o2dhl+j5QnCbSI48cIBnmtnXT+G0I4cx6aCBfP+cg/nVZYdz8ckuG3m0xpDrJk89NLwxcm+oEk2gSXv60fvy0A3HNB9ff+64mPMxhg3wN+Dr5VR1ySmjOH3Sfs3HjsSht6FXYRe+c8KIoIafG78upDO+dSC3XDwh7nHI/C6duHH6IVxz1kER07hFOPmINEwPi0HHUxAJUuQa7DvpsH2a/8fz/RzrbuWluKEwqG93OrsGT/30IBLtzu4/qEewWclNU4T/EFRmp0I4ZWLLHIdBtqnoym+2TNQ/YnR/+1Sxu4CF3TrTq7ALfSPYi7040fUsm8UMBOiUm8NF3zD08lvB2+IVFXTh21OGc0zJYH47Y2Lcj9mtyE6ftB/3/WAyF53coqT+9/wSunbuxNnH7E+n3BxKDuhL357RW91ePHT9MUw/cSQ/scePnJ6UY+qIlyu/1fLMzpw8PMgkOHZ4HyYdHN467+5qyffzWQb3cv0HDCli2IBCpowb1GwisCp5ezkdD7PBiRP2Ceqpugl9Vgfv790z6NI5N2xs674fTKZ3j9jvysh9egaZ/KJx5uT9YidqZVRB+MRtijh/qtskELsKcLr/3V22x3jH/iIlD/WuKO6ZT+e8HM6e4j2gnAzd7Bd7QEhvxq1o8vJyfCm/gvw8HrnxeA7ct3dz2DeOGMrPvnsoR44d0HyOUCXmPuxsP5PikMrlzquP4jczJlJU0JlDTfTBYy/TXbzmObBWCU4FBwy2PLYG9+1OTiBAQX4ex5YMbo4f7bpfEfEhiqNwzdBenHf8Adx80aHNcT+78FAuP210xFM5nm3GNSY1cUx83mK3XDyBWd9v6a14VeZedO3cUrn+7LuH8otLDrPy22EBWr6teM3KB4UohOumjWPU0OBxt5ERxuEK8vOCZEuW3JwAuTk5PHLj8Uw/0Z9VojXocAoikmlnzk/imqvXcj6P0zkfuYOXu+LwQcnNcHWYMi7Y9tylcy5/vOFYSkYk5p66r6ur/9D1lpngxAn7MGFUPwb17c71543juxHMPgD9e3VzdfGDv9ARg62PK9K4Rk4gEHPmr/uM/Xrm8z/fPogrvhm6NJilxO6dOSmsBzDUw/7r0LVzLkeM6c9REUw70RjYx3ug3GtAEqBPlNbmrZcexo3fTcI7LkrFGGhuXbeEfePwoUHyHzC4qNkm7m7tTj1kCCOGFDUvlR9LKU46KLJNf7+BPYJ6Gc1TF2J0ay+2e1OFEe4rtJhjQ8eDQvlmyJjb+BHF/OlHx3gnBn4zYyLXTTs4YryX5NeedVBQTymUwfaKDqfaPWmn+G6vqVQ1PhKhTXgTpYOcQIBrzzqIB5/9JOlz/fiC8dQ3NHLNPa8Hhbs/SvdSHam6bihzfnIcV/xuceQEHh/jzy85rNm7w+k1feeElh7T2P1aWllmn5707ekaWHY+yEB4JQSWSeCh64/xHBgOE83+3GKZwSLNBUiEHt07Nw+w+2HG6WOormlgXJS5Is4YROe8HGrrWiadTTvugKAJUm68zHduV9VkmHbc/sx96bOYlecBg4u46NTRHHJAH667700AptuuvVXV9fTu0ZWC/Dw+XRvsoeUe+7rstNFcdpq/taD8Tnzt3aMrl54yijFRelKXnjqKI8cOiKi0Hb49ZTjfnjI8yJspr1PkdzO05+yH/C65DOrbnTUbd3n2krp3tXrSDrk5OVxz5lj2H5yaBmSydJgehJ8u7HgfrW4/dMrNCepuOq3I7vne+tjxinA4aqzVgg0bMLQ5ycNu7kWi26uaCN3oUH4y/RAuP21MXAPufpRDNJJtS1195liOO2RwxDkX8TBxzACOHT846jjFlHGD+N7pB3L8+OAB4XifTX6XTr5t19E8p44/ZAiP3Hh8UOvdi0AgwLSpI+nRLXxMolvXTpwxab8gU1wgEOCsyftxy8UTwtI7RPPwc75PP3dl8rhBYfMSnAHmTp2sb8/PBM+4SKP/6YRR/YLeqUz6NXW4HkToc77l4gnNPvuJEE3vTDtuf4YP7MGIIT0JBGDSwQN5avEXVj6XHSBUpiu+OcbTbOJw/tQRvPSeP7/5Gd8aQ1OTNR+hvqGJm+e0rCcV6cX74bnj2BvHPcnLy2HfAYXNXfYC27vjiDjt0l4499dRuMm2rPr36saFJxl+9uclYXGt8SHmBAIcMaY/B+7Xm+0V1SxbvbUVrhLM9eeVtPo1vPjW0dEHVQ8ZUew51wZaGg7ub8FZTyy/Syf21tR75nMY1Kcbpx+9r+dgeCx+f81R1NZ7LynSv3c3Vq/f2Tz+Fo2jDhrQ/H07ZMFk6KTocAoilCBvhARqiE65AU4+YiiHjw4fDHXmJwAc4xpoDCVRzxE/TAyxp+d1yqHO+RgilLdzXm6QR1QscgIBfm4PFoLVwpx9/ZS4zhFGiGwF+XnceulhCXXzvfD6cJPt3USjID+Pa84cG3Fy1g3nlVBdG70S9EvooH1rkahC3X9Q+Oz2AwYXsXRVsPJ84Lop5OYGKNtVzU89FHqQLIEAZ05ObPntSF5NABecMIJDRhb7csM9+fChrPiyPGxiZLINj3Sv4OqmwyiI1tLkgUCAc487IO587p5HOl+AkgP68l4aWrHJenTM+OYY/vPW2qBKO6JrbZLcf91knlz0OWdOit4Cbg2cNaQO3M+Hd1KGuO3yw6n2mrWc4GvrXsPogeussZW37Nn6btOb8+wT8SpLFXmdcjnI50Q4xz06KKw1hEojHUZBOER7YO64eAYr2yrZ/PKOH1mc0gHoaHTvmpexjXVS6RoJhLllpoLQZVJCiXfszt1Yc2Y1O3MI+vXKZ1PIUt3xzGtJBZ1yc6hvSG4V266dc6mubWie7d5W6XAKwi+psJ9HI9LiX63Noaa4uQeRya6rknpmXz8lrau1Om9PYTd/M4qjvW6HjCzmR+eX0NQEH67ZHpIvwEM3HENDkpW2X+7/weSk91m/6GTD3poGy+U9Rd9ZJjZvUgXhgZ8Zkcni5R0SlZCX7KZLD4f6+AfXDx/dn4P378P/Lfo8K2ZqKqkj1b2RWEw8cACyfidn+ZyUuf/gIg41xXzbY0mXQCDAmH17s3JtuWfeLnm5kKZl1pMZi3I+0/zOnZg4xhr/G7lPEZ9t2ElRQXL1il9TVyrpMAqioGsekw8eyNgo7m+BQICrzjgwbKJbNtA7xJVy4tiBCW0cAlZFctHJ4evbK+kj3Z03P55A8dIlL5cZcZhiO+XmcG2UdYiAtu/2Y+MuxpmThnPU2IEJO1g4C1e2phNFJDqMgsjJCXDpqaMpLi6MWrEePrp1TUuJctTYATz8/KpMi6EkybVnHWTbp1NbWcfirquPbPFeU9JKTk4gKe+7w0f3p6yimhMO9Tf/KZX4UhDGmAuAm4E8YJaIPBgSXwLMwdol7nXgKhGpN8YMBeYC/QABpovIHmNMT+BxYDiwDThXRMI3R1DoW9SVo8YO0PGCdsKhxhp4f19a35PMTbes3wHRws+ijB2NnJwApx25b2auHSuBMWYwcDswCSjB2iEudBbXXGCmiIzEGru60g6fDcwWkVHAMuAWO/zXwBsiMhr4C/CHZAuSDvZLYg/aRLnz6qOa/btHDe0ZcflrRWkXqH7IKvy4PJwALBKRchGpBJ4GznEijTHDgHwRcWayPApMM8bkAVPs9M3h9v/TsHoQAP8ATrHTZzU3Tj+E+6/zvwlLqvnxBYdw3PjIE+4U/5x+9L4A9C7MjDeZ0s5pJ4rOj4lpEFDqOi4leHtRr/ghQF+gQkTqQ8KD8timqAqgGNgcbwHSSV6n3KiLeflh/Ii+zW58F37DULm3LhWiKXFy5IEDIm7Ik078LnPdUXDuRqLriGUaZ5XZvBiLIbYV/CiIHIL1YQBo9BEfGo4rX+jTDz1nVPr0iT5xJxbFxfGbihLJ48WvrmrZ8ezck8I9ieK5TqpkygY6Wll6lFqOEl26dMrqsqdbth7bqwAoGVmc0munqxzf/84hjF22gWMmDG21ccN0PhM/CmIj4LarDCC4pb8RGOgRvxUoMsbkikiDncbJt8lOt9EY0wkoBMr8Cl1Wtsf38sChxPJiikSiLqWtdZ1Ey5GNdMSyVFTsBaCmpj5ry56J51Jgt7zHH9AnZddOdzmOMMVs376nVc6daFlycgIJNaz99IMWAlONMcXGmG7A2cALTqSIrAOqjTFO0/hCYIGI1AFvAOfZ4RcBC+z/8+1j7Pg37PQdlktPHRW2gYnSnmmbJpTWpk9RVx7+yXEcHWWzISV9xOxBiMgmY8xNwGKgMzBHRJYaY+YDPxeRZcB04C/GmB7AB8B9dvZrgL8ZY24G1gPfscNvAR41xqwEdtr5s5KJY/rz9Y69rX6dyQcPip1IUToA6tKdPfiaByEi84B5IWGnuv5/RPDAtRO+DjjWI7wcOD1OWTNCPDNFFcUvg/paE6ei7UanKJmmw8ykVpRsYmCf7jz4wylRd39TlEyjCkJRMoTfLUQVJVO0D2ddRVEUJeWoglAURVE8UQWhKIqieKIKQlEURfFEFYSiKIriiSoIRVEUxZO25meXC9a6IsmQbP5sob2UA7Qs2Up7KUt7KQckVhZXnrgm3gTa2HLDk7DWd1IURVHiZzLwpt/EbU1BdAEOw9pLoiHDsiiKorQVcrFW1H4PqPGbqa0pCEVRFCVN6CC1oiiK4okqCEVRFMUTVRCKoiiKJ6ogFEVRFE9UQSiKoiieqIJQFEVRPFEFoSiKonjS1pbaSBhjzAXAzUAeMEtEHsywSJ4YY3oAbwPfFJG1xpgTgHuAfOBJEbnZTlcCzAF6AK8DV4lIvTFmKDAX6AcIMF1E9mSgHL8AzrUPnxeRH7fhsvwKOAdoAh4WkXvaallsGX8P9BWRS+KV1xjTE3gcGA5sA84VkS0ZKsdiW746O+h7wP54fOfxPq80l+NbwC+A7sBLIvKDbHm/OkQPwhgzGLgda6mOEmCGMWZMZqUKxxhzBNY0+JH2cT7wCHAGMBo4zBhzip18LjBTREYCAeBKO3w2MFtERgHLgFvSVwIL++U+CRiPdb8PNcZ8h7ZZlmOA44GDgQnA/xhjxtEGywJgjJkKXOwKilfeXwNviMho4C/AH9IieAjGmADWdzJOREpEpATYiMd3nuB3lK5yDAf+CJyJ9Y4dYsuWFe9Xh1AQwAnAIhEpF5FK4GmsFmG2cSVwLbDZPj4cWCMiX9mtmrnANGPMMCBfRJbY6R61w/OAKVjlaw5Pk+xuSoEbRKRWROqAVVgfc5sri4i8Bhxny9wPq9fdkzZYFmNMb6wK9Df2cSLynobVgwD4B3CKnT7dGPv3JWPMR8aYmUT+zuP6jtJaCjgLq4ew0f5WzgOq4pG3Nd+vjqIgBmFVWg6lwJAMyRIREblCRNyLEUaSO1J4X6DC1UXOSDlFZKXzEhtjRmCZmhppg2UBEJE6Y8wvgU+BV2ijzwX4E3ATsMM+TkTe5jx2fAVQ3Lpie9IL61mcBUwFrgKGEt9zyYZ64QAg1xjzH2PMcuCaKHKl/f3qKAoiB8t+7BDAqrCynUhy+w2HDJbTGHMg8DLwv8CXtOGyiMgvsCrCfbB6Q22qLMaYK4ANIvKKKzgReUPXms7ItyQi74jIRSKyS0S2Aw8DvyK555KJsnTC6vlcDhwJHIE1vpMV71dHURAbsVYydBhAixknm4kkd6TwrUCRMcZZ830gGSqnMeZorBbejSLyN9poWYwxo+yBQUSkCngGOJa2V5bzgJPsVuqvgNOBK4hf3k12OowxnYBCoKzVpQ/BGDPJHk9xCABrie+5ZEO9sAVYKCLbRGQv8CyWwsiK96ujKIiFwFRjTLExphtwNvBChmXyw7uAMcYcYD/8C4AFIrIOqLYrYYAL7fA6rP0yzrPDLwIWpFtoY8w+wL+AC0TkCTu4TZYFqzX3F2NMF2NMZ6yBwz/RxsoiIieKyFh7MPfnwH9E5NIE5J1vH2PHv2GnTzc9gbuMMV2NMYVYA+/fxfs7j+vdS3M5ngO+YYzpact2CtZYQla8Xx1CQYjIJizb62JgOTBPRJZmVqrYiEg1cAnwTyz792paBqKmA/caY1YDBcB9dvg1WN4bn2JtDnJzOmW2+RHQFbjHGLPcbrVeQhssi4jMB54HPgTeB962ld4ltLGyRCBeeW8BJhpjVtpprk2zvACIyHMEP5dHROQtPL7zBL+jdJXjXeBOLO/FT4F1wEMJyNsq75fuB6EoiqJ40iF6EIqiKEr8qIJQFEVRPFEFoSiKoniiCkJRFEXxRBWEoiiK4okqCEVRFMUTVRCKoiiKJ6ogFEVRFE/+H2r3L2lKzfizAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(disc_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test without train\n",
    "netD_neg_test = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg_test = NetG(train_100k.shape[1]).cuda()\n",
    "\n",
    "netD_neg_test.eval()\n",
    "netG_neg_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.eval()\n",
    "netG_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuraccy\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda() \n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k > 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake_accur_check = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without train\n",
    "fake_test_accur_check = netG_neg_test(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_accur_check_ = (fake_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_accur_check = (fake_test_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25859, 14532)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_accur_check_ * negative_feedback).sum(), (fake_accur_check_ * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20002, 25273)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_test_accur_check * negative_feedback).sum(), (fake_test_accur_check * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6358405665248715"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items\n",
    "(fake_accur_check_ * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49182423959281024"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items - WITHOUT TRAIN \n",
    "(fake_test_accur_check * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7087833911144066"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items\n",
    "((1-fake_accur_check_) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49353720366325327"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49353720366325327"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5690"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD_neg.state_dict(), './netD_neg-100k')\n",
    "torch.save(netG_neg.state_dict(), './netG_neg-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fake_accur_check\n",
    "del fake_test_accur_check \n",
    "del e_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# predicting on ngetavie feedbacks\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k == 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = (fake > 0.8).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_augment_negative = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13777078364356143, 0.2542968846049817, 0.6079323317514569]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_probs = [(train_100k == 1).sum()/((train_100k > 0) & (train_100k < 4)).sum(), (train_100k == 2).sum()/(((train_100k > 0) & (train_100k < 4))).sum(), (train_100k == 3).sum()/((train_100k > 0) & (train_100k < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_100k = train_100k + to_augment_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 5.743238557340337)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(augmented_train_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "# MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-da219d16cefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMF_SGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExplicitMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_train_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mMF_SGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[1;34m(self, iter_array, test, learning_rate)\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_iter, learning_rate)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpartial_train\u001b[1;34m(self, n_iter)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mctr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, u, i)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sgd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142822581632546\n",
      "Test mse: 1.179626365384201\n",
      "Iteration: 2\n",
      "Train mse: 1.0730309645422471\n",
      "Test mse: 1.1283647354414428\n",
      "Iteration: 5\n",
      "Train mse: 0.976704786728581\n",
      "Test mse: 1.0498895036356708\n",
      "Iteration: 10\n",
      "Train mse: 0.9191190735106731\n",
      "Test mse: 0.99882388507181\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8670975465312848\n",
      "Test mse: 0.9525039875303193\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8395297477072872\n",
      "Test mse: 0.9342662870213886\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7572607163519403\n",
      "Test mse: 0.917558302954112\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40646564364302945\n",
      "Test mse: 0.9047525011076776\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1583192976603522\n",
      "Test mse: 1.1778921160446154\n",
      "Iteration: 2\n",
      "Train mse: 1.0827823219708648\n",
      "Test mse: 1.124826924658217\n",
      "Iteration: 5\n",
      "Train mse: 0.9809962544391533\n",
      "Test mse: 1.0456674228179015\n",
      "Iteration: 10\n",
      "Train mse: 0.9217915122656181\n",
      "Test mse: 0.9960992361685521\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8695835693024129\n",
      "Test mse: 0.9521000726072203\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8421152097706776\n",
      "Test mse: 0.9345373660525688\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.755397379978269\n",
      "Test mse: 0.916590796822304\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40575111332308983\n",
      "Test mse: 0.906762464167835\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.35978032575701\n",
      "Test mse: 8.193157746225948\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359732338000212\n",
      "Test mse: 8.193099816595986\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359687859300425\n",
      "Test mse: 8.193046053932775\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359646525578646\n",
      "Test mse: 8.192996043819882\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3596080230681835\n",
      "Test mse: 8.192949426343956\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393714646752196\n",
      "Test mse: 8.194362304310943\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393666122560651\n",
      "Test mse: 8.194304259396569\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39362115513897\n",
      "Test mse: 8.194250397772203\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393579374306474\n",
      "Test mse: 8.19420030178969\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393540461520644\n",
      "Test mse: 8.19415360957236\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1583192976603522\n",
      "Test mse: 1.1778921160446154\n",
      "Iteration: 2\n",
      "Train mse: 1.0827823219708648\n",
      "Test mse: 1.124826924658217\n",
      "Iteration: 5\n",
      "Train mse: 0.9809962544391533\n",
      "Test mse: 1.0456674228179015\n",
      "Iteration: 10\n",
      "Train mse: 0.9217915122656181\n",
      "Test mse: 0.9960992361685521\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8695835693024129\n",
      "Test mse: 0.9521000726072203\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8421152097706776\n",
      "Test mse: 0.9345373660525688\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.755397379978269\n",
      "Test mse: 0.916590796822304\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6846501387489577\n",
      "Test mse: 0.9047955049119271\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6088625770108296\n",
      "Test mse: 0.8969742625646109\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5326639192133193\n",
      "Test mse: 0.8944780567537973\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4636710714200561\n",
      "Test mse: 0.8979252620915619\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.40575111332308983\n",
      "Test mse: 0.906762464167835\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.35905711689460085\n",
      "Test mse: 0.9198637997234861\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "\n",
    "best_sgd_model = ExplicitMF(train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1545119024961061\n",
      "Test mse: 1.178567399719999\n",
      "Iteration: 2\n",
      "Train mse: 1.0808310124661051\n",
      "Test mse: 1.1261795339578151\n",
      "Iteration: 5\n",
      "Train mse: 0.9808797891168647\n",
      "Test mse: 1.0474046524170364\n",
      "Iteration: 10\n",
      "Train mse: 0.9222637203091965\n",
      "Test mse: 0.997286882756945\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8708585227221491\n",
      "Test mse: 0.9523215310455245\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8464129951238863\n",
      "Test mse: 0.9346230958725521\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7999830140159173\n",
      "Test mse: 0.9226304802126559\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7510374101643228\n",
      "Test mse: 0.9117502158632841\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6909018150472902\n",
      "Test mse: 0.9010086763640088\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6230042394517625\n",
      "Test mse: 0.8922974818153959\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5493002333842844\n",
      "Test mse: 0.8866310890559449\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.47529433983651304\n",
      "Test mse: 0.8848626444854254\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4062577209484476\n",
      "Test mse: 0.8867158452927663\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(augmented_train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.158486007439644\n",
      "Test mse: 1.1779978613000384\n",
      "Iteration: 2\n",
      "Train mse: 1.083171581612149\n",
      "Test mse: 1.125028404345431\n",
      "Iteration: 5\n",
      "Train mse: 0.9817048627613273\n",
      "Test mse: 1.0459868491969369\n",
      "Iteration: 10\n",
      "Train mse: 0.9227319825521801\n",
      "Test mse: 0.9963652377560377\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8713373835757132\n",
      "Test mse: 0.9521701303030687\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.846936197066563\n",
      "Test mse: 0.934619043527751\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.8000609230973601\n",
      "Test mse: 0.9225265969757358\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7503523923902362\n",
      "Test mse: 0.9115515826764651\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.689505366026202\n",
      "Test mse: 0.9008884227444379\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6213034484620569\n",
      "Test mse: 0.8924318463805855\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5475532983376317\n",
      "Test mse: 0.8870274449200776\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4737341552445482\n",
      "Test mse: 0.8854985087833768\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4050262870207811\n",
      "Test mse: 0.8875516903462294\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(augmented_train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142972933039822\n",
      "Test mse: 1.17972828223402\n",
      "Iteration: 2\n",
      "Train mse: 1.0733920132206576\n",
      "Test mse: 1.1285610630076768\n",
      "Iteration: 5\n",
      "Train mse: 0.9773881833559704\n",
      "Test mse: 1.0502187304294268\n",
      "Iteration: 10\n",
      "Train mse: 0.9200456767969902\n",
      "Test mse: 0.999134022874239\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8688346580213652\n",
      "Test mse: 0.9526617108778075\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8442347461991091\n",
      "Test mse: 0.934442842680263\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7999403795934972\n",
      "Test mse: 0.9232297270102171\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7525020754101615\n",
      "Test mse: 0.9130561640976463\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6925429566514093\n",
      "Test mse: 0.902326297805037\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6242821764329887\n",
      "Test mse: 0.8932252838608207\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5502016125007768\n",
      "Test mse: 0.8871597136187851\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4759825330856412\n",
      "Test mse: 0.8851292592272328\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.40684917561304956\n",
      "Test mse: 0.8868481394223161\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393714646752196\n",
      "Test mse: 8.194362304310943\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393666122560651\n",
      "Test mse: 8.194304259396569\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39362115513897\n",
      "Test mse: 8.194250397772203\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393579374306474\n",
      "Test mse: 8.19420030178969\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393540461520644\n",
      "Test mse: 8.19415360957236\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39350414020554\n",
      "Test mse: 8.194110004824077\n"
     ]
    }
   ],
   "source": [
    "best_als_model = ExplicitMF(train_100k, n_factors=10, learning='als', \\\n",
    "                            item_fact_reg=0.1, user_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_als_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.35978032575701\n",
      "Test mse: 8.193157746225948\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359732338000212\n",
      "Test mse: 8.193099816595986\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359687859300425\n",
      "Test mse: 8.193046053932775\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359646525578646\n",
      "Test mse: 8.192996043819882\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3596080230681835\n",
      "Test mse: 8.192949426343956\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359572079254732\n",
      "Test mse: 8.192905886804215\n"
     ]
    }
   ],
   "source": [
    "best_als_model = ExplicitMF(augmented_train_100k, n_factors=10, learning='als', \\\n",
    "                            item_fact_reg=0.1, user_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_als_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "# from sklearn.decomposition import ProjectedGradientNMF\n",
    "from sklearn.decomposition import SparsePCA\n",
    "# model = NMF(n_components=10, max_iter=400, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "# W = model.fit_transform(train_100k)\n",
    "# H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((943, 10), (10, 1682))"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4260560791980288"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NMF(n_components=20, max_iter=800, init='nndsvdar', beta_loss='frobenius', alpha=0.1, random_state=seed)\n",
    "model = NMF(n_components=10, max_iter=5000, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "W = model.fit_transform(augmented_train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42284283620274604"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NMF(n_components=20, max_iter=800, init='nndsvdar', beta_loss='frobenius', alpha=0.1, random_state=seed)\n",
    "model = NMF(n_components=10, max_iter=5000, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "W = model.fit_transform(train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4229995439123597"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py:170: DeprecationWarning: normalize_components=False is a backward-compatible setting that implements a non-standard definition of sparse PCA. This compatibility mode will be removed in 0.22.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-360-3f29c98d23de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparsePCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_train_100k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    185\u001b[0m                                                \u001b[0mcode_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcode_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                                                \u001b[0mdict_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                                                \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m                                                )\n\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\dict_learning.py\u001b[0m in \u001b[0;36mdict_learning\u001b[1;34m(X, n_components, alpha, max_iter, tol, method, n_jobs, dict_init, code_init, callback, verbose, random_state, return_n_iter, positive_dict, positive_code)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Update code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m         code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha,\n\u001b[1;32m--> 574\u001b[1;33m                              init=code, n_jobs=n_jobs, positive=positive_code)\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Update dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         dictionary, residuals = _update_dict(dictionary.T, X.T, code.T,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\dict_learning.py\u001b[0m in \u001b[0;36msparse_encode\u001b[1;34m(X, dictionary, gram, cov, algorithm, n_nonzero_coefs, alpha, copy_cov, init, max_iter, n_jobs, check_input, verbose, positive)\u001b[0m\n\u001b[0;32m    311\u001b[0m                               \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m                               positive=positive)\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\dict_learning.py\u001b[0m in \u001b[0;36m_sparse_encode\u001b[1;34m(X, dictionary, gram, cov, algorithm, regularization, copy_cov, init, max_iter, check_input, verbose, positive)\u001b[0m\n\u001b[0;32m    121\u001b[0m                                    \u001b[0mprecompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                                    positive=positive)\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0mlasso_lars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[0mnew_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlasso_lars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, Xy)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m         self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path,\n\u001b[1;32m--> 708\u001b[1;33m                   Xy=Xy)\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_iter, alpha, fit_path, Xy)\u001b[0m\n\u001b[0;32m    665\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                     \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m                     positive=self.positive)\n\u001b[0m\u001b[0;32m    668\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36mlars_path\u001b[1;34m(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001b[0m\n\u001b[0;32m    288\u001b[0m                                         **solve_triangular_args)\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m             \u001b[0mdiag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_active\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SparsePCA(n_components=10, max_iter=40, alpha=0.1,  random_state=seed)\n",
    "W = model.fit_transform(augmented_train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38982933425042454"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\sparse_pca.py:170: DeprecationWarning: normalize_components=False is a backward-compatible setting that implements a non-standard definition of sparse PCA. This compatibility mode will be removed in 0.22.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = SparsePCA(n_components=10, max_iter=40, alpha=0.1,  random_state=seed)\n",
    "W = model.fit_transform(train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38935982618491954"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
