{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  3 5778k    3  220k    0     0   115k      0  0:00:50  0:00:01  0:00:49  115k\n",
      " 33 5778k   33 1940k    0     0   693k      0  0:00:08  0:00:02  0:00:06  693k\n",
      "100 5778k  100 5778k    0     0  1652k      0  0:00:03  0:00:03 --:--:-- 1652k\n",
      "\"unzip\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "‘ЁбвҐ¬Ґ ­Ґ г¤ Ґвбп ­ ©вЁ гЄ § ­­л© Їгвм.\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 4808k    0 11401    0     0  32855      0  0:02:29 --:--:--  0:02:29 32761\n",
      " 14 4808k   14  699k    0     0   512k      0  0:00:09  0:00:01  0:00:08  511k\n",
      " 22 4808k   22 1104k    0     0   451k      0  0:00:10  0:00:02  0:00:08  451k\n",
      " 59 4808k   59 2864k    0     0   867k      0  0:00:05  0:00:03  0:00:02  867k\n",
      "100 4808k  100 4808k    0     0  1157k      0  0:00:04  0:00:04 --:--:-- 1157k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "    \n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "    \n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following blogpost's independent code to benchmark our experiments https://blog.insightdatascience.com/explicit-matrix-factorization-als-sgd-and-all-that-jazz-b00e4d9b21ea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent MF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1     1193       5\n",
       "1        1      661       3\n",
       "2        1      914       3\n",
       "3        1     3408       4\n",
       "4        1     2355       5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating']\n",
    "df = pd.read_csv('./ml-1m/ratings.dat', sep='::', usecols = [0, 1, 2], names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, (3706,), 3952)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.item_id.unique().max(), df.item_id.unique().shape, df.item_id.unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().max()\n",
    "n_items = df.item_id.unique().max()\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "#     print(row)\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.190220560634904"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    np.random.seed(seed)\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1m, test_1m = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.93718412338794, 0.25303643724696356)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_1m), get_sparsity(test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movielens-100k dataset\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_100k, test_100k = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 0.5945303210463734)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((943, 1682), (6040, 3952))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_100k.shape, train_1m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        np.random.seed(seed)\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * \\\n",
    "                                (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 8.162626454488155\n",
      "Test mse: 11.074297483666612\n",
      "Iteration: 2\n",
      "Train mse: 5.710628199541684\n",
      "Test mse: 8.654294898028265\n",
      "Iteration: 5\n",
      "Train mse: 5.421378301611028\n",
      "Test mse: 8.237957326989877\n",
      "Iteration: 10\n",
      "Train mse: 5.3966006950612435\n",
      "Test mse: 8.199281754444268\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.3948135876025365\n",
      "Test mse: 8.195283711199771\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394753065812939\n",
      "Test mse: 8.195083034461906\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.394526731799798\n",
      "Test mse: 8.194863821632225\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def plot_learning_curve(iter_array, model):\n",
    "    plt.plot(iter_array, model.train_mse, \\\n",
    "             label='Training', linewidth=5)\n",
    "    plt.plot(iter_array, model.test_mse, \\\n",
    "             label='Test', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('MSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(iter_array, MF_ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 8.133017313467287\n",
      "Test mse: 11.385478952329686\n",
      "Iteration: 2\n",
      "Train mse: 6.181422331570321\n",
      "Test mse: 9.510432053967698\n",
      "Iteration: 5\n",
      "Train mse: 5.945868319505223\n",
      "Test mse: 9.181499199125527\n",
      "Iteration: 10\n",
      "Train mse: 5.929921005207816\n",
      "Test mse: 9.163591900058499\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.922879579194776\n",
      "Test mse: 9.155637570112932\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.921741164281515\n",
      "Test mse: 9.152673442345948\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.921434636456923\n",
      "Test mse: 9.152593151189302\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_1m, n_factors=20, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142807793060997\n",
      "Test mse: 1.1796269767247503\n",
      "Iteration: 2\n",
      "Train mse: 1.0730325987217215\n",
      "Test mse: 1.1283621352778417\n",
      "Iteration: 5\n",
      "Train mse: 0.9767157588922124\n",
      "Test mse: 1.0499014864558258\n",
      "Iteration: 10\n",
      "Train mse: 0.9190727366615857\n",
      "Test mse: 0.9988387536431501\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8669457204379917\n",
      "Test mse: 0.9525818812444498\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8387251669562131\n",
      "Test mse: 0.9343161812537658\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7485848531412769\n",
      "Test mse: 0.9170005429005551\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40410579696377263\n",
      "Test mse: 0.9148070379768369\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Proposed GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train_100k == 0)\n",
    "positive_feedback = (train_100k > 3)\n",
    "negative_feedback = ((train_100k < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49901, 40669)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_feedback.sum(), negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback + negative_feedback != zero_mask).all()\n",
    "assert (positive_feedback + negative_feedback == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94.28986095682184, 3.146093059441684, 2.5640459837364746)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback), get_sparsity(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, mat, p=0.5, batch_size=64):\n",
    "        '''\n",
    "        mat is a binary matrix (e.g. positive feedback, or negative feedback)\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.mat = mat\n",
    "        self.p = p\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.mat.shape[0] / self.batch_size))\n",
    "    \n",
    "    def gen_item_GAN(self):\n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y, indexes\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_negative = DataGenerator(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _ = generator_negative.gen_item_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 1682), (64, 1682))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, feat_size):\n",
    "        super(NetD, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "#         self.use_cuda = True\n",
    "#         self.feat_size = feat_size\n",
    "        # top\n",
    "#         print(self.feat_size*2)\n",
    "        self.t1 = torch.nn.Linear(self.feat_size, 512)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(self.feat_size, 512)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 512, self.feat_size)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        \n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "#         if self.use_cuda: \n",
    "        idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "        x = filt * x\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_size):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz + self.feat_size, 512), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "#                                 torch.nn.ReLU(), \n",
    "# #                                 nn.Dropout(0.5),\n",
    "#                                 torch.nn.Linear(2048, 2048),\n",
    "                                torch.nn.ReLU(), \n",
    "#                                 torch.nn.BatchNorm1d(512),\n",
    "#                                 nn.Dropout(0.6),\n",
    "                                torch.nn.Linear(512, self.feat_size), \n",
    "                                torch.nn.Sigmoid()\n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "        \n",
    "    def forward(self, e_mask, x):\n",
    "        x = self.netGen(x)\n",
    "#         print(x.shape, )\n",
    "        x = x * e_mask\n",
    "        return x\n",
    "#         return F.dropout(x, 0.7)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_losses = []\n",
    "disc_losses = []\n",
    "def train_GAN(netD, netG, negative, tr, steps_per_epoch = 200, epochs = 10):\n",
    "    d_iter = 5\n",
    "    g_iter = 1\n",
    "    gen_iterations = 0\n",
    "#     gen_losses = []\n",
    "#     disc_losses = []\n",
    "#     train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for c in range(steps_per_epoch):\n",
    "            data_iter = 100\n",
    "            i = 0\n",
    "#             while i < 100:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in netD.parameters(): # reset requires_grad\n",
    "                p.requires_grad = True # they are set to False below in netG update\n",
    "#             d_iter = d_iter\n",
    "            j = 0\n",
    "            while j < d_iter*5:\n",
    "                j += 1\n",
    "                # load real data\n",
    "                i += 1\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "#                         condition, X, idxs = batch_generator(X_neg, y_neg)\n",
    "#                 X, _ = data_iter.next()\n",
    "#                 X = X.view(X.size(0), -1)\n",
    "#                 X = (X >= 0.5).float()\n",
    "#                     if cuda: \n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "#                     X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "#     + torch.randn(X.size()).cuda() * 0.2\n",
    "#                 print(condition.shape, X_neg.shape, y_neg.shape)\n",
    "                real = Variable(X)\n",
    "\n",
    "                # generate fake data\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "#                     if cuda: \n",
    "                noise = noise.cuda()\n",
    "#                     noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "                with torch.no_grad():\n",
    "                    noisev = Variable(noise) # totally freeze netG\n",
    "                concated = torch.cat((noisev, condition), 1)\n",
    "#                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\n",
    "                e_mask = torch.Tensor(tr[idxs]>0).cuda()\n",
    "#                     print(e_mask.shape, concated.shape, condition.shape)\n",
    "                fake = Variable(netG(e_mask, concated).data)\n",
    "\n",
    "                # compute gradient, take step\n",
    "                netD.zero_grad()\n",
    "#                 concated_real = torch.cat((real, condition), 1)\n",
    "#                 print(concated_real)\n",
    "                out = netD(real, fake)\n",
    "                outputD = torch.mean(out) + lamba * out.norm()\n",
    "                stdD = torch.std(out)\n",
    "                outputD.backward(mone)\n",
    "                optimizerD.step()\n",
    "#                     print('AAAAAAAAA mse:=WWWWWWWWWWWWWWWWWWWWWW')\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "\n",
    "#         g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "                netG.zero_grad()\n",
    "                # load real data\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "\n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "    #                 X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "                real = Variable(X)\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "                noise = noise.cuda()\n",
    "                noisev = Variable(noise)\n",
    "                concated_ = torch.cat((noisev, condition), 1)\n",
    "                e_mask_ = torch.Tensor(tr[idxs]>0).cuda()\n",
    "\n",
    "                fake = netG(e_mask_, concated_)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "                gen_iterations += 1\n",
    "    #             print('AAAAAA')\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "    #             eval_losses.append(eval_loss)\n",
    "    #             print('mse:', eval_loss)\n",
    "    #             print(outputG.item(), outputD.item())\n",
    "                gen_losses.append(outputG.item())\n",
    "                disc_losses.append(outputD.item())\n",
    "                print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, 100, gen_iterations, outputD.item(), outputG.item()))\n",
    "    return gen_losses, disc_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-3\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 10\n",
    "# device = 5\n",
    "seed = 1\n",
    "nz = 8\n",
    "lamba = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=1682, out_features=512, bias=True)\n",
      "  (b1): Linear(in_features=1682, out_features=512, bias=True)\n",
      "  (fc): Linear(in_features=1024, out_features=1682, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=1690, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=1682, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_neg = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg = NetG(train_100k.shape[1]).cuda()\n",
    "print(netD_neg)\n",
    "print(netG_neg)\n",
    "optimizerG = optim.RMSprop(netG_neg.parameters(), lr=lrG, weight_decay=1e-4)\n",
    "optimizerD = optim.RMSprop(netD_neg.parameters(), lr=lrD, weight_decay=1e-4)\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = (-1 * one).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][26/100][1] Loss_D: 0.012087 Loss_G: 0.015088 \n",
      "[0/10][26/100][2] Loss_D: 0.004970 Loss_G: 0.011500 \n",
      "[0/10][26/100][3] Loss_D: 0.008385 Loss_G: 0.009363 \n",
      "[0/10][26/100][4] Loss_D: 0.005430 Loss_G: 0.002678 \n",
      "[0/10][26/100][5] Loss_D: 0.000809 Loss_G: 0.000955 \n",
      "[0/10][26/100][6] Loss_D: 0.001399 Loss_G: 0.002833 \n",
      "[0/10][26/100][7] Loss_D: 0.000421 Loss_G: 0.000294 \n",
      "[0/10][26/100][8] Loss_D: 0.001978 Loss_G: -0.001052 \n",
      "[0/10][26/100][9] Loss_D: 0.002113 Loss_G: 0.003058 \n",
      "[0/10][26/100][10] Loss_D: 0.000651 Loss_G: 0.001420 \n",
      "[0/10][26/100][11] Loss_D: 0.000195 Loss_G: 0.001388 \n",
      "[0/10][26/100][12] Loss_D: 0.002154 Loss_G: 0.000524 \n",
      "[0/10][26/100][13] Loss_D: 0.000834 Loss_G: 0.002114 \n",
      "[0/10][26/100][14] Loss_D: 0.002443 Loss_G: 0.000701 \n",
      "[0/10][26/100][15] Loss_D: 0.000421 Loss_G: 0.002086 \n",
      "[0/10][26/100][16] Loss_D: 0.003219 Loss_G: 0.004532 \n",
      "[0/10][26/100][17] Loss_D: 0.001307 Loss_G: 0.002597 \n",
      "[0/10][26/100][18] Loss_D: 0.001849 Loss_G: 0.004025 \n",
      "[0/10][26/100][19] Loss_D: 0.001137 Loss_G: 0.002981 \n",
      "[0/10][26/100][20] Loss_D: 0.002176 Loss_G: 0.003523 \n",
      "[0/10][26/100][21] Loss_D: 0.005344 Loss_G: 0.001279 \n",
      "[0/10][26/100][22] Loss_D: 0.000561 Loss_G: 0.003828 \n",
      "[0/10][26/100][23] Loss_D: 0.002711 Loss_G: 0.002090 \n",
      "[0/10][26/100][24] Loss_D: 0.003942 Loss_G: 0.001535 \n",
      "[0/10][26/100][25] Loss_D: 0.001516 Loss_G: 0.000637 \n",
      "[0/10][26/100][26] Loss_D: 0.001906 Loss_G: 0.004274 \n",
      "[0/10][26/100][27] Loss_D: 0.002866 Loss_G: 0.002585 \n",
      "[0/10][26/100][28] Loss_D: 0.003539 Loss_G: 0.001308 \n",
      "[0/10][26/100][29] Loss_D: 0.003935 Loss_G: 0.004300 \n",
      "[0/10][26/100][30] Loss_D: 0.002091 Loss_G: 0.002745 \n",
      "[0/10][26/100][31] Loss_D: 0.001988 Loss_G: 0.003094 \n",
      "[0/10][26/100][32] Loss_D: 0.001166 Loss_G: 0.000975 \n",
      "[0/10][26/100][33] Loss_D: 0.003148 Loss_G: 0.002965 \n",
      "[0/10][26/100][34] Loss_D: 0.002721 Loss_G: 0.002830 \n",
      "[0/10][26/100][35] Loss_D: 0.002427 Loss_G: 0.001430 \n",
      "[0/10][26/100][36] Loss_D: 0.002369 Loss_G: 0.003807 \n",
      "[0/10][26/100][37] Loss_D: 0.002598 Loss_G: 0.001102 \n",
      "[0/10][26/100][38] Loss_D: 0.004300 Loss_G: 0.000681 \n",
      "[0/10][26/100][39] Loss_D: 0.003322 Loss_G: 0.001501 \n",
      "[0/10][26/100][40] Loss_D: 0.002213 Loss_G: 0.004457 \n",
      "[0/10][26/100][41] Loss_D: 0.003328 Loss_G: 0.002908 \n",
      "[0/10][26/100][42] Loss_D: 0.002264 Loss_G: 0.002146 \n",
      "[0/10][26/100][43] Loss_D: 0.001929 Loss_G: 0.003310 \n",
      "[0/10][26/100][44] Loss_D: 0.002073 Loss_G: 0.004098 \n",
      "[0/10][26/100][45] Loss_D: 0.001661 Loss_G: 0.002403 \n",
      "[0/10][26/100][46] Loss_D: 0.002230 Loss_G: 0.002238 \n",
      "[0/10][26/100][47] Loss_D: 0.001599 Loss_G: 0.001985 \n",
      "[0/10][26/100][48] Loss_D: 0.003203 Loss_G: 0.002173 \n",
      "[0/10][26/100][49] Loss_D: 0.002380 Loss_G: 0.005350 \n",
      "[0/10][26/100][50] Loss_D: 0.001843 Loss_G: 0.003312 \n",
      "[0/10][26/100][51] Loss_D: 0.000968 Loss_G: 0.002683 \n",
      "[0/10][26/100][52] Loss_D: 0.001063 Loss_G: 0.002344 \n",
      "[0/10][26/100][53] Loss_D: 0.001953 Loss_G: -0.000109 \n",
      "[0/10][26/100][54] Loss_D: 0.001708 Loss_G: 0.002022 \n",
      "[0/10][26/100][55] Loss_D: 0.002629 Loss_G: 0.002920 \n",
      "[0/10][26/100][56] Loss_D: 0.002935 Loss_G: 0.002163 \n",
      "[0/10][26/100][57] Loss_D: 0.002149 Loss_G: 0.003381 \n",
      "[0/10][26/100][58] Loss_D: 0.003306 Loss_G: 0.002217 \n",
      "[0/10][26/100][59] Loss_D: 0.000753 Loss_G: 0.001774 \n",
      "[0/10][26/100][60] Loss_D: 0.001338 Loss_G: 0.003281 \n",
      "[0/10][26/100][61] Loss_D: 0.002081 Loss_G: 0.002978 \n",
      "[0/10][26/100][62] Loss_D: 0.001909 Loss_G: 0.004443 \n",
      "[0/10][26/100][63] Loss_D: 0.002708 Loss_G: 0.004038 \n",
      "[0/10][26/100][64] Loss_D: 0.003233 Loss_G: 0.003535 \n",
      "[0/10][26/100][65] Loss_D: 0.004619 Loss_G: 0.003175 \n",
      "[0/10][26/100][66] Loss_D: 0.002882 Loss_G: 0.004994 \n",
      "[0/10][26/100][67] Loss_D: 0.001213 Loss_G: 0.003453 \n",
      "[0/10][26/100][68] Loss_D: 0.004281 Loss_G: 0.002042 \n",
      "[0/10][26/100][69] Loss_D: -0.000048 Loss_G: 0.002542 \n",
      "[0/10][26/100][70] Loss_D: 0.001781 Loss_G: 0.001548 \n",
      "[0/10][26/100][71] Loss_D: 0.002799 Loss_G: 0.002164 \n",
      "[0/10][26/100][72] Loss_D: 0.002566 Loss_G: 0.002960 \n",
      "[0/10][26/100][73] Loss_D: 0.002119 Loss_G: 0.001053 \n",
      "[0/10][26/100][74] Loss_D: 0.001468 Loss_G: 0.001399 \n",
      "[0/10][26/100][75] Loss_D: 0.002003 Loss_G: 0.002346 \n",
      "[0/10][26/100][76] Loss_D: 0.003245 Loss_G: 0.002019 \n",
      "[0/10][26/100][77] Loss_D: 0.002792 Loss_G: 0.003024 \n",
      "[0/10][26/100][78] Loss_D: 0.002771 Loss_G: 0.003376 \n",
      "[0/10][26/100][79] Loss_D: 0.001678 Loss_G: 0.001753 \n",
      "[0/10][26/100][80] Loss_D: 0.002271 Loss_G: 0.002962 \n",
      "[0/10][26/100][81] Loss_D: 0.003898 Loss_G: 0.002530 \n",
      "[0/10][26/100][82] Loss_D: 0.002300 Loss_G: 0.002978 \n",
      "[0/10][26/100][83] Loss_D: 0.001550 Loss_G: 0.001341 \n",
      "[0/10][26/100][84] Loss_D: 0.001866 Loss_G: 0.001491 \n",
      "[0/10][26/100][85] Loss_D: 0.001497 Loss_G: 0.001815 \n",
      "[0/10][26/100][86] Loss_D: 0.002783 Loss_G: 0.003228 \n",
      "[0/10][26/100][87] Loss_D: 0.003830 Loss_G: 0.002620 \n",
      "[0/10][26/100][88] Loss_D: 0.001889 Loss_G: 0.003152 \n",
      "[0/10][26/100][89] Loss_D: 0.002684 Loss_G: 0.003213 \n",
      "[0/10][26/100][90] Loss_D: 0.002183 Loss_G: 0.004818 \n",
      "[0/10][26/100][91] Loss_D: 0.001295 Loss_G: 0.001614 \n",
      "[0/10][26/100][92] Loss_D: 0.003594 Loss_G: 0.002291 \n",
      "[0/10][26/100][93] Loss_D: 0.005441 Loss_G: 0.003607 \n",
      "[0/10][26/100][94] Loss_D: 0.003080 Loss_G: 0.004285 \n",
      "[0/10][26/100][95] Loss_D: 0.002442 Loss_G: 0.002539 \n",
      "[0/10][26/100][96] Loss_D: 0.001175 Loss_G: 0.001460 \n",
      "[0/10][26/100][97] Loss_D: 0.002534 Loss_G: 0.002829 \n",
      "[0/10][26/100][98] Loss_D: 0.002287 Loss_G: 0.002665 \n",
      "[0/10][26/100][99] Loss_D: 0.002788 Loss_G: 0.003604 \n",
      "[0/10][26/100][100] Loss_D: -0.000608 Loss_G: -0.000142 \n",
      "[0/10][26/100][101] Loss_D: 0.002930 Loss_G: 0.002758 \n",
      "[0/10][26/100][102] Loss_D: 0.003766 Loss_G: 0.002850 \n",
      "[0/10][26/100][103] Loss_D: 0.002994 Loss_G: 0.003137 \n",
      "[0/10][26/100][104] Loss_D: 0.001788 Loss_G: 0.002300 \n",
      "[0/10][26/100][105] Loss_D: 0.003229 Loss_G: 0.006637 \n",
      "[0/10][26/100][106] Loss_D: 0.004211 Loss_G: 0.001850 \n",
      "[0/10][26/100][107] Loss_D: 0.003635 Loss_G: 0.002181 \n",
      "[0/10][26/100][108] Loss_D: 0.001835 Loss_G: 0.004213 \n",
      "[0/10][26/100][109] Loss_D: 0.003168 Loss_G: 0.002969 \n",
      "[0/10][26/100][110] Loss_D: 0.001263 Loss_G: 0.001641 \n",
      "[0/10][26/100][111] Loss_D: 0.002696 Loss_G: 0.003139 \n",
      "[0/10][26/100][112] Loss_D: 0.003121 Loss_G: 0.001367 \n",
      "[0/10][26/100][113] Loss_D: 0.002657 Loss_G: 0.002669 \n",
      "[0/10][26/100][114] Loss_D: 0.003870 Loss_G: 0.004908 \n",
      "[0/10][26/100][115] Loss_D: 0.003585 Loss_G: 0.002559 \n",
      "[0/10][26/100][116] Loss_D: 0.002738 Loss_G: 0.000287 \n",
      "[0/10][26/100][117] Loss_D: 0.001652 Loss_G: 0.001595 \n",
      "[0/10][26/100][118] Loss_D: 0.003225 Loss_G: 0.003035 \n",
      "[0/10][26/100][119] Loss_D: 0.002767 Loss_G: 0.004564 \n",
      "[0/10][26/100][120] Loss_D: 0.002705 Loss_G: 0.002986 \n",
      "[0/10][26/100][121] Loss_D: 0.002441 Loss_G: 0.002174 \n",
      "[0/10][26/100][122] Loss_D: 0.002251 Loss_G: 0.001753 \n",
      "[0/10][26/100][123] Loss_D: 0.004791 Loss_G: 0.002815 \n",
      "[0/10][26/100][124] Loss_D: 0.003866 Loss_G: 0.003538 \n",
      "[0/10][26/100][125] Loss_D: 0.003698 Loss_G: 0.003742 \n",
      "[0/10][26/100][126] Loss_D: 0.001239 Loss_G: 0.000796 \n",
      "[0/10][26/100][127] Loss_D: 0.003256 Loss_G: 0.002107 \n",
      "[0/10][26/100][128] Loss_D: 0.002366 Loss_G: 0.001334 \n",
      "[0/10][26/100][129] Loss_D: 0.002493 Loss_G: 0.006581 \n",
      "[0/10][26/100][130] Loss_D: 0.003176 Loss_G: 0.003231 \n",
      "[0/10][26/100][131] Loss_D: 0.002226 Loss_G: 0.001988 \n",
      "[0/10][26/100][132] Loss_D: 0.002885 Loss_G: 0.005411 \n",
      "[0/10][26/100][133] Loss_D: 0.002019 Loss_G: 0.002145 \n",
      "[0/10][26/100][134] Loss_D: 0.003980 Loss_G: 0.006265 \n",
      "[0/10][26/100][135] Loss_D: 0.003040 Loss_G: 0.002040 \n",
      "[0/10][26/100][136] Loss_D: 0.003360 Loss_G: 0.002554 \n",
      "[0/10][26/100][137] Loss_D: 0.002083 Loss_G: 0.001998 \n",
      "[0/10][26/100][138] Loss_D: 0.002369 Loss_G: 0.003182 \n",
      "[0/10][26/100][139] Loss_D: 0.002360 Loss_G: 0.002186 \n",
      "[0/10][26/100][140] Loss_D: 0.003694 Loss_G: 0.002299 \n",
      "[0/10][26/100][141] Loss_D: 0.002102 Loss_G: 0.001985 \n",
      "[0/10][26/100][142] Loss_D: 0.003676 Loss_G: 0.002353 \n",
      "[0/10][26/100][143] Loss_D: 0.002629 Loss_G: 0.003281 \n",
      "[0/10][26/100][144] Loss_D: 0.001623 Loss_G: 0.002759 \n",
      "[0/10][26/100][145] Loss_D: 0.004280 Loss_G: 0.000811 \n",
      "[0/10][26/100][146] Loss_D: 0.002587 Loss_G: 0.002543 \n",
      "[0/10][26/100][147] Loss_D: 0.002363 Loss_G: 0.001931 \n",
      "[0/10][26/100][148] Loss_D: 0.003586 Loss_G: 0.003699 \n",
      "[0/10][26/100][149] Loss_D: 0.001863 Loss_G: 0.001369 \n",
      "[0/10][26/100][150] Loss_D: 0.002041 Loss_G: 0.001753 \n",
      "[0/10][26/100][151] Loss_D: 0.002645 Loss_G: -0.000193 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][26/100][152] Loss_D: 0.001650 Loss_G: 0.003534 \n",
      "[0/10][26/100][153] Loss_D: 0.002910 Loss_G: 0.002714 \n",
      "[0/10][26/100][154] Loss_D: 0.001373 Loss_G: 0.002033 \n",
      "[0/10][26/100][155] Loss_D: 0.002888 Loss_G: 0.002314 \n",
      "[0/10][26/100][156] Loss_D: 0.003377 Loss_G: 0.001553 \n",
      "[0/10][26/100][157] Loss_D: 0.003323 Loss_G: 0.004364 \n",
      "[0/10][26/100][158] Loss_D: 0.002229 Loss_G: 0.004223 \n",
      "[0/10][26/100][159] Loss_D: 0.002176 Loss_G: 0.002869 \n",
      "[0/10][26/100][160] Loss_D: 0.002034 Loss_G: 0.001248 \n",
      "[0/10][26/100][161] Loss_D: 0.004255 Loss_G: 0.001665 \n",
      "[0/10][26/100][162] Loss_D: 0.002156 Loss_G: 0.002910 \n",
      "[0/10][26/100][163] Loss_D: 0.002349 Loss_G: 0.003328 \n",
      "[0/10][26/100][164] Loss_D: 0.001229 Loss_G: 0.003339 \n",
      "[0/10][26/100][165] Loss_D: 0.002214 Loss_G: 0.002556 \n",
      "[0/10][26/100][166] Loss_D: 0.003886 Loss_G: 0.000184 \n",
      "[0/10][26/100][167] Loss_D: 0.003272 Loss_G: 0.003361 \n",
      "[0/10][26/100][168] Loss_D: 0.001674 Loss_G: 0.001858 \n",
      "[0/10][26/100][169] Loss_D: 0.001446 Loss_G: 0.003461 \n",
      "[0/10][26/100][170] Loss_D: 0.003880 Loss_G: -0.000178 \n",
      "[0/10][26/100][171] Loss_D: 0.001739 Loss_G: 0.002212 \n",
      "[0/10][26/100][172] Loss_D: 0.004657 Loss_G: 0.001855 \n",
      "[0/10][26/100][173] Loss_D: 0.002477 Loss_G: 0.001204 \n",
      "[0/10][26/100][174] Loss_D: 0.003000 Loss_G: 0.001175 \n",
      "[0/10][26/100][175] Loss_D: 0.003666 Loss_G: 0.004786 \n",
      "[0/10][26/100][176] Loss_D: 0.003030 Loss_G: 0.004027 \n",
      "[0/10][26/100][177] Loss_D: 0.004162 Loss_G: 0.004599 \n",
      "[0/10][26/100][178] Loss_D: 0.002253 Loss_G: 0.003814 \n",
      "[0/10][26/100][179] Loss_D: 0.003116 Loss_G: 0.001554 \n",
      "[0/10][26/100][180] Loss_D: 0.001806 Loss_G: 0.001939 \n",
      "[0/10][26/100][181] Loss_D: 0.001058 Loss_G: 0.003360 \n",
      "[0/10][26/100][182] Loss_D: 0.001951 Loss_G: 0.004172 \n",
      "[0/10][26/100][183] Loss_D: 0.004118 Loss_G: 0.002555 \n",
      "[0/10][26/100][184] Loss_D: 0.004415 Loss_G: 0.001223 \n",
      "[0/10][26/100][185] Loss_D: 0.002325 Loss_G: 0.004317 \n",
      "[0/10][26/100][186] Loss_D: 0.002712 Loss_G: 0.003192 \n",
      "[0/10][26/100][187] Loss_D: 0.001948 Loss_G: 0.003982 \n",
      "[0/10][26/100][188] Loss_D: 0.004134 Loss_G: 0.002635 \n",
      "[0/10][26/100][189] Loss_D: 0.002227 Loss_G: 0.003723 \n",
      "[0/10][26/100][190] Loss_D: 0.002306 Loss_G: 0.004182 \n",
      "[0/10][26/100][191] Loss_D: 0.001549 Loss_G: 0.002309 \n",
      "[0/10][26/100][192] Loss_D: 0.003642 Loss_G: -0.001690 \n",
      "[0/10][26/100][193] Loss_D: 0.002651 Loss_G: 0.003279 \n",
      "[0/10][26/100][194] Loss_D: 0.002272 Loss_G: 0.002930 \n",
      "[0/10][26/100][195] Loss_D: 0.002791 Loss_G: 0.003828 \n",
      "[0/10][26/100][196] Loss_D: 0.004261 Loss_G: 0.002877 \n",
      "[0/10][26/100][197] Loss_D: 0.002273 Loss_G: 0.003489 \n",
      "[0/10][26/100][198] Loss_D: 0.003653 Loss_G: 0.002537 \n",
      "[0/10][26/100][199] Loss_D: 0.003151 Loss_G: 0.002430 \n",
      "[0/10][26/100][200] Loss_D: 0.003666 Loss_G: 0.002414 \n",
      "[1/10][26/100][201] Loss_D: 0.003613 Loss_G: 0.005103 \n",
      "[1/10][26/100][202] Loss_D: 0.003777 Loss_G: 0.004087 \n",
      "[1/10][26/100][203] Loss_D: 0.003286 Loss_G: 0.004651 \n",
      "[1/10][26/100][204] Loss_D: 0.001955 Loss_G: 0.001191 \n",
      "[1/10][26/100][205] Loss_D: 0.005152 Loss_G: 0.003108 \n",
      "[1/10][26/100][206] Loss_D: 0.002679 Loss_G: 0.002178 \n",
      "[1/10][26/100][207] Loss_D: 0.004719 Loss_G: 0.003399 \n",
      "[1/10][26/100][208] Loss_D: 0.004361 Loss_G: 0.004417 \n",
      "[1/10][26/100][209] Loss_D: 0.003486 Loss_G: 0.004659 \n",
      "[1/10][26/100][210] Loss_D: 0.003642 Loss_G: 0.002549 \n",
      "[1/10][26/100][211] Loss_D: 0.003019 Loss_G: 0.001410 \n",
      "[1/10][26/100][212] Loss_D: 0.003575 Loss_G: 0.003080 \n",
      "[1/10][26/100][213] Loss_D: 0.002318 Loss_G: 0.003625 \n",
      "[1/10][26/100][214] Loss_D: 0.003069 Loss_G: 0.004025 \n",
      "[1/10][26/100][215] Loss_D: 0.001527 Loss_G: 0.003503 \n",
      "[1/10][26/100][216] Loss_D: 0.001301 Loss_G: 0.001693 \n",
      "[1/10][26/100][217] Loss_D: 0.002098 Loss_G: 0.005696 \n",
      "[1/10][26/100][218] Loss_D: 0.002696 Loss_G: 0.003606 \n",
      "[1/10][26/100][219] Loss_D: 0.004255 Loss_G: 0.003560 \n",
      "[1/10][26/100][220] Loss_D: 0.002356 Loss_G: 0.002271 \n",
      "[1/10][26/100][221] Loss_D: 0.001737 Loss_G: 0.003702 \n",
      "[1/10][26/100][222] Loss_D: 0.001110 Loss_G: 0.001571 \n",
      "[1/10][26/100][223] Loss_D: 0.001173 Loss_G: 0.002781 \n",
      "[1/10][26/100][224] Loss_D: 0.003310 Loss_G: 0.003614 \n",
      "[1/10][26/100][225] Loss_D: 0.003543 Loss_G: 0.003035 \n",
      "[1/10][26/100][226] Loss_D: 0.004338 Loss_G: 0.001212 \n",
      "[1/10][26/100][227] Loss_D: 0.001911 Loss_G: 0.003164 \n",
      "[1/10][26/100][228] Loss_D: 0.004678 Loss_G: 0.000565 \n",
      "[1/10][26/100][229] Loss_D: -0.000572 Loss_G: 0.002678 \n",
      "[1/10][26/100][230] Loss_D: 0.002732 Loss_G: 0.002196 \n",
      "[1/10][26/100][231] Loss_D: 0.003076 Loss_G: 0.003087 \n",
      "[1/10][26/100][232] Loss_D: 0.000095 Loss_G: 0.002435 \n",
      "[1/10][26/100][233] Loss_D: 0.001644 Loss_G: 0.001969 \n",
      "[1/10][26/100][234] Loss_D: 0.003243 Loss_G: 0.002542 \n",
      "[1/10][26/100][235] Loss_D: 0.002425 Loss_G: 0.002136 \n",
      "[1/10][26/100][236] Loss_D: 0.001351 Loss_G: 0.004028 \n",
      "[1/10][26/100][237] Loss_D: 0.003536 Loss_G: 0.002213 \n",
      "[1/10][26/100][238] Loss_D: 0.001731 Loss_G: 0.001864 \n",
      "[1/10][26/100][239] Loss_D: 0.004360 Loss_G: 0.002332 \n",
      "[1/10][26/100][240] Loss_D: 0.003704 Loss_G: 0.002579 \n",
      "[1/10][26/100][241] Loss_D: 0.004440 Loss_G: 0.002366 \n",
      "[1/10][26/100][242] Loss_D: 0.001993 Loss_G: 0.004411 \n",
      "[1/10][26/100][243] Loss_D: 0.001558 Loss_G: 0.003093 \n",
      "[1/10][26/100][244] Loss_D: 0.003271 Loss_G: 0.001728 \n",
      "[1/10][26/100][245] Loss_D: 0.002285 Loss_G: 0.005402 \n",
      "[1/10][26/100][246] Loss_D: 0.003226 Loss_G: 0.003160 \n",
      "[1/10][26/100][247] Loss_D: 0.001450 Loss_G: 0.003483 \n",
      "[1/10][26/100][248] Loss_D: 0.001520 Loss_G: 0.000853 \n",
      "[1/10][26/100][249] Loss_D: 0.003377 Loss_G: 0.002588 \n",
      "[1/10][26/100][250] Loss_D: 0.001808 Loss_G: 0.001068 \n",
      "[1/10][26/100][251] Loss_D: 0.002934 Loss_G: 0.001977 \n",
      "[1/10][26/100][252] Loss_D: 0.001671 Loss_G: 0.002426 \n",
      "[1/10][26/100][253] Loss_D: 0.003108 Loss_G: 0.001735 \n",
      "[1/10][26/100][254] Loss_D: 0.004680 Loss_G: 0.000690 \n",
      "[1/10][26/100][255] Loss_D: -0.000047 Loss_G: -0.000041 \n",
      "[1/10][26/100][256] Loss_D: 0.004467 Loss_G: 0.003850 \n",
      "[1/10][26/100][257] Loss_D: 0.001959 Loss_G: 0.003161 \n",
      "[1/10][26/100][258] Loss_D: 0.004921 Loss_G: 0.004112 \n",
      "[1/10][26/100][259] Loss_D: 0.002366 Loss_G: 0.004195 \n",
      "[1/10][26/100][260] Loss_D: 0.001467 Loss_G: 0.002219 \n",
      "[1/10][26/100][261] Loss_D: -0.002408 Loss_G: 0.002387 \n",
      "[1/10][26/100][262] Loss_D: 0.002640 Loss_G: 0.002107 \n",
      "[1/10][26/100][263] Loss_D: 0.002392 Loss_G: 0.003136 \n",
      "[1/10][26/100][264] Loss_D: 0.001430 Loss_G: 0.002396 \n",
      "[1/10][26/100][265] Loss_D: 0.002113 Loss_G: 0.002825 \n",
      "[1/10][26/100][266] Loss_D: 0.000999 Loss_G: 0.004733 \n",
      "[1/10][26/100][267] Loss_D: 0.002462 Loss_G: 0.003401 \n",
      "[1/10][26/100][268] Loss_D: 0.002153 Loss_G: 0.003778 \n",
      "[1/10][26/100][269] Loss_D: 0.003789 Loss_G: 0.001239 \n",
      "[1/10][26/100][270] Loss_D: 0.001646 Loss_G: 0.002962 \n",
      "[1/10][26/100][271] Loss_D: 0.002741 Loss_G: 0.002553 \n",
      "[1/10][26/100][272] Loss_D: 0.002616 Loss_G: 0.003330 \n",
      "[1/10][26/100][273] Loss_D: 0.003305 Loss_G: 0.003876 \n",
      "[1/10][26/100][274] Loss_D: 0.002235 Loss_G: 0.004005 \n",
      "[1/10][26/100][275] Loss_D: 0.001806 Loss_G: 0.003713 \n",
      "[1/10][26/100][276] Loss_D: 0.002479 Loss_G: 0.001443 \n",
      "[1/10][26/100][277] Loss_D: 0.003897 Loss_G: 0.000504 \n",
      "[1/10][26/100][278] Loss_D: 0.003651 Loss_G: 0.004307 \n",
      "[1/10][26/100][279] Loss_D: 0.004224 Loss_G: 0.002160 \n",
      "[1/10][26/100][280] Loss_D: 0.001742 Loss_G: 0.003283 \n",
      "[1/10][26/100][281] Loss_D: 0.002963 Loss_G: 0.002073 \n",
      "[1/10][26/100][282] Loss_D: 0.003216 Loss_G: 0.003375 \n",
      "[1/10][26/100][283] Loss_D: 0.002502 Loss_G: 0.003076 \n",
      "[1/10][26/100][284] Loss_D: 0.005702 Loss_G: 0.002214 \n",
      "[1/10][26/100][285] Loss_D: 0.000748 Loss_G: 0.002799 \n",
      "[1/10][26/100][286] Loss_D: 0.001417 Loss_G: 0.003712 \n",
      "[1/10][26/100][287] Loss_D: 0.003561 Loss_G: 0.003507 \n",
      "[1/10][26/100][288] Loss_D: 0.001082 Loss_G: 0.003096 \n",
      "[1/10][26/100][289] Loss_D: 0.004102 Loss_G: 0.000994 \n",
      "[1/10][26/100][290] Loss_D: 0.002425 Loss_G: 0.005090 \n",
      "[1/10][26/100][291] Loss_D: 0.001065 Loss_G: 0.002229 \n",
      "[1/10][26/100][292] Loss_D: 0.002028 Loss_G: 0.002357 \n",
      "[1/10][26/100][293] Loss_D: 0.000457 Loss_G: 0.000296 \n",
      "[1/10][26/100][294] Loss_D: 0.003028 Loss_G: 0.002856 \n",
      "[1/10][26/100][295] Loss_D: 0.002762 Loss_G: 0.003176 \n",
      "[1/10][26/100][296] Loss_D: 0.001913 Loss_G: 0.003000 \n",
      "[1/10][26/100][297] Loss_D: 0.004242 Loss_G: 0.003290 \n",
      "[1/10][26/100][298] Loss_D: 0.001720 Loss_G: 0.003354 \n",
      "[1/10][26/100][299] Loss_D: 0.002685 Loss_G: 0.002366 \n",
      "[1/10][26/100][300] Loss_D: 0.001295 Loss_G: 0.002677 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][26/100][301] Loss_D: 0.002372 Loss_G: 0.001548 \n",
      "[1/10][26/100][302] Loss_D: 0.002248 Loss_G: 0.002999 \n",
      "[1/10][26/100][303] Loss_D: 0.003680 Loss_G: 0.002452 \n",
      "[1/10][26/100][304] Loss_D: 0.003587 Loss_G: 0.002650 \n",
      "[1/10][26/100][305] Loss_D: 0.003831 Loss_G: 0.002544 \n",
      "[1/10][26/100][306] Loss_D: 0.004565 Loss_G: 0.002764 \n",
      "[1/10][26/100][307] Loss_D: 0.004842 Loss_G: 0.001201 \n",
      "[1/10][26/100][308] Loss_D: 0.004395 Loss_G: 0.002799 \n",
      "[1/10][26/100][309] Loss_D: 0.001112 Loss_G: 0.002917 \n",
      "[1/10][26/100][310] Loss_D: 0.002635 Loss_G: 0.004290 \n",
      "[1/10][26/100][311] Loss_D: 0.002732 Loss_G: 0.002151 \n",
      "[1/10][26/100][312] Loss_D: 0.003240 Loss_G: 0.001802 \n",
      "[1/10][26/100][313] Loss_D: 0.002164 Loss_G: 0.002715 \n",
      "[1/10][26/100][314] Loss_D: 0.000409 Loss_G: 0.002146 \n",
      "[1/10][26/100][315] Loss_D: 0.003111 Loss_G: 0.004301 \n",
      "[1/10][26/100][316] Loss_D: 0.005148 Loss_G: 0.003258 \n",
      "[1/10][26/100][317] Loss_D: 0.003117 Loss_G: 0.004852 \n",
      "[1/10][26/100][318] Loss_D: 0.002145 Loss_G: 0.002360 \n",
      "[1/10][26/100][319] Loss_D: 0.004303 Loss_G: 0.002981 \n",
      "[1/10][26/100][320] Loss_D: 0.002108 Loss_G: 0.002218 \n",
      "[1/10][26/100][321] Loss_D: 0.004098 Loss_G: 0.004337 \n",
      "[1/10][26/100][322] Loss_D: 0.003547 Loss_G: 0.002697 \n",
      "[1/10][26/100][323] Loss_D: 0.002156 Loss_G: 0.003904 \n",
      "[1/10][26/100][324] Loss_D: 0.002353 Loss_G: 0.002623 \n",
      "[1/10][26/100][325] Loss_D: 0.003591 Loss_G: 0.001187 \n",
      "[1/10][26/100][326] Loss_D: 0.005215 Loss_G: 0.006384 \n",
      "[1/10][26/100][327] Loss_D: 0.000855 Loss_G: 0.003206 \n",
      "[1/10][26/100][328] Loss_D: 0.001764 Loss_G: 0.003224 \n",
      "[1/10][26/100][329] Loss_D: 0.001808 Loss_G: 0.001546 \n",
      "[1/10][26/100][330] Loss_D: 0.006217 Loss_G: 0.002739 \n",
      "[1/10][26/100][331] Loss_D: 0.001972 Loss_G: 0.002608 \n",
      "[1/10][26/100][332] Loss_D: 0.000895 Loss_G: 0.002487 \n",
      "[1/10][26/100][333] Loss_D: 0.004780 Loss_G: 0.002488 \n",
      "[1/10][26/100][334] Loss_D: 0.002302 Loss_G: 0.002753 \n",
      "[1/10][26/100][335] Loss_D: 0.002585 Loss_G: 0.003616 \n",
      "[1/10][26/100][336] Loss_D: 0.003836 Loss_G: 0.002583 \n",
      "[1/10][26/100][337] Loss_D: 0.003488 Loss_G: 0.002769 \n",
      "[1/10][26/100][338] Loss_D: 0.001171 Loss_G: 0.001548 \n",
      "[1/10][26/100][339] Loss_D: 0.000928 Loss_G: 0.002345 \n",
      "[1/10][26/100][340] Loss_D: 0.002787 Loss_G: 0.002100 \n",
      "[1/10][26/100][341] Loss_D: 0.004241 Loss_G: 0.006301 \n",
      "[1/10][26/100][342] Loss_D: 0.003167 Loss_G: 0.002218 \n",
      "[1/10][26/100][343] Loss_D: 0.000391 Loss_G: 0.002456 \n",
      "[1/10][26/100][344] Loss_D: 0.006350 Loss_G: 0.002903 \n",
      "[1/10][26/100][345] Loss_D: 0.004913 Loss_G: 0.003697 \n",
      "[1/10][26/100][346] Loss_D: 0.003877 Loss_G: 0.003264 \n",
      "[1/10][26/100][347] Loss_D: 0.001568 Loss_G: 0.003582 \n",
      "[1/10][26/100][348] Loss_D: 0.001975 Loss_G: 0.002722 \n",
      "[1/10][26/100][349] Loss_D: 0.003631 Loss_G: 0.002827 \n",
      "[1/10][26/100][350] Loss_D: 0.002777 Loss_G: 0.002624 \n",
      "[1/10][26/100][351] Loss_D: 0.001993 Loss_G: 0.002749 \n",
      "[1/10][26/100][352] Loss_D: 0.002452 Loss_G: 0.002752 \n",
      "[1/10][26/100][353] Loss_D: 0.001252 Loss_G: 0.002071 \n",
      "[1/10][26/100][354] Loss_D: 0.003528 Loss_G: 0.003168 \n",
      "[1/10][26/100][355] Loss_D: 0.001426 Loss_G: 0.002608 \n",
      "[1/10][26/100][356] Loss_D: 0.004885 Loss_G: 0.001434 \n",
      "[1/10][26/100][357] Loss_D: 0.002158 Loss_G: 0.003004 \n",
      "[1/10][26/100][358] Loss_D: 0.003554 Loss_G: 0.001601 \n",
      "[1/10][26/100][359] Loss_D: 0.004104 Loss_G: 0.005525 \n",
      "[1/10][26/100][360] Loss_D: 0.001477 Loss_G: 0.002323 \n",
      "[1/10][26/100][361] Loss_D: 0.002993 Loss_G: 0.001134 \n",
      "[1/10][26/100][362] Loss_D: 0.002968 Loss_G: 0.002065 \n",
      "[1/10][26/100][363] Loss_D: 0.002346 Loss_G: 0.002660 \n",
      "[1/10][26/100][364] Loss_D: 0.002957 Loss_G: 0.002769 \n",
      "[1/10][26/100][365] Loss_D: 0.001874 Loss_G: 0.002296 \n",
      "[1/10][26/100][366] Loss_D: 0.004329 Loss_G: 0.002012 \n",
      "[1/10][26/100][367] Loss_D: 0.002693 Loss_G: 0.001945 \n",
      "[1/10][26/100][368] Loss_D: 0.003427 Loss_G: 0.003789 \n",
      "[1/10][26/100][369] Loss_D: 0.002999 Loss_G: 0.001459 \n",
      "[1/10][26/100][370] Loss_D: 0.002217 Loss_G: 0.005574 \n",
      "[1/10][26/100][371] Loss_D: 0.002610 Loss_G: 0.003487 \n",
      "[1/10][26/100][372] Loss_D: 0.002303 Loss_G: -0.000223 \n",
      "[1/10][26/100][373] Loss_D: 0.003183 Loss_G: 0.000489 \n",
      "[1/10][26/100][374] Loss_D: 0.001487 Loss_G: 0.001802 \n",
      "[1/10][26/100][375] Loss_D: 0.002855 Loss_G: 0.002871 \n",
      "[1/10][26/100][376] Loss_D: 0.001909 Loss_G: 0.003021 \n",
      "[1/10][26/100][377] Loss_D: 0.003576 Loss_G: 0.001657 \n",
      "[1/10][26/100][378] Loss_D: 0.005240 Loss_G: 0.002391 \n",
      "[1/10][26/100][379] Loss_D: 0.004698 Loss_G: -0.000413 \n",
      "[1/10][26/100][380] Loss_D: -0.000215 Loss_G: 0.004047 \n",
      "[1/10][26/100][381] Loss_D: 0.002156 Loss_G: 0.004210 \n",
      "[1/10][26/100][382] Loss_D: 0.004484 Loss_G: 0.005766 \n",
      "[1/10][26/100][383] Loss_D: 0.001795 Loss_G: 0.004091 \n",
      "[1/10][26/100][384] Loss_D: 0.000638 Loss_G: 0.002408 \n",
      "[1/10][26/100][385] Loss_D: 0.001960 Loss_G: 0.001023 \n",
      "[1/10][26/100][386] Loss_D: 0.002697 Loss_G: 0.002916 \n",
      "[1/10][26/100][387] Loss_D: 0.002759 Loss_G: 0.002728 \n",
      "[1/10][26/100][388] Loss_D: 0.003711 Loss_G: 0.002349 \n",
      "[1/10][26/100][389] Loss_D: 0.002289 Loss_G: 0.001207 \n",
      "[1/10][26/100][390] Loss_D: 0.003151 Loss_G: 0.002570 \n",
      "[1/10][26/100][391] Loss_D: 0.001562 Loss_G: 0.003021 \n",
      "[1/10][26/100][392] Loss_D: 0.003304 Loss_G: 0.003337 \n",
      "[1/10][26/100][393] Loss_D: 0.004053 Loss_G: 0.000800 \n",
      "[1/10][26/100][394] Loss_D: 0.002486 Loss_G: 0.002576 \n",
      "[1/10][26/100][395] Loss_D: 0.003719 Loss_G: 0.003663 \n",
      "[1/10][26/100][396] Loss_D: 0.002078 Loss_G: 0.002283 \n",
      "[1/10][26/100][397] Loss_D: 0.000216 Loss_G: 0.002639 \n",
      "[1/10][26/100][398] Loss_D: 0.001943 Loss_G: 0.003108 \n",
      "[1/10][26/100][399] Loss_D: 0.002427 Loss_G: 0.004065 \n",
      "[1/10][26/100][400] Loss_D: 0.001756 Loss_G: 0.000223 \n",
      "[2/10][26/100][401] Loss_D: 0.003783 Loss_G: 0.001573 \n",
      "[2/10][26/100][402] Loss_D: 0.002617 Loss_G: 0.003161 \n",
      "[2/10][26/100][403] Loss_D: 0.003037 Loss_G: 0.003237 \n",
      "[2/10][26/100][404] Loss_D: 0.003390 Loss_G: 0.001647 \n",
      "[2/10][26/100][405] Loss_D: 0.002778 Loss_G: 0.002081 \n",
      "[2/10][26/100][406] Loss_D: 0.002417 Loss_G: 0.001766 \n",
      "[2/10][26/100][407] Loss_D: 0.001695 Loss_G: 0.001355 \n",
      "[2/10][26/100][408] Loss_D: 0.002826 Loss_G: 0.001900 \n",
      "[2/10][26/100][409] Loss_D: 0.003007 Loss_G: 0.003648 \n",
      "[2/10][26/100][410] Loss_D: 0.003041 Loss_G: 0.001561 \n",
      "[2/10][26/100][411] Loss_D: 0.003061 Loss_G: 0.004625 \n",
      "[2/10][26/100][412] Loss_D: 0.003079 Loss_G: 0.003250 \n",
      "[2/10][26/100][413] Loss_D: 0.000510 Loss_G: 0.002968 \n",
      "[2/10][26/100][414] Loss_D: 0.001175 Loss_G: 0.001650 \n",
      "[2/10][26/100][415] Loss_D: 0.004410 Loss_G: 0.004662 \n",
      "[2/10][26/100][416] Loss_D: 0.002838 Loss_G: 0.003313 \n",
      "[2/10][26/100][417] Loss_D: 0.001256 Loss_G: 0.004512 \n",
      "[2/10][26/100][418] Loss_D: 0.004320 Loss_G: 0.003530 \n",
      "[2/10][26/100][419] Loss_D: 0.002323 Loss_G: 0.002658 \n",
      "[2/10][26/100][420] Loss_D: 0.000832 Loss_G: 0.003326 \n",
      "[2/10][26/100][421] Loss_D: 0.001527 Loss_G: 0.002911 \n",
      "[2/10][26/100][422] Loss_D: 0.002716 Loss_G: 0.004049 \n",
      "[2/10][26/100][423] Loss_D: 0.003158 Loss_G: 0.003405 \n",
      "[2/10][26/100][424] Loss_D: 0.001701 Loss_G: 0.001310 \n",
      "[2/10][26/100][425] Loss_D: 0.002140 Loss_G: 0.003237 \n",
      "[2/10][26/100][426] Loss_D: 0.001813 Loss_G: 0.002800 \n",
      "[2/10][26/100][427] Loss_D: 0.002852 Loss_G: 0.003392 \n",
      "[2/10][26/100][428] Loss_D: 0.004901 Loss_G: 0.003760 \n",
      "[2/10][26/100][429] Loss_D: 0.002494 Loss_G: 0.003461 \n",
      "[2/10][26/100][430] Loss_D: 0.001072 Loss_G: 0.001322 \n",
      "[2/10][26/100][431] Loss_D: 0.002217 Loss_G: 0.002434 \n",
      "[2/10][26/100][432] Loss_D: 0.000265 Loss_G: 0.002529 \n",
      "[2/10][26/100][433] Loss_D: 0.001000 Loss_G: 0.002088 \n",
      "[2/10][26/100][434] Loss_D: 0.000688 Loss_G: 0.001593 \n",
      "[2/10][26/100][435] Loss_D: 0.002590 Loss_G: 0.002610 \n",
      "[2/10][26/100][436] Loss_D: 0.002304 Loss_G: 0.001701 \n",
      "[2/10][26/100][437] Loss_D: 0.001232 Loss_G: 0.002891 \n",
      "[2/10][26/100][438] Loss_D: 0.002822 Loss_G: 0.002417 \n",
      "[2/10][26/100][439] Loss_D: 0.002734 Loss_G: 0.002396 \n",
      "[2/10][26/100][440] Loss_D: 0.003878 Loss_G: 0.003651 \n",
      "[2/10][26/100][441] Loss_D: 0.002579 Loss_G: 0.005343 \n",
      "[2/10][26/100][442] Loss_D: 0.006315 Loss_G: 0.003239 \n",
      "[2/10][26/100][443] Loss_D: 0.003980 Loss_G: 0.004422 \n",
      "[2/10][26/100][444] Loss_D: 0.002393 Loss_G: 0.002323 \n",
      "[2/10][26/100][445] Loss_D: 0.002932 Loss_G: 0.001481 \n",
      "[2/10][26/100][446] Loss_D: -0.000095 Loss_G: 0.003826 \n",
      "[2/10][26/100][447] Loss_D: 0.003107 Loss_G: 0.001309 \n",
      "[2/10][26/100][448] Loss_D: 0.002752 Loss_G: 0.001094 \n",
      "[2/10][26/100][449] Loss_D: 0.001590 Loss_G: 0.003399 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][26/100][450] Loss_D: 0.001516 Loss_G: 0.003678 \n",
      "[2/10][26/100][451] Loss_D: 0.001981 Loss_G: 0.002764 \n",
      "[2/10][26/100][452] Loss_D: 0.002041 Loss_G: 0.001292 \n",
      "[2/10][26/100][453] Loss_D: 0.002544 Loss_G: 0.002003 \n",
      "[2/10][26/100][454] Loss_D: 0.003786 Loss_G: 0.002933 \n",
      "[2/10][26/100][455] Loss_D: 0.005093 Loss_G: 0.003009 \n",
      "[2/10][26/100][456] Loss_D: 0.001415 Loss_G: 0.002430 \n",
      "[2/10][26/100][457] Loss_D: 0.004683 Loss_G: 0.003706 \n",
      "[2/10][26/100][458] Loss_D: 0.001700 Loss_G: 0.003699 \n",
      "[2/10][26/100][459] Loss_D: 0.003692 Loss_G: 0.003024 \n",
      "[2/10][26/100][460] Loss_D: 0.000586 Loss_G: 0.003178 \n",
      "[2/10][26/100][461] Loss_D: 0.004567 Loss_G: 0.004311 \n",
      "[2/10][26/100][462] Loss_D: 0.004065 Loss_G: -0.000172 \n",
      "[2/10][26/100][463] Loss_D: 0.001691 Loss_G: 0.001705 \n",
      "[2/10][26/100][464] Loss_D: 0.001830 Loss_G: 0.003100 \n",
      "[2/10][26/100][465] Loss_D: 0.004785 Loss_G: 0.001930 \n",
      "[2/10][26/100][466] Loss_D: 0.003047 Loss_G: 0.003423 \n",
      "[2/10][26/100][467] Loss_D: 0.004276 Loss_G: 0.003520 \n",
      "[2/10][26/100][468] Loss_D: 0.000423 Loss_G: 0.002795 \n",
      "[2/10][26/100][469] Loss_D: 0.002086 Loss_G: 0.001526 \n",
      "[2/10][26/100][470] Loss_D: 0.003741 Loss_G: 0.001029 \n",
      "[2/10][26/100][471] Loss_D: 0.001107 Loss_G: 0.002590 \n",
      "[2/10][26/100][472] Loss_D: 0.003177 Loss_G: 0.000170 \n",
      "[2/10][26/100][473] Loss_D: 0.002162 Loss_G: 0.002089 \n",
      "[2/10][26/100][474] Loss_D: 0.001798 Loss_G: 0.001545 \n",
      "[2/10][26/100][475] Loss_D: 0.001504 Loss_G: 0.003233 \n",
      "[2/10][26/100][476] Loss_D: 0.002724 Loss_G: 0.001287 \n",
      "[2/10][26/100][477] Loss_D: 0.002150 Loss_G: 0.003784 \n",
      "[2/10][26/100][478] Loss_D: 0.001400 Loss_G: 0.002460 \n",
      "[2/10][26/100][479] Loss_D: 0.003131 Loss_G: 0.003699 \n",
      "[2/10][26/100][480] Loss_D: 0.003106 Loss_G: 0.001021 \n",
      "[2/10][26/100][481] Loss_D: 0.003100 Loss_G: 0.003900 \n",
      "[2/10][26/100][482] Loss_D: 0.003215 Loss_G: 0.002533 \n",
      "[2/10][26/100][483] Loss_D: 0.003251 Loss_G: 0.003165 \n",
      "[2/10][26/100][484] Loss_D: 0.001321 Loss_G: 0.001123 \n",
      "[2/10][26/100][485] Loss_D: 0.003289 Loss_G: 0.001244 \n",
      "[2/10][26/100][486] Loss_D: 0.003299 Loss_G: 0.000438 \n",
      "[2/10][26/100][487] Loss_D: 0.001486 Loss_G: 0.002576 \n",
      "[2/10][26/100][488] Loss_D: 0.002709 Loss_G: 0.002143 \n",
      "[2/10][26/100][489] Loss_D: 0.001923 Loss_G: 0.002951 \n",
      "[2/10][26/100][490] Loss_D: 0.001823 Loss_G: 0.004268 \n",
      "[2/10][26/100][491] Loss_D: 0.001973 Loss_G: 0.003050 \n",
      "[2/10][26/100][492] Loss_D: 0.002513 Loss_G: 0.001596 \n",
      "[2/10][26/100][493] Loss_D: 0.003194 Loss_G: 0.002682 \n",
      "[2/10][26/100][494] Loss_D: 0.003258 Loss_G: 0.003937 \n",
      "[2/10][26/100][495] Loss_D: 0.000169 Loss_G: 0.002897 \n",
      "[2/10][26/100][496] Loss_D: 0.000782 Loss_G: 0.002216 \n",
      "[2/10][26/100][497] Loss_D: 0.002592 Loss_G: 0.001356 \n",
      "[2/10][26/100][498] Loss_D: 0.003936 Loss_G: 0.001147 \n",
      "[2/10][26/100][499] Loss_D: 0.004084 Loss_G: 0.003672 \n",
      "[2/10][26/100][500] Loss_D: 0.005053 Loss_G: 0.001796 \n",
      "[2/10][26/100][501] Loss_D: 0.002636 Loss_G: 0.003426 \n",
      "[2/10][26/100][502] Loss_D: 0.002256 Loss_G: 0.004818 \n",
      "[2/10][26/100][503] Loss_D: 0.003372 Loss_G: 0.003206 \n",
      "[2/10][26/100][504] Loss_D: 0.003149 Loss_G: 0.002504 \n",
      "[2/10][26/100][505] Loss_D: 0.002489 Loss_G: 0.001973 \n",
      "[2/10][26/100][506] Loss_D: 0.001805 Loss_G: 0.001605 \n",
      "[2/10][26/100][507] Loss_D: 0.001894 Loss_G: 0.002922 \n",
      "[2/10][26/100][508] Loss_D: 0.001775 Loss_G: 0.003066 \n",
      "[2/10][26/100][509] Loss_D: 0.000946 Loss_G: 0.004161 \n",
      "[2/10][26/100][510] Loss_D: 0.001509 Loss_G: 0.003175 \n",
      "[2/10][26/100][511] Loss_D: 0.004073 Loss_G: 0.001589 \n",
      "[2/10][26/100][512] Loss_D: 0.001775 Loss_G: 0.002400 \n",
      "[2/10][26/100][513] Loss_D: 0.001596 Loss_G: 0.003704 \n",
      "[2/10][26/100][514] Loss_D: 0.005757 Loss_G: 0.004823 \n",
      "[2/10][26/100][515] Loss_D: 0.002388 Loss_G: 0.004062 \n",
      "[2/10][26/100][516] Loss_D: 0.003683 Loss_G: 0.003038 \n",
      "[2/10][26/100][517] Loss_D: 0.002813 Loss_G: 0.003966 \n",
      "[2/10][26/100][518] Loss_D: 0.001366 Loss_G: 0.003450 \n",
      "[2/10][26/100][519] Loss_D: 0.001880 Loss_G: -0.000379 \n",
      "[2/10][26/100][520] Loss_D: 0.002756 Loss_G: 0.003577 \n",
      "[2/10][26/100][521] Loss_D: 0.006529 Loss_G: 0.002376 \n",
      "[2/10][26/100][522] Loss_D: 0.003314 Loss_G: 0.003939 \n",
      "[2/10][26/100][523] Loss_D: 0.003520 Loss_G: 0.003328 \n",
      "[2/10][26/100][524] Loss_D: 0.004039 Loss_G: 0.003005 \n",
      "[2/10][26/100][525] Loss_D: 0.001858 Loss_G: 0.004642 \n",
      "[2/10][26/100][526] Loss_D: 0.004004 Loss_G: 0.002215 \n",
      "[2/10][26/100][527] Loss_D: 0.002022 Loss_G: 0.003834 \n",
      "[2/10][26/100][528] Loss_D: 0.004163 Loss_G: 0.002117 \n",
      "[2/10][26/100][529] Loss_D: 0.005234 Loss_G: 0.002549 \n",
      "[2/10][26/100][530] Loss_D: 0.002087 Loss_G: 0.001586 \n",
      "[2/10][26/100][531] Loss_D: 0.002150 Loss_G: 0.000980 \n",
      "[2/10][26/100][532] Loss_D: 0.001463 Loss_G: 0.001604 \n",
      "[2/10][26/100][533] Loss_D: 0.001680 Loss_G: 0.004834 \n",
      "[2/10][26/100][534] Loss_D: 0.001781 Loss_G: 0.001026 \n",
      "[2/10][26/100][535] Loss_D: 0.002149 Loss_G: 0.002063 \n",
      "[2/10][26/100][536] Loss_D: 0.002011 Loss_G: 0.002753 \n",
      "[2/10][26/100][537] Loss_D: 0.002195 Loss_G: 0.001899 \n",
      "[2/10][26/100][538] Loss_D: 0.001823 Loss_G: 0.002638 \n",
      "[2/10][26/100][539] Loss_D: 0.001677 Loss_G: 0.002077 \n",
      "[2/10][26/100][540] Loss_D: 0.002311 Loss_G: 0.002469 \n",
      "[2/10][26/100][541] Loss_D: 0.003878 Loss_G: 0.002353 \n",
      "[2/10][26/100][542] Loss_D: 0.004088 Loss_G: 0.003466 \n",
      "[2/10][26/100][543] Loss_D: 0.006474 Loss_G: 0.000114 \n",
      "[2/10][26/100][544] Loss_D: 0.003529 Loss_G: 0.004676 \n",
      "[2/10][26/100][545] Loss_D: 0.002908 Loss_G: 0.003412 \n",
      "[2/10][26/100][546] Loss_D: 0.002366 Loss_G: 0.003484 \n",
      "[2/10][26/100][547] Loss_D: 0.000792 Loss_G: 0.002939 \n",
      "[2/10][26/100][548] Loss_D: 0.002380 Loss_G: 0.003706 \n",
      "[2/10][26/100][549] Loss_D: 0.004491 Loss_G: 0.001391 \n",
      "[2/10][26/100][550] Loss_D: 0.001532 Loss_G: 0.005097 \n",
      "[2/10][26/100][551] Loss_D: 0.001149 Loss_G: 0.001187 \n",
      "[2/10][26/100][552] Loss_D: 0.002172 Loss_G: 0.003285 \n",
      "[2/10][26/100][553] Loss_D: 0.001062 Loss_G: 0.002039 \n",
      "[2/10][26/100][554] Loss_D: 0.001761 Loss_G: 0.001535 \n",
      "[2/10][26/100][555] Loss_D: 0.002724 Loss_G: 0.002610 \n",
      "[2/10][26/100][556] Loss_D: 0.001729 Loss_G: 0.002477 \n",
      "[2/10][26/100][557] Loss_D: 0.004134 Loss_G: 0.004495 \n",
      "[2/10][26/100][558] Loss_D: 0.002507 Loss_G: 0.002410 \n",
      "[2/10][26/100][559] Loss_D: 0.002366 Loss_G: 0.003383 \n",
      "[2/10][26/100][560] Loss_D: 0.002216 Loss_G: 0.002070 \n",
      "[2/10][26/100][561] Loss_D: 0.002257 Loss_G: 0.001262 \n",
      "[2/10][26/100][562] Loss_D: 0.000949 Loss_G: 0.002912 \n",
      "[2/10][26/100][563] Loss_D: 0.001359 Loss_G: 0.002088 \n",
      "[2/10][26/100][564] Loss_D: 0.001905 Loss_G: 0.003155 \n",
      "[2/10][26/100][565] Loss_D: 0.000768 Loss_G: 0.003335 \n",
      "[2/10][26/100][566] Loss_D: 0.003388 Loss_G: 0.002138 \n",
      "[2/10][26/100][567] Loss_D: 0.003297 Loss_G: 0.005112 \n",
      "[2/10][26/100][568] Loss_D: 0.000503 Loss_G: 0.002571 \n",
      "[2/10][26/100][569] Loss_D: 0.003255 Loss_G: 0.003590 \n",
      "[2/10][26/100][570] Loss_D: 0.002578 Loss_G: 0.004573 \n",
      "[2/10][26/100][571] Loss_D: 0.004704 Loss_G: 0.001051 \n",
      "[2/10][26/100][572] Loss_D: 0.002626 Loss_G: 0.001825 \n",
      "[2/10][26/100][573] Loss_D: 0.003094 Loss_G: 0.004640 \n",
      "[2/10][26/100][574] Loss_D: 0.003686 Loss_G: 0.001119 \n",
      "[2/10][26/100][575] Loss_D: 0.002773 Loss_G: 0.003207 \n",
      "[2/10][26/100][576] Loss_D: 0.002491 Loss_G: 0.003009 \n",
      "[2/10][26/100][577] Loss_D: 0.000496 Loss_G: 0.001708 \n",
      "[2/10][26/100][578] Loss_D: 0.003791 Loss_G: 0.003429 \n",
      "[2/10][26/100][579] Loss_D: 0.002851 Loss_G: 0.002451 \n",
      "[2/10][26/100][580] Loss_D: 0.002999 Loss_G: 0.004092 \n",
      "[2/10][26/100][581] Loss_D: 0.002710 Loss_G: 0.001046 \n",
      "[2/10][26/100][582] Loss_D: 0.006969 Loss_G: 0.004174 \n",
      "[2/10][26/100][583] Loss_D: 0.002441 Loss_G: 0.003132 \n",
      "[2/10][26/100][584] Loss_D: 0.003433 Loss_G: 0.007324 \n",
      "[2/10][26/100][585] Loss_D: 0.002433 Loss_G: 0.000810 \n",
      "[2/10][26/100][586] Loss_D: 0.002426 Loss_G: 0.002928 \n",
      "[2/10][26/100][587] Loss_D: 0.001792 Loss_G: 0.002149 \n",
      "[2/10][26/100][588] Loss_D: 0.000872 Loss_G: 0.002451 \n",
      "[2/10][26/100][589] Loss_D: 0.003802 Loss_G: 0.002997 \n",
      "[2/10][26/100][590] Loss_D: 0.003452 Loss_G: 0.003097 \n",
      "[2/10][26/100][591] Loss_D: 0.003392 Loss_G: 0.002596 \n",
      "[2/10][26/100][592] Loss_D: 0.003545 Loss_G: 0.001261 \n",
      "[2/10][26/100][593] Loss_D: 0.002062 Loss_G: 0.003637 \n",
      "[2/10][26/100][594] Loss_D: 0.004187 Loss_G: 0.004249 \n",
      "[2/10][26/100][595] Loss_D: 0.002689 Loss_G: 0.003307 \n",
      "[2/10][26/100][596] Loss_D: 0.001547 Loss_G: 0.002153 \n",
      "[2/10][26/100][597] Loss_D: 0.001493 Loss_G: 0.002771 \n",
      "[2/10][26/100][598] Loss_D: 0.002543 Loss_G: 0.002233 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][26/100][599] Loss_D: 0.001638 Loss_G: 0.003764 \n",
      "[2/10][26/100][600] Loss_D: 0.002327 Loss_G: 0.002195 \n",
      "[3/10][26/100][601] Loss_D: 0.001475 Loss_G: 0.003072 \n",
      "[3/10][26/100][602] Loss_D: 0.003281 Loss_G: 0.001635 \n",
      "[3/10][26/100][603] Loss_D: 0.002632 Loss_G: 0.001150 \n",
      "[3/10][26/100][604] Loss_D: 0.003867 Loss_G: 0.001764 \n",
      "[3/10][26/100][605] Loss_D: 0.003179 Loss_G: 0.002596 \n",
      "[3/10][26/100][606] Loss_D: 0.002367 Loss_G: 0.003969 \n",
      "[3/10][26/100][607] Loss_D: 0.003617 Loss_G: 0.004697 \n",
      "[3/10][26/100][608] Loss_D: 0.001709 Loss_G: 0.001449 \n",
      "[3/10][26/100][609] Loss_D: 0.000370 Loss_G: 0.001799 \n",
      "[3/10][26/100][610] Loss_D: 0.001792 Loss_G: 0.002036 \n",
      "[3/10][26/100][611] Loss_D: 0.002417 Loss_G: 0.001967 \n",
      "[3/10][26/100][612] Loss_D: 0.002970 Loss_G: 0.001681 \n",
      "[3/10][26/100][613] Loss_D: 0.002919 Loss_G: 0.002511 \n",
      "[3/10][26/100][614] Loss_D: 0.002392 Loss_G: 0.002992 \n",
      "[3/10][26/100][615] Loss_D: 0.002207 Loss_G: 0.002047 \n",
      "[3/10][26/100][616] Loss_D: 0.002638 Loss_G: 0.002525 \n",
      "[3/10][26/100][617] Loss_D: 0.002348 Loss_G: 0.001698 \n",
      "[3/10][26/100][618] Loss_D: 0.003201 Loss_G: 0.003766 \n",
      "[3/10][26/100][619] Loss_D: 0.001304 Loss_G: 0.004983 \n",
      "[3/10][26/100][620] Loss_D: 0.002228 Loss_G: 0.002204 \n",
      "[3/10][26/100][621] Loss_D: 0.002023 Loss_G: 0.001951 \n",
      "[3/10][26/100][622] Loss_D: 0.002479 Loss_G: 0.001548 \n",
      "[3/10][26/100][623] Loss_D: 0.003558 Loss_G: 0.001831 \n",
      "[3/10][26/100][624] Loss_D: 0.002545 Loss_G: 0.001637 \n",
      "[3/10][26/100][625] Loss_D: 0.003053 Loss_G: 0.001200 \n",
      "[3/10][26/100][626] Loss_D: 0.002093 Loss_G: 0.004863 \n",
      "[3/10][26/100][627] Loss_D: 0.002353 Loss_G: 0.003266 \n",
      "[3/10][26/100][628] Loss_D: 0.005705 Loss_G: 0.004113 \n",
      "[3/10][26/100][629] Loss_D: 0.002843 Loss_G: 0.003863 \n",
      "[3/10][26/100][630] Loss_D: 0.001294 Loss_G: 0.003377 \n",
      "[3/10][26/100][631] Loss_D: 0.003378 Loss_G: 0.001760 \n",
      "[3/10][26/100][632] Loss_D: 0.004736 Loss_G: 0.002123 \n",
      "[3/10][26/100][633] Loss_D: 0.001930 Loss_G: 0.003013 \n",
      "[3/10][26/100][634] Loss_D: 0.002832 Loss_G: 0.002760 \n",
      "[3/10][26/100][635] Loss_D: 0.004438 Loss_G: 0.000347 \n",
      "[3/10][26/100][636] Loss_D: 0.002987 Loss_G: 0.004036 \n",
      "[3/10][26/100][637] Loss_D: 0.003382 Loss_G: 0.001843 \n",
      "[3/10][26/100][638] Loss_D: 0.001735 Loss_G: 0.001804 \n",
      "[3/10][26/100][639] Loss_D: 0.003685 Loss_G: 0.001964 \n",
      "[3/10][26/100][640] Loss_D: 0.001821 Loss_G: 0.005317 \n",
      "[3/10][26/100][641] Loss_D: 0.002278 Loss_G: 0.002413 \n",
      "[3/10][26/100][642] Loss_D: 0.001707 Loss_G: 0.002856 \n",
      "[3/10][26/100][643] Loss_D: 0.002641 Loss_G: 0.002346 \n",
      "[3/10][26/100][644] Loss_D: 0.001337 Loss_G: 0.001957 \n",
      "[3/10][26/100][645] Loss_D: 0.002283 Loss_G: 0.003128 \n",
      "[3/10][26/100][646] Loss_D: 0.005567 Loss_G: 0.003905 \n",
      "[3/10][26/100][647] Loss_D: 0.005946 Loss_G: 0.001881 \n",
      "[3/10][26/100][648] Loss_D: -0.001551 Loss_G: 0.002195 \n",
      "[3/10][26/100][649] Loss_D: 0.003200 Loss_G: 0.002417 \n",
      "[3/10][26/100][650] Loss_D: 0.001363 Loss_G: 0.004254 \n",
      "[3/10][26/100][651] Loss_D: 0.004127 Loss_G: 0.001877 \n",
      "[3/10][26/100][652] Loss_D: 0.002451 Loss_G: 0.001392 \n",
      "[3/10][26/100][653] Loss_D: 0.001529 Loss_G: 0.003680 \n",
      "[3/10][26/100][654] Loss_D: 0.002803 Loss_G: 0.001800 \n",
      "[3/10][26/100][655] Loss_D: 0.003544 Loss_G: 0.004358 \n",
      "[3/10][26/100][656] Loss_D: 0.004287 Loss_G: 0.004292 \n",
      "[3/10][26/100][657] Loss_D: 0.000929 Loss_G: 0.001303 \n",
      "[3/10][26/100][658] Loss_D: 0.002912 Loss_G: 0.001563 \n",
      "[3/10][26/100][659] Loss_D: 0.003036 Loss_G: 0.001018 \n",
      "[3/10][26/100][660] Loss_D: 0.003443 Loss_G: 0.003081 \n",
      "[3/10][26/100][661] Loss_D: 0.001857 Loss_G: 0.003799 \n",
      "[3/10][26/100][662] Loss_D: 0.002471 Loss_G: 0.003153 \n",
      "[3/10][26/100][663] Loss_D: 0.002522 Loss_G: 0.002358 \n",
      "[3/10][26/100][664] Loss_D: 0.002294 Loss_G: 0.001721 \n",
      "[3/10][26/100][665] Loss_D: 0.004557 Loss_G: 0.003107 \n",
      "[3/10][26/100][666] Loss_D: 0.002061 Loss_G: 0.001688 \n",
      "[3/10][26/100][667] Loss_D: 0.002708 Loss_G: 0.004059 \n",
      "[3/10][26/100][668] Loss_D: 0.002690 Loss_G: 0.002992 \n",
      "[3/10][26/100][669] Loss_D: 0.004334 Loss_G: 0.001573 \n",
      "[3/10][26/100][670] Loss_D: 0.004457 Loss_G: 0.001513 \n",
      "[3/10][26/100][671] Loss_D: 0.003861 Loss_G: 0.002274 \n",
      "[3/10][26/100][672] Loss_D: 0.002884 Loss_G: 0.003095 \n",
      "[3/10][26/100][673] Loss_D: 0.003326 Loss_G: 0.005120 \n",
      "[3/10][26/100][674] Loss_D: 0.001554 Loss_G: 0.004296 \n",
      "[3/10][26/100][675] Loss_D: 0.002830 Loss_G: 0.003594 \n",
      "[3/10][26/100][676] Loss_D: 0.002848 Loss_G: 0.003414 \n",
      "[3/10][26/100][677] Loss_D: 0.003705 Loss_G: 0.000598 \n",
      "[3/10][26/100][678] Loss_D: 0.002708 Loss_G: 0.002737 \n",
      "[3/10][26/100][679] Loss_D: 0.000079 Loss_G: 0.003714 \n",
      "[3/10][26/100][680] Loss_D: 0.000447 Loss_G: 0.002031 \n",
      "[3/10][26/100][681] Loss_D: 0.003693 Loss_G: 0.004133 \n",
      "[3/10][26/100][682] Loss_D: 0.002783 Loss_G: 0.001860 \n",
      "[3/10][26/100][683] Loss_D: 0.002175 Loss_G: 0.005095 \n",
      "[3/10][26/100][684] Loss_D: 0.002662 Loss_G: 0.001443 \n",
      "[3/10][26/100][685] Loss_D: 0.002751 Loss_G: 0.001967 \n",
      "[3/10][26/100][686] Loss_D: 0.004511 Loss_G: 0.001777 \n",
      "[3/10][26/100][687] Loss_D: 0.002454 Loss_G: 0.003230 \n",
      "[3/10][26/100][688] Loss_D: 0.001532 Loss_G: 0.001834 \n",
      "[3/10][26/100][689] Loss_D: 0.002209 Loss_G: 0.003895 \n",
      "[3/10][26/100][690] Loss_D: 0.002802 Loss_G: 0.004258 \n",
      "[3/10][26/100][691] Loss_D: 0.002184 Loss_G: 0.002544 \n",
      "[3/10][26/100][692] Loss_D: 0.002558 Loss_G: 0.002111 \n",
      "[3/10][26/100][693] Loss_D: 0.002249 Loss_G: 0.005673 \n",
      "[3/10][26/100][694] Loss_D: 0.003822 Loss_G: 0.003122 \n",
      "[3/10][26/100][695] Loss_D: 0.003709 Loss_G: 0.002642 \n",
      "[3/10][26/100][696] Loss_D: 0.002928 Loss_G: 0.000225 \n",
      "[3/10][26/100][697] Loss_D: 0.004009 Loss_G: 0.004589 \n",
      "[3/10][26/100][698] Loss_D: 0.002115 Loss_G: 0.005305 \n",
      "[3/10][26/100][699] Loss_D: 0.002220 Loss_G: 0.001290 \n",
      "[3/10][26/100][700] Loss_D: 0.002254 Loss_G: 0.002120 \n",
      "[3/10][26/100][701] Loss_D: 0.001421 Loss_G: 0.004509 \n",
      "[3/10][26/100][702] Loss_D: 0.005236 Loss_G: 0.002438 \n",
      "[3/10][26/100][703] Loss_D: 0.004155 Loss_G: 0.002249 \n",
      "[3/10][26/100][704] Loss_D: 0.003908 Loss_G: 0.003429 \n",
      "[3/10][26/100][705] Loss_D: 0.003569 Loss_G: 0.003120 \n",
      "[3/10][26/100][706] Loss_D: 0.001264 Loss_G: 0.003578 \n",
      "[3/10][26/100][707] Loss_D: 0.003948 Loss_G: 0.003935 \n",
      "[3/10][26/100][708] Loss_D: 0.003631 Loss_G: 0.001317 \n",
      "[3/10][26/100][709] Loss_D: 0.003358 Loss_G: 0.003186 \n",
      "[3/10][26/100][710] Loss_D: 0.002904 Loss_G: 0.001592 \n",
      "[3/10][26/100][711] Loss_D: 0.001228 Loss_G: 0.004271 \n",
      "[3/10][26/100][712] Loss_D: 0.002049 Loss_G: 0.003439 \n",
      "[3/10][26/100][713] Loss_D: 0.003907 Loss_G: 0.002437 \n",
      "[3/10][26/100][714] Loss_D: 0.002323 Loss_G: 0.003376 \n",
      "[3/10][26/100][715] Loss_D: 0.002552 Loss_G: 0.001575 \n",
      "[3/10][26/100][716] Loss_D: 0.002166 Loss_G: 0.002550 \n",
      "[3/10][26/100][717] Loss_D: 0.002740 Loss_G: 0.002719 \n",
      "[3/10][26/100][718] Loss_D: 0.003532 Loss_G: 0.002067 \n",
      "[3/10][26/100][719] Loss_D: 0.002539 Loss_G: 0.004745 \n",
      "[3/10][26/100][720] Loss_D: 0.003021 Loss_G: 0.002588 \n",
      "[3/10][26/100][721] Loss_D: 0.002493 Loss_G: 0.001371 \n",
      "[3/10][26/100][722] Loss_D: 0.002814 Loss_G: 0.003208 \n",
      "[3/10][26/100][723] Loss_D: 0.005006 Loss_G: 0.002328 \n",
      "[3/10][26/100][724] Loss_D: 0.002912 Loss_G: 0.002890 \n",
      "[3/10][26/100][725] Loss_D: 0.002037 Loss_G: 0.003718 \n",
      "[3/10][26/100][726] Loss_D: 0.003076 Loss_G: 0.003699 \n",
      "[3/10][26/100][727] Loss_D: 0.003163 Loss_G: 0.003698 \n",
      "[3/10][26/100][728] Loss_D: 0.003497 Loss_G: 0.002486 \n",
      "[3/10][26/100][729] Loss_D: 0.002729 Loss_G: 0.002129 \n",
      "[3/10][26/100][730] Loss_D: 0.003792 Loss_G: 0.001936 \n",
      "[3/10][26/100][731] Loss_D: 0.002483 Loss_G: 0.001411 \n",
      "[3/10][26/100][732] Loss_D: 0.003935 Loss_G: 0.002764 \n",
      "[3/10][26/100][733] Loss_D: 0.003167 Loss_G: 0.002197 \n",
      "[3/10][26/100][734] Loss_D: 0.002924 Loss_G: 0.001223 \n",
      "[3/10][26/100][735] Loss_D: 0.001683 Loss_G: 0.002607 \n",
      "[3/10][26/100][736] Loss_D: 0.004931 Loss_G: 0.002670 \n",
      "[3/10][26/100][737] Loss_D: 0.005331 Loss_G: 0.001576 \n",
      "[3/10][26/100][738] Loss_D: 0.001227 Loss_G: 0.001731 \n",
      "[3/10][26/100][739] Loss_D: 0.003437 Loss_G: 0.002282 \n",
      "[3/10][26/100][740] Loss_D: 0.006914 Loss_G: 0.002852 \n",
      "[3/10][26/100][741] Loss_D: -0.000216 Loss_G: 0.004286 \n",
      "[3/10][26/100][742] Loss_D: 0.003496 Loss_G: 0.000605 \n",
      "[3/10][26/100][743] Loss_D: 0.003010 Loss_G: 0.004054 \n",
      "[3/10][26/100][744] Loss_D: 0.005305 Loss_G: 0.002662 \n",
      "[3/10][26/100][745] Loss_D: 0.003303 Loss_G: 0.001932 \n",
      "[3/10][26/100][746] Loss_D: 0.001899 Loss_G: 0.001932 \n",
      "[3/10][26/100][747] Loss_D: 0.001513 Loss_G: 0.003617 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][26/100][748] Loss_D: 0.002993 Loss_G: 0.003807 \n",
      "[3/10][26/100][749] Loss_D: 0.001752 Loss_G: 0.001853 \n",
      "[3/10][26/100][750] Loss_D: 0.001851 Loss_G: 0.001587 \n",
      "[3/10][26/100][751] Loss_D: 0.002166 Loss_G: 0.000517 \n",
      "[3/10][26/100][752] Loss_D: 0.003723 Loss_G: 0.002335 \n",
      "[3/10][26/100][753] Loss_D: 0.002990 Loss_G: 0.002285 \n",
      "[3/10][26/100][754] Loss_D: 0.001990 Loss_G: 0.001991 \n",
      "[3/10][26/100][755] Loss_D: 0.002955 Loss_G: 0.002651 \n",
      "[3/10][26/100][756] Loss_D: 0.000676 Loss_G: 0.003252 \n",
      "[3/10][26/100][757] Loss_D: 0.001853 Loss_G: 0.006936 \n",
      "[3/10][26/100][758] Loss_D: 0.003297 Loss_G: 0.001921 \n",
      "[3/10][26/100][759] Loss_D: 0.002130 Loss_G: 0.001457 \n",
      "[3/10][26/100][760] Loss_D: 0.003723 Loss_G: 0.003164 \n",
      "[3/10][26/100][761] Loss_D: 0.001319 Loss_G: 0.003865 \n",
      "[3/10][26/100][762] Loss_D: 0.002516 Loss_G: 0.001967 \n",
      "[3/10][26/100][763] Loss_D: 0.002654 Loss_G: 0.003954 \n",
      "[3/10][26/100][764] Loss_D: 0.004762 Loss_G: 0.003516 \n",
      "[3/10][26/100][765] Loss_D: 0.002453 Loss_G: 0.001947 \n",
      "[3/10][26/100][766] Loss_D: 0.002731 Loss_G: 0.002738 \n",
      "[3/10][26/100][767] Loss_D: 0.001944 Loss_G: 0.003707 \n",
      "[3/10][26/100][768] Loss_D: 0.002608 Loss_G: 0.001627 \n",
      "[3/10][26/100][769] Loss_D: 0.001721 Loss_G: 0.001821 \n",
      "[3/10][26/100][770] Loss_D: 0.001330 Loss_G: 0.003112 \n",
      "[3/10][26/100][771] Loss_D: 0.002355 Loss_G: 0.002076 \n",
      "[3/10][26/100][772] Loss_D: 0.002926 Loss_G: 0.003020 \n",
      "[3/10][26/100][773] Loss_D: 0.002613 Loss_G: 0.005064 \n",
      "[3/10][26/100][774] Loss_D: 0.002732 Loss_G: 0.002237 \n",
      "[3/10][26/100][775] Loss_D: 0.003546 Loss_G: 0.003189 \n",
      "[3/10][26/100][776] Loss_D: 0.006877 Loss_G: 0.004132 \n",
      "[3/10][26/100][777] Loss_D: 0.002985 Loss_G: 0.001825 \n",
      "[3/10][26/100][778] Loss_D: 0.001114 Loss_G: 0.003577 \n",
      "[3/10][26/100][779] Loss_D: 0.003423 Loss_G: 0.002671 \n",
      "[3/10][26/100][780] Loss_D: 0.001843 Loss_G: 0.002972 \n",
      "[3/10][26/100][781] Loss_D: 0.002696 Loss_G: 0.003177 \n",
      "[3/10][26/100][782] Loss_D: 0.001791 Loss_G: 0.003671 \n",
      "[3/10][26/100][783] Loss_D: 0.004942 Loss_G: 0.004009 \n",
      "[3/10][26/100][784] Loss_D: 0.003356 Loss_G: 0.001588 \n",
      "[3/10][26/100][785] Loss_D: 0.002402 Loss_G: 0.003275 \n",
      "[3/10][26/100][786] Loss_D: 0.003506 Loss_G: 0.003660 \n",
      "[3/10][26/100][787] Loss_D: 0.003838 Loss_G: 0.004363 \n",
      "[3/10][26/100][788] Loss_D: 0.002418 Loss_G: 0.002482 \n",
      "[3/10][26/100][789] Loss_D: 0.002425 Loss_G: 0.001593 \n",
      "[3/10][26/100][790] Loss_D: 0.003186 Loss_G: 0.001625 \n",
      "[3/10][26/100][791] Loss_D: 0.001175 Loss_G: 0.001941 \n",
      "[3/10][26/100][792] Loss_D: 0.002740 Loss_G: 0.002081 \n",
      "[3/10][26/100][793] Loss_D: 0.001620 Loss_G: 0.004001 \n",
      "[3/10][26/100][794] Loss_D: 0.000963 Loss_G: 0.003275 \n",
      "[3/10][26/100][795] Loss_D: 0.005081 Loss_G: 0.003382 \n",
      "[3/10][26/100][796] Loss_D: 0.000815 Loss_G: 0.001837 \n",
      "[3/10][26/100][797] Loss_D: 0.003228 Loss_G: 0.002903 \n",
      "[3/10][26/100][798] Loss_D: 0.002031 Loss_G: 0.003207 \n",
      "[3/10][26/100][799] Loss_D: 0.004315 Loss_G: 0.002346 \n",
      "[3/10][26/100][800] Loss_D: 0.002773 Loss_G: 0.001354 \n",
      "[4/10][26/100][801] Loss_D: 0.001399 Loss_G: 0.000928 \n",
      "[4/10][26/100][802] Loss_D: 0.000754 Loss_G: 0.002784 \n",
      "[4/10][26/100][803] Loss_D: 0.002527 Loss_G: 0.003514 \n",
      "[4/10][26/100][804] Loss_D: 0.002098 Loss_G: 0.004166 \n",
      "[4/10][26/100][805] Loss_D: 0.002291 Loss_G: 0.003690 \n",
      "[4/10][26/100][806] Loss_D: 0.003392 Loss_G: 0.003277 \n",
      "[4/10][26/100][807] Loss_D: 0.001465 Loss_G: 0.001821 \n",
      "[4/10][26/100][808] Loss_D: 0.003559 Loss_G: 0.003220 \n",
      "[4/10][26/100][809] Loss_D: 0.003318 Loss_G: 0.001576 \n",
      "[4/10][26/100][810] Loss_D: 0.002263 Loss_G: 0.003400 \n",
      "[4/10][26/100][811] Loss_D: 0.001664 Loss_G: 0.003157 \n",
      "[4/10][26/100][812] Loss_D: 0.002041 Loss_G: 0.002634 \n",
      "[4/10][26/100][813] Loss_D: 0.003715 Loss_G: 0.001893 \n",
      "[4/10][26/100][814] Loss_D: 0.001570 Loss_G: 0.001854 \n",
      "[4/10][26/100][815] Loss_D: 0.003111 Loss_G: 0.001995 \n",
      "[4/10][26/100][816] Loss_D: 0.002605 Loss_G: 0.002166 \n",
      "[4/10][26/100][817] Loss_D: 0.003011 Loss_G: 0.002654 \n",
      "[4/10][26/100][818] Loss_D: 0.002418 Loss_G: 0.001885 \n",
      "[4/10][26/100][819] Loss_D: 0.003091 Loss_G: 0.003250 \n",
      "[4/10][26/100][820] Loss_D: 0.003022 Loss_G: 0.001804 \n",
      "[4/10][26/100][821] Loss_D: 0.002632 Loss_G: 0.005948 \n",
      "[4/10][26/100][822] Loss_D: 0.001352 Loss_G: 0.004307 \n",
      "[4/10][26/100][823] Loss_D: 0.001764 Loss_G: 0.001868 \n",
      "[4/10][26/100][824] Loss_D: 0.004621 Loss_G: 0.002473 \n",
      "[4/10][26/100][825] Loss_D: 0.002001 Loss_G: 0.002000 \n",
      "[4/10][26/100][826] Loss_D: 0.002254 Loss_G: 0.002625 \n",
      "[4/10][26/100][827] Loss_D: -0.000405 Loss_G: 0.002211 \n",
      "[4/10][26/100][828] Loss_D: 0.002610 Loss_G: 0.001372 \n",
      "[4/10][26/100][829] Loss_D: 0.001308 Loss_G: 0.002451 \n",
      "[4/10][26/100][830] Loss_D: 0.002478 Loss_G: 0.002829 \n",
      "[4/10][26/100][831] Loss_D: 0.003726 Loss_G: 0.000289 \n",
      "[4/10][26/100][832] Loss_D: 0.004345 Loss_G: 0.003328 \n",
      "[4/10][26/100][833] Loss_D: 0.000702 Loss_G: 0.001640 \n",
      "[4/10][26/100][834] Loss_D: 0.001863 Loss_G: 0.002155 \n",
      "[4/10][26/100][835] Loss_D: 0.006160 Loss_G: 0.002797 \n",
      "[4/10][26/100][836] Loss_D: 0.001066 Loss_G: 0.004116 \n",
      "[4/10][26/100][837] Loss_D: 0.003154 Loss_G: 0.002325 \n",
      "[4/10][26/100][838] Loss_D: 0.002402 Loss_G: 0.003462 \n",
      "[4/10][26/100][839] Loss_D: 0.002230 Loss_G: 0.004973 \n",
      "[4/10][26/100][840] Loss_D: 0.002868 Loss_G: 0.003339 \n",
      "[4/10][26/100][841] Loss_D: 0.002846 Loss_G: 0.003567 \n",
      "[4/10][26/100][842] Loss_D: 0.003066 Loss_G: 0.004164 \n",
      "[4/10][26/100][843] Loss_D: 0.001844 Loss_G: 0.000535 \n",
      "[4/10][26/100][844] Loss_D: 0.002176 Loss_G: 0.002710 \n",
      "[4/10][26/100][845] Loss_D: 0.001270 Loss_G: 0.002488 \n",
      "[4/10][26/100][846] Loss_D: 0.002459 Loss_G: 0.002368 \n",
      "[4/10][26/100][847] Loss_D: 0.005221 Loss_G: 0.002457 \n",
      "[4/10][26/100][848] Loss_D: 0.005012 Loss_G: 0.005267 \n",
      "[4/10][26/100][849] Loss_D: 0.004820 Loss_G: 0.002055 \n",
      "[4/10][26/100][850] Loss_D: 0.002351 Loss_G: 0.001540 \n",
      "[4/10][26/100][851] Loss_D: 0.002088 Loss_G: 0.002304 \n",
      "[4/10][26/100][852] Loss_D: 0.001883 Loss_G: 0.001524 \n",
      "[4/10][26/100][853] Loss_D: 0.000980 Loss_G: 0.002004 \n",
      "[4/10][26/100][854] Loss_D: 0.003248 Loss_G: 0.005010 \n",
      "[4/10][26/100][855] Loss_D: 0.003384 Loss_G: 0.001735 \n",
      "[4/10][26/100][856] Loss_D: 0.002257 Loss_G: 0.005042 \n",
      "[4/10][26/100][857] Loss_D: 0.003304 Loss_G: 0.002143 \n",
      "[4/10][26/100][858] Loss_D: 0.002918 Loss_G: 0.002015 \n",
      "[4/10][26/100][859] Loss_D: 0.004503 Loss_G: 0.003023 \n",
      "[4/10][26/100][860] Loss_D: 0.002356 Loss_G: 0.004234 \n",
      "[4/10][26/100][861] Loss_D: 0.003753 Loss_G: 0.001737 \n",
      "[4/10][26/100][862] Loss_D: 0.002114 Loss_G: 0.004022 \n",
      "[4/10][26/100][863] Loss_D: 0.002475 Loss_G: 0.002623 \n",
      "[4/10][26/100][864] Loss_D: 0.003748 Loss_G: -0.001688 \n",
      "[4/10][26/100][865] Loss_D: 0.004025 Loss_G: 0.002516 \n",
      "[4/10][26/100][866] Loss_D: 0.003073 Loss_G: 0.001819 \n",
      "[4/10][26/100][867] Loss_D: 0.003719 Loss_G: 0.003351 \n",
      "[4/10][26/100][868] Loss_D: 0.002792 Loss_G: 0.002128 \n",
      "[4/10][26/100][869] Loss_D: 0.002553 Loss_G: 0.001802 \n",
      "[4/10][26/100][870] Loss_D: 0.002870 Loss_G: 0.004369 \n",
      "[4/10][26/100][871] Loss_D: 0.004126 Loss_G: 0.002938 \n",
      "[4/10][26/100][872] Loss_D: -0.000033 Loss_G: 0.002061 \n",
      "[4/10][26/100][873] Loss_D: 0.003983 Loss_G: 0.002171 \n",
      "[4/10][26/100][874] Loss_D: 0.003171 Loss_G: 0.000553 \n",
      "[4/10][26/100][875] Loss_D: 0.004314 Loss_G: 0.000408 \n",
      "[4/10][26/100][876] Loss_D: 0.002013 Loss_G: 0.003258 \n",
      "[4/10][26/100][877] Loss_D: 0.001185 Loss_G: 0.002427 \n",
      "[4/10][26/100][878] Loss_D: 0.003662 Loss_G: 0.003200 \n",
      "[4/10][26/100][879] Loss_D: 0.002914 Loss_G: 0.002126 \n",
      "[4/10][26/100][880] Loss_D: 0.001235 Loss_G: 0.003800 \n",
      "[4/10][26/100][881] Loss_D: 0.002731 Loss_G: 0.002739 \n",
      "[4/10][26/100][882] Loss_D: 0.002923 Loss_G: 0.002861 \n",
      "[4/10][26/100][883] Loss_D: 0.002526 Loss_G: 0.001355 \n",
      "[4/10][26/100][884] Loss_D: 0.004062 Loss_G: 0.003363 \n",
      "[4/10][26/100][885] Loss_D: 0.002181 Loss_G: 0.002008 \n",
      "[4/10][26/100][886] Loss_D: 0.002055 Loss_G: 0.003179 \n",
      "[4/10][26/100][887] Loss_D: 0.007551 Loss_G: 0.000780 \n",
      "[4/10][26/100][888] Loss_D: 0.003334 Loss_G: 0.001596 \n",
      "[4/10][26/100][889] Loss_D: 0.002764 Loss_G: 0.003993 \n",
      "[4/10][26/100][890] Loss_D: 0.003008 Loss_G: 0.001811 \n",
      "[4/10][26/100][891] Loss_D: 0.002251 Loss_G: 0.002760 \n",
      "[4/10][26/100][892] Loss_D: 0.000801 Loss_G: 0.001012 \n",
      "[4/10][26/100][893] Loss_D: 0.002303 Loss_G: 0.002839 \n",
      "[4/10][26/100][894] Loss_D: 0.003586 Loss_G: 0.002632 \n",
      "[4/10][26/100][895] Loss_D: 0.003308 Loss_G: 0.002988 \n",
      "[4/10][26/100][896] Loss_D: 0.004049 Loss_G: 0.004171 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10][26/100][897] Loss_D: 0.004270 Loss_G: 0.003112 \n",
      "[4/10][26/100][898] Loss_D: 0.002882 Loss_G: 0.001647 \n",
      "[4/10][26/100][899] Loss_D: 0.004530 Loss_G: 0.002929 \n",
      "[4/10][26/100][900] Loss_D: 0.000293 Loss_G: -0.000045 \n",
      "[4/10][26/100][901] Loss_D: 0.003035 Loss_G: 0.003477 \n",
      "[4/10][26/100][902] Loss_D: 0.000969 Loss_G: 0.001896 \n",
      "[4/10][26/100][903] Loss_D: 0.002874 Loss_G: 0.001321 \n",
      "[4/10][26/100][904] Loss_D: 0.003027 Loss_G: 0.002142 \n",
      "[4/10][26/100][905] Loss_D: 0.003356 Loss_G: 0.001568 \n",
      "[4/10][26/100][906] Loss_D: 0.002885 Loss_G: 0.002049 \n",
      "[4/10][26/100][907] Loss_D: 0.001506 Loss_G: 0.003561 \n",
      "[4/10][26/100][908] Loss_D: 0.001879 Loss_G: 0.003072 \n",
      "[4/10][26/100][909] Loss_D: 0.001921 Loss_G: 0.000947 \n",
      "[4/10][26/100][910] Loss_D: 0.003334 Loss_G: 0.001589 \n",
      "[4/10][26/100][911] Loss_D: 0.003564 Loss_G: 0.000838 \n",
      "[4/10][26/100][912] Loss_D: 0.003155 Loss_G: 0.006000 \n",
      "[4/10][26/100][913] Loss_D: 0.001431 Loss_G: 0.002085 \n",
      "[4/10][26/100][914] Loss_D: 0.000598 Loss_G: 0.003080 \n",
      "[4/10][26/100][915] Loss_D: 0.004504 Loss_G: 0.003394 \n",
      "[4/10][26/100][916] Loss_D: 0.002646 Loss_G: 0.002824 \n",
      "[4/10][26/100][917] Loss_D: 0.003827 Loss_G: 0.001381 \n",
      "[4/10][26/100][918] Loss_D: 0.002336 Loss_G: 0.002320 \n",
      "[4/10][26/100][919] Loss_D: 0.002118 Loss_G: 0.000138 \n",
      "[4/10][26/100][920] Loss_D: 0.003652 Loss_G: 0.001702 \n",
      "[4/10][26/100][921] Loss_D: 0.000852 Loss_G: 0.003129 \n",
      "[4/10][26/100][922] Loss_D: 0.001815 Loss_G: 0.001969 \n",
      "[4/10][26/100][923] Loss_D: 0.003383 Loss_G: 0.005647 \n",
      "[4/10][26/100][924] Loss_D: 0.002662 Loss_G: 0.003435 \n",
      "[4/10][26/100][925] Loss_D: 0.001108 Loss_G: 0.002472 \n",
      "[4/10][26/100][926] Loss_D: 0.003296 Loss_G: 0.002233 \n",
      "[4/10][26/100][927] Loss_D: 0.002714 Loss_G: 0.000647 \n",
      "[4/10][26/100][928] Loss_D: 0.002127 Loss_G: 0.002576 \n",
      "[4/10][26/100][929] Loss_D: 0.001956 Loss_G: 0.003091 \n",
      "[4/10][26/100][930] Loss_D: 0.003032 Loss_G: 0.002885 \n",
      "[4/10][26/100][931] Loss_D: 0.002666 Loss_G: 0.003165 \n",
      "[4/10][26/100][932] Loss_D: 0.003393 Loss_G: 0.002077 \n",
      "[4/10][26/100][933] Loss_D: 0.002802 Loss_G: 0.003365 \n",
      "[4/10][26/100][934] Loss_D: 0.004278 Loss_G: 0.003258 \n",
      "[4/10][26/100][935] Loss_D: 0.004374 Loss_G: 0.001583 \n",
      "[4/10][26/100][936] Loss_D: 0.001269 Loss_G: 0.001934 \n",
      "[4/10][26/100][937] Loss_D: 0.004799 Loss_G: 0.000726 \n",
      "[4/10][26/100][938] Loss_D: 0.002369 Loss_G: 0.000875 \n",
      "[4/10][26/100][939] Loss_D: 0.003055 Loss_G: 0.001981 \n",
      "[4/10][26/100][940] Loss_D: 0.004387 Loss_G: -0.000225 \n",
      "[4/10][26/100][941] Loss_D: 0.004300 Loss_G: 0.003341 \n",
      "[4/10][26/100][942] Loss_D: 0.000603 Loss_G: 0.004152 \n",
      "[4/10][26/100][943] Loss_D: 0.001497 Loss_G: 0.002332 \n",
      "[4/10][26/100][944] Loss_D: 0.004561 Loss_G: 0.001869 \n",
      "[4/10][26/100][945] Loss_D: 0.001847 Loss_G: 0.002961 \n",
      "[4/10][26/100][946] Loss_D: 0.006519 Loss_G: 0.001774 \n",
      "[4/10][26/100][947] Loss_D: 0.005560 Loss_G: 0.003431 \n",
      "[4/10][26/100][948] Loss_D: 0.003927 Loss_G: 0.003459 \n",
      "[4/10][26/100][949] Loss_D: 0.002616 Loss_G: 0.001326 \n",
      "[4/10][26/100][950] Loss_D: 0.002684 Loss_G: 0.004638 \n",
      "[4/10][26/100][951] Loss_D: 0.003700 Loss_G: 0.001940 \n",
      "[4/10][26/100][952] Loss_D: 0.002453 Loss_G: 0.003146 \n",
      "[4/10][26/100][953] Loss_D: 0.004392 Loss_G: 0.004237 \n",
      "[4/10][26/100][954] Loss_D: 0.004065 Loss_G: 0.002673 \n",
      "[4/10][26/100][955] Loss_D: 0.003514 Loss_G: 0.002777 \n",
      "[4/10][26/100][956] Loss_D: 0.002679 Loss_G: 0.004890 \n",
      "[4/10][26/100][957] Loss_D: 0.002800 Loss_G: 0.003636 \n",
      "[4/10][26/100][958] Loss_D: 0.003586 Loss_G: 0.002043 \n",
      "[4/10][26/100][959] Loss_D: 0.004222 Loss_G: 0.001692 \n",
      "[4/10][26/100][960] Loss_D: 0.001752 Loss_G: 0.001266 \n",
      "[4/10][26/100][961] Loss_D: 0.003031 Loss_G: 0.002987 \n",
      "[4/10][26/100][962] Loss_D: 0.003612 Loss_G: 0.003741 \n",
      "[4/10][26/100][963] Loss_D: 0.002017 Loss_G: 0.001771 \n",
      "[4/10][26/100][964] Loss_D: 0.003834 Loss_G: 0.003428 \n",
      "[4/10][26/100][965] Loss_D: 0.001608 Loss_G: 0.004237 \n",
      "[4/10][26/100][966] Loss_D: 0.002872 Loss_G: 0.002956 \n",
      "[4/10][26/100][967] Loss_D: 0.003002 Loss_G: 0.002529 \n",
      "[4/10][26/100][968] Loss_D: 0.003191 Loss_G: 0.003624 \n",
      "[4/10][26/100][969] Loss_D: 0.002167 Loss_G: 0.002357 \n",
      "[4/10][26/100][970] Loss_D: 0.002004 Loss_G: 0.004964 \n",
      "[4/10][26/100][971] Loss_D: 0.002412 Loss_G: 0.004288 \n",
      "[4/10][26/100][972] Loss_D: 0.002665 Loss_G: 0.003906 \n",
      "[4/10][26/100][973] Loss_D: 0.001655 Loss_G: 0.001398 \n",
      "[4/10][26/100][974] Loss_D: 0.001575 Loss_G: 0.002002 \n",
      "[4/10][26/100][975] Loss_D: 0.002005 Loss_G: 0.004230 \n",
      "[4/10][26/100][976] Loss_D: 0.002964 Loss_G: 0.002563 \n",
      "[4/10][26/100][977] Loss_D: 0.001876 Loss_G: 0.004726 \n",
      "[4/10][26/100][978] Loss_D: 0.001344 Loss_G: 0.001471 \n",
      "[4/10][26/100][979] Loss_D: 0.002942 Loss_G: 0.004385 \n",
      "[4/10][26/100][980] Loss_D: 0.002261 Loss_G: 0.002692 \n",
      "[4/10][26/100][981] Loss_D: 0.005222 Loss_G: 0.002160 \n",
      "[4/10][26/100][982] Loss_D: 0.002326 Loss_G: 0.000143 \n",
      "[4/10][26/100][983] Loss_D: 0.002457 Loss_G: 0.003633 \n",
      "[4/10][26/100][984] Loss_D: 0.003002 Loss_G: 0.003076 \n",
      "[4/10][26/100][985] Loss_D: 0.002913 Loss_G: 0.001801 \n",
      "[4/10][26/100][986] Loss_D: 0.001441 Loss_G: 0.000894 \n",
      "[4/10][26/100][987] Loss_D: 0.002319 Loss_G: 0.004932 \n",
      "[4/10][26/100][988] Loss_D: 0.001983 Loss_G: 0.002167 \n",
      "[4/10][26/100][989] Loss_D: 0.002262 Loss_G: 0.002154 \n",
      "[4/10][26/100][990] Loss_D: 0.002914 Loss_G: 0.003089 \n",
      "[4/10][26/100][991] Loss_D: 0.001004 Loss_G: 0.002606 \n",
      "[4/10][26/100][992] Loss_D: 0.003276 Loss_G: 0.004500 \n",
      "[4/10][26/100][993] Loss_D: 0.002338 Loss_G: 0.003310 \n",
      "[4/10][26/100][994] Loss_D: 0.003135 Loss_G: 0.000168 \n",
      "[4/10][26/100][995] Loss_D: 0.001313 Loss_G: 0.002589 \n",
      "[4/10][26/100][996] Loss_D: 0.003988 Loss_G: 0.002438 \n",
      "[4/10][26/100][997] Loss_D: 0.002912 Loss_G: 0.002475 \n",
      "[4/10][26/100][998] Loss_D: 0.002217 Loss_G: 0.004479 \n",
      "[4/10][26/100][999] Loss_D: 0.001785 Loss_G: 0.002992 \n",
      "[4/10][26/100][1000] Loss_D: 0.001402 Loss_G: 0.002058 \n",
      "[5/10][26/100][1001] Loss_D: 0.004366 Loss_G: 0.002309 \n",
      "[5/10][26/100][1002] Loss_D: 0.001542 Loss_G: 0.003283 \n",
      "[5/10][26/100][1003] Loss_D: 0.002381 Loss_G: 0.003598 \n",
      "[5/10][26/100][1004] Loss_D: 0.003786 Loss_G: 0.002889 \n",
      "[5/10][26/100][1005] Loss_D: 0.002192 Loss_G: 0.001678 \n",
      "[5/10][26/100][1006] Loss_D: 0.003849 Loss_G: 0.002354 \n",
      "[5/10][26/100][1007] Loss_D: 0.002909 Loss_G: 0.004633 \n",
      "[5/10][26/100][1008] Loss_D: 0.001228 Loss_G: 0.003128 \n",
      "[5/10][26/100][1009] Loss_D: 0.001728 Loss_G: 0.002486 \n",
      "[5/10][26/100][1010] Loss_D: 0.001030 Loss_G: 0.002934 \n",
      "[5/10][26/100][1011] Loss_D: 0.002340 Loss_G: 0.003737 \n",
      "[5/10][26/100][1012] Loss_D: 0.002896 Loss_G: 0.003312 \n",
      "[5/10][26/100][1013] Loss_D: 0.003575 Loss_G: 0.001603 \n",
      "[5/10][26/100][1014] Loss_D: 0.002656 Loss_G: 0.001397 \n",
      "[5/10][26/100][1015] Loss_D: 0.002645 Loss_G: 0.003414 \n",
      "[5/10][26/100][1016] Loss_D: 0.002753 Loss_G: 0.003927 \n",
      "[5/10][26/100][1017] Loss_D: 0.002977 Loss_G: 0.003462 \n",
      "[5/10][26/100][1018] Loss_D: 0.001486 Loss_G: 0.001610 \n",
      "[5/10][26/100][1019] Loss_D: 0.002135 Loss_G: 0.001647 \n",
      "[5/10][26/100][1020] Loss_D: 0.002221 Loss_G: 0.004540 \n",
      "[5/10][26/100][1021] Loss_D: 0.001627 Loss_G: 0.003276 \n",
      "[5/10][26/100][1022] Loss_D: 0.003065 Loss_G: 0.002276 \n",
      "[5/10][26/100][1023] Loss_D: 0.003357 Loss_G: 0.002496 \n",
      "[5/10][26/100][1024] Loss_D: 0.002024 Loss_G: 0.002561 \n",
      "[5/10][26/100][1025] Loss_D: 0.003273 Loss_G: 0.002394 \n",
      "[5/10][26/100][1026] Loss_D: 0.002366 Loss_G: 0.003764 \n",
      "[5/10][26/100][1027] Loss_D: 0.003932 Loss_G: 0.002766 \n",
      "[5/10][26/100][1028] Loss_D: 0.002603 Loss_G: 0.002822 \n",
      "[5/10][26/100][1029] Loss_D: 0.001680 Loss_G: 0.004401 \n",
      "[5/10][26/100][1030] Loss_D: 0.003023 Loss_G: 0.001477 \n",
      "[5/10][26/100][1031] Loss_D: 0.003089 Loss_G: 0.003343 \n",
      "[5/10][26/100][1032] Loss_D: 0.002295 Loss_G: 0.002642 \n",
      "[5/10][26/100][1033] Loss_D: 0.002606 Loss_G: 0.001224 \n",
      "[5/10][26/100][1034] Loss_D: 0.002640 Loss_G: 0.002735 \n",
      "[5/10][26/100][1035] Loss_D: 0.002827 Loss_G: 0.003275 \n",
      "[5/10][26/100][1036] Loss_D: 0.002213 Loss_G: 0.004717 \n",
      "[5/10][26/100][1037] Loss_D: 0.004766 Loss_G: 0.002267 \n",
      "[5/10][26/100][1038] Loss_D: 0.002620 Loss_G: 0.002059 \n",
      "[5/10][26/100][1039] Loss_D: 0.003275 Loss_G: 0.003093 \n",
      "[5/10][26/100][1040] Loss_D: 0.001499 Loss_G: 0.002637 \n",
      "[5/10][26/100][1041] Loss_D: 0.001885 Loss_G: 0.004329 \n",
      "[5/10][26/100][1042] Loss_D: 0.001286 Loss_G: 0.002480 \n",
      "[5/10][26/100][1043] Loss_D: 0.003510 Loss_G: 0.001209 \n",
      "[5/10][26/100][1044] Loss_D: 0.002778 Loss_G: 0.001978 \n",
      "[5/10][26/100][1045] Loss_D: 0.002850 Loss_G: 0.002635 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10][26/100][1046] Loss_D: 0.001192 Loss_G: 0.003445 \n",
      "[5/10][26/100][1047] Loss_D: 0.002580 Loss_G: 0.001147 \n",
      "[5/10][26/100][1048] Loss_D: 0.002004 Loss_G: 0.001482 \n",
      "[5/10][26/100][1049] Loss_D: 0.001884 Loss_G: 0.003323 \n",
      "[5/10][26/100][1050] Loss_D: 0.004350 Loss_G: -0.001761 \n",
      "[5/10][26/100][1051] Loss_D: 0.002169 Loss_G: 0.001142 \n",
      "[5/10][26/100][1052] Loss_D: 0.002459 Loss_G: 0.000947 \n",
      "[5/10][26/100][1053] Loss_D: 0.004246 Loss_G: 0.003220 \n",
      "[5/10][26/100][1054] Loss_D: 0.005561 Loss_G: 0.002164 \n",
      "[5/10][26/100][1055] Loss_D: 0.003240 Loss_G: 0.003203 \n",
      "[5/10][26/100][1056] Loss_D: 0.002464 Loss_G: 0.001015 \n",
      "[5/10][26/100][1057] Loss_D: 0.002413 Loss_G: 0.003185 \n",
      "[5/10][26/100][1058] Loss_D: 0.004572 Loss_G: 0.002314 \n",
      "[5/10][26/100][1059] Loss_D: 0.002217 Loss_G: 0.003163 \n",
      "[5/10][26/100][1060] Loss_D: 0.002359 Loss_G: 0.003968 \n",
      "[5/10][26/100][1061] Loss_D: 0.002931 Loss_G: 0.002584 \n",
      "[5/10][26/100][1062] Loss_D: 0.003633 Loss_G: 0.001561 \n",
      "[5/10][26/100][1063] Loss_D: 0.006169 Loss_G: 0.003081 \n",
      "[5/10][26/100][1064] Loss_D: 0.002359 Loss_G: 0.001648 \n",
      "[5/10][26/100][1065] Loss_D: 0.003997 Loss_G: 0.002109 \n",
      "[5/10][26/100][1066] Loss_D: 0.001483 Loss_G: 0.003100 \n",
      "[5/10][26/100][1067] Loss_D: 0.002488 Loss_G: 0.004728 \n",
      "[5/10][26/100][1068] Loss_D: 0.001551 Loss_G: 0.002197 \n",
      "[5/10][26/100][1069] Loss_D: 0.002136 Loss_G: 0.003538 \n",
      "[5/10][26/100][1070] Loss_D: 0.002703 Loss_G: 0.001784 \n",
      "[5/10][26/100][1071] Loss_D: 0.001275 Loss_G: 0.002000 \n",
      "[5/10][26/100][1072] Loss_D: 0.002443 Loss_G: 0.002137 \n",
      "[5/10][26/100][1073] Loss_D: 0.003170 Loss_G: 0.002696 \n",
      "[5/10][26/100][1074] Loss_D: 0.004201 Loss_G: 0.002032 \n",
      "[5/10][26/100][1075] Loss_D: 0.002535 Loss_G: 0.004347 \n",
      "[5/10][26/100][1076] Loss_D: 0.001827 Loss_G: 0.001714 \n",
      "[5/10][26/100][1077] Loss_D: 0.001578 Loss_G: 0.002246 \n",
      "[5/10][26/100][1078] Loss_D: 0.002111 Loss_G: 0.003049 \n",
      "[5/10][26/100][1079] Loss_D: 0.005779 Loss_G: 0.004026 \n",
      "[5/10][26/100][1080] Loss_D: 0.002357 Loss_G: 0.002165 \n",
      "[5/10][26/100][1081] Loss_D: 0.005141 Loss_G: 0.004093 \n",
      "[5/10][26/100][1082] Loss_D: 0.002891 Loss_G: 0.003986 \n",
      "[5/10][26/100][1083] Loss_D: 0.006675 Loss_G: 0.002285 \n",
      "[5/10][26/100][1084] Loss_D: 0.002256 Loss_G: 0.002495 \n",
      "[5/10][26/100][1085] Loss_D: 0.001997 Loss_G: 0.002032 \n",
      "[5/10][26/100][1086] Loss_D: 0.004628 Loss_G: 0.003273 \n",
      "[5/10][26/100][1087] Loss_D: 0.001755 Loss_G: 0.003117 \n",
      "[5/10][26/100][1088] Loss_D: 0.004417 Loss_G: 0.005766 \n",
      "[5/10][26/100][1089] Loss_D: 0.003891 Loss_G: 0.003273 \n",
      "[5/10][26/100][1090] Loss_D: 0.001092 Loss_G: 0.003656 \n",
      "[5/10][26/100][1091] Loss_D: 0.003443 Loss_G: 0.002987 \n",
      "[5/10][26/100][1092] Loss_D: 0.004773 Loss_G: 0.004212 \n",
      "[5/10][26/100][1093] Loss_D: 0.002227 Loss_G: 0.005108 \n",
      "[5/10][26/100][1094] Loss_D: 0.003582 Loss_G: 0.002425 \n",
      "[5/10][26/100][1095] Loss_D: 0.003113 Loss_G: 0.003602 \n",
      "[5/10][26/100][1096] Loss_D: 0.004593 Loss_G: 0.004418 \n",
      "[5/10][26/100][1097] Loss_D: 0.002113 Loss_G: 0.001681 \n",
      "[5/10][26/100][1098] Loss_D: 0.000668 Loss_G: 0.002410 \n",
      "[5/10][26/100][1099] Loss_D: 0.001580 Loss_G: 0.002763 \n",
      "[5/10][26/100][1100] Loss_D: 0.001777 Loss_G: 0.002247 \n",
      "[5/10][26/100][1101] Loss_D: -0.001145 Loss_G: 0.002242 \n",
      "[5/10][26/100][1102] Loss_D: 0.002173 Loss_G: 0.001146 \n",
      "[5/10][26/100][1103] Loss_D: 0.001835 Loss_G: 0.002414 \n",
      "[5/10][26/100][1104] Loss_D: 0.002837 Loss_G: 0.003012 \n",
      "[5/10][26/100][1105] Loss_D: 0.001311 Loss_G: 0.004747 \n",
      "[5/10][26/100][1106] Loss_D: 0.002325 Loss_G: 0.003537 \n",
      "[5/10][26/100][1107] Loss_D: 0.002658 Loss_G: 0.003557 \n",
      "[5/10][26/100][1108] Loss_D: 0.002970 Loss_G: -0.000235 \n",
      "[5/10][26/100][1109] Loss_D: 0.002661 Loss_G: 0.001883 \n",
      "[5/10][26/100][1110] Loss_D: 0.001527 Loss_G: 0.003389 \n",
      "[5/10][26/100][1111] Loss_D: 0.002829 Loss_G: 0.003956 \n",
      "[5/10][26/100][1112] Loss_D: 0.003081 Loss_G: 0.002504 \n",
      "[5/10][26/100][1113] Loss_D: 0.001394 Loss_G: 0.003641 \n",
      "[5/10][26/100][1114] Loss_D: 0.003175 Loss_G: 0.004049 \n",
      "[5/10][26/100][1115] Loss_D: 0.002291 Loss_G: 0.003059 \n",
      "[5/10][26/100][1116] Loss_D: 0.001915 Loss_G: 0.003042 \n",
      "[5/10][26/100][1117] Loss_D: 0.003451 Loss_G: 0.001423 \n",
      "[5/10][26/100][1118] Loss_D: 0.002365 Loss_G: 0.002511 \n",
      "[5/10][26/100][1119] Loss_D: 0.002619 Loss_G: 0.002917 \n",
      "[5/10][26/100][1120] Loss_D: 0.002273 Loss_G: 0.004296 \n",
      "[5/10][26/100][1121] Loss_D: 0.002360 Loss_G: 0.001307 \n",
      "[5/10][26/100][1122] Loss_D: 0.001418 Loss_G: 0.001903 \n",
      "[5/10][26/100][1123] Loss_D: 0.003085 Loss_G: 0.001773 \n",
      "[5/10][26/100][1124] Loss_D: 0.002915 Loss_G: 0.003752 \n",
      "[5/10][26/100][1125] Loss_D: 0.001943 Loss_G: 0.000867 \n",
      "[5/10][26/100][1126] Loss_D: 0.003326 Loss_G: 0.003918 \n",
      "[5/10][26/100][1127] Loss_D: 0.004288 Loss_G: 0.001945 \n",
      "[5/10][26/100][1128] Loss_D: 0.004157 Loss_G: 0.002169 \n",
      "[5/10][26/100][1129] Loss_D: -0.000623 Loss_G: 0.002378 \n",
      "[5/10][26/100][1130] Loss_D: 0.003026 Loss_G: 0.003084 \n",
      "[5/10][26/100][1131] Loss_D: 0.002854 Loss_G: 0.003010 \n",
      "[5/10][26/100][1132] Loss_D: 0.002555 Loss_G: 0.001381 \n",
      "[5/10][26/100][1133] Loss_D: 0.002481 Loss_G: 0.001735 \n",
      "[5/10][26/100][1134] Loss_D: 0.002512 Loss_G: 0.002463 \n",
      "[5/10][26/100][1135] Loss_D: 0.002007 Loss_G: 0.001963 \n",
      "[5/10][26/100][1136] Loss_D: 0.002921 Loss_G: 0.002150 \n",
      "[5/10][26/100][1137] Loss_D: 0.001687 Loss_G: 0.004022 \n",
      "[5/10][26/100][1138] Loss_D: 0.002313 Loss_G: 0.004753 \n",
      "[5/10][26/100][1139] Loss_D: 0.003994 Loss_G: 0.003667 \n",
      "[5/10][26/100][1140] Loss_D: 0.003481 Loss_G: 0.003394 \n",
      "[5/10][26/100][1141] Loss_D: 0.001475 Loss_G: 0.002254 \n",
      "[5/10][26/100][1142] Loss_D: 0.000996 Loss_G: 0.004325 \n",
      "[5/10][26/100][1143] Loss_D: 0.003762 Loss_G: 0.002937 \n",
      "[5/10][26/100][1144] Loss_D: 0.001786 Loss_G: 0.002150 \n",
      "[5/10][26/100][1145] Loss_D: 0.001095 Loss_G: 0.003279 \n",
      "[5/10][26/100][1146] Loss_D: 0.003729 Loss_G: 0.004469 \n",
      "[5/10][26/100][1147] Loss_D: 0.000885 Loss_G: 0.002711 \n",
      "[5/10][26/100][1148] Loss_D: 0.002571 Loss_G: 0.002715 \n",
      "[5/10][26/100][1149] Loss_D: 0.002931 Loss_G: 0.003652 \n",
      "[5/10][26/100][1150] Loss_D: 0.004392 Loss_G: 0.001700 \n",
      "[5/10][26/100][1151] Loss_D: 0.002621 Loss_G: 0.002454 \n",
      "[5/10][26/100][1152] Loss_D: 0.001618 Loss_G: 0.003483 \n",
      "[5/10][26/100][1153] Loss_D: 0.003625 Loss_G: 0.002872 \n",
      "[5/10][26/100][1154] Loss_D: 0.002753 Loss_G: 0.002381 \n",
      "[5/10][26/100][1155] Loss_D: 0.002109 Loss_G: 0.002889 \n",
      "[5/10][26/100][1156] Loss_D: 0.001631 Loss_G: 0.003328 \n",
      "[5/10][26/100][1157] Loss_D: 0.001837 Loss_G: 0.002364 \n",
      "[5/10][26/100][1158] Loss_D: 0.002111 Loss_G: 0.001847 \n",
      "[5/10][26/100][1159] Loss_D: 0.003301 Loss_G: 0.001969 \n",
      "[5/10][26/100][1160] Loss_D: 0.005186 Loss_G: 0.002291 \n",
      "[5/10][26/100][1161] Loss_D: 0.004851 Loss_G: 0.003771 \n",
      "[5/10][26/100][1162] Loss_D: 0.002085 Loss_G: 0.002210 \n",
      "[5/10][26/100][1163] Loss_D: 0.002729 Loss_G: 0.005623 \n",
      "[5/10][26/100][1164] Loss_D: 0.002269 Loss_G: 0.002821 \n",
      "[5/10][26/100][1165] Loss_D: 0.002228 Loss_G: 0.002336 \n",
      "[5/10][26/100][1166] Loss_D: 0.002243 Loss_G: 0.001895 \n",
      "[5/10][26/100][1167] Loss_D: 0.003685 Loss_G: 0.003938 \n",
      "[5/10][26/100][1168] Loss_D: 0.003723 Loss_G: 0.002616 \n",
      "[5/10][26/100][1169] Loss_D: 0.001568 Loss_G: 0.003494 \n",
      "[5/10][26/100][1170] Loss_D: 0.002480 Loss_G: 0.002132 \n",
      "[5/10][26/100][1171] Loss_D: 0.001768 Loss_G: 0.002629 \n",
      "[5/10][26/100][1172] Loss_D: 0.002194 Loss_G: 0.001919 \n",
      "[5/10][26/100][1173] Loss_D: 0.002670 Loss_G: 0.002761 \n",
      "[5/10][26/100][1174] Loss_D: 0.003617 Loss_G: 0.002496 \n",
      "[5/10][26/100][1175] Loss_D: 0.001789 Loss_G: 0.002824 \n",
      "[5/10][26/100][1176] Loss_D: 0.005451 Loss_G: 0.001669 \n",
      "[5/10][26/100][1177] Loss_D: 0.003220 Loss_G: 0.004659 \n",
      "[5/10][26/100][1178] Loss_D: 0.004773 Loss_G: 0.003348 \n",
      "[5/10][26/100][1179] Loss_D: 0.003257 Loss_G: 0.002410 \n",
      "[5/10][26/100][1180] Loss_D: 0.001898 Loss_G: 0.002723 \n",
      "[5/10][26/100][1181] Loss_D: 0.001679 Loss_G: 0.002090 \n",
      "[5/10][26/100][1182] Loss_D: 0.003298 Loss_G: 0.003480 \n",
      "[5/10][26/100][1183] Loss_D: 0.001860 Loss_G: 0.002566 \n",
      "[5/10][26/100][1184] Loss_D: 0.002653 Loss_G: 0.002713 \n",
      "[5/10][26/100][1185] Loss_D: 0.001960 Loss_G: 0.002513 \n",
      "[5/10][26/100][1186] Loss_D: 0.003567 Loss_G: 0.001281 \n",
      "[5/10][26/100][1187] Loss_D: 0.000279 Loss_G: 0.004061 \n",
      "[5/10][26/100][1188] Loss_D: 0.001435 Loss_G: 0.004372 \n",
      "[5/10][26/100][1189] Loss_D: 0.001129 Loss_G: 0.001090 \n",
      "[5/10][26/100][1190] Loss_D: 0.003952 Loss_G: 0.005146 \n",
      "[5/10][26/100][1191] Loss_D: 0.000756 Loss_G: 0.001282 \n",
      "[5/10][26/100][1192] Loss_D: 0.000517 Loss_G: 0.002031 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/10][26/100][1193] Loss_D: 0.001601 Loss_G: 0.002308 \n",
      "[5/10][26/100][1194] Loss_D: 0.002325 Loss_G: 0.001954 \n",
      "[5/10][26/100][1195] Loss_D: 0.004919 Loss_G: 0.002290 \n",
      "[5/10][26/100][1196] Loss_D: 0.002682 Loss_G: 0.007882 \n",
      "[5/10][26/100][1197] Loss_D: 0.002396 Loss_G: 0.001871 \n",
      "[5/10][26/100][1198] Loss_D: 0.003366 Loss_G: 0.004840 \n",
      "[5/10][26/100][1199] Loss_D: 0.001838 Loss_G: 0.001419 \n",
      "[5/10][26/100][1200] Loss_D: 0.000953 Loss_G: 0.001666 \n",
      "[6/10][26/100][1201] Loss_D: 0.003362 Loss_G: -0.001339 \n",
      "[6/10][26/100][1202] Loss_D: 0.003113 Loss_G: 0.002715 \n",
      "[6/10][26/100][1203] Loss_D: 0.001119 Loss_G: 0.001512 \n",
      "[6/10][26/100][1204] Loss_D: 0.001360 Loss_G: 0.001884 \n",
      "[6/10][26/100][1205] Loss_D: 0.001229 Loss_G: 0.002255 \n",
      "[6/10][26/100][1206] Loss_D: 0.007470 Loss_G: 0.002098 \n",
      "[6/10][26/100][1207] Loss_D: 0.002776 Loss_G: 0.003473 \n",
      "[6/10][26/100][1208] Loss_D: 0.001968 Loss_G: 0.001492 \n",
      "[6/10][26/100][1209] Loss_D: 0.004213 Loss_G: 0.001387 \n",
      "[6/10][26/100][1210] Loss_D: 0.002931 Loss_G: 0.002362 \n",
      "[6/10][26/100][1211] Loss_D: 0.002070 Loss_G: 0.004853 \n",
      "[6/10][26/100][1212] Loss_D: 0.003623 Loss_G: 0.003715 \n",
      "[6/10][26/100][1213] Loss_D: 0.002286 Loss_G: 0.003348 \n",
      "[6/10][26/100][1214] Loss_D: 0.003914 Loss_G: 0.001917 \n",
      "[6/10][26/100][1215] Loss_D: 0.002312 Loss_G: 0.001648 \n",
      "[6/10][26/100][1216] Loss_D: 0.002752 Loss_G: 0.001732 \n",
      "[6/10][26/100][1217] Loss_D: 0.003884 Loss_G: 0.005041 \n",
      "[6/10][26/100][1218] Loss_D: 0.001845 Loss_G: 0.002240 \n",
      "[6/10][26/100][1219] Loss_D: 0.002889 Loss_G: 0.002334 \n",
      "[6/10][26/100][1220] Loss_D: 0.003316 Loss_G: 0.003499 \n",
      "[6/10][26/100][1221] Loss_D: 0.004060 Loss_G: 0.003133 \n",
      "[6/10][26/100][1222] Loss_D: 0.001710 Loss_G: 0.002998 \n",
      "[6/10][26/100][1223] Loss_D: 0.002769 Loss_G: 0.002046 \n",
      "[6/10][26/100][1224] Loss_D: 0.001807 Loss_G: 0.002548 \n",
      "[6/10][26/100][1225] Loss_D: 0.001337 Loss_G: 0.002182 \n",
      "[6/10][26/100][1226] Loss_D: 0.003381 Loss_G: 0.002619 \n",
      "[6/10][26/100][1227] Loss_D: 0.003075 Loss_G: 0.001468 \n",
      "[6/10][26/100][1228] Loss_D: 0.002270 Loss_G: 0.002004 \n",
      "[6/10][26/100][1229] Loss_D: 0.003973 Loss_G: 0.000023 \n",
      "[6/10][26/100][1230] Loss_D: -0.000680 Loss_G: 0.002444 \n",
      "[6/10][26/100][1231] Loss_D: 0.003971 Loss_G: 0.003044 \n",
      "[6/10][26/100][1232] Loss_D: 0.001400 Loss_G: 0.002856 \n",
      "[6/10][26/100][1233] Loss_D: 0.002101 Loss_G: 0.005099 \n",
      "[6/10][26/100][1234] Loss_D: 0.002193 Loss_G: 0.002017 \n",
      "[6/10][26/100][1235] Loss_D: 0.003613 Loss_G: 0.003260 \n",
      "[6/10][26/100][1236] Loss_D: 0.002977 Loss_G: 0.004946 \n",
      "[6/10][26/100][1237] Loss_D: 0.003325 Loss_G: 0.003818 \n",
      "[6/10][26/100][1238] Loss_D: 0.002834 Loss_G: 0.003786 \n",
      "[6/10][26/100][1239] Loss_D: 0.001393 Loss_G: 0.001431 \n",
      "[6/10][26/100][1240] Loss_D: 0.001814 Loss_G: 0.002098 \n",
      "[6/10][26/100][1241] Loss_D: 0.003928 Loss_G: 0.003656 \n",
      "[6/10][26/100][1242] Loss_D: 0.001590 Loss_G: 0.003674 \n",
      "[6/10][26/100][1243] Loss_D: 0.005148 Loss_G: 0.005017 \n",
      "[6/10][26/100][1244] Loss_D: 0.003700 Loss_G: 0.001319 \n",
      "[6/10][26/100][1245] Loss_D: 0.002311 Loss_G: 0.002778 \n",
      "[6/10][26/100][1246] Loss_D: 0.002867 Loss_G: 0.003371 \n",
      "[6/10][26/100][1247] Loss_D: 0.003434 Loss_G: 0.002386 \n",
      "[6/10][26/100][1248] Loss_D: 0.001868 Loss_G: 0.002904 \n",
      "[6/10][26/100][1249] Loss_D: 0.002590 Loss_G: 0.001909 \n",
      "[6/10][26/100][1250] Loss_D: 0.002552 Loss_G: 0.003735 \n",
      "[6/10][26/100][1251] Loss_D: 0.002001 Loss_G: 0.003023 \n",
      "[6/10][26/100][1252] Loss_D: 0.004985 Loss_G: 0.003402 \n",
      "[6/10][26/100][1253] Loss_D: 0.003505 Loss_G: 0.002649 \n",
      "[6/10][26/100][1254] Loss_D: 0.003155 Loss_G: 0.003257 \n",
      "[6/10][26/100][1255] Loss_D: 0.002659 Loss_G: 0.001881 \n",
      "[6/10][26/100][1256] Loss_D: 0.002918 Loss_G: 0.001297 \n",
      "[6/10][26/100][1257] Loss_D: 0.003016 Loss_G: 0.002770 \n",
      "[6/10][26/100][1258] Loss_D: 0.004170 Loss_G: 0.003456 \n",
      "[6/10][26/100][1259] Loss_D: 0.003458 Loss_G: 0.000495 \n",
      "[6/10][26/100][1260] Loss_D: 0.001902 Loss_G: 0.003272 \n",
      "[6/10][26/100][1261] Loss_D: 0.001641 Loss_G: 0.002940 \n",
      "[6/10][26/100][1262] Loss_D: 0.002463 Loss_G: 0.001773 \n",
      "[6/10][26/100][1263] Loss_D: 0.003470 Loss_G: 0.001191 \n",
      "[6/10][26/100][1264] Loss_D: 0.003605 Loss_G: 0.001865 \n",
      "[6/10][26/100][1265] Loss_D: 0.002751 Loss_G: 0.001442 \n",
      "[6/10][26/100][1266] Loss_D: 0.002766 Loss_G: 0.002028 \n",
      "[6/10][26/100][1267] Loss_D: 0.002959 Loss_G: 0.003891 \n",
      "[6/10][26/100][1268] Loss_D: 0.002223 Loss_G: 0.002005 \n",
      "[6/10][26/100][1269] Loss_D: 0.004210 Loss_G: 0.002294 \n",
      "[6/10][26/100][1270] Loss_D: 0.001857 Loss_G: 0.002722 \n",
      "[6/10][26/100][1271] Loss_D: 0.001916 Loss_G: 0.000926 \n",
      "[6/10][26/100][1272] Loss_D: 0.001364 Loss_G: 0.001330 \n",
      "[6/10][26/100][1273] Loss_D: 0.004475 Loss_G: 0.003334 \n",
      "[6/10][26/100][1274] Loss_D: 0.002275 Loss_G: 0.004770 \n",
      "[6/10][26/100][1275] Loss_D: 0.002690 Loss_G: 0.003257 \n",
      "[6/10][26/100][1276] Loss_D: 0.001409 Loss_G: 0.001920 \n",
      "[6/10][26/100][1277] Loss_D: 0.001586 Loss_G: 0.001372 \n",
      "[6/10][26/100][1278] Loss_D: 0.007182 Loss_G: 0.001611 \n",
      "[6/10][26/100][1279] Loss_D: 0.001819 Loss_G: 0.000621 \n",
      "[6/10][26/100][1280] Loss_D: 0.003584 Loss_G: 0.004137 \n",
      "[6/10][26/100][1281] Loss_D: 0.003101 Loss_G: 0.001255 \n",
      "[6/10][26/100][1282] Loss_D: 0.003004 Loss_G: 0.001988 \n",
      "[6/10][26/100][1283] Loss_D: 0.001639 Loss_G: 0.005615 \n",
      "[6/10][26/100][1284] Loss_D: 0.003205 Loss_G: 0.002636 \n",
      "[6/10][26/100][1285] Loss_D: 0.004723 Loss_G: 0.003889 \n",
      "[6/10][26/100][1286] Loss_D: 0.001620 Loss_G: 0.003139 \n",
      "[6/10][26/100][1287] Loss_D: 0.002204 Loss_G: 0.004243 \n",
      "[6/10][26/100][1288] Loss_D: 0.002195 Loss_G: 0.001889 \n",
      "[6/10][26/100][1289] Loss_D: 0.002952 Loss_G: 0.002272 \n",
      "[6/10][26/100][1290] Loss_D: 0.003501 Loss_G: 0.002035 \n",
      "[6/10][26/100][1291] Loss_D: 0.003316 Loss_G: 0.002771 \n",
      "[6/10][26/100][1292] Loss_D: 0.002930 Loss_G: 0.002663 \n",
      "[6/10][26/100][1293] Loss_D: 0.002280 Loss_G: 0.001653 \n",
      "[6/10][26/100][1294] Loss_D: 0.001526 Loss_G: 0.002265 \n",
      "[6/10][26/100][1295] Loss_D: 0.002028 Loss_G: 0.000861 \n",
      "[6/10][26/100][1296] Loss_D: 0.003956 Loss_G: 0.000758 \n",
      "[6/10][26/100][1297] Loss_D: 0.002167 Loss_G: 0.002702 \n",
      "[6/10][26/100][1298] Loss_D: 0.003312 Loss_G: 0.002355 \n",
      "[6/10][26/100][1299] Loss_D: 0.002069 Loss_G: 0.003312 \n",
      "[6/10][26/100][1300] Loss_D: 0.002678 Loss_G: 0.002042 \n",
      "[6/10][26/100][1301] Loss_D: 0.001658 Loss_G: 0.001398 \n",
      "[6/10][26/100][1302] Loss_D: 0.003379 Loss_G: -0.002324 \n",
      "[6/10][26/100][1303] Loss_D: 0.003462 Loss_G: 0.001225 \n",
      "[6/10][26/100][1304] Loss_D: 0.002011 Loss_G: 0.002167 \n",
      "[6/10][26/100][1305] Loss_D: 0.002860 Loss_G: 0.004505 \n",
      "[6/10][26/100][1306] Loss_D: 0.001947 Loss_G: 0.001873 \n",
      "[6/10][26/100][1307] Loss_D: 0.003868 Loss_G: 0.002211 \n",
      "[6/10][26/100][1308] Loss_D: 0.000719 Loss_G: 0.003306 \n",
      "[6/10][26/100][1309] Loss_D: 0.004052 Loss_G: 0.001408 \n",
      "[6/10][26/100][1310] Loss_D: 0.004293 Loss_G: 0.002573 \n",
      "[6/10][26/100][1311] Loss_D: 0.003118 Loss_G: 0.003804 \n",
      "[6/10][26/100][1312] Loss_D: 0.003535 Loss_G: 0.001146 \n",
      "[6/10][26/100][1313] Loss_D: 0.002561 Loss_G: 0.003483 \n",
      "[6/10][26/100][1314] Loss_D: 0.001321 Loss_G: 0.003627 \n",
      "[6/10][26/100][1315] Loss_D: 0.003647 Loss_G: 0.001752 \n",
      "[6/10][26/100][1316] Loss_D: 0.003238 Loss_G: 0.003290 \n",
      "[6/10][26/100][1317] Loss_D: 0.003831 Loss_G: 0.002489 \n",
      "[6/10][26/100][1318] Loss_D: 0.002547 Loss_G: 0.005008 \n",
      "[6/10][26/100][1319] Loss_D: 0.003478 Loss_G: 0.001343 \n",
      "[6/10][26/100][1320] Loss_D: 0.003278 Loss_G: 0.003001 \n",
      "[6/10][26/100][1321] Loss_D: 0.002000 Loss_G: 0.001542 \n",
      "[6/10][26/100][1322] Loss_D: 0.003174 Loss_G: 0.002348 \n",
      "[6/10][26/100][1323] Loss_D: 0.003893 Loss_G: 0.003592 \n",
      "[6/10][26/100][1324] Loss_D: 0.003368 Loss_G: 0.002744 \n",
      "[6/10][26/100][1325] Loss_D: 0.002184 Loss_G: 0.001941 \n",
      "[6/10][26/100][1326] Loss_D: 0.005825 Loss_G: 0.003015 \n",
      "[6/10][26/100][1327] Loss_D: 0.004748 Loss_G: 0.002365 \n",
      "[6/10][26/100][1328] Loss_D: 0.001674 Loss_G: 0.002787 \n",
      "[6/10][26/100][1329] Loss_D: 0.001532 Loss_G: 0.003979 \n",
      "[6/10][26/100][1330] Loss_D: 0.003189 Loss_G: 0.003040 \n",
      "[6/10][26/100][1331] Loss_D: 0.002614 Loss_G: 0.002268 \n",
      "[6/10][26/100][1332] Loss_D: 0.003084 Loss_G: 0.003924 \n",
      "[6/10][26/100][1333] Loss_D: 0.003927 Loss_G: 0.003944 \n",
      "[6/10][26/100][1334] Loss_D: 0.003851 Loss_G: 0.001491 \n",
      "[6/10][26/100][1335] Loss_D: 0.004094 Loss_G: 0.003244 \n",
      "[6/10][26/100][1336] Loss_D: 0.003286 Loss_G: 0.004056 \n",
      "[6/10][26/100][1337] Loss_D: 0.001379 Loss_G: 0.003519 \n",
      "[6/10][26/100][1338] Loss_D: 0.001637 Loss_G: 0.002799 \n",
      "[6/10][26/100][1339] Loss_D: 0.001803 Loss_G: 0.003860 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/10][26/100][1340] Loss_D: 0.003652 Loss_G: 0.003030 \n",
      "[6/10][26/100][1341] Loss_D: 0.002465 Loss_G: 0.005252 \n",
      "[6/10][26/100][1342] Loss_D: 0.003879 Loss_G: 0.003069 \n",
      "[6/10][26/100][1343] Loss_D: 0.002397 Loss_G: 0.001710 \n",
      "[6/10][26/100][1344] Loss_D: 0.001659 Loss_G: 0.002197 \n",
      "[6/10][26/100][1345] Loss_D: 0.003360 Loss_G: 0.003900 \n",
      "[6/10][26/100][1346] Loss_D: 0.003048 Loss_G: 0.002625 \n",
      "[6/10][26/100][1347] Loss_D: 0.001544 Loss_G: 0.001043 \n",
      "[6/10][26/100][1348] Loss_D: 0.004113 Loss_G: 0.003787 \n",
      "[6/10][26/100][1349] Loss_D: 0.003951 Loss_G: 0.002537 \n",
      "[6/10][26/100][1350] Loss_D: 0.004574 Loss_G: 0.001215 \n",
      "[6/10][26/100][1351] Loss_D: 0.002629 Loss_G: 0.004124 \n",
      "[6/10][26/100][1352] Loss_D: 0.003218 Loss_G: 0.001994 \n",
      "[6/10][26/100][1353] Loss_D: 0.003685 Loss_G: 0.002315 \n",
      "[6/10][26/100][1354] Loss_D: 0.002697 Loss_G: 0.002458 \n",
      "[6/10][26/100][1355] Loss_D: 0.003520 Loss_G: 0.001900 \n",
      "[6/10][26/100][1356] Loss_D: 0.000992 Loss_G: 0.001363 \n",
      "[6/10][26/100][1357] Loss_D: 0.004675 Loss_G: 0.005109 \n",
      "[6/10][26/100][1358] Loss_D: 0.004051 Loss_G: 0.000787 \n",
      "[6/10][26/100][1359] Loss_D: 0.002710 Loss_G: 0.003233 \n",
      "[6/10][26/100][1360] Loss_D: 0.002411 Loss_G: 0.002946 \n",
      "[6/10][26/100][1361] Loss_D: 0.003628 Loss_G: 0.002368 \n",
      "[6/10][26/100][1362] Loss_D: 0.003992 Loss_G: 0.003224 \n",
      "[6/10][26/100][1363] Loss_D: 0.001430 Loss_G: 0.001922 \n",
      "[6/10][26/100][1364] Loss_D: 0.001905 Loss_G: 0.001193 \n",
      "[6/10][26/100][1365] Loss_D: 0.001430 Loss_G: 0.001668 \n",
      "[6/10][26/100][1366] Loss_D: 0.002323 Loss_G: 0.000446 \n",
      "[6/10][26/100][1367] Loss_D: 0.000790 Loss_G: -0.000017 \n",
      "[6/10][26/100][1368] Loss_D: 0.002472 Loss_G: 0.002994 \n",
      "[6/10][26/100][1369] Loss_D: 0.001612 Loss_G: 0.002349 \n",
      "[6/10][26/100][1370] Loss_D: 0.003644 Loss_G: 0.003185 \n",
      "[6/10][26/100][1371] Loss_D: 0.003644 Loss_G: 0.002902 \n",
      "[6/10][26/100][1372] Loss_D: 0.002057 Loss_G: 0.003293 \n",
      "[6/10][26/100][1373] Loss_D: 0.000980 Loss_G: 0.000471 \n",
      "[6/10][26/100][1374] Loss_D: 0.002780 Loss_G: 0.002133 \n",
      "[6/10][26/100][1375] Loss_D: 0.001066 Loss_G: 0.002568 \n",
      "[6/10][26/100][1376] Loss_D: 0.001215 Loss_G: 0.003512 \n",
      "[6/10][26/100][1377] Loss_D: 0.002007 Loss_G: 0.001451 \n",
      "[6/10][26/100][1378] Loss_D: 0.002660 Loss_G: 0.005639 \n",
      "[6/10][26/100][1379] Loss_D: 0.001716 Loss_G: 0.001365 \n",
      "[6/10][26/100][1380] Loss_D: 0.003627 Loss_G: 0.002299 \n",
      "[6/10][26/100][1381] Loss_D: 0.002132 Loss_G: 0.003877 \n",
      "[6/10][26/100][1382] Loss_D: 0.003073 Loss_G: 0.002578 \n",
      "[6/10][26/100][1383] Loss_D: 0.004508 Loss_G: 0.000878 \n",
      "[6/10][26/100][1384] Loss_D: 0.002195 Loss_G: 0.003024 \n",
      "[6/10][26/100][1385] Loss_D: 0.004947 Loss_G: 0.004508 \n",
      "[6/10][26/100][1386] Loss_D: 0.002617 Loss_G: 0.003340 \n",
      "[6/10][26/100][1387] Loss_D: 0.002313 Loss_G: 0.001457 \n",
      "[6/10][26/100][1388] Loss_D: 0.003124 Loss_G: 0.001129 \n",
      "[6/10][26/100][1389] Loss_D: 0.004426 Loss_G: 0.003951 \n",
      "[6/10][26/100][1390] Loss_D: 0.002442 Loss_G: 0.004237 \n",
      "[6/10][26/100][1391] Loss_D: 0.001533 Loss_G: 0.001342 \n",
      "[6/10][26/100][1392] Loss_D: 0.003763 Loss_G: 0.004333 \n",
      "[6/10][26/100][1393] Loss_D: 0.002682 Loss_G: 0.003026 \n",
      "[6/10][26/100][1394] Loss_D: 0.003412 Loss_G: 0.002268 \n",
      "[6/10][26/100][1395] Loss_D: 0.001427 Loss_G: 0.001074 \n",
      "[6/10][26/100][1396] Loss_D: 0.003364 Loss_G: 0.000808 \n",
      "[6/10][26/100][1397] Loss_D: 0.003418 Loss_G: 0.000188 \n",
      "[6/10][26/100][1398] Loss_D: 0.003401 Loss_G: 0.000706 \n",
      "[6/10][26/100][1399] Loss_D: 0.004557 Loss_G: 0.001773 \n",
      "[6/10][26/100][1400] Loss_D: 0.003314 Loss_G: 0.004379 \n",
      "[7/10][26/100][1401] Loss_D: 0.002359 Loss_G: 0.002256 \n",
      "[7/10][26/100][1402] Loss_D: 0.003851 Loss_G: 0.001288 \n",
      "[7/10][26/100][1403] Loss_D: 0.003047 Loss_G: 0.006175 \n",
      "[7/10][26/100][1404] Loss_D: 0.003121 Loss_G: 0.004864 \n",
      "[7/10][26/100][1405] Loss_D: 0.002776 Loss_G: 0.002336 \n",
      "[7/10][26/100][1406] Loss_D: 0.000750 Loss_G: 0.003207 \n",
      "[7/10][26/100][1407] Loss_D: 0.001176 Loss_G: 0.001086 \n",
      "[7/10][26/100][1408] Loss_D: 0.005175 Loss_G: 0.004161 \n",
      "[7/10][26/100][1409] Loss_D: 0.003860 Loss_G: 0.002942 \n",
      "[7/10][26/100][1410] Loss_D: 0.003798 Loss_G: 0.002137 \n",
      "[7/10][26/100][1411] Loss_D: 0.002793 Loss_G: 0.000701 \n",
      "[7/10][26/100][1412] Loss_D: 0.004748 Loss_G: 0.002034 \n",
      "[7/10][26/100][1413] Loss_D: 0.005015 Loss_G: 0.003521 \n",
      "[7/10][26/100][1414] Loss_D: 0.003957 Loss_G: 0.003645 \n",
      "[7/10][26/100][1415] Loss_D: 0.003428 Loss_G: 0.003656 \n",
      "[7/10][26/100][1416] Loss_D: 0.004446 Loss_G: 0.003851 \n",
      "[7/10][26/100][1417] Loss_D: 0.003389 Loss_G: 0.002694 \n",
      "[7/10][26/100][1418] Loss_D: 0.002777 Loss_G: 0.002846 \n",
      "[7/10][26/100][1419] Loss_D: 0.002498 Loss_G: 0.000590 \n",
      "[7/10][26/100][1420] Loss_D: 0.002580 Loss_G: 0.001445 \n",
      "[7/10][26/100][1421] Loss_D: 0.003701 Loss_G: 0.002644 \n",
      "[7/10][26/100][1422] Loss_D: 0.002114 Loss_G: 0.002287 \n",
      "[7/10][26/100][1423] Loss_D: 0.001715 Loss_G: 0.001075 \n",
      "[7/10][26/100][1424] Loss_D: 0.000084 Loss_G: 0.001840 \n",
      "[7/10][26/100][1425] Loss_D: 0.003434 Loss_G: 0.003905 \n",
      "[7/10][26/100][1426] Loss_D: 0.002746 Loss_G: 0.003372 \n",
      "[7/10][26/100][1427] Loss_D: 0.003664 Loss_G: 0.001983 \n",
      "[7/10][26/100][1428] Loss_D: 0.003282 Loss_G: 0.000975 \n",
      "[7/10][26/100][1429] Loss_D: 0.002919 Loss_G: 0.003311 \n",
      "[7/10][26/100][1430] Loss_D: 0.002597 Loss_G: 0.004094 \n",
      "[7/10][26/100][1431] Loss_D: 0.003876 Loss_G: 0.003198 \n",
      "[7/10][26/100][1432] Loss_D: 0.002335 Loss_G: 0.003181 \n",
      "[7/10][26/100][1433] Loss_D: 0.002626 Loss_G: 0.002536 \n",
      "[7/10][26/100][1434] Loss_D: 0.001707 Loss_G: 0.002362 \n",
      "[7/10][26/100][1435] Loss_D: 0.002572 Loss_G: 0.003966 \n",
      "[7/10][26/100][1436] Loss_D: 0.006572 Loss_G: 0.005982 \n",
      "[7/10][26/100][1437] Loss_D: 0.001669 Loss_G: 0.001457 \n",
      "[7/10][26/100][1438] Loss_D: 0.002984 Loss_G: 0.003089 \n",
      "[7/10][26/100][1439] Loss_D: 0.003264 Loss_G: 0.000935 \n",
      "[7/10][26/100][1440] Loss_D: 0.000379 Loss_G: 0.002531 \n",
      "[7/10][26/100][1441] Loss_D: 0.001991 Loss_G: 0.001913 \n",
      "[7/10][26/100][1442] Loss_D: 0.005586 Loss_G: 0.002694 \n",
      "[7/10][26/100][1443] Loss_D: 0.003254 Loss_G: 0.002012 \n",
      "[7/10][26/100][1444] Loss_D: 0.003246 Loss_G: 0.001595 \n",
      "[7/10][26/100][1445] Loss_D: 0.002255 Loss_G: 0.002293 \n",
      "[7/10][26/100][1446] Loss_D: 0.001735 Loss_G: 0.004622 \n",
      "[7/10][26/100][1447] Loss_D: 0.004114 Loss_G: 0.002804 \n",
      "[7/10][26/100][1448] Loss_D: 0.002085 Loss_G: 0.002338 \n",
      "[7/10][26/100][1449] Loss_D: 0.001452 Loss_G: 0.002877 \n",
      "[7/10][26/100][1450] Loss_D: 0.003023 Loss_G: 0.002936 \n",
      "[7/10][26/100][1451] Loss_D: 0.003075 Loss_G: 0.002044 \n",
      "[7/10][26/100][1452] Loss_D: 0.003244 Loss_G: 0.002725 \n",
      "[7/10][26/100][1453] Loss_D: 0.003216 Loss_G: 0.001123 \n",
      "[7/10][26/100][1454] Loss_D: 0.004338 Loss_G: 0.003050 \n",
      "[7/10][26/100][1455] Loss_D: 0.001787 Loss_G: 0.003540 \n",
      "[7/10][26/100][1456] Loss_D: 0.001382 Loss_G: 0.002785 \n",
      "[7/10][26/100][1457] Loss_D: 0.001754 Loss_G: 0.002472 \n",
      "[7/10][26/100][1458] Loss_D: 0.004542 Loss_G: 0.003161 \n",
      "[7/10][26/100][1459] Loss_D: 0.002957 Loss_G: 0.005020 \n",
      "[7/10][26/100][1460] Loss_D: 0.002703 Loss_G: 0.003689 \n",
      "[7/10][26/100][1461] Loss_D: 0.001895 Loss_G: 0.005143 \n",
      "[7/10][26/100][1462] Loss_D: 0.003423 Loss_G: 0.001211 \n",
      "[7/10][26/100][1463] Loss_D: 0.002943 Loss_G: 0.001848 \n",
      "[7/10][26/100][1464] Loss_D: 0.003415 Loss_G: 0.001325 \n",
      "[7/10][26/100][1465] Loss_D: 0.003401 Loss_G: 0.002924 \n",
      "[7/10][26/100][1466] Loss_D: 0.000556 Loss_G: 0.001338 \n",
      "[7/10][26/100][1467] Loss_D: 0.001404 Loss_G: 0.003303 \n",
      "[7/10][26/100][1468] Loss_D: 0.002068 Loss_G: 0.002325 \n",
      "[7/10][26/100][1469] Loss_D: 0.001704 Loss_G: 0.002073 \n",
      "[7/10][26/100][1470] Loss_D: 0.001061 Loss_G: 0.003889 \n",
      "[7/10][26/100][1471] Loss_D: 0.002696 Loss_G: 0.003814 \n",
      "[7/10][26/100][1472] Loss_D: 0.001489 Loss_G: 0.001580 \n",
      "[7/10][26/100][1473] Loss_D: 0.001972 Loss_G: 0.002101 \n",
      "[7/10][26/100][1474] Loss_D: 0.001387 Loss_G: 0.002329 \n",
      "[7/10][26/100][1475] Loss_D: 0.004680 Loss_G: 0.002506 \n",
      "[7/10][26/100][1476] Loss_D: 0.000751 Loss_G: 0.002518 \n",
      "[7/10][26/100][1477] Loss_D: 0.002673 Loss_G: 0.003414 \n",
      "[7/10][26/100][1478] Loss_D: 0.002002 Loss_G: 0.001500 \n",
      "[7/10][26/100][1479] Loss_D: 0.003337 Loss_G: -0.000421 \n",
      "[7/10][26/100][1480] Loss_D: 0.003437 Loss_G: 0.004502 \n",
      "[7/10][26/100][1481] Loss_D: 0.001991 Loss_G: 0.004912 \n",
      "[7/10][26/100][1482] Loss_D: 0.004285 Loss_G: 0.000407 \n",
      "[7/10][26/100][1483] Loss_D: 0.004441 Loss_G: 0.003634 \n",
      "[7/10][26/100][1484] Loss_D: 0.003387 Loss_G: 0.000356 \n",
      "[7/10][26/100][1485] Loss_D: 0.005232 Loss_G: 0.003959 \n",
      "[7/10][26/100][1486] Loss_D: 0.002706 Loss_G: 0.004525 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/10][26/100][1487] Loss_D: 0.001965 Loss_G: 0.002849 \n",
      "[7/10][26/100][1488] Loss_D: 0.002913 Loss_G: 0.002156 \n",
      "[7/10][26/100][1489] Loss_D: 0.001691 Loss_G: 0.002613 \n",
      "[7/10][26/100][1490] Loss_D: 0.001634 Loss_G: 0.002130 \n",
      "[7/10][26/100][1491] Loss_D: 0.003907 Loss_G: 0.002053 \n",
      "[7/10][26/100][1492] Loss_D: 0.002752 Loss_G: 0.001381 \n",
      "[7/10][26/100][1493] Loss_D: 0.002140 Loss_G: 0.003838 \n",
      "[7/10][26/100][1494] Loss_D: 0.003575 Loss_G: 0.003588 \n",
      "[7/10][26/100][1495] Loss_D: 0.002519 Loss_G: 0.002288 \n",
      "[7/10][26/100][1496] Loss_D: 0.001994 Loss_G: 0.002685 \n",
      "[7/10][26/100][1497] Loss_D: 0.001699 Loss_G: 0.002890 \n",
      "[7/10][26/100][1498] Loss_D: 0.001647 Loss_G: 0.005059 \n",
      "[7/10][26/100][1499] Loss_D: 0.002003 Loss_G: 0.002173 \n",
      "[7/10][26/100][1500] Loss_D: 0.001918 Loss_G: 0.001773 \n",
      "[7/10][26/100][1501] Loss_D: 0.001763 Loss_G: 0.002832 \n",
      "[7/10][26/100][1502] Loss_D: 0.004306 Loss_G: 0.002392 \n",
      "[7/10][26/100][1503] Loss_D: 0.003089 Loss_G: 0.003696 \n",
      "[7/10][26/100][1504] Loss_D: 0.005219 Loss_G: 0.003695 \n",
      "[7/10][26/100][1505] Loss_D: 0.001999 Loss_G: 0.002977 \n",
      "[7/10][26/100][1506] Loss_D: 0.004387 Loss_G: 0.004212 \n",
      "[7/10][26/100][1507] Loss_D: 0.001968 Loss_G: 0.005840 \n",
      "[7/10][26/100][1508] Loss_D: 0.000975 Loss_G: 0.001685 \n",
      "[7/10][26/100][1509] Loss_D: 0.000962 Loss_G: 0.001834 \n",
      "[7/10][26/100][1510] Loss_D: 0.001210 Loss_G: 0.001951 \n",
      "[7/10][26/100][1511] Loss_D: 0.003268 Loss_G: 0.003340 \n",
      "[7/10][26/100][1512] Loss_D: 0.002713 Loss_G: 0.004848 \n",
      "[7/10][26/100][1513] Loss_D: 0.004266 Loss_G: 0.002467 \n",
      "[7/10][26/100][1514] Loss_D: 0.003238 Loss_G: 0.001551 \n",
      "[7/10][26/100][1515] Loss_D: 0.001666 Loss_G: 0.002505 \n",
      "[7/10][26/100][1516] Loss_D: 0.001555 Loss_G: 0.001435 \n",
      "[7/10][26/100][1517] Loss_D: 0.002868 Loss_G: 0.001889 \n",
      "[7/10][26/100][1518] Loss_D: 0.002629 Loss_G: 0.001541 \n",
      "[7/10][26/100][1519] Loss_D: 0.008301 Loss_G: 0.003580 \n",
      "[7/10][26/100][1520] Loss_D: 0.002549 Loss_G: 0.002656 \n",
      "[7/10][26/100][1521] Loss_D: 0.004342 Loss_G: 0.005019 \n",
      "[7/10][26/100][1522] Loss_D: 0.002624 Loss_G: 0.004910 \n",
      "[7/10][26/100][1523] Loss_D: 0.003106 Loss_G: 0.001993 \n",
      "[7/10][26/100][1524] Loss_D: 0.002725 Loss_G: 0.002957 \n",
      "[7/10][26/100][1525] Loss_D: 0.002440 Loss_G: 0.002636 \n",
      "[7/10][26/100][1526] Loss_D: 0.002733 Loss_G: 0.004405 \n",
      "[7/10][26/100][1527] Loss_D: 0.003124 Loss_G: 0.002915 \n",
      "[7/10][26/100][1528] Loss_D: 0.001043 Loss_G: 0.001359 \n",
      "[7/10][26/100][1529] Loss_D: 0.002219 Loss_G: 0.001633 \n",
      "[7/10][26/100][1530] Loss_D: 0.002980 Loss_G: 0.002630 \n",
      "[7/10][26/100][1531] Loss_D: 0.005236 Loss_G: 0.005986 \n",
      "[7/10][26/100][1532] Loss_D: 0.003299 Loss_G: 0.002524 \n",
      "[7/10][26/100][1533] Loss_D: 0.001928 Loss_G: 0.002955 \n",
      "[7/10][26/100][1534] Loss_D: 0.001712 Loss_G: 0.001267 \n",
      "[7/10][26/100][1535] Loss_D: 0.002160 Loss_G: 0.005335 \n",
      "[7/10][26/100][1536] Loss_D: 0.001715 Loss_G: 0.004598 \n",
      "[7/10][26/100][1537] Loss_D: 0.004934 Loss_G: 0.003597 \n",
      "[7/10][26/100][1538] Loss_D: 0.004904 Loss_G: 0.004554 \n",
      "[7/10][26/100][1539] Loss_D: 0.002276 Loss_G: 0.003382 \n",
      "[7/10][26/100][1540] Loss_D: 0.003399 Loss_G: 0.003951 \n",
      "[7/10][26/100][1541] Loss_D: 0.003684 Loss_G: 0.002567 \n",
      "[7/10][26/100][1542] Loss_D: 0.002294 Loss_G: 0.000600 \n",
      "[7/10][26/100][1543] Loss_D: 0.001855 Loss_G: 0.000687 \n",
      "[7/10][26/100][1544] Loss_D: 0.003146 Loss_G: 0.003483 \n",
      "[7/10][26/100][1545] Loss_D: 0.004660 Loss_G: 0.001739 \n",
      "[7/10][26/100][1546] Loss_D: 0.003312 Loss_G: 0.001288 \n",
      "[7/10][26/100][1547] Loss_D: 0.002466 Loss_G: 0.002076 \n",
      "[7/10][26/100][1548] Loss_D: 0.002892 Loss_G: 0.001198 \n",
      "[7/10][26/100][1549] Loss_D: 0.001968 Loss_G: 0.002377 \n",
      "[7/10][26/100][1550] Loss_D: 0.001723 Loss_G: 0.002106 \n",
      "[7/10][26/100][1551] Loss_D: 0.001808 Loss_G: 0.003753 \n",
      "[7/10][26/100][1552] Loss_D: 0.003143 Loss_G: 0.002484 \n",
      "[7/10][26/100][1553] Loss_D: 0.003019 Loss_G: 0.004743 \n",
      "[7/10][26/100][1554] Loss_D: 0.003908 Loss_G: 0.002188 \n",
      "[7/10][26/100][1555] Loss_D: 0.001021 Loss_G: 0.002931 \n",
      "[7/10][26/100][1556] Loss_D: 0.004190 Loss_G: 0.001557 \n",
      "[7/10][26/100][1557] Loss_D: 0.002378 Loss_G: 0.002776 \n",
      "[7/10][26/100][1558] Loss_D: 0.001465 Loss_G: 0.001851 \n",
      "[7/10][26/100][1559] Loss_D: 0.002570 Loss_G: 0.001824 \n",
      "[7/10][26/100][1560] Loss_D: 0.002767 Loss_G: 0.004289 \n",
      "[7/10][26/100][1561] Loss_D: 0.004904 Loss_G: 0.005052 \n",
      "[7/10][26/100][1562] Loss_D: 0.000332 Loss_G: 0.002616 \n",
      "[7/10][26/100][1563] Loss_D: 0.002094 Loss_G: 0.003783 \n",
      "[7/10][26/100][1564] Loss_D: 0.003233 Loss_G: 0.002766 \n",
      "[7/10][26/100][1565] Loss_D: 0.003452 Loss_G: 0.001958 \n",
      "[7/10][26/100][1566] Loss_D: 0.000683 Loss_G: 0.002124 \n",
      "[7/10][26/100][1567] Loss_D: 0.005388 Loss_G: 0.005333 \n",
      "[7/10][26/100][1568] Loss_D: 0.002518 Loss_G: 0.003697 \n",
      "[7/10][26/100][1569] Loss_D: -0.000500 Loss_G: 0.002431 \n",
      "[7/10][26/100][1570] Loss_D: 0.003232 Loss_G: 0.002214 \n",
      "[7/10][26/100][1571] Loss_D: 0.005242 Loss_G: 0.002138 \n",
      "[7/10][26/100][1572] Loss_D: 0.002863 Loss_G: 0.003139 \n",
      "[7/10][26/100][1573] Loss_D: 0.002394 Loss_G: 0.002109 \n",
      "[7/10][26/100][1574] Loss_D: 0.001724 Loss_G: 0.003537 \n",
      "[7/10][26/100][1575] Loss_D: 0.002545 Loss_G: 0.002770 \n",
      "[7/10][26/100][1576] Loss_D: 0.005154 Loss_G: 0.001907 \n",
      "[7/10][26/100][1577] Loss_D: 0.003854 Loss_G: 0.000801 \n",
      "[7/10][26/100][1578] Loss_D: 0.003985 Loss_G: 0.004038 \n",
      "[7/10][26/100][1579] Loss_D: 0.001034 Loss_G: 0.002826 \n",
      "[7/10][26/100][1580] Loss_D: 0.002419 Loss_G: 0.003157 \n",
      "[7/10][26/100][1581] Loss_D: 0.006787 Loss_G: 0.004358 \n",
      "[7/10][26/100][1582] Loss_D: 0.002132 Loss_G: 0.002573 \n",
      "[7/10][26/100][1583] Loss_D: 0.003323 Loss_G: 0.001641 \n",
      "[7/10][26/100][1584] Loss_D: 0.002439 Loss_G: 0.003353 \n",
      "[7/10][26/100][1585] Loss_D: 0.002960 Loss_G: 0.002585 \n",
      "[7/10][26/100][1586] Loss_D: 0.001110 Loss_G: 0.003353 \n",
      "[7/10][26/100][1587] Loss_D: 0.002309 Loss_G: 0.002297 \n",
      "[7/10][26/100][1588] Loss_D: 0.002797 Loss_G: 0.003235 \n",
      "[7/10][26/100][1589] Loss_D: 0.003804 Loss_G: 0.001710 \n",
      "[7/10][26/100][1590] Loss_D: 0.002404 Loss_G: 0.001616 \n",
      "[7/10][26/100][1591] Loss_D: 0.002389 Loss_G: 0.003350 \n",
      "[7/10][26/100][1592] Loss_D: 0.002392 Loss_G: 0.000994 \n",
      "[7/10][26/100][1593] Loss_D: 0.002134 Loss_G: 0.002856 \n",
      "[7/10][26/100][1594] Loss_D: 0.001706 Loss_G: 0.002183 \n",
      "[7/10][26/100][1595] Loss_D: 0.003181 Loss_G: 0.001372 \n",
      "[7/10][26/100][1596] Loss_D: 0.003285 Loss_G: 0.005380 \n",
      "[7/10][26/100][1597] Loss_D: 0.000255 Loss_G: 0.003547 \n",
      "[7/10][26/100][1598] Loss_D: 0.001246 Loss_G: 0.003274 \n",
      "[7/10][26/100][1599] Loss_D: 0.002134 Loss_G: 0.001621 \n",
      "[7/10][26/100][1600] Loss_D: 0.002465 Loss_G: 0.003092 \n",
      "[8/10][26/100][1601] Loss_D: 0.004769 Loss_G: 0.001870 \n",
      "[8/10][26/100][1602] Loss_D: 0.001961 Loss_G: 0.002303 \n",
      "[8/10][26/100][1603] Loss_D: 0.002582 Loss_G: 0.004122 \n",
      "[8/10][26/100][1604] Loss_D: 0.002345 Loss_G: 0.003611 \n",
      "[8/10][26/100][1605] Loss_D: 0.004868 Loss_G: 0.001886 \n",
      "[8/10][26/100][1606] Loss_D: 0.002800 Loss_G: 0.002134 \n",
      "[8/10][26/100][1607] Loss_D: 0.002537 Loss_G: 0.003836 \n",
      "[8/10][26/100][1608] Loss_D: 0.003031 Loss_G: 0.002585 \n",
      "[8/10][26/100][1609] Loss_D: 0.004095 Loss_G: 0.003163 \n",
      "[8/10][26/100][1610] Loss_D: 0.002774 Loss_G: 0.003591 \n",
      "[8/10][26/100][1611] Loss_D: 0.002470 Loss_G: 0.000615 \n",
      "[8/10][26/100][1612] Loss_D: 0.003017 Loss_G: 0.003225 \n",
      "[8/10][26/100][1613] Loss_D: 0.002845 Loss_G: 0.003946 \n",
      "[8/10][26/100][1614] Loss_D: 0.002598 Loss_G: 0.002498 \n",
      "[8/10][26/100][1615] Loss_D: 0.003790 Loss_G: 0.000287 \n",
      "[8/10][26/100][1616] Loss_D: 0.002037 Loss_G: 0.004450 \n",
      "[8/10][26/100][1617] Loss_D: 0.002957 Loss_G: 0.003229 \n",
      "[8/10][26/100][1618] Loss_D: 0.002026 Loss_G: 0.001833 \n",
      "[8/10][26/100][1619] Loss_D: 0.001910 Loss_G: -0.000007 \n",
      "[8/10][26/100][1620] Loss_D: 0.001235 Loss_G: 0.003320 \n",
      "[8/10][26/100][1621] Loss_D: 0.002687 Loss_G: 0.003363 \n",
      "[8/10][26/100][1622] Loss_D: 0.002276 Loss_G: 0.003823 \n",
      "[8/10][26/100][1623] Loss_D: 0.002316 Loss_G: 0.003336 \n",
      "[8/10][26/100][1624] Loss_D: 0.003962 Loss_G: 0.001609 \n",
      "[8/10][26/100][1625] Loss_D: 0.001733 Loss_G: 0.000793 \n",
      "[8/10][26/100][1626] Loss_D: 0.000159 Loss_G: 0.004401 \n",
      "[8/10][26/100][1627] Loss_D: 0.001282 Loss_G: 0.004669 \n",
      "[8/10][26/100][1628] Loss_D: 0.003416 Loss_G: 0.003881 \n",
      "[8/10][26/100][1629] Loss_D: 0.001674 Loss_G: 0.002474 \n",
      "[8/10][26/100][1630] Loss_D: 0.002841 Loss_G: 0.002452 \n",
      "[8/10][26/100][1631] Loss_D: 0.003124 Loss_G: 0.004237 \n",
      "[8/10][26/100][1632] Loss_D: 0.004578 Loss_G: 0.002707 \n",
      "[8/10][26/100][1633] Loss_D: 0.003530 Loss_G: 0.002075 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][26/100][1634] Loss_D: 0.003664 Loss_G: 0.002980 \n",
      "[8/10][26/100][1635] Loss_D: 0.003218 Loss_G: 0.004434 \n",
      "[8/10][26/100][1636] Loss_D: 0.002367 Loss_G: 0.002041 \n",
      "[8/10][26/100][1637] Loss_D: 0.003369 Loss_G: 0.005300 \n",
      "[8/10][26/100][1638] Loss_D: 0.002763 Loss_G: 0.002825 \n",
      "[8/10][26/100][1639] Loss_D: 0.004250 Loss_G: 0.001523 \n",
      "[8/10][26/100][1640] Loss_D: 0.003487 Loss_G: 0.004189 \n",
      "[8/10][26/100][1641] Loss_D: 0.004970 Loss_G: 0.002829 \n",
      "[8/10][26/100][1642] Loss_D: 0.001995 Loss_G: 0.002354 \n",
      "[8/10][26/100][1643] Loss_D: 0.002439 Loss_G: 0.005154 \n",
      "[8/10][26/100][1644] Loss_D: 0.005568 Loss_G: 0.001317 \n",
      "[8/10][26/100][1645] Loss_D: 0.001403 Loss_G: 0.002422 \n",
      "[8/10][26/100][1646] Loss_D: 0.000532 Loss_G: 0.001972 \n",
      "[8/10][26/100][1647] Loss_D: 0.002941 Loss_G: 0.002522 \n",
      "[8/10][26/100][1648] Loss_D: 0.003760 Loss_G: 0.003727 \n",
      "[8/10][26/100][1649] Loss_D: 0.002537 Loss_G: 0.002009 \n",
      "[8/10][26/100][1650] Loss_D: 0.002320 Loss_G: 0.002986 \n",
      "[8/10][26/100][1651] Loss_D: 0.000739 Loss_G: 0.004228 \n",
      "[8/10][26/100][1652] Loss_D: 0.001948 Loss_G: 0.002092 \n",
      "[8/10][26/100][1653] Loss_D: 0.001673 Loss_G: 0.002715 \n",
      "[8/10][26/100][1654] Loss_D: 0.003627 Loss_G: 0.001347 \n",
      "[8/10][26/100][1655] Loss_D: 0.002181 Loss_G: 0.003760 \n",
      "[8/10][26/100][1656] Loss_D: 0.002377 Loss_G: 0.002653 \n",
      "[8/10][26/100][1657] Loss_D: 0.003104 Loss_G: 0.003248 \n",
      "[8/10][26/100][1658] Loss_D: 0.002020 Loss_G: 0.003796 \n",
      "[8/10][26/100][1659] Loss_D: 0.002700 Loss_G: 0.002261 \n",
      "[8/10][26/100][1660] Loss_D: 0.002205 Loss_G: 0.003312 \n",
      "[8/10][26/100][1661] Loss_D: 0.002263 Loss_G: 0.002770 \n",
      "[8/10][26/100][1662] Loss_D: 0.003203 Loss_G: 0.001546 \n",
      "[8/10][26/100][1663] Loss_D: 0.001324 Loss_G: 0.002431 \n",
      "[8/10][26/100][1664] Loss_D: 0.000890 Loss_G: 0.003083 \n",
      "[8/10][26/100][1665] Loss_D: 0.004270 Loss_G: 0.005768 \n",
      "[8/10][26/100][1666] Loss_D: 0.004867 Loss_G: 0.001097 \n",
      "[8/10][26/100][1667] Loss_D: 0.002099 Loss_G: 0.003004 \n",
      "[8/10][26/100][1668] Loss_D: 0.002631 Loss_G: 0.003199 \n",
      "[8/10][26/100][1669] Loss_D: 0.003847 Loss_G: 0.005683 \n",
      "[8/10][26/100][1670] Loss_D: 0.001120 Loss_G: 0.002168 \n",
      "[8/10][26/100][1671] Loss_D: 0.002304 Loss_G: 0.004179 \n",
      "[8/10][26/100][1672] Loss_D: 0.002134 Loss_G: 0.003347 \n",
      "[8/10][26/100][1673] Loss_D: 0.002667 Loss_G: 0.001724 \n",
      "[8/10][26/100][1674] Loss_D: 0.001635 Loss_G: 0.002246 \n",
      "[8/10][26/100][1675] Loss_D: 0.001964 Loss_G: 0.003573 \n",
      "[8/10][26/100][1676] Loss_D: 0.003467 Loss_G: 0.002209 \n",
      "[8/10][26/100][1677] Loss_D: 0.001988 Loss_G: 0.002034 \n",
      "[8/10][26/100][1678] Loss_D: 0.002620 Loss_G: 0.002714 \n",
      "[8/10][26/100][1679] Loss_D: 0.000738 Loss_G: 0.001137 \n",
      "[8/10][26/100][1680] Loss_D: 0.001115 Loss_G: 0.002424 \n",
      "[8/10][26/100][1681] Loss_D: 0.002490 Loss_G: 0.004011 \n",
      "[8/10][26/100][1682] Loss_D: 0.002438 Loss_G: 0.001342 \n",
      "[8/10][26/100][1683] Loss_D: 0.004093 Loss_G: 0.003729 \n",
      "[8/10][26/100][1684] Loss_D: 0.003692 Loss_G: 0.004804 \n",
      "[8/10][26/100][1685] Loss_D: 0.003001 Loss_G: 0.001178 \n",
      "[8/10][26/100][1686] Loss_D: 0.002322 Loss_G: 0.001802 \n",
      "[8/10][26/100][1687] Loss_D: 0.001939 Loss_G: 0.002646 \n",
      "[8/10][26/100][1688] Loss_D: 0.004481 Loss_G: 0.002082 \n",
      "[8/10][26/100][1689] Loss_D: 0.001931 Loss_G: 0.002056 \n",
      "[8/10][26/100][1690] Loss_D: 0.001474 Loss_G: 0.003242 \n",
      "[8/10][26/100][1691] Loss_D: 0.002001 Loss_G: 0.002538 \n",
      "[8/10][26/100][1692] Loss_D: 0.002615 Loss_G: 0.005586 \n",
      "[8/10][26/100][1693] Loss_D: 0.000957 Loss_G: 0.002200 \n",
      "[8/10][26/100][1694] Loss_D: 0.002774 Loss_G: 0.002697 \n",
      "[8/10][26/100][1695] Loss_D: 0.002559 Loss_G: 0.003606 \n",
      "[8/10][26/100][1696] Loss_D: 0.002620 Loss_G: 0.003205 \n",
      "[8/10][26/100][1697] Loss_D: 0.003750 Loss_G: 0.004588 \n",
      "[8/10][26/100][1698] Loss_D: 0.002104 Loss_G: 0.002151 \n",
      "[8/10][26/100][1699] Loss_D: 0.002341 Loss_G: 0.002385 \n",
      "[8/10][26/100][1700] Loss_D: 0.001651 Loss_G: 0.001977 \n",
      "[8/10][26/100][1701] Loss_D: 0.003125 Loss_G: 0.002933 \n",
      "[8/10][26/100][1702] Loss_D: 0.003839 Loss_G: 0.004339 \n",
      "[8/10][26/100][1703] Loss_D: 0.002807 Loss_G: 0.001664 \n",
      "[8/10][26/100][1704] Loss_D: 0.005889 Loss_G: 0.001820 \n",
      "[8/10][26/100][1705] Loss_D: 0.001176 Loss_G: 0.002574 \n",
      "[8/10][26/100][1706] Loss_D: 0.002528 Loss_G: 0.002159 \n",
      "[8/10][26/100][1707] Loss_D: 0.002308 Loss_G: 0.002867 \n",
      "[8/10][26/100][1708] Loss_D: 0.001961 Loss_G: 0.003456 \n",
      "[8/10][26/100][1709] Loss_D: 0.002376 Loss_G: 0.004753 \n",
      "[8/10][26/100][1710] Loss_D: 0.001702 Loss_G: 0.002107 \n",
      "[8/10][26/100][1711] Loss_D: 0.002263 Loss_G: 0.003895 \n",
      "[8/10][26/100][1712] Loss_D: 0.002449 Loss_G: 0.000701 \n",
      "[8/10][26/100][1713] Loss_D: 0.003102 Loss_G: 0.003495 \n",
      "[8/10][26/100][1714] Loss_D: 0.004107 Loss_G: 0.004025 \n",
      "[8/10][26/100][1715] Loss_D: 0.002989 Loss_G: 0.004026 \n",
      "[8/10][26/100][1716] Loss_D: 0.004537 Loss_G: 0.003604 \n",
      "[8/10][26/100][1717] Loss_D: 0.002462 Loss_G: 0.001671 \n",
      "[8/10][26/100][1718] Loss_D: 0.003049 Loss_G: 0.002410 \n",
      "[8/10][26/100][1719] Loss_D: 0.001846 Loss_G: 0.002325 \n",
      "[8/10][26/100][1720] Loss_D: 0.004447 Loss_G: 0.002759 \n",
      "[8/10][26/100][1721] Loss_D: 0.003661 Loss_G: 0.001444 \n",
      "[8/10][26/100][1722] Loss_D: 0.003328 Loss_G: 0.004339 \n",
      "[8/10][26/100][1723] Loss_D: 0.001958 Loss_G: 0.002780 \n",
      "[8/10][26/100][1724] Loss_D: 0.001216 Loss_G: 0.004314 \n",
      "[8/10][26/100][1725] Loss_D: 0.002875 Loss_G: 0.002251 \n",
      "[8/10][26/100][1726] Loss_D: 0.001495 Loss_G: 0.001726 \n",
      "[8/10][26/100][1727] Loss_D: 0.002895 Loss_G: 0.001048 \n",
      "[8/10][26/100][1728] Loss_D: 0.002281 Loss_G: 0.003246 \n",
      "[8/10][26/100][1729] Loss_D: 0.004949 Loss_G: 0.001784 \n",
      "[8/10][26/100][1730] Loss_D: 0.003480 Loss_G: 0.002473 \n",
      "[8/10][26/100][1731] Loss_D: 0.003199 Loss_G: 0.003311 \n",
      "[8/10][26/100][1732] Loss_D: 0.003037 Loss_G: 0.002581 \n",
      "[8/10][26/100][1733] Loss_D: 0.001577 Loss_G: 0.001910 \n",
      "[8/10][26/100][1734] Loss_D: 0.001702 Loss_G: 0.001864 \n",
      "[8/10][26/100][1735] Loss_D: 0.004961 Loss_G: 0.001012 \n",
      "[8/10][26/100][1736] Loss_D: 0.002195 Loss_G: 0.004222 \n",
      "[8/10][26/100][1737] Loss_D: 0.003271 Loss_G: 0.000670 \n",
      "[8/10][26/100][1738] Loss_D: 0.003564 Loss_G: 0.000710 \n",
      "[8/10][26/100][1739] Loss_D: 0.003576 Loss_G: 0.002375 \n",
      "[8/10][26/100][1740] Loss_D: -0.000178 Loss_G: 0.003070 \n",
      "[8/10][26/100][1741] Loss_D: 0.003312 Loss_G: 0.002827 \n",
      "[8/10][26/100][1742] Loss_D: 0.002159 Loss_G: 0.002206 \n",
      "[8/10][26/100][1743] Loss_D: 0.001520 Loss_G: 0.001360 \n",
      "[8/10][26/100][1744] Loss_D: 0.003286 Loss_G: 0.003969 \n",
      "[8/10][26/100][1745] Loss_D: 0.001998 Loss_G: 0.002518 \n",
      "[8/10][26/100][1746] Loss_D: 0.001989 Loss_G: 0.001963 \n",
      "[8/10][26/100][1747] Loss_D: 0.002862 Loss_G: 0.001287 \n",
      "[8/10][26/100][1748] Loss_D: 0.002900 Loss_G: 0.003260 \n",
      "[8/10][26/100][1749] Loss_D: 0.001689 Loss_G: 0.002112 \n",
      "[8/10][26/100][1750] Loss_D: 0.001947 Loss_G: 0.002971 \n",
      "[8/10][26/100][1751] Loss_D: 0.001595 Loss_G: 0.002539 \n",
      "[8/10][26/100][1752] Loss_D: 0.002904 Loss_G: 0.001642 \n",
      "[8/10][26/100][1753] Loss_D: 0.001856 Loss_G: 0.004603 \n",
      "[8/10][26/100][1754] Loss_D: 0.001983 Loss_G: 0.001172 \n",
      "[8/10][26/100][1755] Loss_D: 0.003571 Loss_G: 0.004014 \n",
      "[8/10][26/100][1756] Loss_D: 0.002567 Loss_G: 0.003363 \n",
      "[8/10][26/100][1757] Loss_D: 0.003901 Loss_G: 0.001485 \n",
      "[8/10][26/100][1758] Loss_D: 0.002859 Loss_G: 0.003171 \n",
      "[8/10][26/100][1759] Loss_D: 0.002282 Loss_G: 0.002324 \n",
      "[8/10][26/100][1760] Loss_D: 0.004346 Loss_G: 0.005003 \n",
      "[8/10][26/100][1761] Loss_D: 0.001957 Loss_G: 0.003167 \n",
      "[8/10][26/100][1762] Loss_D: 0.003364 Loss_G: 0.003047 \n",
      "[8/10][26/100][1763] Loss_D: 0.002067 Loss_G: 0.002735 \n",
      "[8/10][26/100][1764] Loss_D: 0.001901 Loss_G: 0.003191 \n",
      "[8/10][26/100][1765] Loss_D: 0.001349 Loss_G: 0.002247 \n",
      "[8/10][26/100][1766] Loss_D: 0.001561 Loss_G: 0.002153 \n",
      "[8/10][26/100][1767] Loss_D: 0.002350 Loss_G: 0.001392 \n",
      "[8/10][26/100][1768] Loss_D: 0.002150 Loss_G: 0.005071 \n",
      "[8/10][26/100][1769] Loss_D: 0.001721 Loss_G: 0.002970 \n",
      "[8/10][26/100][1770] Loss_D: 0.002024 Loss_G: 0.001216 \n",
      "[8/10][26/100][1771] Loss_D: 0.002557 Loss_G: 0.003621 \n",
      "[8/10][26/100][1772] Loss_D: 0.003015 Loss_G: 0.002871 \n",
      "[8/10][26/100][1773] Loss_D: 0.003149 Loss_G: 0.001879 \n",
      "[8/10][26/100][1774] Loss_D: 0.001988 Loss_G: 0.001829 \n",
      "[8/10][26/100][1775] Loss_D: 0.003246 Loss_G: 0.000537 \n",
      "[8/10][26/100][1776] Loss_D: 0.001871 Loss_G: 0.002150 \n",
      "[8/10][26/100][1777] Loss_D: 0.002873 Loss_G: 0.002351 \n",
      "[8/10][26/100][1778] Loss_D: -0.000433 Loss_G: 0.001658 \n",
      "[8/10][26/100][1779] Loss_D: 0.002195 Loss_G: 0.002823 \n",
      "[8/10][26/100][1780] Loss_D: 0.003137 Loss_G: 0.001735 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/10][26/100][1781] Loss_D: 0.002389 Loss_G: 0.002148 \n",
      "[8/10][26/100][1782] Loss_D: 0.002012 Loss_G: 0.004329 \n",
      "[8/10][26/100][1783] Loss_D: 0.004873 Loss_G: 0.002662 \n",
      "[8/10][26/100][1784] Loss_D: 0.001523 Loss_G: 0.001375 \n",
      "[8/10][26/100][1785] Loss_D: 0.000127 Loss_G: 0.002760 \n",
      "[8/10][26/100][1786] Loss_D: 0.002576 Loss_G: 0.003632 \n",
      "[8/10][26/100][1787] Loss_D: 0.001384 Loss_G: 0.004447 \n",
      "[8/10][26/100][1788] Loss_D: 0.002252 Loss_G: 0.002149 \n",
      "[8/10][26/100][1789] Loss_D: 0.001989 Loss_G: 0.004832 \n",
      "[8/10][26/100][1790] Loss_D: 0.002693 Loss_G: 0.003308 \n",
      "[8/10][26/100][1791] Loss_D: 0.003539 Loss_G: 0.002051 \n",
      "[8/10][26/100][1792] Loss_D: 0.003006 Loss_G: 0.003700 \n",
      "[8/10][26/100][1793] Loss_D: 0.002093 Loss_G: 0.001422 \n",
      "[8/10][26/100][1794] Loss_D: 0.003773 Loss_G: 0.002367 \n",
      "[8/10][26/100][1795] Loss_D: 0.002315 Loss_G: 0.002852 \n",
      "[8/10][26/100][1796] Loss_D: 0.003472 Loss_G: 0.003191 \n",
      "[8/10][26/100][1797] Loss_D: 0.002012 Loss_G: 0.005275 \n",
      "[8/10][26/100][1798] Loss_D: 0.002361 Loss_G: 0.000004 \n",
      "[8/10][26/100][1799] Loss_D: 0.002967 Loss_G: 0.000937 \n",
      "[8/10][26/100][1800] Loss_D: 0.002372 Loss_G: 0.003100 \n",
      "[9/10][26/100][1801] Loss_D: 0.002152 Loss_G: 0.003122 \n",
      "[9/10][26/100][1802] Loss_D: 0.005378 Loss_G: 0.004773 \n",
      "[9/10][26/100][1803] Loss_D: 0.005040 Loss_G: 0.002459 \n",
      "[9/10][26/100][1804] Loss_D: 0.002463 Loss_G: 0.005391 \n",
      "[9/10][26/100][1805] Loss_D: 0.000994 Loss_G: 0.003469 \n",
      "[9/10][26/100][1806] Loss_D: 0.004529 Loss_G: 0.003431 \n",
      "[9/10][26/100][1807] Loss_D: 0.004572 Loss_G: 0.002489 \n",
      "[9/10][26/100][1808] Loss_D: 0.002720 Loss_G: 0.006707 \n",
      "[9/10][26/100][1809] Loss_D: 0.003353 Loss_G: 0.001465 \n",
      "[9/10][26/100][1810] Loss_D: 0.002032 Loss_G: 0.002228 \n",
      "[9/10][26/100][1811] Loss_D: 0.002914 Loss_G: 0.002822 \n",
      "[9/10][26/100][1812] Loss_D: 0.004742 Loss_G: 0.004039 \n",
      "[9/10][26/100][1813] Loss_D: 0.004110 Loss_G: 0.002368 \n",
      "[9/10][26/100][1814] Loss_D: -0.000072 Loss_G: 0.004159 \n",
      "[9/10][26/100][1815] Loss_D: 0.001426 Loss_G: 0.001095 \n",
      "[9/10][26/100][1816] Loss_D: 0.002010 Loss_G: 0.004241 \n",
      "[9/10][26/100][1817] Loss_D: 0.003140 Loss_G: 0.001247 \n",
      "[9/10][26/100][1818] Loss_D: 0.005152 Loss_G: 0.002065 \n",
      "[9/10][26/100][1819] Loss_D: 0.002432 Loss_G: 0.000769 \n",
      "[9/10][26/100][1820] Loss_D: 0.001782 Loss_G: 0.002647 \n",
      "[9/10][26/100][1821] Loss_D: 0.001126 Loss_G: 0.002870 \n",
      "[9/10][26/100][1822] Loss_D: 0.002450 Loss_G: 0.002180 \n",
      "[9/10][26/100][1823] Loss_D: 0.003307 Loss_G: 0.000503 \n",
      "[9/10][26/100][1824] Loss_D: 0.003894 Loss_G: 0.002287 \n",
      "[9/10][26/100][1825] Loss_D: 0.003201 Loss_G: 0.001873 \n",
      "[9/10][26/100][1826] Loss_D: 0.004979 Loss_G: 0.000790 \n",
      "[9/10][26/100][1827] Loss_D: 0.003349 Loss_G: 0.003097 \n",
      "[9/10][26/100][1828] Loss_D: 0.002943 Loss_G: 0.003168 \n",
      "[9/10][26/100][1829] Loss_D: 0.002548 Loss_G: 0.005873 \n",
      "[9/10][26/100][1830] Loss_D: 0.001560 Loss_G: 0.003786 \n",
      "[9/10][26/100][1831] Loss_D: 0.003471 Loss_G: 0.004040 \n",
      "[9/10][26/100][1832] Loss_D: 0.002695 Loss_G: 0.002466 \n",
      "[9/10][26/100][1833] Loss_D: 0.003532 Loss_G: 0.002798 \n",
      "[9/10][26/100][1834] Loss_D: 0.003330 Loss_G: 0.002024 \n",
      "[9/10][26/100][1835] Loss_D: 0.003682 Loss_G: 0.003285 \n",
      "[9/10][26/100][1836] Loss_D: 0.002210 Loss_G: 0.005304 \n",
      "[9/10][26/100][1837] Loss_D: 0.002612 Loss_G: 0.003457 \n",
      "[9/10][26/100][1838] Loss_D: 0.003250 Loss_G: 0.004639 \n",
      "[9/10][26/100][1839] Loss_D: 0.002390 Loss_G: 0.003225 \n",
      "[9/10][26/100][1840] Loss_D: 0.000597 Loss_G: 0.002320 \n",
      "[9/10][26/100][1841] Loss_D: 0.002736 Loss_G: 0.002493 \n",
      "[9/10][26/100][1842] Loss_D: 0.003961 Loss_G: 0.006513 \n",
      "[9/10][26/100][1843] Loss_D: 0.001968 Loss_G: 0.006260 \n",
      "[9/10][26/100][1844] Loss_D: 0.002924 Loss_G: 0.003488 \n",
      "[9/10][26/100][1845] Loss_D: 0.002350 Loss_G: 0.002566 \n",
      "[9/10][26/100][1846] Loss_D: 0.002343 Loss_G: 0.002657 \n",
      "[9/10][26/100][1847] Loss_D: 0.002010 Loss_G: 0.003678 \n",
      "[9/10][26/100][1848] Loss_D: 0.003951 Loss_G: 0.002077 \n",
      "[9/10][26/100][1849] Loss_D: 0.004412 Loss_G: 0.001977 \n",
      "[9/10][26/100][1850] Loss_D: 0.004310 Loss_G: 0.001595 \n",
      "[9/10][26/100][1851] Loss_D: 0.002904 Loss_G: 0.003320 \n",
      "[9/10][26/100][1852] Loss_D: 0.002688 Loss_G: 0.002858 \n",
      "[9/10][26/100][1853] Loss_D: 0.001901 Loss_G: 0.003464 \n",
      "[9/10][26/100][1854] Loss_D: 0.000541 Loss_G: 0.003305 \n",
      "[9/10][26/100][1855] Loss_D: 0.002682 Loss_G: 0.002290 \n",
      "[9/10][26/100][1856] Loss_D: 0.003966 Loss_G: 0.004353 \n",
      "[9/10][26/100][1857] Loss_D: 0.001448 Loss_G: 0.001297 \n",
      "[9/10][26/100][1858] Loss_D: 0.004301 Loss_G: 0.002998 \n",
      "[9/10][26/100][1859] Loss_D: 0.002147 Loss_G: 0.003835 \n",
      "[9/10][26/100][1860] Loss_D: 0.002653 Loss_G: 0.003963 \n",
      "[9/10][26/100][1861] Loss_D: 0.004735 Loss_G: 0.004867 \n",
      "[9/10][26/100][1862] Loss_D: 0.002039 Loss_G: 0.004001 \n",
      "[9/10][26/100][1863] Loss_D: 0.002496 Loss_G: 0.002042 \n",
      "[9/10][26/100][1864] Loss_D: 0.003297 Loss_G: 0.002215 \n",
      "[9/10][26/100][1865] Loss_D: 0.002580 Loss_G: 0.002765 \n",
      "[9/10][26/100][1866] Loss_D: 0.004176 Loss_G: 0.003660 \n",
      "[9/10][26/100][1867] Loss_D: 0.001716 Loss_G: 0.002161 \n",
      "[9/10][26/100][1868] Loss_D: 0.002769 Loss_G: 0.001968 \n",
      "[9/10][26/100][1869] Loss_D: 0.001769 Loss_G: 0.004245 \n",
      "[9/10][26/100][1870] Loss_D: 0.003344 Loss_G: 0.003993 \n",
      "[9/10][26/100][1871] Loss_D: 0.003593 Loss_G: 0.002718 \n",
      "[9/10][26/100][1872] Loss_D: 0.002773 Loss_G: 0.000926 \n",
      "[9/10][26/100][1873] Loss_D: 0.002404 Loss_G: 0.001925 \n",
      "[9/10][26/100][1874] Loss_D: 0.003138 Loss_G: 0.004070 \n",
      "[9/10][26/100][1875] Loss_D: 0.004390 Loss_G: 0.004530 \n",
      "[9/10][26/100][1876] Loss_D: 0.002103 Loss_G: 0.001362 \n",
      "[9/10][26/100][1877] Loss_D: 0.002694 Loss_G: 0.003332 \n",
      "[9/10][26/100][1878] Loss_D: 0.001602 Loss_G: 0.003051 \n",
      "[9/10][26/100][1879] Loss_D: 0.001243 Loss_G: 0.001015 \n",
      "[9/10][26/100][1880] Loss_D: 0.003783 Loss_G: 0.002021 \n",
      "[9/10][26/100][1881] Loss_D: 0.003858 Loss_G: 0.002317 \n",
      "[9/10][26/100][1882] Loss_D: 0.001762 Loss_G: 0.002095 \n",
      "[9/10][26/100][1883] Loss_D: 0.002466 Loss_G: 0.003707 \n",
      "[9/10][26/100][1884] Loss_D: 0.002761 Loss_G: 0.004584 \n",
      "[9/10][26/100][1885] Loss_D: 0.004164 Loss_G: 0.004101 \n",
      "[9/10][26/100][1886] Loss_D: 0.003734 Loss_G: 0.001778 \n",
      "[9/10][26/100][1887] Loss_D: 0.001096 Loss_G: 0.002884 \n",
      "[9/10][26/100][1888] Loss_D: 0.002028 Loss_G: 0.002566 \n",
      "[9/10][26/100][1889] Loss_D: 0.001207 Loss_G: 0.003613 \n",
      "[9/10][26/100][1890] Loss_D: 0.002516 Loss_G: 0.002473 \n",
      "[9/10][26/100][1891] Loss_D: 0.002006 Loss_G: 0.006173 \n",
      "[9/10][26/100][1892] Loss_D: 0.002573 Loss_G: 0.003799 \n",
      "[9/10][26/100][1893] Loss_D: 0.003307 Loss_G: 0.002277 \n",
      "[9/10][26/100][1894] Loss_D: 0.001938 Loss_G: 0.003101 \n",
      "[9/10][26/100][1895] Loss_D: 0.004923 Loss_G: 0.003109 \n",
      "[9/10][26/100][1896] Loss_D: 0.000417 Loss_G: 0.001896 \n",
      "[9/10][26/100][1897] Loss_D: 0.001273 Loss_G: 0.002114 \n",
      "[9/10][26/100][1898] Loss_D: 0.004226 Loss_G: 0.002225 \n",
      "[9/10][26/100][1899] Loss_D: 0.000011 Loss_G: 0.001402 \n",
      "[9/10][26/100][1900] Loss_D: 0.001226 Loss_G: 0.004585 \n",
      "[9/10][26/100][1901] Loss_D: 0.004272 Loss_G: 0.002806 \n",
      "[9/10][26/100][1902] Loss_D: 0.004515 Loss_G: 0.001869 \n",
      "[9/10][26/100][1903] Loss_D: 0.003339 Loss_G: 0.004231 \n",
      "[9/10][26/100][1904] Loss_D: 0.004134 Loss_G: 0.002327 \n",
      "[9/10][26/100][1905] Loss_D: 0.002594 Loss_G: 0.001642 \n",
      "[9/10][26/100][1906] Loss_D: 0.000851 Loss_G: 0.003373 \n",
      "[9/10][26/100][1907] Loss_D: 0.002428 Loss_G: 0.001766 \n",
      "[9/10][26/100][1908] Loss_D: 0.002776 Loss_G: 0.005044 \n",
      "[9/10][26/100][1909] Loss_D: 0.002763 Loss_G: 0.001990 \n",
      "[9/10][26/100][1910] Loss_D: 0.001237 Loss_G: 0.001835 \n",
      "[9/10][26/100][1911] Loss_D: 0.003310 Loss_G: 0.002665 \n",
      "[9/10][26/100][1912] Loss_D: 0.003804 Loss_G: 0.002492 \n",
      "[9/10][26/100][1913] Loss_D: 0.003521 Loss_G: 0.009949 \n",
      "[9/10][26/100][1914] Loss_D: 0.002632 Loss_G: 0.003865 \n",
      "[9/10][26/100][1915] Loss_D: 0.003151 Loss_G: 0.003132 \n",
      "[9/10][26/100][1916] Loss_D: 0.004405 Loss_G: 0.004626 \n",
      "[9/10][26/100][1917] Loss_D: 0.000515 Loss_G: 0.004075 \n",
      "[9/10][26/100][1918] Loss_D: 0.002341 Loss_G: 0.001842 \n",
      "[9/10][26/100][1919] Loss_D: 0.004108 Loss_G: 0.002844 \n",
      "[9/10][26/100][1920] Loss_D: 0.003102 Loss_G: 0.001294 \n",
      "[9/10][26/100][1921] Loss_D: 0.001428 Loss_G: 0.002262 \n",
      "[9/10][26/100][1922] Loss_D: 0.003274 Loss_G: 0.004857 \n",
      "[9/10][26/100][1923] Loss_D: 0.002737 Loss_G: 0.002398 \n",
      "[9/10][26/100][1924] Loss_D: 0.001647 Loss_G: 0.002881 \n",
      "[9/10][26/100][1925] Loss_D: 0.002999 Loss_G: 0.004120 \n",
      "[9/10][26/100][1926] Loss_D: 0.002737 Loss_G: 0.002032 \n",
      "[9/10][26/100][1927] Loss_D: 0.002221 Loss_G: 0.001571 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/10][26/100][1928] Loss_D: 0.004302 Loss_G: 0.001951 \n",
      "[9/10][26/100][1929] Loss_D: 0.001869 Loss_G: 0.003298 \n",
      "[9/10][26/100][1930] Loss_D: 0.000162 Loss_G: 0.003871 \n",
      "[9/10][26/100][1931] Loss_D: 0.002967 Loss_G: 0.002514 \n",
      "[9/10][26/100][1932] Loss_D: 0.004325 Loss_G: 0.003446 \n",
      "[9/10][26/100][1933] Loss_D: 0.003358 Loss_G: 0.005165 \n",
      "[9/10][26/100][1934] Loss_D: 0.000997 Loss_G: 0.003416 \n",
      "[9/10][26/100][1935] Loss_D: 0.002714 Loss_G: 0.000408 \n",
      "[9/10][26/100][1936] Loss_D: 0.003771 Loss_G: 0.002444 \n",
      "[9/10][26/100][1937] Loss_D: 0.001834 Loss_G: 0.003084 \n",
      "[9/10][26/100][1938] Loss_D: 0.002493 Loss_G: 0.008069 \n",
      "[9/10][26/100][1939] Loss_D: 0.000990 Loss_G: 0.001743 \n",
      "[9/10][26/100][1940] Loss_D: 0.003918 Loss_G: 0.002804 \n",
      "[9/10][26/100][1941] Loss_D: 0.003465 Loss_G: 0.004021 \n",
      "[9/10][26/100][1942] Loss_D: 0.002551 Loss_G: 0.001750 \n",
      "[9/10][26/100][1943] Loss_D: 0.002792 Loss_G: 0.001495 \n",
      "[9/10][26/100][1944] Loss_D: 0.002222 Loss_G: 0.003684 \n",
      "[9/10][26/100][1945] Loss_D: 0.002112 Loss_G: 0.001990 \n",
      "[9/10][26/100][1946] Loss_D: 0.001755 Loss_G: 0.001420 \n",
      "[9/10][26/100][1947] Loss_D: 0.001497 Loss_G: 0.003521 \n",
      "[9/10][26/100][1948] Loss_D: 0.002522 Loss_G: 0.004497 \n",
      "[9/10][26/100][1949] Loss_D: 0.001969 Loss_G: 0.002634 \n",
      "[9/10][26/100][1950] Loss_D: 0.002217 Loss_G: 0.002747 \n",
      "[9/10][26/100][1951] Loss_D: 0.004962 Loss_G: 0.002682 \n",
      "[9/10][26/100][1952] Loss_D: 0.003747 Loss_G: 0.003750 \n",
      "[9/10][26/100][1953] Loss_D: 0.002429 Loss_G: 0.000232 \n",
      "[9/10][26/100][1954] Loss_D: 0.003970 Loss_G: 0.002486 \n",
      "[9/10][26/100][1955] Loss_D: 0.002702 Loss_G: 0.002323 \n",
      "[9/10][26/100][1956] Loss_D: 0.002771 Loss_G: 0.003010 \n",
      "[9/10][26/100][1957] Loss_D: 0.001559 Loss_G: 0.003505 \n",
      "[9/10][26/100][1958] Loss_D: 0.002043 Loss_G: 0.001915 \n",
      "[9/10][26/100][1959] Loss_D: 0.003553 Loss_G: 0.003036 \n",
      "[9/10][26/100][1960] Loss_D: 0.001308 Loss_G: 0.001345 \n",
      "[9/10][26/100][1961] Loss_D: 0.003378 Loss_G: 0.001902 \n",
      "[9/10][26/100][1962] Loss_D: 0.003743 Loss_G: 0.004569 \n",
      "[9/10][26/100][1963] Loss_D: 0.002242 Loss_G: 0.001484 \n",
      "[9/10][26/100][1964] Loss_D: 0.002241 Loss_G: 0.000560 \n",
      "[9/10][26/100][1965] Loss_D: 0.001010 Loss_G: 0.002896 \n",
      "[9/10][26/100][1966] Loss_D: 0.002845 Loss_G: 0.001396 \n",
      "[9/10][26/100][1967] Loss_D: 0.002910 Loss_G: 0.004233 \n",
      "[9/10][26/100][1968] Loss_D: 0.005154 Loss_G: 0.002970 \n",
      "[9/10][26/100][1969] Loss_D: 0.004248 Loss_G: 0.001849 \n",
      "[9/10][26/100][1970] Loss_D: 0.003789 Loss_G: 0.001577 \n",
      "[9/10][26/100][1971] Loss_D: 0.004062 Loss_G: 0.001435 \n",
      "[9/10][26/100][1972] Loss_D: 0.002508 Loss_G: 0.006358 \n",
      "[9/10][26/100][1973] Loss_D: 0.004319 Loss_G: 0.002487 \n",
      "[9/10][26/100][1974] Loss_D: 0.003340 Loss_G: 0.002797 \n",
      "[9/10][26/100][1975] Loss_D: 0.004819 Loss_G: 0.003164 \n",
      "[9/10][26/100][1976] Loss_D: 0.002024 Loss_G: 0.002966 \n",
      "[9/10][26/100][1977] Loss_D: 0.002849 Loss_G: 0.001959 \n",
      "[9/10][26/100][1978] Loss_D: 0.001846 Loss_G: 0.006404 \n",
      "[9/10][26/100][1979] Loss_D: 0.003807 Loss_G: 0.002207 \n",
      "[9/10][26/100][1980] Loss_D: 0.003785 Loss_G: 0.002793 \n",
      "[9/10][26/100][1981] Loss_D: 0.003554 Loss_G: 0.005381 \n",
      "[9/10][26/100][1982] Loss_D: 0.003438 Loss_G: 0.004209 \n",
      "[9/10][26/100][1983] Loss_D: 0.004951 Loss_G: 0.003049 \n",
      "[9/10][26/100][1984] Loss_D: 0.002992 Loss_G: 0.001958 \n",
      "[9/10][26/100][1985] Loss_D: 0.003895 Loss_G: 0.001141 \n",
      "[9/10][26/100][1986] Loss_D: 0.001768 Loss_G: 0.005517 \n",
      "[9/10][26/100][1987] Loss_D: 0.001231 Loss_G: 0.002804 \n",
      "[9/10][26/100][1988] Loss_D: 0.004268 Loss_G: 0.002827 \n",
      "[9/10][26/100][1989] Loss_D: 0.004625 Loss_G: 0.001526 \n",
      "[9/10][26/100][1990] Loss_D: 0.002849 Loss_G: 0.002493 \n",
      "[9/10][26/100][1991] Loss_D: 0.002543 Loss_G: 0.004542 \n",
      "[9/10][26/100][1992] Loss_D: 0.003637 Loss_G: 0.003047 \n",
      "[9/10][26/100][1993] Loss_D: 0.002758 Loss_G: 0.002548 \n",
      "[9/10][26/100][1994] Loss_D: 0.002654 Loss_G: 0.001533 \n",
      "[9/10][26/100][1995] Loss_D: 0.004534 Loss_G: 0.003235 \n",
      "[9/10][26/100][1996] Loss_D: 0.004457 Loss_G: 0.002810 \n",
      "[9/10][26/100][1997] Loss_D: 0.003904 Loss_G: 0.002373 \n",
      "[9/10][26/100][1998] Loss_D: 0.004310 Loss_G: 0.001950 \n",
      "[9/10][26/100][1999] Loss_D: 0.001120 Loss_G: 0.002134 \n",
      "[9/10][26/100][2000] Loss_D: 0.001712 Loss_G: 0.002819 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.015087549574673176,\n",
       "  0.011499668471515179,\n",
       "  0.009362959302961826,\n",
       "  0.0026781747583299875,\n",
       "  0.0009550042450428009,\n",
       "  0.0028326816391199827,\n",
       "  0.00029393803561106324,\n",
       "  -0.001052424544468522,\n",
       "  0.003058132715523243,\n",
       "  0.001420168555341661,\n",
       "  0.0013879818143323064,\n",
       "  0.0005235265707597136,\n",
       "  0.0021142871119081974,\n",
       "  0.0007014574948698282,\n",
       "  0.0020857159979641438,\n",
       "  0.004531828220933676,\n",
       "  0.0025970120914280415,\n",
       "  0.004024721682071686,\n",
       "  0.0029805772937834263,\n",
       "  0.0035233059898018837,\n",
       "  0.001278858631849289,\n",
       "  0.003827510867267847,\n",
       "  0.0020897332578897476,\n",
       "  0.0015348732704296708,\n",
       "  0.0006372160860337317,\n",
       "  0.0042740413919091225,\n",
       "  0.0025846052449196577,\n",
       "  0.00130808528047055,\n",
       "  0.00429991539567709,\n",
       "  0.002744811587035656,\n",
       "  0.0030942216981202364,\n",
       "  0.0009747726144269109,\n",
       "  0.0029646819457411766,\n",
       "  0.0028301337733864784,\n",
       "  0.0014303538482636213,\n",
       "  0.003806662280112505,\n",
       "  0.0011024657869711518,\n",
       "  0.0006811631610617042,\n",
       "  0.0015012789517641068,\n",
       "  0.004456625785678625,\n",
       "  0.002907722955569625,\n",
       "  0.00214564916677773,\n",
       "  0.003309686668217182,\n",
       "  0.004098483361303806,\n",
       "  0.0024026280734688044,\n",
       "  0.002238468499854207,\n",
       "  0.0019854195415973663,\n",
       "  0.0021725718397647142,\n",
       "  0.0053499117493629456,\n",
       "  0.0033116089180111885,\n",
       "  0.002683257218450308,\n",
       "  0.002343984553590417,\n",
       "  -0.0001086168922483921,\n",
       "  0.002021811669692397,\n",
       "  0.0029198029078543186,\n",
       "  0.0021631435956805944,\n",
       "  0.003381245071068406,\n",
       "  0.002217477187514305,\n",
       "  0.0017742663621902466,\n",
       "  0.003281224984675646,\n",
       "  0.0029783067293465137,\n",
       "  0.00444334652274847,\n",
       "  0.0040382095612585545,\n",
       "  0.0035348155070096254,\n",
       "  0.0031749398913234472,\n",
       "  0.00499408645555377,\n",
       "  0.0034529692493379116,\n",
       "  0.002041585510596633,\n",
       "  0.0025418256409466267,\n",
       "  0.0015480882721021771,\n",
       "  0.002164134755730629,\n",
       "  0.0029603561852127314,\n",
       "  0.0010527822887524962,\n",
       "  0.0013993369648233056,\n",
       "  0.0023456497583538294,\n",
       "  0.0020189527422189713,\n",
       "  0.003024318255484104,\n",
       "  0.003375554922968149,\n",
       "  0.001753364340402186,\n",
       "  0.0029618979897350073,\n",
       "  0.0025304409209638834,\n",
       "  0.002977958181872964,\n",
       "  0.0013410933315753937,\n",
       "  0.0014906671131029725,\n",
       "  0.0018150032265111804,\n",
       "  0.0032283621840178967,\n",
       "  0.0026200730353593826,\n",
       "  0.003152105025947094,\n",
       "  0.0032129341270774603,\n",
       "  0.004817925859242678,\n",
       "  0.0016137341735884547,\n",
       "  0.0022907694801688194,\n",
       "  0.0036065238527953625,\n",
       "  0.004285107832401991,\n",
       "  0.0025387885980308056,\n",
       "  0.0014598974958062172,\n",
       "  0.0028294117655605078,\n",
       "  0.0026646729093044996,\n",
       "  0.003604175988584757,\n",
       "  -0.00014190208457875997,\n",
       "  0.002757506910711527,\n",
       "  0.002849681070074439,\n",
       "  0.0031368432100862265,\n",
       "  0.0023000380024313927,\n",
       "  0.0066371518187224865,\n",
       "  0.0018498302670195699,\n",
       "  0.002180620562285185,\n",
       "  0.004213268868625164,\n",
       "  0.0029690624214708805,\n",
       "  0.0016407602233812213,\n",
       "  0.0031393649987876415,\n",
       "  0.0013670797925442457,\n",
       "  0.002668540459126234,\n",
       "  0.004908152390271425,\n",
       "  0.00255937990732491,\n",
       "  0.00028734648367390037,\n",
       "  0.0015947511419653893,\n",
       "  0.003035173984244466,\n",
       "  0.004564377944916487,\n",
       "  0.0029863733798265457,\n",
       "  0.002174094319343567,\n",
       "  0.0017527621239423752,\n",
       "  0.002814968815073371,\n",
       "  0.003537977347150445,\n",
       "  0.003742342349141836,\n",
       "  0.0007958366186358035,\n",
       "  0.002107326639816165,\n",
       "  0.001333865337073803,\n",
       "  0.006581267807632685,\n",
       "  0.0032313340343534946,\n",
       "  0.0019876165315508842,\n",
       "  0.005411230027675629,\n",
       "  0.0021451562643051147,\n",
       "  0.006265247706323862,\n",
       "  0.0020404960960149765,\n",
       "  0.00255391513928771,\n",
       "  0.001998408930376172,\n",
       "  0.003181617008522153,\n",
       "  0.0021862389985471964,\n",
       "  0.0022986752446740866,\n",
       "  0.001985472161322832,\n",
       "  0.002353087067604065,\n",
       "  0.003280945122241974,\n",
       "  0.002759253839030862,\n",
       "  0.0008111154893413186,\n",
       "  0.0025430480018258095,\n",
       "  0.0019306824542582035,\n",
       "  0.003698840970173478,\n",
       "  0.0013688699109479785,\n",
       "  0.001753412070684135,\n",
       "  -0.00019253205391578376,\n",
       "  0.003534272313117981,\n",
       "  0.002714032307267189,\n",
       "  0.002032735152170062,\n",
       "  0.002314362907782197,\n",
       "  0.0015531624667346478,\n",
       "  0.004363758489489555,\n",
       "  0.004222723189741373,\n",
       "  0.002869043732061982,\n",
       "  0.0012475828407332301,\n",
       "  0.0016648933524265885,\n",
       "  0.002910084556788206,\n",
       "  0.0033277010079473257,\n",
       "  0.003339371643960476,\n",
       "  0.002556409453973174,\n",
       "  0.00018419981643091887,\n",
       "  0.003360697068274021,\n",
       "  0.0018580828327685595,\n",
       "  0.003460725536569953,\n",
       "  -0.00017819626373238862,\n",
       "  0.002211917657405138,\n",
       "  0.0018549344968050718,\n",
       "  0.0012037043925374746,\n",
       "  0.0011748661054298282,\n",
       "  0.004785573575645685,\n",
       "  0.004026871174573898,\n",
       "  0.0045989276841282845,\n",
       "  0.003814299590885639,\n",
       "  0.001553690293803811,\n",
       "  0.0019387378124520183,\n",
       "  0.0033603329211473465,\n",
       "  0.00417158380150795,\n",
       "  0.0025549898855388165,\n",
       "  0.0012226203689351678,\n",
       "  0.004316728562116623,\n",
       "  0.0031921849586069584,\n",
       "  0.003982025198638439,\n",
       "  0.002634995384141803,\n",
       "  0.003722772467881441,\n",
       "  0.00418180413544178,\n",
       "  0.002308879280462861,\n",
       "  -0.0016896628076210618,\n",
       "  0.0032790498808026314,\n",
       "  0.0029297207947820425,\n",
       "  0.003827671054750681,\n",
       "  0.0028770798817276955,\n",
       "  0.003488740185275674,\n",
       "  0.0025372602976858616,\n",
       "  0.0024298785720020533,\n",
       "  0.0024135250132530928,\n",
       "  0.005103117786347866,\n",
       "  0.004087310284376144,\n",
       "  0.004651092924177647,\n",
       "  0.001191046554595232,\n",
       "  0.003108165692538023,\n",
       "  0.0021777921356260777,\n",
       "  0.0033985457848757505,\n",
       "  0.00441697658970952,\n",
       "  0.004659183789044619,\n",
       "  0.0025494126603007317,\n",
       "  0.0014096364611759782,\n",
       "  0.0030799892265349627,\n",
       "  0.0036251286510378122,\n",
       "  0.004024562891572714,\n",
       "  0.0035025370307266712,\n",
       "  0.0016926280222833157,\n",
       "  0.005695510655641556,\n",
       "  0.0036064614541828632,\n",
       "  0.0035599633120000362,\n",
       "  0.0022706601303070784,\n",
       "  0.0037024253979325294,\n",
       "  0.0015706995036453009,\n",
       "  0.002781063551083207,\n",
       "  0.003614481072872877,\n",
       "  0.0030350720044225454,\n",
       "  0.001211689435876906,\n",
       "  0.003163788001984358,\n",
       "  0.0005650041857734323,\n",
       "  0.002677887910977006,\n",
       "  0.0021959329023957253,\n",
       "  0.003086672630161047,\n",
       "  0.002435175469145179,\n",
       "  0.001968566793948412,\n",
       "  0.0025424708146601915,\n",
       "  0.002135597402229905,\n",
       "  0.004028365481644869,\n",
       "  0.002213012194260955,\n",
       "  0.001864416990429163,\n",
       "  0.002331966068595648,\n",
       "  0.002579093910753727,\n",
       "  0.0023663840256631374,\n",
       "  0.004411312751471996,\n",
       "  0.0030928710475564003,\n",
       "  0.001727789407595992,\n",
       "  0.005401959177106619,\n",
       "  0.0031604485120624304,\n",
       "  0.0034830872900784016,\n",
       "  0.000853120582178235,\n",
       "  0.002587946131825447,\n",
       "  0.0010677716927602887,\n",
       "  0.001977305393666029,\n",
       "  0.00242609647102654,\n",
       "  0.001735358964651823,\n",
       "  0.0006901367451064289,\n",
       "  -4.105106563656591e-05,\n",
       "  0.0038499655202031136,\n",
       "  0.0031612201128154993,\n",
       "  0.004112428519874811,\n",
       "  0.004195270594209433,\n",
       "  0.0022191498428583145,\n",
       "  0.0023870032746344805,\n",
       "  0.002106631873175502,\n",
       "  0.003136133775115013,\n",
       "  0.002395557938143611,\n",
       "  0.0028252622578293085,\n",
       "  0.0047331941314041615,\n",
       "  0.0034014591947197914,\n",
       "  0.0037778348196297884,\n",
       "  0.00123873061966151,\n",
       "  0.0029619920533150434,\n",
       "  0.0025527586694806814,\n",
       "  0.0033304563257843256,\n",
       "  0.0038756374269723892,\n",
       "  0.004005286376923323,\n",
       "  0.003712942125275731,\n",
       "  0.0014427556889131665,\n",
       "  0.0005035077920183539,\n",
       "  0.004307363647967577,\n",
       "  0.002160232048481703,\n",
       "  0.0032829968258738518,\n",
       "  0.0020726954098790884,\n",
       "  0.0033752075396478176,\n",
       "  0.003075656248256564,\n",
       "  0.002213884610682726,\n",
       "  0.002799436915665865,\n",
       "  0.003711507422849536,\n",
       "  0.003506881184875965,\n",
       "  0.00309599912725389,\n",
       "  0.0009944478515535593,\n",
       "  0.005090039223432541,\n",
       "  0.0022293052170425653,\n",
       "  0.0023574738297611475,\n",
       "  0.00029622571310028434,\n",
       "  0.002855561673641205,\n",
       "  0.003175947582349181,\n",
       "  0.0029999096877872944,\n",
       "  0.0032901735976338387,\n",
       "  0.0033540234435349703,\n",
       "  0.0023656697012484074,\n",
       "  0.002677211305126548,\n",
       "  0.001547988853417337,\n",
       "  0.0029988603200763464,\n",
       "  0.002452016342431307,\n",
       "  0.0026498050428926945,\n",
       "  0.00254351319745183,\n",
       "  0.0027643260546028614,\n",
       "  0.0012007764307782054,\n",
       "  0.0027992650866508484,\n",
       "  0.0029167644679546356,\n",
       "  0.004289892036467791,\n",
       "  0.0021514040417969227,\n",
       "  0.0018016411922872066,\n",
       "  0.002714804606512189,\n",
       "  0.0021457874681800604,\n",
       "  0.004301473498344421,\n",
       "  0.0032583854626864195,\n",
       "  0.004851860459893942,\n",
       "  0.0023600971326231956,\n",
       "  0.002980924444273114,\n",
       "  0.0022176525089889765,\n",
       "  0.004336581565439701,\n",
       "  0.0026970384642481804,\n",
       "  0.0039043030701577663,\n",
       "  0.002622695406898856,\n",
       "  0.0011865395354107022,\n",
       "  0.006384368985891342,\n",
       "  0.003205991117283702,\n",
       "  0.003223506035283208,\n",
       "  0.0015461434377357364,\n",
       "  0.0027394008357077837,\n",
       "  0.002607592847198248,\n",
       "  0.0024865956511348486,\n",
       "  0.0024884319864213467,\n",
       "  0.002753386041149497,\n",
       "  0.003615575609728694,\n",
       "  0.0025825926568359137,\n",
       "  0.0027693805750459433,\n",
       "  0.0015483669703826308,\n",
       "  0.002345199231058359,\n",
       "  0.002100020181387663,\n",
       "  0.006300847977399826,\n",
       "  0.002217647386714816,\n",
       "  0.002456429647281766,\n",
       "  0.0029034148901700974,\n",
       "  0.0036968474742025137,\n",
       "  0.003264061640948057,\n",
       "  0.0035815073642879725,\n",
       "  0.0027218852192163467,\n",
       "  0.0028274792712181807,\n",
       "  0.0026238674763590097,\n",
       "  0.0027492085937410593,\n",
       "  0.0027518209535628557,\n",
       "  0.002070763846859336,\n",
       "  0.0031684187706559896,\n",
       "  0.0026081367395818233,\n",
       "  0.0014339991612359881,\n",
       "  0.0030040214769542217,\n",
       "  0.0016013362910598516,\n",
       "  0.005524602252990007,\n",
       "  0.00232344469986856,\n",
       "  0.0011336080497130752,\n",
       "  0.002064711879938841,\n",
       "  0.0026598088443279266,\n",
       "  0.0027692029252648354,\n",
       "  0.002295618411153555,\n",
       "  0.0020117624662816525,\n",
       "  0.0019451058469712734,\n",
       "  0.0037888456135988235,\n",
       "  0.0014588150661438704,\n",
       "  0.005574121605604887,\n",
       "  0.00348681746982038,\n",
       "  -0.00022266236192081124,\n",
       "  0.0004886457463726401,\n",
       "  0.001801922800950706,\n",
       "  0.002870960859581828,\n",
       "  0.0030208833049982786,\n",
       "  0.0016571837477385998,\n",
       "  0.0023909860756248236,\n",
       "  -0.0004129853332415223,\n",
       "  0.0040468513034284115,\n",
       "  0.004209952894598246,\n",
       "  0.005766009446233511,\n",
       "  0.0040911422111094,\n",
       "  0.002407689346000552,\n",
       "  0.00102338008582592,\n",
       "  0.002916316967457533,\n",
       "  0.0027282964438199997,\n",
       "  0.0023494248744100332,\n",
       "  0.0012066841591149569,\n",
       "  0.002569518517702818,\n",
       "  0.0030213785357773304,\n",
       "  0.003336806083098054,\n",
       "  0.0007995663909241557,\n",
       "  0.0025764228776097298,\n",
       "  0.003663051873445511,\n",
       "  0.0022826024796813726,\n",
       "  0.00263927411288023,\n",
       "  0.003107756609097123,\n",
       "  0.004064721055328846,\n",
       "  0.00022278018877841532,\n",
       "  0.0015731825260445476,\n",
       "  0.0031609481666237116,\n",
       "  0.0032372530549764633,\n",
       "  0.0016465825028717518,\n",
       "  0.002080660779029131,\n",
       "  0.0017655264819040895,\n",
       "  0.0013549268478527665,\n",
       "  0.0019004706991836429,\n",
       "  0.0036482131108641624,\n",
       "  0.0015614776639267802,\n",
       "  0.0046247877180576324,\n",
       "  0.003249693661928177,\n",
       "  0.0029677473939955235,\n",
       "  0.0016501358477398753,\n",
       "  0.004661884158849716,\n",
       "  0.003313482739031315,\n",
       "  0.004512254148721695,\n",
       "  0.003529792185872793,\n",
       "  0.0026583995204418898,\n",
       "  0.003326395759359002,\n",
       "  0.0029108028393238783,\n",
       "  0.004049367271363735,\n",
       "  0.00340479239821434,\n",
       "  0.0013096873881295323,\n",
       "  0.0032367908861488104,\n",
       "  0.0027995954733341932,\n",
       "  0.0033920081332325935,\n",
       "  0.003760137129575014,\n",
       "  0.0034609700087457895,\n",
       "  0.0013222453417256474,\n",
       "  0.002434351248666644,\n",
       "  0.0025287410244345665,\n",
       "  0.0020879688672721386,\n",
       "  0.0015930489171296358,\n",
       "  0.0026098533999174833,\n",
       "  0.0017014285549521446,\n",
       "  0.0028913922142237425,\n",
       "  0.0024168079253286123,\n",
       "  0.0023961823899298906,\n",
       "  0.00365122826769948,\n",
       "  0.005342882126569748,\n",
       "  0.0032389138359576464,\n",
       "  0.004421878140419722,\n",
       "  0.0023234644904732704,\n",
       "  0.0014805622631683946,\n",
       "  0.003825797699391842,\n",
       "  0.0013094305759295821,\n",
       "  0.0010941949440166354,\n",
       "  0.003399122040718794,\n",
       "  0.003678125562146306,\n",
       "  0.0027637388557195663,\n",
       "  0.0012918425491079688,\n",
       "  0.0020034725312143564,\n",
       "  0.00293306028470397,\n",
       "  0.003009430132806301,\n",
       "  0.0024303756654262543,\n",
       "  0.003706173272803426,\n",
       "  0.0036993115209043026,\n",
       "  0.003023643745109439,\n",
       "  0.0031784845050424337,\n",
       "  0.0043113213032484055,\n",
       "  -0.00017223175382241607,\n",
       "  0.0017050276510417461,\n",
       "  0.0030999991577118635,\n",
       "  0.0019295573001727462,\n",
       "  0.003422762732952833,\n",
       "  0.003519853577017784,\n",
       "  0.0027945823967456818,\n",
       "  0.0015260197687894106,\n",
       "  0.0010285035241395235,\n",
       "  0.0025899822358042,\n",
       "  0.00017020318773575127,\n",
       "  0.002088550478219986,\n",
       "  0.001544736442156136,\n",
       "  0.003233203198760748,\n",
       "  0.0012867787154391408,\n",
       "  0.003784331725910306,\n",
       "  0.0024602198973298073,\n",
       "  0.0036985320039093494,\n",
       "  0.001021124771796167,\n",
       "  0.0039003556594252586,\n",
       "  0.002532679121941328,\n",
       "  0.003164976602420211,\n",
       "  0.0011233346303924918,\n",
       "  0.0012444336898624897,\n",
       "  0.0004377636360004544,\n",
       "  0.0025757013354450464,\n",
       "  0.0021425390150398016,\n",
       "  0.0029508681036531925,\n",
       "  0.004268473945558071,\n",
       "  0.003049552673473954,\n",
       "  0.0015961864264681935,\n",
       "  0.002682448597624898,\n",
       "  0.0039368378929793835,\n",
       "  0.0028971638530492783,\n",
       "  0.002215632703155279,\n",
       "  0.001356462249532342,\n",
       "  0.0011465519201010466,\n",
       "  0.003671968588605523,\n",
       "  0.0017964188009500504,\n",
       "  0.003425787203013897,\n",
       "  0.004817718639969826,\n",
       "  0.003206090070307255,\n",
       "  0.002503973664715886,\n",
       "  0.0019727563485503197,\n",
       "  0.00160535320173949,\n",
       "  0.0029219710268080235,\n",
       "  0.00306618376635015,\n",
       "  0.004161106422543526,\n",
       "  0.003174719400703907,\n",
       "  0.0015889264177531004,\n",
       "  0.0023995640221983194,\n",
       "  0.003703559748828411,\n",
       "  0.004822740331292152,\n",
       "  0.0040618544444441795,\n",
       "  0.0030380256939679384,\n",
       "  0.003965965937823057,\n",
       "  0.003449501935392618,\n",
       "  -0.00037913871346972883,\n",
       "  0.003577187890186906,\n",
       "  0.0023759035393595695,\n",
       "  0.00393893476575613,\n",
       "  0.0033282036893069744,\n",
       "  0.0030047029722481966,\n",
       "  0.004642166662961245,\n",
       "  0.0022145716939121485,\n",
       "  0.0038343477062880993,\n",
       "  0.0021174161229282618,\n",
       "  0.002548999385908246,\n",
       "  0.0015863805310800672,\n",
       "  0.000980003853328526,\n",
       "  0.001604169374331832,\n",
       "  0.00483445730060339,\n",
       "  0.0010256815003231168,\n",
       "  0.002063196152448654,\n",
       "  0.0027530156075954437,\n",
       "  0.0018987999064847827,\n",
       "  0.0026384873781353235,\n",
       "  0.002076543867588043,\n",
       "  0.0024691722355782986,\n",
       "  0.0023531701881438494,\n",
       "  0.0034658547956496477,\n",
       "  0.00011385257676010951,\n",
       "  0.004675762262195349,\n",
       "  0.0034115163143724203,\n",
       "  0.0034841771703213453,\n",
       "  0.002938821678981185,\n",
       "  0.003705982118844986,\n",
       "  0.0013906406238675117,\n",
       "  0.005096557550132275,\n",
       "  0.0011871365131810308,\n",
       "  0.0032853989396244287,\n",
       "  0.0020385300740599632,\n",
       "  0.001535294926725328,\n",
       "  0.0026097584050148726,\n",
       "  0.0024767969734966755,\n",
       "  0.004495428409427404,\n",
       "  0.0024097224231809378,\n",
       "  0.00338277081027627,\n",
       "  0.0020704076159745455,\n",
       "  0.0012619318440556526,\n",
       "  0.0029118049424141645,\n",
       "  0.0020876845810562372,\n",
       "  0.003154678735882044,\n",
       "  0.003334642853587866,\n",
       "  0.002137736650183797,\n",
       "  0.005111921578645706,\n",
       "  0.0025709380861371756,\n",
       "  0.003589688567444682,\n",
       "  0.004572657402604818,\n",
       "  0.0010510716820135713,\n",
       "  0.0018246291438117623,\n",
       "  0.004639629740267992,\n",
       "  0.0011192525271326303,\n",
       "  0.0032066632993519306,\n",
       "  0.0030085782054811716,\n",
       "  0.001708477851934731,\n",
       "  0.0034289699979126453,\n",
       "  0.0024509159848093987,\n",
       "  0.004092238377779722,\n",
       "  0.001045741606503725,\n",
       "  0.00417399313300848,\n",
       "  0.003131738631054759,\n",
       "  0.007324059493839741,\n",
       "  0.0008097197278402746,\n",
       "  0.002928223693743348,\n",
       "  0.0021486433688551188,\n",
       "  0.002450559288263321,\n",
       "  0.0029969827737659216,\n",
       "  0.0030971411615610123,\n",
       "  0.0025962027721107006,\n",
       "  0.0012606531381607056,\n",
       "  0.0036369299050420523,\n",
       "  0.0042488123290240765,\n",
       "  0.0033070542849600315,\n",
       "  0.0021530522499233484,\n",
       "  0.0027709954883903265,\n",
       "  0.002233496867120266,\n",
       "  0.0037637215573340654,\n",
       "  0.0021950085647404194,\n",
       "  0.0030720795039087534,\n",
       "  0.0016354245599359274,\n",
       "  0.0011501586996018887,\n",
       "  0.0017641923623159528,\n",
       "  0.0025956472381949425,\n",
       "  0.0039687324315309525,\n",
       "  0.004697276279330254,\n",
       "  0.0014493180206045508,\n",
       "  0.0017993546789512038,\n",
       "  0.0020356234163045883,\n",
       "  0.0019674019422382116,\n",
       "  0.0016808981308713555,\n",
       "  0.0025109383277595043,\n",
       "  0.002991768764331937,\n",
       "  0.0020474547054618597,\n",
       "  0.0025254543870687485,\n",
       "  0.00169836834538728,\n",
       "  0.003766207955777645,\n",
       "  0.004983121529221535,\n",
       "  0.0022044717334210873,\n",
       "  0.0019513951847329736,\n",
       "  0.001547694206237793,\n",
       "  0.0018310819286853075,\n",
       "  0.0016368009382858872,\n",
       "  0.0012003164738416672,\n",
       "  0.004863442853093147,\n",
       "  0.0032658393029123545,\n",
       "  0.004113044589757919,\n",
       "  0.0038626925088465214,\n",
       "  0.0033772638998925686,\n",
       "  0.0017596118850633502,\n",
       "  0.00212296680547297,\n",
       "  0.0030131435487419367,\n",
       "  0.002759607508778572,\n",
       "  0.00034659041557461023,\n",
       "  0.0040358551777899265,\n",
       "  0.0018426176393404603,\n",
       "  0.0018038530834019184,\n",
       "  0.001964295981451869,\n",
       "  0.005316563416272402,\n",
       "  0.0024126472417265177,\n",
       "  0.002855957020074129,\n",
       "  0.0023460134398192167,\n",
       "  0.0019573590252548456,\n",
       "  0.0031284152064472437,\n",
       "  0.003904640441760421,\n",
       "  0.0018814848735928535,\n",
       "  0.0021953487303107977,\n",
       "  0.0024169031530618668,\n",
       "  0.00425424799323082,\n",
       "  0.0018771624891087413,\n",
       "  0.001392106874845922,\n",
       "  0.0036799428053200245,\n",
       "  0.0018002194119617343,\n",
       "  0.004357792437076569,\n",
       "  0.0042924536392092705,\n",
       "  0.0013033094583079219,\n",
       "  0.0015632770955562592,\n",
       "  0.0010178255615755916,\n",
       "  0.003080531256273389,\n",
       "  0.003798715304583311,\n",
       "  0.003153217723593116,\n",
       "  0.0023579823318868876,\n",
       "  0.0017213381361216307,\n",
       "  0.003107104217633605,\n",
       "  0.001688246033154428,\n",
       "  0.004059396218508482,\n",
       "  0.002992010908201337,\n",
       "  0.0015733689069747925,\n",
       "  0.0015128697268664837,\n",
       "  0.0022743227891623974,\n",
       "  0.0030948163475841284,\n",
       "  0.0051201446913182735,\n",
       "  0.0042959535494446754,\n",
       "  0.003594496287405491,\n",
       "  0.0034141112118959427,\n",
       "  0.0005984745803289115,\n",
       "  0.0027367889415472746,\n",
       "  0.003713684855028987,\n",
       "  0.002031061565503478,\n",
       "  0.004133382812142372,\n",
       "  0.0018598369788378477,\n",
       "  0.005095348227769136,\n",
       "  0.001442565699107945,\n",
       "  0.0019668187014758587,\n",
       "  0.001777219818904996,\n",
       "  0.00323026767000556,\n",
       "  0.0018336367793381214,\n",
       "  0.003894676687195897,\n",
       "  0.004258287604898214,\n",
       "  0.002543911337852478,\n",
       "  0.0021107513457536697,\n",
       "  0.0056733740493655205,\n",
       "  0.003121942514553666,\n",
       "  0.0026415842585265636,\n",
       "  0.00022494611039292067,\n",
       "  0.004588616080582142,\n",
       "  0.005305038299411535,\n",
       "  0.0012897003907710314,\n",
       "  0.002120404737070203,\n",
       "  0.004509377293288708,\n",
       "  0.0024382593110203743,\n",
       "  0.002248983597382903,\n",
       "  0.0034294123761355877,\n",
       "  0.0031195522751659155,\n",
       "  0.0035775527358055115,\n",
       "  0.003935071174055338,\n",
       "  0.001316908630542457,\n",
       "  0.0031860561575740576,\n",
       "  0.001591943670064211,\n",
       "  0.004270671401172876,\n",
       "  0.0034391938243061304,\n",
       "  0.0024369806051254272,\n",
       "  0.0033755083568394184,\n",
       "  0.001575478003360331,\n",
       "  0.002549634547904134,\n",
       "  0.002718935254961252,\n",
       "  0.0020668860524892807,\n",
       "  0.004744930192828178,\n",
       "  0.0025878730230033398,\n",
       "  0.0013708302285522223,\n",
       "  0.0032083597034215927,\n",
       "  0.002327981870621443,\n",
       "  0.0028902937192469835,\n",
       "  0.0037183272652328014,\n",
       "  0.0036986751947551966,\n",
       "  0.003698221407830715,\n",
       "  0.0024862780701369047,\n",
       "  0.0021285186521708965,\n",
       "  0.001935861655510962,\n",
       "  0.0014112356584519148,\n",
       "  0.0027641153428703547,\n",
       "  0.002196633955463767,\n",
       "  0.001223288127221167,\n",
       "  0.0026073057670146227,\n",
       "  0.0026704350020736456,\n",
       "  0.0015758257359266281,\n",
       "  0.0017313219141215086,\n",
       "  0.0022821042221039534,\n",
       "  0.002852467820048332,\n",
       "  0.004286252893507481,\n",
       "  0.0006052391836419702,\n",
       "  0.004053979180753231,\n",
       "  0.0026621241122484207,\n",
       "  0.001932162675075233,\n",
       "  0.0019318172708153725,\n",
       "  0.003617060836404562,\n",
       "  0.0038074005860835314,\n",
       "  0.0018533671973273158,\n",
       "  0.0015866255853325129,\n",
       "  0.0005173920653760433,\n",
       "  0.002334811259061098,\n",
       "  0.002284678863361478,\n",
       "  0.0019913308788090944,\n",
       "  0.0026507172733545303,\n",
       "  0.0032523698173463345,\n",
       "  0.006936328951269388,\n",
       "  0.0019212273182347417,\n",
       "  0.0014572381041944027,\n",
       "  0.003164277644827962,\n",
       "  0.003865073202177882,\n",
       "  0.00196694559417665,\n",
       "  0.003954397514462471,\n",
       "  0.0035163990687578917,\n",
       "  0.0019465944496914744,\n",
       "  0.002738068113103509,\n",
       "  0.0037074661813676357,\n",
       "  0.0016270293854176998,\n",
       "  0.0018211439019069076,\n",
       "  0.0031119761988520622,\n",
       "  0.00207624863833189,\n",
       "  0.0030204923823475838,\n",
       "  0.005063667427748442,\n",
       "  0.002236513886600733,\n",
       "  0.0031893281266093254,\n",
       "  0.004132211674004793,\n",
       "  0.0018246702384203672,\n",
       "  0.003577154828235507,\n",
       "  0.0026710706297308207,\n",
       "  0.0029715343844145536,\n",
       "  0.0031767431646585464,\n",
       "  0.003671155543997884,\n",
       "  0.0040089101530611515,\n",
       "  0.0015881935833021998,\n",
       "  0.0032751341350376606,\n",
       "  0.0036599119193851948,\n",
       "  0.004363266285508871,\n",
       "  0.002481966745108366,\n",
       "  0.0015934660332277417,\n",
       "  0.0016253793146461248,\n",
       "  0.001941123278811574,\n",
       "  0.0020813955925405025,\n",
       "  0.004001423716545105,\n",
       "  0.0032748610246926546,\n",
       "  0.003381674410775304,\n",
       "  0.001837184652686119,\n",
       "  0.0029032223392277956,\n",
       "  0.0032067252323031425,\n",
       "  0.0023458513896912336,\n",
       "  0.0013542968081310391,\n",
       "  0.0009277350036427379,\n",
       "  0.002783640520647168,\n",
       "  0.0035140709951519966,\n",
       "  0.0041663264855742455,\n",
       "  0.00368960527703166,\n",
       "  0.0032772759441286325,\n",
       "  0.00182127277366817,\n",
       "  0.0032199248671531677,\n",
       "  0.0015763584524393082,\n",
       "  0.003399531589820981,\n",
       "  0.0031565679237246513,\n",
       "  0.0026340673211961985,\n",
       "  0.0018930525984615088,\n",
       "  0.0018536127172410488,\n",
       "  0.001994900405406952,\n",
       "  0.0021658011246472597,\n",
       "  0.0026542902924120426,\n",
       "  0.0018845293670892715,\n",
       "  0.0032501344103366137,\n",
       "  0.001803692663088441,\n",
       "  0.0059484876692295074,\n",
       "  0.004306954797357321,\n",
       "  0.0018683470552787185,\n",
       "  0.0024732633028179407,\n",
       "  0.001999905798584223,\n",
       "  0.002625098219141364,\n",
       "  0.002210975158959627,\n",
       "  0.0013715107925236225,\n",
       "  0.002451248001307249,\n",
       "  0.002829375909641385,\n",
       "  0.0002889177412725985,\n",
       "  0.003328405087813735,\n",
       "  0.0016399201704189181,\n",
       "  0.0021551470272243023,\n",
       "  0.002796717919409275,\n",
       "  0.004115710034966469,\n",
       "  0.002325258916243911,\n",
       "  0.003462256398051977,\n",
       "  0.004973267670720816,\n",
       "  0.003339300863444805,\n",
       "  0.0035669717472046614,\n",
       "  0.00416423799470067,\n",
       "  0.0005345367826521397,\n",
       "  0.0027100450824946165,\n",
       "  0.0024877970572561026,\n",
       "  0.0023683798499405384,\n",
       "  0.0024573036935180426,\n",
       "  0.005267185624688864,\n",
       "  0.0020549283362925053,\n",
       "  0.001539780991151929,\n",
       "  0.002304417546838522,\n",
       "  0.0015239244094118476,\n",
       "  0.0020037509966641665,\n",
       "  0.0050103673711419106,\n",
       "  0.0017351562855765224,\n",
       "  0.005042491480708122,\n",
       "  0.002142616081982851,\n",
       "  0.0020146090537309647,\n",
       "  0.003023152006790042,\n",
       "  0.004234377760440111,\n",
       "  0.001736501813866198,\n",
       "  0.004022447858005762,\n",
       "  0.0026226069312542677,\n",
       "  -0.0016877573216333985,\n",
       "  0.002515681553632021,\n",
       "  0.0018193480791524053,\n",
       "  0.003351037623360753,\n",
       "  0.0021279321517795324,\n",
       "  0.0018018538830801845,\n",
       "  0.004368660971522331,\n",
       "  0.002937617478892207,\n",
       "  0.0020605316385626793,\n",
       "  0.002170636085793376,\n",
       "  0.0005531345959752798,\n",
       "  0.00040771483327262104,\n",
       "  0.003258240409195423,\n",
       "  0.0024265071842819452,\n",
       "  0.003199860220775008,\n",
       "  0.0021256711333990097,\n",
       "  0.0037995262537151575,\n",
       "  0.0027391707990318537,\n",
       "  0.0028609803412109613,\n",
       "  0.0013549849390983582,\n",
       "  0.0033631830010563135,\n",
       "  0.0020077372901141644,\n",
       "  0.0031786051113158464,\n",
       "  0.0007803561165928841,\n",
       "  0.0015959249576553702,\n",
       "  0.003992760553956032,\n",
       "  0.0018114892300218344,\n",
       "  0.00276001775637269,\n",
       "  0.0010118312202394009,\n",
       "  0.002838656073436141,\n",
       "  0.002632188145071268,\n",
       "  0.0029881815426051617,\n",
       "  0.004170853178948164,\n",
       "  0.0031119671184569597,\n",
       "  0.001646537217311561,\n",
       "  0.002929066773504019,\n",
       "  -4.479351264308207e-05,\n",
       "  0.0034771766513586044,\n",
       "  0.0018964505288749933,\n",
       "  0.0013212526682764292,\n",
       "  0.00214215787127614,\n",
       "  0.0015684374375268817,\n",
       "  0.0020489885937422514,\n",
       "  0.0035611274652183056,\n",
       "  0.003071679500862956,\n",
       "  0.0009472046513110399,\n",
       "  0.0015886550536379218,\n",
       "  0.0008382815285585821,\n",
       "  0.005999913439154625,\n",
       "  0.0020849069114774466,\n",
       "  0.00308002601377666,\n",
       "  0.003393657971173525,\n",
       "  0.0028238233644515276,\n",
       "  0.0013808993389829993,\n",
       "  0.0023196672555059195,\n",
       "  0.00013811989629175514,\n",
       "  0.0017024048138409853,\n",
       "  0.0031289139296859503,\n",
       "  0.0019687118474394083,\n",
       "  0.005646700970828533,\n",
       "  0.003435452003031969,\n",
       "  0.0024722993839532137,\n",
       "  0.0022325359750539064,\n",
       "  0.0006471889792010188,\n",
       "  0.0025758484844118357,\n",
       "  0.003091118996962905,\n",
       "  0.002884682733565569,\n",
       "  0.003165067872032523,\n",
       "  0.002076620003208518,\n",
       "  0.003365462413057685,\n",
       "  0.0032580439001321793,\n",
       "  0.0015829838812351227,\n",
       "  0.0019336818950250745,\n",
       "  0.0007264529704116285,\n",
       "  0.0008751348941586912,\n",
       "  0.0019814055413007736,\n",
       "  -0.00022534580784849823,\n",
       "  0.003340532071888447,\n",
       "  0.004151642322540283,\n",
       "  0.002332000993192196,\n",
       "  0.0018693562597036362,\n",
       "  0.0029609401244670153,\n",
       "  0.0017744505312293768,\n",
       "  0.0034309267066419125,\n",
       "  0.003458712249994278,\n",
       "  0.0013255438534542918,\n",
       "  0.004637638572603464,\n",
       "  0.001940048998221755,\n",
       "  0.0031455017160624266,\n",
       "  0.004236522130668163,\n",
       "  0.002673428738489747,\n",
       "  0.002776942914351821,\n",
       "  0.004889815580099821,\n",
       "  0.003636400680989027,\n",
       "  0.002042700070887804,\n",
       "  0.001691692043095827,\n",
       "  0.0012661950895562768,\n",
       "  0.0029867631383240223,\n",
       "  0.003741262713447213,\n",
       "  0.0017708631930872798,\n",
       "  0.0034281746484339237,\n",
       "  0.004237382672727108,\n",
       "  0.002956126118078828,\n",
       "  0.002529480727389455,\n",
       "  0.0036240345798432827,\n",
       "  0.002356720855459571,\n",
       "  0.004963968880474567,\n",
       "  0.00428802240639925,\n",
       "  0.003905767574906349,\n",
       "  0.0013980900403112173,\n",
       "  0.002001540968194604,\n",
       "  0.004229632671922445,\n",
       "  0.0025632428005337715,\n",
       "  0.004725826904177666,\n",
       "  0.0014709056122228503,\n",
       "  0.004385041072964668,\n",
       "  0.002692077774554491,\n",
       "  0.0021596034057438374,\n",
       "  0.00014263257617130876,\n",
       "  0.003632972715422511,\n",
       "  0.0030758630018681288,\n",
       "  0.0018013899680227041,\n",
       "  0.0008938199025578797,\n",
       "  0.004932354670017958,\n",
       "  0.0021673261653631926,\n",
       "  0.0021537223365157843,\n",
       "  0.0030890614725649357,\n",
       "  0.0026063036639243364,\n",
       "  0.004500001203268766,\n",
       "  0.0033101344015449286,\n",
       "  0.00016845566278789192,\n",
       "  0.0025893624406307936,\n",
       "  0.002438146388158202,\n",
       "  0.0024747613351792097,\n",
       "  0.004478930030018091,\n",
       "  0.0029915315099060535,\n",
       "  0.0020577500108629465,\n",
       "  ...],\n",
       " [0.012086743488907814,\n",
       "  0.004970039241015911,\n",
       "  0.008385415188968182,\n",
       "  0.00543027650564909,\n",
       "  0.0008086375310085714,\n",
       "  0.0013989608269184828,\n",
       "  0.0004213491047266871,\n",
       "  0.0019780199509114027,\n",
       "  0.0021131616085767746,\n",
       "  0.0006513543194159865,\n",
       "  0.0001947795390151441,\n",
       "  0.0021543006878346205,\n",
       "  0.0008340358617715538,\n",
       "  0.0024431818164885044,\n",
       "  0.00042132078669965267,\n",
       "  0.0032187928445637226,\n",
       "  0.0013071169378235936,\n",
       "  0.0018492487724870443,\n",
       "  0.0011369207641109824,\n",
       "  0.002176394686102867,\n",
       "  0.00534409424290061,\n",
       "  0.0005614444962702692,\n",
       "  0.0027111771050840616,\n",
       "  0.003941631875932217,\n",
       "  0.0015159955946728587,\n",
       "  0.001906185643747449,\n",
       "  0.002865581074729562,\n",
       "  0.003538682358339429,\n",
       "  0.003935209009796381,\n",
       "  0.002090965397655964,\n",
       "  0.0019878351595252752,\n",
       "  0.0011661286698654294,\n",
       "  0.003148026764392853,\n",
       "  0.002720612334087491,\n",
       "  0.0024274864699691534,\n",
       "  0.0023687826469540596,\n",
       "  0.0025976519100368023,\n",
       "  0.004299751948565245,\n",
       "  0.0033221635967493057,\n",
       "  0.002213227329775691,\n",
       "  0.0033280947245657444,\n",
       "  0.0022644384298473597,\n",
       "  0.0019285306334495544,\n",
       "  0.002073227893561125,\n",
       "  0.0016607248689979315,\n",
       "  0.0022298842668533325,\n",
       "  0.0015992196276783943,\n",
       "  0.003202557796612382,\n",
       "  0.0023800265043973923,\n",
       "  0.0018427888862788677,\n",
       "  0.000967974541708827,\n",
       "  0.0010628413874655962,\n",
       "  0.001953081227838993,\n",
       "  0.0017078762175515294,\n",
       "  0.002628911752253771,\n",
       "  0.0029354116413742304,\n",
       "  0.0021485004108399153,\n",
       "  0.00330588617362082,\n",
       "  0.0007528333808295429,\n",
       "  0.0013384252088144422,\n",
       "  0.002081470098346472,\n",
       "  0.0019086305983364582,\n",
       "  0.0027079209685325623,\n",
       "  0.0032334146089851856,\n",
       "  0.004619284998625517,\n",
       "  0.0028822263702750206,\n",
       "  0.001212507369928062,\n",
       "  0.004281456116586924,\n",
       "  -4.826710937777534e-05,\n",
       "  0.001781292143277824,\n",
       "  0.002799486042931676,\n",
       "  0.002565552480518818,\n",
       "  0.0021185048390179873,\n",
       "  0.0014678963925689459,\n",
       "  0.0020025165285915136,\n",
       "  0.0032451199367642403,\n",
       "  0.002792393323034048,\n",
       "  0.002770892111584544,\n",
       "  0.0016775953117758036,\n",
       "  0.002270740456879139,\n",
       "  0.003897697664797306,\n",
       "  0.0022998987697064877,\n",
       "  0.0015500516165047884,\n",
       "  0.0018662215443328023,\n",
       "  0.0014968329342082143,\n",
       "  0.0027833397034555674,\n",
       "  0.0038297991268336773,\n",
       "  0.0018887544283643365,\n",
       "  0.0026839864440262318,\n",
       "  0.0021831165067851543,\n",
       "  0.0012949785450473428,\n",
       "  0.003594136331230402,\n",
       "  0.005441050510853529,\n",
       "  0.003079941961914301,\n",
       "  0.002441541524603963,\n",
       "  0.0011751450365409255,\n",
       "  0.0025336514227092266,\n",
       "  0.0022869985550642014,\n",
       "  0.0027876202948391438,\n",
       "  -0.0006078426376916468,\n",
       "  0.0029302400071173906,\n",
       "  0.003766171168535948,\n",
       "  0.0029942423570901155,\n",
       "  0.0017882204847410321,\n",
       "  0.0032290732488036156,\n",
       "  0.0042106215842068195,\n",
       "  0.0036350369919091463,\n",
       "  0.0018354043131694198,\n",
       "  0.0031683638226240873,\n",
       "  0.0012634079903364182,\n",
       "  0.0026955492794513702,\n",
       "  0.003121360670775175,\n",
       "  0.002656916156411171,\n",
       "  0.0038695461116731167,\n",
       "  0.0035854806192219257,\n",
       "  0.0027376702055335045,\n",
       "  0.0016521846409887075,\n",
       "  0.0032253749668598175,\n",
       "  0.002767052035778761,\n",
       "  0.002705412684008479,\n",
       "  0.0024408188182860613,\n",
       "  0.0022513740696012974,\n",
       "  0.0047907754778862,\n",
       "  0.003865851555019617,\n",
       "  0.0036979226861149073,\n",
       "  0.0012389736948534846,\n",
       "  0.0032559700775891542,\n",
       "  0.0023661996237933636,\n",
       "  0.002492781262844801,\n",
       "  0.0031763946171849966,\n",
       "  0.002225805539637804,\n",
       "  0.002885135356336832,\n",
       "  0.002019384875893593,\n",
       "  0.003979949746280909,\n",
       "  0.0030400024261325598,\n",
       "  0.003360337344929576,\n",
       "  0.0020831585861742496,\n",
       "  0.0023692436516284943,\n",
       "  0.0023600796703249216,\n",
       "  0.0036941098514944315,\n",
       "  0.0021018623374402523,\n",
       "  0.0036764328833669424,\n",
       "  0.0026286193169653416,\n",
       "  0.0016229614848271012,\n",
       "  0.004279590677469969,\n",
       "  0.002587146358564496,\n",
       "  0.0023628578055649996,\n",
       "  0.0035859011113643646,\n",
       "  0.0018626144155859947,\n",
       "  0.002041412517428398,\n",
       "  0.002645123051479459,\n",
       "  0.0016503637889400125,\n",
       "  0.0029100480023771524,\n",
       "  0.0013732005609199405,\n",
       "  0.002887569833546877,\n",
       "  0.003376511624082923,\n",
       "  0.003323060693219304,\n",
       "  0.0022290523629635572,\n",
       "  0.002176334150135517,\n",
       "  0.0020337144378572702,\n",
       "  0.004254629369825125,\n",
       "  0.002155811758711934,\n",
       "  0.002348992507904768,\n",
       "  0.0012294662883505225,\n",
       "  0.0022143423557281494,\n",
       "  0.003885969053953886,\n",
       "  0.003271888941526413,\n",
       "  0.0016744356835260987,\n",
       "  0.0014460699167102575,\n",
       "  0.0038799080066382885,\n",
       "  0.0017390111461281776,\n",
       "  0.00465657003223896,\n",
       "  0.002476716646924615,\n",
       "  0.0029998996760696173,\n",
       "  0.003665577620267868,\n",
       "  0.003029740182682872,\n",
       "  0.0041621653363108635,\n",
       "  0.002253203885629773,\n",
       "  0.0031159534119069576,\n",
       "  0.0018058524001389742,\n",
       "  0.0010584131814539433,\n",
       "  0.0019509823760017753,\n",
       "  0.004117841832339764,\n",
       "  0.004415031522512436,\n",
       "  0.0023250123485922813,\n",
       "  0.002711882581934333,\n",
       "  0.0019484139047563076,\n",
       "  0.004134408663958311,\n",
       "  0.0022266453597694635,\n",
       "  0.002306225011125207,\n",
       "  0.001548938569612801,\n",
       "  0.0036424677819013596,\n",
       "  0.002650889568030834,\n",
       "  0.0022724780719727278,\n",
       "  0.002790795173496008,\n",
       "  0.004261129070073366,\n",
       "  0.0022731672506779432,\n",
       "  0.0036531141959130764,\n",
       "  0.0031510558910667896,\n",
       "  0.003665843280032277,\n",
       "  0.0036134333349764347,\n",
       "  0.003776762867346406,\n",
       "  0.0032857158221304417,\n",
       "  0.0019548977725207806,\n",
       "  0.005151823163032532,\n",
       "  0.0026790364645421505,\n",
       "  0.0047185118310153484,\n",
       "  0.004361468367278576,\n",
       "  0.0034863564651459455,\n",
       "  0.003642465453594923,\n",
       "  0.0030190818943083286,\n",
       "  0.0035746896173805,\n",
       "  0.002317912643775344,\n",
       "  0.003068791003897786,\n",
       "  0.001527264597825706,\n",
       "  0.0013014001306146383,\n",
       "  0.002097772667184472,\n",
       "  0.0026955055072903633,\n",
       "  0.004255486652255058,\n",
       "  0.0023555592633783817,\n",
       "  0.0017369198612868786,\n",
       "  0.0011099126422777772,\n",
       "  0.0011728089302778244,\n",
       "  0.003309752093628049,\n",
       "  0.00354301487095654,\n",
       "  0.004338052123785019,\n",
       "  0.0019105473766103387,\n",
       "  0.004678466822952032,\n",
       "  -0.0005724962684325874,\n",
       "  0.002732260152697563,\n",
       "  0.0030756136402487755,\n",
       "  9.52905320446007e-05,\n",
       "  0.0016444625798612833,\n",
       "  0.003242509439587593,\n",
       "  0.002425306709483266,\n",
       "  0.001351208658888936,\n",
       "  0.003536029253154993,\n",
       "  0.001731020980514586,\n",
       "  0.004360175225883722,\n",
       "  0.0037044754717499018,\n",
       "  0.004439528565853834,\n",
       "  0.0019932612776756287,\n",
       "  0.001558160176500678,\n",
       "  0.0032708027865737677,\n",
       "  0.002284992951899767,\n",
       "  0.0032257859129458666,\n",
       "  0.001450162846595049,\n",
       "  0.0015199017943814397,\n",
       "  0.003377396846190095,\n",
       "  0.00180785299744457,\n",
       "  0.002934209769591689,\n",
       "  0.001670718309469521,\n",
       "  0.0031084036454558372,\n",
       "  0.00468016229569912,\n",
       "  -4.65375924250111e-05,\n",
       "  0.004467083606868982,\n",
       "  0.0019586856942623854,\n",
       "  0.004920723848044872,\n",
       "  0.002366134664043784,\n",
       "  0.001467033289372921,\n",
       "  -0.0024077824782580137,\n",
       "  0.0026397905312478542,\n",
       "  0.0023920093663036823,\n",
       "  0.0014302299823611975,\n",
       "  0.0021134712733328342,\n",
       "  0.0009990409016609192,\n",
       "  0.002461615251377225,\n",
       "  0.002153398236259818,\n",
       "  0.0037886735517531633,\n",
       "  0.0016462096245959401,\n",
       "  0.0027414211072027683,\n",
       "  0.0026163964066654444,\n",
       "  0.0033049634657800198,\n",
       "  0.002235387684777379,\n",
       "  0.0018063277238979936,\n",
       "  0.0024786910507827997,\n",
       "  0.003897308837622404,\n",
       "  0.0036510585341602564,\n",
       "  0.004224410280585289,\n",
       "  0.001742288121022284,\n",
       "  0.0029625026509165764,\n",
       "  0.003216128097847104,\n",
       "  0.002502191113308072,\n",
       "  0.005702256225049496,\n",
       "  0.0007484225789085031,\n",
       "  0.001416920917108655,\n",
       "  0.0035606552846729755,\n",
       "  0.001081589492969215,\n",
       "  0.004101800732314587,\n",
       "  0.0024250075221061707,\n",
       "  0.0010651623597368598,\n",
       "  0.002027734648436308,\n",
       "  0.00045704597141593695,\n",
       "  0.0030284631066024303,\n",
       "  0.0027617206797003746,\n",
       "  0.0019127849955111742,\n",
       "  0.004242194816470146,\n",
       "  0.0017201497685164213,\n",
       "  0.002685385523363948,\n",
       "  0.001295087393373251,\n",
       "  0.002372352872043848,\n",
       "  0.0022482089698314667,\n",
       "  0.0036795055493712425,\n",
       "  0.003586740465834737,\n",
       "  0.0038311548996716738,\n",
       "  0.004565385635942221,\n",
       "  0.004842457827180624,\n",
       "  0.0043945214711129665,\n",
       "  0.001111540594138205,\n",
       "  0.0026350272819399834,\n",
       "  0.0027322506066411734,\n",
       "  0.0032400074414908886,\n",
       "  0.002164073521271348,\n",
       "  0.00040888460353016853,\n",
       "  0.0031111768912523985,\n",
       "  0.0051477630622684956,\n",
       "  0.0031165664549916983,\n",
       "  0.0021449937485158443,\n",
       "  0.004302706103771925,\n",
       "  0.002107559470459819,\n",
       "  0.004098120145499706,\n",
       "  0.0035468372516334057,\n",
       "  0.002155760768800974,\n",
       "  0.0023528907913714647,\n",
       "  0.003590817330405116,\n",
       "  0.00521467300131917,\n",
       "  0.0008554222877137363,\n",
       "  0.0017639079596847296,\n",
       "  0.0018075136467814445,\n",
       "  0.006217468995600939,\n",
       "  0.0019720050040632486,\n",
       "  0.0008954538498073816,\n",
       "  0.004780338145792484,\n",
       "  0.0023017253261059523,\n",
       "  0.002585236681625247,\n",
       "  0.0038359304890036583,\n",
       "  0.003487704088911414,\n",
       "  0.0011710557155311108,\n",
       "  0.000928421679418534,\n",
       "  0.002786822384223342,\n",
       "  0.004240512847900391,\n",
       "  0.003167141228914261,\n",
       "  0.0003913777181878686,\n",
       "  0.006349507253617048,\n",
       "  0.004913310054689646,\n",
       "  0.0038765636272728443,\n",
       "  0.0015678562922403216,\n",
       "  0.001974953105673194,\n",
       "  0.0036308420822024345,\n",
       "  0.00277687213383615,\n",
       "  0.001992726232856512,\n",
       "  0.0024522633757442236,\n",
       "  0.0012523088371381164,\n",
       "  0.0035277772694826126,\n",
       "  0.001425968250259757,\n",
       "  0.004884962923824787,\n",
       "  0.0021578529849648476,\n",
       "  0.0035540075041353703,\n",
       "  0.004103895742446184,\n",
       "  0.001477146870456636,\n",
       "  0.0029928036965429783,\n",
       "  0.0029678947757929564,\n",
       "  0.0023463889956474304,\n",
       "  0.002957269549369812,\n",
       "  0.0018735122866928577,\n",
       "  0.004328508395701647,\n",
       "  0.0026932815089821815,\n",
       "  0.003427216550335288,\n",
       "  0.0029994335491210222,\n",
       "  0.0022174660116434097,\n",
       "  0.0026096913497895002,\n",
       "  0.002302828710526228,\n",
       "  0.003183079417794943,\n",
       "  0.0014872775645926595,\n",
       "  0.002855168655514717,\n",
       "  0.0019094907911494374,\n",
       "  0.003575829556211829,\n",
       "  0.00523950532078743,\n",
       "  0.004697752185165882,\n",
       "  -0.00021517195273190737,\n",
       "  0.0021558047737926245,\n",
       "  0.004483516793698072,\n",
       "  0.0017952520865947008,\n",
       "  0.0006381922867149115,\n",
       "  0.001960473833605647,\n",
       "  0.0026972803752869368,\n",
       "  0.002758711576461792,\n",
       "  0.003710661083459854,\n",
       "  0.0022885422222316265,\n",
       "  0.003150549717247486,\n",
       "  0.0015618493780493736,\n",
       "  0.003304067999124527,\n",
       "  0.004052998498082161,\n",
       "  0.0024864589795470238,\n",
       "  0.0037190818693488836,\n",
       "  0.002077814657241106,\n",
       "  0.00021558655134867877,\n",
       "  0.0019428315572440624,\n",
       "  0.0024267989210784435,\n",
       "  0.0017561977729201317,\n",
       "  0.003783449297770858,\n",
       "  0.002616827841848135,\n",
       "  0.003036792855709791,\n",
       "  0.0033901785500347614,\n",
       "  0.0027779133524745703,\n",
       "  0.0024172787088900805,\n",
       "  0.0016947714611887932,\n",
       "  0.00282588554546237,\n",
       "  0.0030065462924540043,\n",
       "  0.003041016636416316,\n",
       "  0.003061064053326845,\n",
       "  0.0030794846825301647,\n",
       "  0.0005098439287394285,\n",
       "  0.0011753018479794264,\n",
       "  0.004409605171531439,\n",
       "  0.0028376076370477676,\n",
       "  0.0012561114272102714,\n",
       "  0.004319921135902405,\n",
       "  0.0023230218794196844,\n",
       "  0.0008316217572428286,\n",
       "  0.0015271863667294383,\n",
       "  0.0027160088066011667,\n",
       "  0.003157817991450429,\n",
       "  0.001701079192571342,\n",
       "  0.002139580901712179,\n",
       "  0.0018125574570149183,\n",
       "  0.002852282952517271,\n",
       "  0.004901427309960127,\n",
       "  0.0024943812750279903,\n",
       "  0.0010718152625486255,\n",
       "  0.00221692374907434,\n",
       "  0.0002645973290782422,\n",
       "  0.0010003234492614865,\n",
       "  0.0006884383037686348,\n",
       "  0.0025901140179485083,\n",
       "  0.002303709276020527,\n",
       "  0.0012322309194132686,\n",
       "  0.002822081558406353,\n",
       "  0.002734464593231678,\n",
       "  0.0038777717854827642,\n",
       "  0.002578870626166463,\n",
       "  0.006314838770776987,\n",
       "  0.003979991190135479,\n",
       "  0.0023934380151331425,\n",
       "  0.002932494506239891,\n",
       "  -9.512074029771611e-05,\n",
       "  0.0031065649818629026,\n",
       "  0.0027518183924257755,\n",
       "  0.0015903584426268935,\n",
       "  0.0015162202762439847,\n",
       "  0.0019805151969194412,\n",
       "  0.0020405221730470657,\n",
       "  0.00254394905641675,\n",
       "  0.0037857280112802982,\n",
       "  0.005092897452414036,\n",
       "  0.001414944534189999,\n",
       "  0.004682773724198341,\n",
       "  0.0017003907123580575,\n",
       "  0.003692400408908725,\n",
       "  0.0005860378732904792,\n",
       "  0.004567326977849007,\n",
       "  0.004065193701535463,\n",
       "  0.0016907290555536747,\n",
       "  0.0018300536321476102,\n",
       "  0.004785000812262297,\n",
       "  0.0030474665109068155,\n",
       "  0.004275628365576267,\n",
       "  0.0004233199288137257,\n",
       "  0.002086085034534335,\n",
       "  0.0037414554972201586,\n",
       "  0.0011071970220655203,\n",
       "  0.0031773988157510757,\n",
       "  0.002161894692108035,\n",
       "  0.0017978694522753358,\n",
       "  0.0015044400934129953,\n",
       "  0.0027241315692663193,\n",
       "  0.0021497546695172787,\n",
       "  0.0014001887757331133,\n",
       "  0.0031311805360019207,\n",
       "  0.003105697687715292,\n",
       "  0.003099757479503751,\n",
       "  0.003215180244296789,\n",
       "  0.003250862006098032,\n",
       "  0.0013205928262323141,\n",
       "  0.003289085580036044,\n",
       "  0.003298672614619136,\n",
       "  0.001485538436099887,\n",
       "  0.0027092071250081062,\n",
       "  0.0019231120822951198,\n",
       "  0.0018225787207484245,\n",
       "  0.001972561003640294,\n",
       "  0.0025128398556262255,\n",
       "  0.003194315591827035,\n",
       "  0.0032575130462646484,\n",
       "  0.00016908087127376348,\n",
       "  0.0007821423350833356,\n",
       "  0.0025920113548636436,\n",
       "  0.003935972694307566,\n",
       "  0.0040839132852852345,\n",
       "  0.005052936729043722,\n",
       "  0.0026364477816969156,\n",
       "  0.002256017876788974,\n",
       "  0.0033720009960234165,\n",
       "  0.003148996038362384,\n",
       "  0.0024892345536500216,\n",
       "  0.0018049709033221006,\n",
       "  0.0018937964923679829,\n",
       "  0.001775057869963348,\n",
       "  0.0009455094113945961,\n",
       "  0.00150882953312248,\n",
       "  0.004073432646691799,\n",
       "  0.0017750884871929884,\n",
       "  0.0015963659388944507,\n",
       "  0.005757446400821209,\n",
       "  0.002387937158346176,\n",
       "  0.0036828897427767515,\n",
       "  0.0028134637977927923,\n",
       "  0.0013659739634022117,\n",
       "  0.0018795604119077325,\n",
       "  0.002755681285634637,\n",
       "  0.00652942294254899,\n",
       "  0.0033140776213258505,\n",
       "  0.0035201285500079393,\n",
       "  0.004039468709379435,\n",
       "  0.0018577899318188429,\n",
       "  0.004003989044576883,\n",
       "  0.0020223320461809635,\n",
       "  0.004163234960287809,\n",
       "  0.005234044510871172,\n",
       "  0.002086531836539507,\n",
       "  0.0021499451249837875,\n",
       "  0.001463104272261262,\n",
       "  0.0016802194295451045,\n",
       "  0.001780595164746046,\n",
       "  0.0021487693302333355,\n",
       "  0.0020110022742301226,\n",
       "  0.00219484674744308,\n",
       "  0.001822889782488346,\n",
       "  0.0016772946109995246,\n",
       "  0.0023110450711101294,\n",
       "  0.0038775415159761906,\n",
       "  0.004087928682565689,\n",
       "  0.006474283058196306,\n",
       "  0.0035293896216899157,\n",
       "  0.0029083958361297846,\n",
       "  0.0023660920560359955,\n",
       "  0.0007922571967355907,\n",
       "  0.0023800767958164215,\n",
       "  0.00449131615459919,\n",
       "  0.0015323834959417582,\n",
       "  0.0011488629970699549,\n",
       "  0.0021723012905567884,\n",
       "  0.0010619622189551592,\n",
       "  0.001760910963639617,\n",
       "  0.0027240850031375885,\n",
       "  0.0017294026911258698,\n",
       "  0.0041337814182043076,\n",
       "  0.0025065524969249964,\n",
       "  0.0023660236038267612,\n",
       "  0.002215556800365448,\n",
       "  0.0022567338310182095,\n",
       "  0.0009486160706728697,\n",
       "  0.0013592640170827508,\n",
       "  0.0019046651432290673,\n",
       "  0.0007675131782889366,\n",
       "  0.0033876486122608185,\n",
       "  0.0032970847096294165,\n",
       "  0.0005028528976254165,\n",
       "  0.0032549062743782997,\n",
       "  0.002578018233180046,\n",
       "  0.004704018589109182,\n",
       "  0.0026264023035764694,\n",
       "  0.0030943085439503193,\n",
       "  0.0036861090920865536,\n",
       "  0.0027727617416530848,\n",
       "  0.0024909768253564835,\n",
       "  0.0004962590173818171,\n",
       "  0.0037905266508460045,\n",
       "  0.0028510724660009146,\n",
       "  0.002999440999701619,\n",
       "  0.00271036010235548,\n",
       "  0.006968747824430466,\n",
       "  0.0024409170728176832,\n",
       "  0.003432678058743477,\n",
       "  0.002432591514661908,\n",
       "  0.002425733720883727,\n",
       "  0.0017920327372848988,\n",
       "  0.000871721189469099,\n",
       "  0.0038024899549782276,\n",
       "  0.003452458418905735,\n",
       "  0.0033915312960743904,\n",
       "  0.003544693812727928,\n",
       "  0.0020623975433409214,\n",
       "  0.004186994396150112,\n",
       "  0.0026894842740148306,\n",
       "  0.0015474481042474508,\n",
       "  0.0014925261493772268,\n",
       "  0.002543419599533081,\n",
       "  0.001637511420994997,\n",
       "  0.002327029360458255,\n",
       "  0.001475079683586955,\n",
       "  0.0032805113587528467,\n",
       "  0.0026322545018047094,\n",
       "  0.003866806859150529,\n",
       "  0.003179352032020688,\n",
       "  0.0023667661007493734,\n",
       "  0.0036171949468553066,\n",
       "  0.0017087542219087481,\n",
       "  0.0003700981615111232,\n",
       "  0.0017922507831826806,\n",
       "  0.002417145762592554,\n",
       "  0.0029700607992708683,\n",
       "  0.0029193717055022717,\n",
       "  0.0023922233376652002,\n",
       "  0.002207479439675808,\n",
       "  0.0026382296346127987,\n",
       "  0.0023482353426516056,\n",
       "  0.0032005009707063437,\n",
       "  0.0013043837388977408,\n",
       "  0.0022278984542936087,\n",
       "  0.0020233942195773125,\n",
       "  0.002478900132700801,\n",
       "  0.0035576941445469856,\n",
       "  0.0025445332285016775,\n",
       "  0.0030528386123478413,\n",
       "  0.0020930799655616283,\n",
       "  0.002352612093091011,\n",
       "  0.005704572889953852,\n",
       "  0.0028425203636288643,\n",
       "  0.0012937441933900118,\n",
       "  0.003377856919541955,\n",
       "  0.004736068192869425,\n",
       "  0.0019303497392684221,\n",
       "  0.0028322087600827217,\n",
       "  0.004438081290572882,\n",
       "  0.0029866881668567657,\n",
       "  0.003382092574611306,\n",
       "  0.001734706456772983,\n",
       "  0.003684582654386759,\n",
       "  0.0018208322580903769,\n",
       "  0.0022784830071032047,\n",
       "  0.0017074490897357464,\n",
       "  0.002640745835378766,\n",
       "  0.0013371552340686321,\n",
       "  0.00228257873095572,\n",
       "  0.005566623527556658,\n",
       "  0.0059455460868775845,\n",
       "  -0.0015506396302953362,\n",
       "  0.003200306324288249,\n",
       "  0.0013634847709909081,\n",
       "  0.004127195104956627,\n",
       "  0.0024514177348464727,\n",
       "  0.0015291253803297877,\n",
       "  0.0028033056296408176,\n",
       "  0.0035440910141915083,\n",
       "  0.004287051502615213,\n",
       "  0.0009289460722357035,\n",
       "  0.0029124098364263773,\n",
       "  0.003035962348803878,\n",
       "  0.0034429901279509068,\n",
       "  0.001856520539149642,\n",
       "  0.0024712278973311186,\n",
       "  0.0025216902140527964,\n",
       "  0.002293880097568035,\n",
       "  0.0045567662455141544,\n",
       "  0.0020606929901987314,\n",
       "  0.00270828977227211,\n",
       "  0.0026895971968770027,\n",
       "  0.004334304016083479,\n",
       "  0.004457446746528149,\n",
       "  0.0038613241631537676,\n",
       "  0.002884030807763338,\n",
       "  0.0033259608317166567,\n",
       "  0.0015538425650447607,\n",
       "  0.002830201992765069,\n",
       "  0.002847810974344611,\n",
       "  0.003704687347635627,\n",
       "  0.0027078790590167046,\n",
       "  7.941447984194383e-05,\n",
       "  0.00044658308615908027,\n",
       "  0.0036925862077623606,\n",
       "  0.002783215371891856,\n",
       "  0.0021748768631368876,\n",
       "  0.002662011655047536,\n",
       "  0.0027506856713443995,\n",
       "  0.004511434584856033,\n",
       "  0.0024536377750337124,\n",
       "  0.0015317648649215698,\n",
       "  0.002208899473771453,\n",
       "  0.0028022737242281437,\n",
       "  0.002183600328862667,\n",
       "  0.0025580385699868202,\n",
       "  0.002249345649033785,\n",
       "  0.003822073107585311,\n",
       "  0.0037085944786667824,\n",
       "  0.002927898894995451,\n",
       "  0.0040093399584293365,\n",
       "  0.0021154165733605623,\n",
       "  0.002219643909484148,\n",
       "  0.002253894926980138,\n",
       "  0.001421211170963943,\n",
       "  0.005236055701971054,\n",
       "  0.004154554568231106,\n",
       "  0.0039082481525838375,\n",
       "  0.003568944986909628,\n",
       "  0.0012641523499041796,\n",
       "  0.003948049619793892,\n",
       "  0.003631210420280695,\n",
       "  0.003357785055413842,\n",
       "  0.0029043364338576794,\n",
       "  0.0012281248345971107,\n",
       "  0.002049335977062583,\n",
       "  0.0039067864418029785,\n",
       "  0.002322621177881956,\n",
       "  0.002552420599386096,\n",
       "  0.0021661780774593353,\n",
       "  0.0027402706909924746,\n",
       "  0.0035324026830494404,\n",
       "  0.0025392952375113964,\n",
       "  0.003020877717062831,\n",
       "  0.002492669504135847,\n",
       "  0.0028144712559878826,\n",
       "  0.005006488412618637,\n",
       "  0.0029115534853190184,\n",
       "  0.0020365212112665176,\n",
       "  0.003075717017054558,\n",
       "  0.0031634278129786253,\n",
       "  0.0034971078857779503,\n",
       "  0.0027293364983052015,\n",
       "  0.003792164148762822,\n",
       "  0.002482563955709338,\n",
       "  0.003934585954993963,\n",
       "  0.003166621783748269,\n",
       "  0.0029244907200336456,\n",
       "  0.0016833135159686208,\n",
       "  0.004931158386170864,\n",
       "  0.005330652464181185,\n",
       "  0.0012265652185305953,\n",
       "  0.003436831757426262,\n",
       "  0.0069136968813836575,\n",
       "  -0.00021584186470136046,\n",
       "  0.0034956312738358974,\n",
       "  0.0030098336283117533,\n",
       "  0.005304527468979359,\n",
       "  0.003303451696410775,\n",
       "  0.0018990905955433846,\n",
       "  0.0015134020941331983,\n",
       "  0.002992896130308509,\n",
       "  0.0017516271909698844,\n",
       "  0.001850984524935484,\n",
       "  0.0021658821497112513,\n",
       "  0.0037232066970318556,\n",
       "  0.0029904665425419807,\n",
       "  0.0019900635816156864,\n",
       "  0.0029549782630056143,\n",
       "  0.0006757156806997955,\n",
       "  0.0018533660331740975,\n",
       "  0.0032974963542073965,\n",
       "  0.002130459528416395,\n",
       "  0.003723349655047059,\n",
       "  0.0013192726764827967,\n",
       "  0.0025160263758152723,\n",
       "  0.0026537892408668995,\n",
       "  0.004761591088026762,\n",
       "  0.0024530363734811544,\n",
       "  0.002731291577219963,\n",
       "  0.0019435212016105652,\n",
       "  0.0026084366254508495,\n",
       "  0.0017205982003360987,\n",
       "  0.001329763326793909,\n",
       "  0.0023551159538328648,\n",
       "  0.0029256532434374094,\n",
       "  0.002613113261759281,\n",
       "  0.0027317116037011147,\n",
       "  0.003545827465131879,\n",
       "  0.006876931991428137,\n",
       "  0.0029853354208171368,\n",
       "  0.0011136564426124096,\n",
       "  0.0034226104617118835,\n",
       "  0.0018431524513289332,\n",
       "  0.0026962498668581247,\n",
       "  0.0017912673065438867,\n",
       "  0.004942051600664854,\n",
       "  0.003356063738465309,\n",
       "  0.002401921432465315,\n",
       "  0.003506127744913101,\n",
       "  0.0038380781188607216,\n",
       "  0.0024184074718505144,\n",
       "  0.002424831036478281,\n",
       "  0.003185940207913518,\n",
       "  0.0011752195423468947,\n",
       "  0.0027395363431423903,\n",
       "  0.001620127004571259,\n",
       "  0.0009634601883590221,\n",
       "  0.005081404931843281,\n",
       "  0.0008152943337336183,\n",
       "  0.0032280099112540483,\n",
       "  0.002031044801697135,\n",
       "  0.00431507034227252,\n",
       "  0.002772529376670718,\n",
       "  0.0013990348670631647,\n",
       "  0.0007543406682088971,\n",
       "  0.0025265614967793226,\n",
       "  0.0020980590488761663,\n",
       "  0.0022914111614227295,\n",
       "  0.0033917503897100687,\n",
       "  0.0014645273331552744,\n",
       "  0.003558998228982091,\n",
       "  0.0033179321326315403,\n",
       "  0.002263155998662114,\n",
       "  0.0016639360692352057,\n",
       "  0.0020407114643603563,\n",
       "  0.003715366357937455,\n",
       "  0.0015702391974627972,\n",
       "  0.003110898658633232,\n",
       "  0.0026048023719340563,\n",
       "  0.003011066932231188,\n",
       "  0.002417762065306306,\n",
       "  0.003091139020398259,\n",
       "  0.003022086573764682,\n",
       "  0.002632088027894497,\n",
       "  0.0013523314846679568,\n",
       "  0.0017637094715610147,\n",
       "  0.004620825406163931,\n",
       "  0.002001248998567462,\n",
       "  0.002254356862977147,\n",
       "  -0.0004049060808029026,\n",
       "  0.002610130701214075,\n",
       "  0.0013076385948807001,\n",
       "  0.0024780952371656895,\n",
       "  0.003725848626345396,\n",
       "  0.0043446095660328865,\n",
       "  0.0007023961516097188,\n",
       "  0.001862674718722701,\n",
       "  0.006159926299005747,\n",
       "  0.0010664876317605376,\n",
       "  0.0031542733777314425,\n",
       "  0.0024023670703172684,\n",
       "  0.002229618839919567,\n",
       "  0.002868121489882469,\n",
       "  0.0028459434397518635,\n",
       "  0.0030661821365356445,\n",
       "  0.0018439649138599634,\n",
       "  0.0021761159878224134,\n",
       "  0.0012704351684078574,\n",
       "  0.0024585865903645754,\n",
       "  0.005221372935920954,\n",
       "  0.0050117517821490765,\n",
       "  0.004819583613425493,\n",
       "  0.00235057738609612,\n",
       "  0.0020876009948551655,\n",
       "  0.0018829465843737125,\n",
       "  0.0009797009406611323,\n",
       "  0.0032477350905537605,\n",
       "  0.003384135663509369,\n",
       "  0.002257260959595442,\n",
       "  0.0033039948903024197,\n",
       "  0.0029184985905885696,\n",
       "  0.0045027402229607105,\n",
       "  0.002355672186240554,\n",
       "  0.0037529952824115753,\n",
       "  0.0021141732577234507,\n",
       "  0.0024748474825173616,\n",
       "  0.0037480294704437256,\n",
       "  0.0040246485732495785,\n",
       "  0.003073001280426979,\n",
       "  0.0037188152782619,\n",
       "  0.002791755134239793,\n",
       "  0.002552814781665802,\n",
       "  0.002869857009500265,\n",
       "  0.004125670529901981,\n",
       "  -3.291647954029031e-05,\n",
       "  0.003983393777161837,\n",
       "  0.00317081855610013,\n",
       "  0.004314076621085405,\n",
       "  0.002012933138757944,\n",
       "  0.0011846506968140602,\n",
       "  0.00366205838508904,\n",
       "  0.0029141195118427277,\n",
       "  0.0012345814611762762,\n",
       "  0.0027308878488838673,\n",
       "  0.0029227419290691614,\n",
       "  0.0025264837313443422,\n",
       "  0.004061713814735413,\n",
       "  0.0021812082268297672,\n",
       "  0.002055057790130377,\n",
       "  0.007551321294158697,\n",
       "  0.003333945292979479,\n",
       "  0.002763928147032857,\n",
       "  0.0030084378086030483,\n",
       "  0.002251442288979888,\n",
       "  0.0008007988217286766,\n",
       "  0.0023025269620120525,\n",
       "  0.0035855614114552736,\n",
       "  0.0033083376474678516,\n",
       "  0.0040491847321391106,\n",
       "  0.004269788973033428,\n",
       "  0.0028817146085202694,\n",
       "  0.004529586993157864,\n",
       "  0.00029283255571499467,\n",
       "  0.0030347956344485283,\n",
       "  0.0009692910243757069,\n",
       "  0.002873798133805394,\n",
       "  0.003026846330612898,\n",
       "  0.003356238594278693,\n",
       "  0.0028852554969489574,\n",
       "  0.0015058435965329409,\n",
       "  0.0018788744928315282,\n",
       "  0.001921400660648942,\n",
       "  0.003333667293190956,\n",
       "  0.0035635263193398714,\n",
       "  0.0031546554528176785,\n",
       "  0.001430949312634766,\n",
       "  0.0005982265574857593,\n",
       "  0.0045042275451123714,\n",
       "  0.002645576372742653,\n",
       "  0.0038271211087703705,\n",
       "  0.0023364571388810873,\n",
       "  0.0021184878423810005,\n",
       "  0.0036519740242511034,\n",
       "  0.0008521716808900237,\n",
       "  0.001815233496017754,\n",
       "  0.0033826762810349464,\n",
       "  0.0026620018761605024,\n",
       "  0.001108012511394918,\n",
       "  0.0032955289352685213,\n",
       "  0.0027139713056385517,\n",
       "  0.002126564271748066,\n",
       "  0.0019561026711016893,\n",
       "  0.0030321329832077026,\n",
       "  0.0026658661663532257,\n",
       "  0.0033927999902516603,\n",
       "  0.002801611553877592,\n",
       "  0.0042778798379004,\n",
       "  0.004373665899038315,\n",
       "  0.001269170781597495,\n",
       "  0.004798675887286663,\n",
       "  0.002369458554312587,\n",
       "  0.0030545289628207684,\n",
       "  0.004387314897030592,\n",
       "  0.0042999121360480785,\n",
       "  0.0006032208912074566,\n",
       "  0.0014969224575906992,\n",
       "  0.004561390727758408,\n",
       "  0.0018471445655450225,\n",
       "  0.006518805865198374,\n",
       "  0.0055602299980819225,\n",
       "  0.0039269705303013325,\n",
       "  0.00261591630987823,\n",
       "  0.002683823462575674,\n",
       "  0.0037003271281719208,\n",
       "  0.002452580723911524,\n",
       "  0.004391627386212349,\n",
       "  0.004064803011715412,\n",
       "  0.003513940377160907,\n",
       "  0.002678884193301201,\n",
       "  0.002799606416374445,\n",
       "  0.003586239879950881,\n",
       "  0.004222020506858826,\n",
       "  0.0017522730631753802,\n",
       "  0.003030574880540371,\n",
       "  0.0036117208655923605,\n",
       "  0.002016533399000764,\n",
       "  0.0038344699423760176,\n",
       "  0.0016080205095931888,\n",
       "  0.002872220706194639,\n",
       "  0.00300163053907454,\n",
       "  0.003190565388649702,\n",
       "  0.002166623715311289,\n",
       "  0.002004485810175538,\n",
       "  0.0024115885607898235,\n",
       "  0.0026650121435523033,\n",
       "  0.0016552594024688005,\n",
       "  0.0015749054728075862,\n",
       "  0.002004869282245636,\n",
       "  0.002964275423437357,\n",
       "  0.0018760513048619032,\n",
       "  0.0013436170993372798,\n",
       "  0.002941971644759178,\n",
       "  0.0022607347927987576,\n",
       "  0.005221500992774963,\n",
       "  0.002325730863958597,\n",
       "  0.0024567607324570417,\n",
       "  0.003001806326210499,\n",
       "  0.0029127069283276796,\n",
       "  0.0014411904849112034,\n",
       "  0.002318887272849679,\n",
       "  0.0019831527024507523,\n",
       "  0.00226192525587976,\n",
       "  0.0029141074046492577,\n",
       "  0.0010036445455625653,\n",
       "  0.0032758305314928293,\n",
       "  0.0023379535414278507,\n",
       "  0.0031351116485893726,\n",
       "  0.0013125987024977803,\n",
       "  0.003988323267549276,\n",
       "  0.002912080381065607,\n",
       "  0.0022171970922499895,\n",
       "  0.0017853425815701485,\n",
       "  0.0014022606192156672,\n",
       "  ...])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.train()\n",
    "netG_neg.train()\n",
    "# gen_losses, disc_losses = train_GAN(netD_neg, netG_neg, negative=True)\n",
    "train_GAN(netD_neg, netG_neg, tr=train_100k, epochs=10, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gen_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(disc_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test without train\n",
    "netD_neg_test = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg_test = NetG(train_100k.shape[1]).cuda()\n",
    "\n",
    "netD_neg_test.eval()\n",
    "netG_neg_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.eval()\n",
    "netG_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuraccy\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda() \n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k > 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake_accur_check = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without train\n",
    "fake_test_accur_check = netG_neg_test(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_accur_check_ = (fake_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_accur_check = (fake_test_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24362, 13579)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_accur_check_ * negative_feedback).sum(), (fake_accur_check_ * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20075, 25478)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_test_accur_check * negative_feedback).sum(), (fake_test_accur_check * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5990312031276894"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items\n",
    "(fake_accur_check_ * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49361921856942637"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items - WITHOUT TRAIN \n",
    "(fake_test_accur_check * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7278812047854752"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items\n",
    "((1-fake_accur_check_) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4894290695577243"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4894290695577243"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10569"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fake_accur_check\n",
    "del fake_test_accur_check \n",
    "del e_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# predicting on ngetavie feedbacks\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k == 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = (fake >0.9).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_augment_negative = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13777078364356143, 0.2542968846049817, 0.6079323317514569]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_probs = [(train_100k == 1).sum()/((train_100k > 0) & (train_100k < 4)).sum(), (train_100k == 2).sum()/(((train_100k > 0) & (train_100k < 4))).sum(), (train_100k == 3).sum()/((train_100k > 0) & (train_100k < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_100k = train_100k + to_augment_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 5.743932070970402)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(augmented_train_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "\tcurrent iteration: 110\n",
      "\tcurrent iteration: 120\n",
      "\tcurrent iteration: 130\n",
      "\tcurrent iteration: 140\n",
      "\tcurrent iteration: 150\n",
      "\tcurrent iteration: 160\n",
      "\tcurrent iteration: 170\n",
      "\tcurrent iteration: 180\n",
      "\tcurrent iteration: 190\n",
      "\tcurrent iteration: 200\n",
      "\tcurrent iteration: 210\n",
      "\tcurrent iteration: 220\n",
      "\tcurrent iteration: 230\n",
      "\tcurrent iteration: 240\n",
      "\tcurrent iteration: 250\n",
      "\tcurrent iteration: 260\n",
      "\tcurrent iteration: 270\n",
      "\tcurrent iteration: 280\n",
      "\tcurrent iteration: 290\n",
      "\tcurrent iteration: 300\n",
      "Train mse: 0.24780276795013206\n",
      "Test mse: 0.9911321751624483\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-da219d16cefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMF_SGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExplicitMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_train_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mMF_SGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[1;34m(self, iter_array, test, learning_rate)\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_iter, learning_rate)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpartial_train\u001b[1;34m(self, n_iter)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mctr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, u, i)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sgd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142822581632546\n",
      "Test mse: 1.179626365384201\n",
      "Iteration: 2\n",
      "Train mse: 1.0730309645422471\n",
      "Test mse: 1.1283647354414428\n",
      "Iteration: 5\n",
      "Train mse: 0.976704786728581\n",
      "Test mse: 1.0498895036356708\n",
      "Iteration: 10\n",
      "Train mse: 0.9191190735106731\n",
      "Test mse: 0.99882388507181\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8670975465312848\n",
      "Test mse: 0.9525039875303193\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8395297477072872\n",
      "Test mse: 0.9342662870213886\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7572607163519403\n",
      "Test mse: 0.917558302954112\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40646564364302945\n",
      "Test mse: 0.9047525011076776\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1583192976603522\n",
      "Test mse: 1.1778921160446154\n",
      "Iteration: 2\n",
      "Train mse: 1.0827823219708648\n",
      "Test mse: 1.124826924658217\n",
      "Iteration: 5\n",
      "Train mse: 0.9809962544391533\n",
      "Test mse: 1.0456674228179015\n",
      "Iteration: 10\n",
      "Train mse: 0.9217915122656181\n",
      "Test mse: 0.9960992361685521\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8695835693024129\n",
      "Test mse: 0.9521000726072203\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8421152097706776\n",
      "Test mse: 0.9345373660525688\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.755397379978269\n",
      "Test mse: 0.916590796822304\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40575111332308983\n",
      "Test mse: 0.906762464167835\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.35978032575701\n",
      "Test mse: 8.193157746225948\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359732338000212\n",
      "Test mse: 8.193099816595986\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359687859300425\n",
      "Test mse: 8.193046053932775\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359646525578646\n",
      "Test mse: 8.192996043819882\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3596080230681835\n",
      "Test mse: 8.192949426343956\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393714646752196\n",
      "Test mse: 8.194362304310943\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393666122560651\n",
      "Test mse: 8.194304259396569\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39362115513897\n",
      "Test mse: 8.194250397772203\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393579374306474\n",
      "Test mse: 8.19420030178969\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393540461520644\n",
      "Test mse: 8.19415360957236\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='sgd',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
