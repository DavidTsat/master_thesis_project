{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from dataLoader import loadData\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  3 5778k    3  220k    0     0   115k      0  0:00:50  0:00:01  0:00:49  115k\n",
      " 33 5778k   33 1940k    0     0   693k      0  0:00:08  0:00:02  0:00:06  693k\n",
      "100 5778k  100 5778k    0     0  1652k      0  0:00:03  0:00:03 --:--:-- 1652k\n",
      "\"unzip\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "‘ЁбвҐ¬Ґ ­Ґ г¤ Ґвбп ­ ©вЁ гЄ § ­­л© Їгвм.\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-1m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0 4808k    0 11401    0     0  32855      0  0:02:29 --:--:--  0:02:29 32761\n",
      " 14 4808k   14  699k    0     0   512k      0  0:00:09  0:00:01  0:00:08  511k\n",
      " 22 4808k   22 1104k    0     0   451k      0  0:00:10  0:00:02  0:00:08  451k\n",
      " 59 4808k   59 2864k    0     0   867k      0  0:00:05  0:00:03  0:00:02  867k\n",
      "100 4808k  100 4808k    0     0  1157k      0  0:00:04  0:00:04 --:--:-- 1157k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ml-1m.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "    \n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ml-100k.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "    \n",
    "# !cd ml-1m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 47\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following blogpost's independent code to benchmark our experiments https://blog.insightdatascience.com/explicit-matrix-factorization-als-sgd-and-all-that-jazz-b00e4d9b21ea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent MF code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        1     1193       5\n",
       "1        1      661       3\n",
       "2        1      914       3\n",
       "3        1     3408       4\n",
       "4        1     2355       5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating']\n",
    "df = pd.read_csv('./ml-1m/ratings.dat', sep='::', usecols = [0, 1, 2], names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3952, (3706,), 3952)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.item_id.unique().max(), df.item_id.unique().shape, df.item_id.unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().max()\n",
    "n_items = df.item_id.unique().max()\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "#     print(row)\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparsity(mat):\n",
    "    sparsity = float(len(mat.nonzero()[0]))\n",
    "    sparsity /= (mat.shape[0] * mat.shape[1])\n",
    "    sparsity *= 100\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.190220560634904"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    np.random.seed(seed)\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1m, test_1m = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.93718412338794, 0.25303643724696356)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_1m), get_sparsity(test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movielens-100k dataset\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_100k, test_100k = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 0.5945303210463734)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((943, 1682), (6040, 3952))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_100k.shape, train_1m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitMF():\n",
    "    def __init__(self, \n",
    "                 ratings,\n",
    "                 n_factors=40,\n",
    "                 learning='sgd',\n",
    "                 item_fact_reg=0.0, \n",
    "                 user_fact_reg=0.0,\n",
    "                 item_bias_reg=0.0,\n",
    "                 user_bias_reg=0.0,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict empty \n",
    "        entries in a matrix. The terminology assumes a \n",
    "        ratings matrix which is ~ user x item\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors to use in matrix \n",
    "            factorization model\n",
    "        learning : (str)\n",
    "            Method of optimization. Options include \n",
    "            'sgd' or 'als'.\n",
    "        \n",
    "        item_fact_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_fact_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "            \n",
    "        item_bias_reg : (float)\n",
    "            Regularization term for item biases\n",
    "        \n",
    "        user_bias_reg : (float)\n",
    "            Regularization term for user biases\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_fact_reg = item_fact_reg\n",
    "        self.user_fact_reg = user_fact_reg\n",
    "        self.item_bias_reg = item_bias_reg\n",
    "        self.user_bias_reg = user_bias_reg\n",
    "        self.learning = learning\n",
    "        if self.learning == 'sgd':\n",
    "            self.sample_row, self.sample_col = self.ratings.nonzero()\n",
    "            self.n_samples = len(self.sample_row)\n",
    "        self._v = verbose\n",
    "\n",
    "    def als_step(self,\n",
    "                 latent_vectors,\n",
    "                 fixed_vecs,\n",
    "                 ratings,\n",
    "                 _lambda,\n",
    "                 type='user'):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        One of the two ALS steps. Solve for the latent vectors\n",
    "        specified by type.\n",
    "        \"\"\"\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), \n",
    "                                             ratings[u, :].dot(fixed_vecs))\n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            \n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), \n",
    "                                             ratings[:, i].T.dot(fixed_vecs))\n",
    "        return latent_vectors\n",
    "\n",
    "    def train(self, n_iter=10, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors        \n",
    "        self.user_vecs = np.random.normal(scale=1./self.n_factors,\\\n",
    "                                          size=(self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.normal(scale=1./self.n_factors,\n",
    "                                          size=(self.n_items, self.n_factors))\n",
    "        \n",
    "        if self.learning == 'als':\n",
    "            self.partial_train(n_iter)\n",
    "        elif self.learning == 'sgd':\n",
    "            self.learning_rate = learning_rate\n",
    "            self.user_bias = np.zeros(self.n_users)\n",
    "            self.item_bias = np.zeros(self.n_items)\n",
    "            self.global_bias = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "            self.partial_train(n_iter)\n",
    "    \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\" \n",
    "        Train model for n_iter iterations. Can be \n",
    "        called multiple times for further training.\n",
    "        \"\"\"\n",
    "        ctr = 1\n",
    "        while ctr <= n_iter:\n",
    "            if ctr % 10 == 0 and self._v:\n",
    "                print ('\\tcurrent iteration: {}'.format(ctr))\n",
    "            if self.learning == 'als':\n",
    "                self.user_vecs = self.als_step(self.user_vecs, \n",
    "                                               self.item_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.user_fact_reg, \n",
    "                                               type='user')\n",
    "                self.item_vecs = self.als_step(self.item_vecs, \n",
    "                                               self.user_vecs, \n",
    "                                               self.ratings, \n",
    "                                               self.item_fact_reg, \n",
    "                                               type='item')\n",
    "            elif self.learning == 'sgd':\n",
    "                self.training_indices = np.arange(self.n_samples)\n",
    "                np.random.shuffle(self.training_indices)\n",
    "                self.sgd()\n",
    "            ctr += 1\n",
    "\n",
    "    def sgd(self):\n",
    "        np.random.seed(seed)\n",
    "        for idx in self.training_indices:\n",
    "            u = self.sample_row[idx]\n",
    "            i = self.sample_col[idx]\n",
    "            prediction = self.predict(u, i)\n",
    "            e = (self.ratings[u,i] - prediction) # error\n",
    "            \n",
    "            # Update biases\n",
    "            self.user_bias[u] += self.learning_rate * \\\n",
    "                                (e - self.user_bias_reg * self.user_bias[u])\n",
    "            self.item_bias[i] += self.learning_rate * \\\n",
    "                                (e - self.item_bias_reg * self.item_bias[i])\n",
    "            \n",
    "            #Update latent factors\n",
    "            self.user_vecs[u, :] += self.learning_rate * \\\n",
    "                                    (e * self.item_vecs[i, :] - \\\n",
    "                                     self.user_fact_reg * self.user_vecs[u,:])\n",
    "            self.item_vecs[i, :] += self.learning_rate * \\\n",
    "                                    (e * self.user_vecs[u, :] - \\\n",
    "                                     self.item_fact_reg * self.item_vecs[i,:])\n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction.\"\"\"\n",
    "        if self.learning == 'als':\n",
    "            return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "        elif self.learning == 'sgd':\n",
    "            prediction = self.global_bias + self.user_bias[u] + self.item_bias[i]\n",
    "            prediction += self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "            return prediction\n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item.\"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], \n",
    "                                self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    def calculate_learning_curve(self, iter_array, test, learning_rate=0.1):\n",
    "        np.random.seed(seed)\n",
    "        \"\"\"\n",
    "        Keep track of MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        iter_array : (list)\n",
    "            List of numbers of iterations to train for each step of \n",
    "            the learning curve. e.g. [1, 5, 10, 20]\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be user x item).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iter_array\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iter_array\n",
    "        \"\"\"\n",
    "        iter_array.sort()\n",
    "        self.train_mse =[]\n",
    "        self.test_mse = []\n",
    "        iter_diff = 0\n",
    "        for (i, n_iter) in enumerate(iter_array):\n",
    "            if self._v:\n",
    "                print ('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                self.train(n_iter - iter_diff, learning_rate)\n",
    "            else:\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse += [get_mse(predictions, test)]\n",
    "            if self._v:\n",
    "                print ('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print ('Test mse: ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 8.162626454488155\n",
      "Test mse: 11.074297483666612\n",
      "Iteration: 2\n",
      "Train mse: 5.710628199541684\n",
      "Test mse: 8.654294898028265\n",
      "Iteration: 5\n",
      "Train mse: 5.421378301611028\n",
      "Test mse: 8.237957326989877\n",
      "Iteration: 10\n",
      "Train mse: 5.3966006950612435\n",
      "Test mse: 8.199281754444268\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.3948135876025365\n",
      "Test mse: 8.195283711199771\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394753065812939\n",
      "Test mse: 8.195083034461906\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.394526731799798\n",
      "Test mse: 8.194863821632225\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def plot_learning_curve(iter_array, model):\n",
    "    plt.plot(iter_array, model.train_mse, \\\n",
    "             label='Training', linewidth=5)\n",
    "    plt.plot(iter_array, model.test_mse, \\\n",
    "             label='Test', linewidth=5)\n",
    "\n",
    "\n",
    "    plt.xticks(fontsize=16);\n",
    "    plt.yticks(fontsize=16);\n",
    "    plt.xlabel('iterations', fontsize=30);\n",
    "    plt.ylabel('MSE', fontsize=30);\n",
    "    plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(iter_array, MF_ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 8.133017313467287\n",
      "Test mse: 11.385478952329686\n",
      "Iteration: 2\n",
      "Train mse: 6.181422331570321\n",
      "Test mse: 9.510432053967698\n",
      "Iteration: 5\n",
      "Train mse: 5.945868319505223\n",
      "Test mse: 9.181499199125527\n",
      "Iteration: 10\n",
      "Train mse: 5.929921005207816\n",
      "Test mse: 9.163591900058499\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.922879579194776\n",
      "Test mse: 9.155637570112932\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.921741164281515\n",
      "Test mse: 9.152673442345948\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.921434636456923\n",
      "Test mse: 9.152593151189302\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_1m, n_factors=20, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142807793060997\n",
      "Test mse: 1.1796269767247503\n",
      "Iteration: 2\n",
      "Train mse: 1.0730325987217215\n",
      "Test mse: 1.1283621352778417\n",
      "Iteration: 5\n",
      "Train mse: 0.9767157588922124\n",
      "Test mse: 1.0499014864558258\n",
      "Iteration: 10\n",
      "Train mse: 0.9190727366615857\n",
      "Test mse: 0.9988387536431501\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8669457204379917\n",
      "Test mse: 0.9525818812444498\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8387251669562131\n",
      "Test mse: 0.9343161812537658\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7485848531412769\n",
      "Test mse: 0.9170005429005551\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40410579696377263\n",
      "Test mse: 0.9148070379768369\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Proposed GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() == True:\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mask = (train_100k == 0)\n",
    "positive_feedback = (train_100k > 3)\n",
    "negative_feedback = ((train_100k < 4) * (1 - zero_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49901, 40669)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_feedback.sum(), negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (positive_feedback + negative_feedback != zero_mask).all()\n",
    "assert (positive_feedback + negative_feedback == 1 - zero_mask).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94.28986095682184, 3.146093059441684, 2.5640459837364746)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(zero_mask), get_sparsity(positive_feedback), get_sparsity(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, mat, p=0.4, batch_size=64):\n",
    "        '''\n",
    "        mat is a binary matrix (e.g. positive feedback, or negative feedback)\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.mat = mat\n",
    "        self.p = p\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.mat.shape[0] / self.batch_size))\n",
    "    \n",
    "    def gen_item_GAN(self):\n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y, indexes\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        indexes = np.random.randint(self.mat.shape[0], size=self.batch_size)\n",
    "\n",
    "        mask = (np.random.rand(self.batch_size, self.mat.shape[1]) > self.p)\n",
    "        \n",
    "        y = self.mat[indexes]\n",
    "        X = self.mat[indexes]*mask # corrupting\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_negative = DataGenerator(negative_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _ = generator_negative.gen_item_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 1682), (64, 1682))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NetD(torch.nn.Module):\n",
    "    def __init__(self, feat_size):\n",
    "        super(NetD, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "#         self.use_cuda = True\n",
    "#         self.feat_size = feat_size\n",
    "        # top\n",
    "#         print(self.feat_size*2)\n",
    "        self.t1 = torch.nn.Linear(self.feat_size, 512)\n",
    "        # bottom\n",
    "        self.b1 = torch.nn.Linear(self.feat_size, 512)\n",
    "        # combined\n",
    "        self.fc = torch.nn.Linear(2 * 512, self.feat_size)\n",
    "    def forward(self, xr, xf):\n",
    "        # get filt\n",
    "        \n",
    "        filt = 1 - (xr * (xf >= 0.5).float()) - ((1-xr) * (xf < 0.5).float())\n",
    "        # random swap\n",
    "        idr = torch.multinomial(torch.Tensor([0.5,0.5]), xr.size(0), replacement=True)\n",
    "        idrx = idr.float().unsqueeze(1).expand_as(xr)\n",
    "#         if self.use_cuda: \n",
    "        idrx = idrx.cuda()\n",
    "        idrx = Variable(idrx)\n",
    "        xt = xr * idrx + xf * (1 - idrx)\n",
    "        xb = xr * (1 - idrx) + xf * idrx\n",
    "        # top : real\n",
    "        xt = F.relu(self.t1(xt))\n",
    "        # bottom : fake\n",
    "        xb = F.relu(self.b1(xb))\n",
    "        # combined\n",
    "        x = torch.cat((xt, xb), 1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        # apply filter, aggregate\n",
    "        x = filt * x\n",
    "        x = x.mean(dim = 1).squeeze()\n",
    "        # use sign, because of swapping\n",
    "        sgn = idr * 2 - 1\n",
    "        sgn = sgn.cuda()\n",
    "        sgn = Variable(sgn.float())\n",
    "        x = sgn * x\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_size):\n",
    "\n",
    "        super(NetG, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.netGen = torch.nn.Sequential( \n",
    "                                torch.nn.Linear(nz + self.feat_size, 512), \n",
    "#                                 torch.nn.BatchNorm1d(1024),\n",
    "#                                 torch.nn.ReLU(), \n",
    "# #                                 nn.Dropout(0.5),\n",
    "#                                 torch.nn.Linear(2048, 2048),\n",
    "                                torch.nn.ReLU(), \n",
    "#                                 torch.nn.BatchNorm1d(512),\n",
    "#                                 nn.Dropout(0.6),\n",
    "                                torch.nn.Linear(512, self.feat_size), \n",
    "                                torch.nn.Sigmoid()\n",
    "#                                 torch.nn.BatchNorm1d(features_length),\n",
    "#                                 nn.Dropout(0.7),\n",
    "#                                 torch.nn.Sigmoid()\n",
    "                                )\n",
    "\n",
    "        \n",
    "    def forward(self, e_mask, x):\n",
    "        x = self.netGen(x)\n",
    "#         print(x.shape, )\n",
    "        x = x * e_mask\n",
    "        return x\n",
    "#         return F.dropout(x, 0.7)\n",
    "#         return 5 * self.netGen(x)\n",
    "#         return torch.sigmoid(x) \n",
    "#         return x*5 # to get values in range [0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_losses = []\n",
    "disc_losses = []\n",
    "def train_GAN(netD, netG, negative, tr, steps_per_epoch = 200, epochs = 10):\n",
    "    d_iter = 5\n",
    "    g_iter = 1\n",
    "    gen_iterations = 0\n",
    "#     gen_losses = []\n",
    "#     disc_losses = []\n",
    "#     train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for c in range(steps_per_epoch):\n",
    "            data_iter = 100\n",
    "            i = 0\n",
    "#             while i < 100:\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            for p in netD.parameters(): # reset requires_grad\n",
    "                p.requires_grad = True # they are set to False below in netG update\n",
    "#             d_iter = d_iter\n",
    "            j = 0\n",
    "            while j < d_iter*5:\n",
    "                j += 1\n",
    "                # load real data\n",
    "                i += 1\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "#                         condition, X, idxs = batch_generator(X_neg, y_neg)\n",
    "#                 X, _ = data_iter.next()\n",
    "#                 X = X.view(X.size(0), -1)\n",
    "#                 X = (X >= 0.5).float()\n",
    "#                     if cuda: \n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "#                     X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "#     + torch.randn(X.size()).cuda() * 0.2\n",
    "#                 print(condition.shape, X_neg.shape, y_neg.shape)\n",
    "                real = Variable(X)\n",
    "\n",
    "                # generate fake data\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "#                     if cuda: \n",
    "                noise = noise.cuda()\n",
    "#                     noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "                with torch.no_grad():\n",
    "                    noisev = Variable(noise) # totally freeze netG\n",
    "                concated = torch.cat((noisev, condition), 1)\n",
    "#                 print(condition.shape, condition.shape, X.shape, noisev.shape, )\n",
    "                e_mask = torch.Tensor(tr[idxs]>0).cuda()\n",
    "#                     print(e_mask.shape, concated.shape, condition.shape)\n",
    "                fake = Variable(netG(e_mask, concated).data)\n",
    "\n",
    "                # compute gradient, take step\n",
    "                netD.zero_grad()\n",
    "#                 concated_real = torch.cat((real, condition), 1)\n",
    "#                 print(concated_real)\n",
    "                out = netD(real, fake)\n",
    "                outputD = torch.mean(out) + lamba * out.norm()\n",
    "                stdD = torch.std(out)\n",
    "                outputD.backward(mone)\n",
    "                optimizerD.step()\n",
    "#                     print('AAAAAAAAA mse:=WWWWWWWWWWWWWWWWWWWWWW')\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "\n",
    "#         g_iter = g_iter\n",
    "            j = 0\n",
    "            while j < g_iter:\n",
    "                j += 1\n",
    "                for p in netD.parameters():\n",
    "                    p.requires_grad = False # to avoid computation\n",
    "                netG.zero_grad()\n",
    "                # load real data\n",
    "\n",
    "                i += 1\n",
    "\n",
    "                if negative:\n",
    "                    condition, X, idxs = generator_negative.gen_item_GAN()\n",
    "\n",
    "                X = torch.from_numpy(X).float().cuda()\n",
    "    #                 X = X + torch.randn(X.size()).cuda() * 0.2\n",
    "                condition = torch.from_numpy(condition).float().cuda() \n",
    "                real = Variable(X)\n",
    "\n",
    "                # update generator\n",
    "                noise = torch.randn(batch_size, nz)\n",
    "                noise = noise.cuda()\n",
    "                noisev = Variable(noise)\n",
    "                concated_ = torch.cat((noisev, condition), 1)\n",
    "                e_mask_ = torch.Tensor(tr[idxs]>0).cuda()\n",
    "\n",
    "                fake = netG(e_mask_, concated_)\n",
    "                out = netD(real, fake)\n",
    "                outputG = torch.mean(out) + lamba * out.norm()\n",
    "                stdG = torch.std(out)\n",
    "                outputG.backward(one)\n",
    "                optimizerG.step()\n",
    "                gen_iterations += 1\n",
    "    #             print('AAAAAA')\n",
    "                eval_loss = F.mse_loss(fake, real, reduction='mean')\n",
    "    #             eval_losses.append(eval_loss)\n",
    "    #             print('mse:', eval_loss)\n",
    "    #             print(outputG.item(), outputD.item())\n",
    "                gen_losses.append(outputG.item())\n",
    "                disc_losses.append(outputD.item())\n",
    "                print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f '% (epoch, epochs, i, 100, gen_iterations, outputD.item(), outputG.item()), eval_loss)\n",
    "    return gen_losses, disc_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrD = 5e-4\n",
    "lrG = 5e-4\n",
    "batch_size = 64\n",
    "cuda = True\n",
    "epochs = 10\n",
    "# device = 5\n",
    "seed = 1\n",
    "nz = 8\n",
    "lamba = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetD(\n",
      "  (t1): Linear(in_features=1682, out_features=512, bias=True)\n",
      "  (b1): Linear(in_features=1682, out_features=512, bias=True)\n",
      "  (fc): Linear(in_features=1024, out_features=1682, bias=True)\n",
      ")\n",
      "NetG(\n",
      "  (netGen): Sequential(\n",
      "    (0): Linear(in_features=1690, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=1682, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# networks\n",
    "netD_neg = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg = NetG(train_100k.shape[1]).cuda()\n",
    "print(netD_neg)\n",
    "print(netG_neg)\n",
    "optimizerG = optim.RMSprop(netG_neg.parameters(), lr=lrG, weight_decay=1e-4)\n",
    "optimizerD = optim.RMSprop(netD_neg.parameters(), lr=lrD, weight_decay=1e-4)\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = (-1 * one).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][26/100][1] Loss_D: 0.015396 Loss_G: 0.012399 \n",
      "[0/10][26/100][2] Loss_D: 0.008699 Loss_G: 0.009984 \n",
      "[0/10][26/100][3] Loss_D: 0.010232 Loss_G: 0.012696 \n",
      "[0/10][26/100][4] Loss_D: 0.007697 Loss_G: 0.004789 \n",
      "[0/10][26/100][5] Loss_D: 0.002852 Loss_G: 0.003720 \n",
      "[0/10][26/100][6] Loss_D: 0.001300 Loss_G: 0.001948 \n",
      "[0/10][26/100][7] Loss_D: 0.003648 Loss_G: 0.002399 \n",
      "[0/10][26/100][8] Loss_D: 0.001874 Loss_G: 0.003110 \n",
      "[0/10][26/100][9] Loss_D: 0.001165 Loss_G: 0.001428 \n",
      "[0/10][26/100][10] Loss_D: 0.001839 Loss_G: 0.002717 \n",
      "[0/10][26/100][11] Loss_D: 0.002029 Loss_G: 0.001121 \n",
      "[0/10][26/100][12] Loss_D: 0.001037 Loss_G: -0.000022 \n",
      "[0/10][26/100][13] Loss_D: 0.000467 Loss_G: 0.000503 \n",
      "[0/10][26/100][14] Loss_D: 0.002084 Loss_G: 0.002133 \n",
      "[0/10][26/100][15] Loss_D: 0.001187 Loss_G: 0.000905 \n",
      "[0/10][26/100][16] Loss_D: 0.000913 Loss_G: 0.002143 \n",
      "[0/10][26/100][17] Loss_D: 0.005114 Loss_G: 0.001427 \n",
      "[0/10][26/100][18] Loss_D: 0.003440 Loss_G: 0.002312 \n",
      "[0/10][26/100][19] Loss_D: 0.001970 Loss_G: 0.002330 \n",
      "[0/10][26/100][20] Loss_D: 0.002683 Loss_G: 0.000356 \n",
      "[0/10][26/100][21] Loss_D: 0.003398 Loss_G: 0.001479 \n",
      "[0/10][26/100][22] Loss_D: 0.004616 Loss_G: 0.003429 \n",
      "[0/10][26/100][23] Loss_D: 0.002514 Loss_G: 0.003152 \n",
      "[0/10][26/100][24] Loss_D: 0.002706 Loss_G: 0.001956 \n",
      "[0/10][26/100][25] Loss_D: 0.001575 Loss_G: 0.002517 \n",
      "[0/10][26/100][26] Loss_D: 0.001971 Loss_G: 0.004477 \n",
      "[0/10][26/100][27] Loss_D: 0.002128 Loss_G: 0.003369 \n",
      "[0/10][26/100][28] Loss_D: 0.003767 Loss_G: 0.002828 \n",
      "[0/10][26/100][29] Loss_D: 0.004764 Loss_G: 0.002792 \n",
      "[0/10][26/100][30] Loss_D: 0.001255 Loss_G: 0.002290 \n",
      "[0/10][26/100][31] Loss_D: 0.001118 Loss_G: 0.001991 \n",
      "[0/10][26/100][32] Loss_D: 0.003251 Loss_G: 0.002075 \n",
      "[0/10][26/100][33] Loss_D: 0.001590 Loss_G: 0.001594 \n",
      "[0/10][26/100][34] Loss_D: 0.000982 Loss_G: 0.003058 \n",
      "[0/10][26/100][35] Loss_D: 0.005063 Loss_G: -0.000976 \n",
      "[0/10][26/100][36] Loss_D: 0.003605 Loss_G: 0.003342 \n",
      "[0/10][26/100][37] Loss_D: 0.001352 Loss_G: 0.002059 \n",
      "[0/10][26/100][38] Loss_D: 0.004509 Loss_G: 0.004062 \n",
      "[0/10][26/100][39] Loss_D: 0.001611 Loss_G: 0.004333 \n",
      "[0/10][26/100][40] Loss_D: 0.002724 Loss_G: 0.005663 \n",
      "[0/10][26/100][41] Loss_D: 0.001453 Loss_G: 0.002492 \n",
      "[0/10][26/100][42] Loss_D: 0.001329 Loss_G: 0.001825 \n",
      "[0/10][26/100][43] Loss_D: 0.004419 Loss_G: 0.004652 \n",
      "[0/10][26/100][44] Loss_D: 0.001511 Loss_G: 0.000823 \n",
      "[0/10][26/100][45] Loss_D: 0.004014 Loss_G: 0.004798 \n",
      "[0/10][26/100][46] Loss_D: 0.001740 Loss_G: 0.001904 \n",
      "[0/10][26/100][47] Loss_D: 0.002114 Loss_G: 0.002247 \n",
      "[0/10][26/100][48] Loss_D: 0.002432 Loss_G: 0.002182 \n",
      "[0/10][26/100][49] Loss_D: 0.003106 Loss_G: 0.003064 \n",
      "[0/10][26/100][50] Loss_D: 0.002457 Loss_G: 0.003863 \n",
      "[0/10][26/100][51] Loss_D: 0.003679 Loss_G: 0.001456 \n",
      "[0/10][26/100][52] Loss_D: 0.002794 Loss_G: 0.002696 \n",
      "[0/10][26/100][53] Loss_D: 0.003719 Loss_G: 0.002600 \n",
      "[0/10][26/100][54] Loss_D: 0.003187 Loss_G: 0.004338 \n",
      "[0/10][26/100][55] Loss_D: 0.002752 Loss_G: 0.005054 \n",
      "[0/10][26/100][56] Loss_D: 0.005331 Loss_G: 0.002348 \n",
      "[0/10][26/100][57] Loss_D: 0.005581 Loss_G: 0.002214 \n",
      "[0/10][26/100][58] Loss_D: 0.002546 Loss_G: 0.002701 \n",
      "[0/10][26/100][59] Loss_D: 0.002447 Loss_G: 0.003260 \n",
      "[0/10][26/100][60] Loss_D: 0.002093 Loss_G: 0.002233 \n",
      "[0/10][26/100][61] Loss_D: 0.001220 Loss_G: 0.003107 \n",
      "[0/10][26/100][62] Loss_D: 0.003240 Loss_G: 0.000936 \n",
      "[0/10][26/100][63] Loss_D: 0.003147 Loss_G: 0.003763 \n",
      "[0/10][26/100][64] Loss_D: 0.002225 Loss_G: 0.003726 \n",
      "[0/10][26/100][65] Loss_D: 0.003249 Loss_G: 0.002099 \n",
      "[0/10][26/100][66] Loss_D: 0.004395 Loss_G: 0.003927 \n",
      "[0/10][26/100][67] Loss_D: 0.002671 Loss_G: 0.003174 \n",
      "[0/10][26/100][68] Loss_D: 0.004313 Loss_G: 0.002985 \n",
      "[0/10][26/100][69] Loss_D: 0.003293 Loss_G: 0.002054 \n",
      "[0/10][26/100][70] Loss_D: 0.001496 Loss_G: 0.003269 \n",
      "[0/10][26/100][71] Loss_D: 0.001787 Loss_G: 0.003404 \n",
      "[0/10][26/100][72] Loss_D: 0.002026 Loss_G: 0.004093 \n",
      "[0/10][26/100][73] Loss_D: 0.002444 Loss_G: 0.005453 \n",
      "[0/10][26/100][74] Loss_D: 0.000796 Loss_G: 0.002373 \n",
      "[0/10][26/100][75] Loss_D: 0.002518 Loss_G: 0.002672 \n",
      "[0/10][26/100][76] Loss_D: 0.003974 Loss_G: 0.001957 \n",
      "[0/10][26/100][77] Loss_D: 0.002191 Loss_G: 0.003017 \n",
      "[0/10][26/100][78] Loss_D: 0.003189 Loss_G: 0.003614 \n",
      "[0/10][26/100][79] Loss_D: 0.001700 Loss_G: 0.003429 \n",
      "[0/10][26/100][80] Loss_D: 0.000193 Loss_G: 0.003528 \n",
      "[0/10][26/100][81] Loss_D: 0.003931 Loss_G: 0.002791 \n",
      "[0/10][26/100][82] Loss_D: 0.004077 Loss_G: 0.002786 \n",
      "[0/10][26/100][83] Loss_D: 0.004234 Loss_G: 0.003731 \n",
      "[0/10][26/100][84] Loss_D: 0.003436 Loss_G: 0.003548 \n",
      "[0/10][26/100][85] Loss_D: 0.002203 Loss_G: 0.003017 \n",
      "[0/10][26/100][86] Loss_D: 0.003179 Loss_G: 0.003716 \n",
      "[0/10][26/100][87] Loss_D: 0.003323 Loss_G: 0.002709 \n",
      "[0/10][26/100][88] Loss_D: 0.003886 Loss_G: 0.001440 \n",
      "[0/10][26/100][89] Loss_D: 0.003226 Loss_G: 0.004230 \n",
      "[0/10][26/100][90] Loss_D: 0.003641 Loss_G: 0.002886 \n",
      "[0/10][26/100][91] Loss_D: 0.003522 Loss_G: 0.002323 \n",
      "[0/10][26/100][92] Loss_D: 0.004424 Loss_G: 0.001882 \n",
      "[0/10][26/100][93] Loss_D: 0.006579 Loss_G: 0.003337 \n",
      "[0/10][26/100][94] Loss_D: 0.002973 Loss_G: 0.000238 \n",
      "[0/10][26/100][95] Loss_D: 0.003709 Loss_G: 0.005404 \n",
      "[0/10][26/100][96] Loss_D: 0.005483 Loss_G: 0.002807 \n",
      "[0/10][26/100][97] Loss_D: 0.004259 Loss_G: 0.004288 \n",
      "[0/10][26/100][98] Loss_D: 0.001522 Loss_G: 0.002529 \n",
      "[0/10][26/100][99] Loss_D: 0.002189 Loss_G: 0.004709 \n",
      "[0/10][26/100][100] Loss_D: 0.001829 Loss_G: 0.004343 \n",
      "[0/10][26/100][101] Loss_D: 0.006062 Loss_G: 0.006248 \n",
      "[0/10][26/100][102] Loss_D: 0.002499 Loss_G: 0.003663 \n",
      "[0/10][26/100][103] Loss_D: 0.001266 Loss_G: 0.003362 \n",
      "[0/10][26/100][104] Loss_D: 0.003422 Loss_G: 0.003810 \n",
      "[0/10][26/100][105] Loss_D: 0.004041 Loss_G: 0.004914 \n",
      "[0/10][26/100][106] Loss_D: 0.001696 Loss_G: 0.004496 \n",
      "[0/10][26/100][107] Loss_D: 0.002772 Loss_G: 0.004670 \n",
      "[0/10][26/100][108] Loss_D: 0.003354 Loss_G: 0.004373 \n",
      "[0/10][26/100][109] Loss_D: 0.000982 Loss_G: 0.004363 \n",
      "[0/10][26/100][110] Loss_D: 0.002981 Loss_G: 0.003296 \n",
      "[0/10][26/100][111] Loss_D: 0.002518 Loss_G: 0.003114 \n",
      "[0/10][26/100][112] Loss_D: 0.004183 Loss_G: 0.004366 \n",
      "[0/10][26/100][113] Loss_D: 0.004294 Loss_G: 0.004313 \n",
      "[0/10][26/100][114] Loss_D: 0.003539 Loss_G: 0.004678 \n",
      "[0/10][26/100][115] Loss_D: 0.003206 Loss_G: 0.002047 \n",
      "[0/10][26/100][116] Loss_D: 0.003964 Loss_G: 0.004334 \n",
      "[0/10][26/100][117] Loss_D: 0.001858 Loss_G: 0.002287 \n",
      "[0/10][26/100][118] Loss_D: 0.002235 Loss_G: 0.001368 \n",
      "[0/10][26/100][119] Loss_D: 0.001725 Loss_G: 0.004655 \n",
      "[0/10][26/100][120] Loss_D: 0.003855 Loss_G: 0.000858 \n",
      "[0/10][26/100][121] Loss_D: 0.002764 Loss_G: 0.001902 \n",
      "[0/10][26/100][122] Loss_D: 0.001367 Loss_G: 0.001736 \n",
      "[0/10][26/100][123] Loss_D: 0.004296 Loss_G: 0.002594 \n",
      "[0/10][26/100][124] Loss_D: 0.003311 Loss_G: 0.002509 \n",
      "[0/10][26/100][125] Loss_D: 0.002318 Loss_G: 0.002469 \n",
      "[0/10][26/100][126] Loss_D: 0.002765 Loss_G: 0.005119 \n",
      "[0/10][26/100][127] Loss_D: 0.001633 Loss_G: 0.004719 \n",
      "[0/10][26/100][128] Loss_D: 0.002465 Loss_G: 0.004062 \n",
      "[0/10][26/100][129] Loss_D: 0.004204 Loss_G: 0.002930 \n",
      "[0/10][26/100][130] Loss_D: 0.002189 Loss_G: 0.002334 \n",
      "[0/10][26/100][131] Loss_D: 0.002089 Loss_G: 0.004550 \n",
      "[0/10][26/100][132] Loss_D: 0.003321 Loss_G: 0.006101 \n",
      "[0/10][26/100][133] Loss_D: 0.003409 Loss_G: 0.002272 \n",
      "[0/10][26/100][134] Loss_D: 0.001471 Loss_G: 0.003933 \n",
      "[0/10][26/100][135] Loss_D: 0.003469 Loss_G: 0.005808 \n",
      "[0/10][26/100][136] Loss_D: 0.001542 Loss_G: 0.002615 \n",
      "[0/10][26/100][137] Loss_D: 0.003553 Loss_G: 0.003774 \n",
      "[0/10][26/100][138] Loss_D: 0.003743 Loss_G: 0.004906 \n",
      "[0/10][26/100][139] Loss_D: 0.004094 Loss_G: 0.003999 \n",
      "[0/10][26/100][140] Loss_D: 0.003618 Loss_G: 0.001596 \n",
      "[0/10][26/100][141] Loss_D: 0.004723 Loss_G: 0.004121 \n",
      "[0/10][26/100][142] Loss_D: 0.002782 Loss_G: 0.002790 \n",
      "[0/10][26/100][143] Loss_D: 0.003007 Loss_G: 0.003645 \n",
      "[0/10][26/100][144] Loss_D: 0.003095 Loss_G: 0.003407 \n",
      "[0/10][26/100][145] Loss_D: 0.001295 Loss_G: 0.002476 \n",
      "[0/10][26/100][146] Loss_D: 0.000251 Loss_G: 0.002395 \n",
      "[0/10][26/100][147] Loss_D: 0.003687 Loss_G: 0.003249 \n",
      "[0/10][26/100][148] Loss_D: 0.002798 Loss_G: 0.003512 \n",
      "[0/10][26/100][149] Loss_D: 0.004171 Loss_G: 0.002722 \n",
      "[0/10][26/100][150] Loss_D: 0.003549 Loss_G: 0.002204 \n",
      "[0/10][26/100][151] Loss_D: 0.003890 Loss_G: 0.004833 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][26/100][152] Loss_D: 0.005536 Loss_G: 0.003550 \n",
      "[0/10][26/100][153] Loss_D: 0.002474 Loss_G: 0.001116 \n",
      "[0/10][26/100][154] Loss_D: 0.004742 Loss_G: 0.003278 \n",
      "[0/10][26/100][155] Loss_D: 0.003442 Loss_G: 0.004659 \n",
      "[0/10][26/100][156] Loss_D: 0.003816 Loss_G: 0.002365 \n",
      "[0/10][26/100][157] Loss_D: 0.002468 Loss_G: 0.001261 \n",
      "[0/10][26/100][158] Loss_D: 0.002932 Loss_G: 0.004094 \n",
      "[0/10][26/100][159] Loss_D: 0.002777 Loss_G: 0.002601 \n",
      "[0/10][26/100][160] Loss_D: 0.000639 Loss_G: 0.003937 \n",
      "[0/10][26/100][161] Loss_D: 0.003193 Loss_G: 0.004437 \n",
      "[0/10][26/100][162] Loss_D: 0.003551 Loss_G: 0.002200 \n",
      "[0/10][26/100][163] Loss_D: 0.003474 Loss_G: 0.003950 \n",
      "[0/10][26/100][164] Loss_D: 0.004544 Loss_G: 0.003059 \n",
      "[0/10][26/100][165] Loss_D: 0.001576 Loss_G: 0.003488 \n",
      "[0/10][26/100][166] Loss_D: 0.003523 Loss_G: 0.003235 \n",
      "[0/10][26/100][167] Loss_D: 0.002024 Loss_G: 0.002194 \n",
      "[0/10][26/100][168] Loss_D: 0.001142 Loss_G: 0.005256 \n",
      "[0/10][26/100][169] Loss_D: 0.005600 Loss_G: 0.002734 \n",
      "[0/10][26/100][170] Loss_D: 0.001416 Loss_G: 0.003424 \n",
      "[0/10][26/100][171] Loss_D: 0.003393 Loss_G: 0.001240 \n",
      "[0/10][26/100][172] Loss_D: 0.003495 Loss_G: 0.003376 \n",
      "[0/10][26/100][173] Loss_D: 0.002716 Loss_G: 0.001195 \n",
      "[0/10][26/100][174] Loss_D: 0.004954 Loss_G: 0.001793 \n",
      "[0/10][26/100][175] Loss_D: 0.004187 Loss_G: 0.001107 \n",
      "[0/10][26/100][176] Loss_D: 0.002672 Loss_G: 0.003053 \n",
      "[0/10][26/100][177] Loss_D: 0.004144 Loss_G: 0.006476 \n",
      "[0/10][26/100][178] Loss_D: 0.002613 Loss_G: 0.004063 \n",
      "[0/10][26/100][179] Loss_D: 0.002505 Loss_G: 0.003282 \n",
      "[0/10][26/100][180] Loss_D: 0.001779 Loss_G: 0.003162 \n",
      "[0/10][26/100][181] Loss_D: 0.001603 Loss_G: 0.005629 \n",
      "[0/10][26/100][182] Loss_D: 0.002557 Loss_G: 0.002718 \n",
      "[0/10][26/100][183] Loss_D: 0.003060 Loss_G: 0.003659 \n",
      "[0/10][26/100][184] Loss_D: 0.000917 Loss_G: 0.000946 \n",
      "[0/10][26/100][185] Loss_D: 0.003564 Loss_G: 0.003028 \n",
      "[0/10][26/100][186] Loss_D: 0.002526 Loss_G: 0.002020 \n",
      "[0/10][26/100][187] Loss_D: 0.001974 Loss_G: 0.002992 \n",
      "[0/10][26/100][188] Loss_D: 0.001537 Loss_G: 0.004211 \n",
      "[0/10][26/100][189] Loss_D: 0.001903 Loss_G: 0.004031 \n",
      "[0/10][26/100][190] Loss_D: 0.000974 Loss_G: 0.001990 \n",
      "[0/10][26/100][191] Loss_D: 0.005132 Loss_G: 0.003408 \n",
      "[0/10][26/100][192] Loss_D: 0.003850 Loss_G: 0.000908 \n",
      "[0/10][26/100][193] Loss_D: 0.003786 Loss_G: 0.001683 \n",
      "[0/10][26/100][194] Loss_D: 0.003023 Loss_G: 0.003126 \n",
      "[0/10][26/100][195] Loss_D: 0.001831 Loss_G: 0.001733 \n",
      "[0/10][26/100][196] Loss_D: 0.002256 Loss_G: 0.005580 \n",
      "[0/10][26/100][197] Loss_D: 0.004649 Loss_G: 0.003733 \n",
      "[0/10][26/100][198] Loss_D: 0.002239 Loss_G: 0.002411 \n",
      "[0/10][26/100][199] Loss_D: 0.004605 Loss_G: 0.004446 \n",
      "[0/10][26/100][200] Loss_D: 0.002830 Loss_G: 0.004455 \n",
      "[1/10][26/100][201] Loss_D: 0.002137 Loss_G: 0.003136 \n",
      "[1/10][26/100][202] Loss_D: 0.002831 Loss_G: 0.003501 \n",
      "[1/10][26/100][203] Loss_D: 0.002571 Loss_G: 0.003591 \n",
      "[1/10][26/100][204] Loss_D: 0.002972 Loss_G: 0.002084 \n",
      "[1/10][26/100][205] Loss_D: 0.002121 Loss_G: 0.003139 \n",
      "[1/10][26/100][206] Loss_D: 0.004659 Loss_G: 0.004246 \n",
      "[1/10][26/100][207] Loss_D: 0.002615 Loss_G: 0.002040 \n",
      "[1/10][26/100][208] Loss_D: 0.002654 Loss_G: 0.000957 \n",
      "[1/10][26/100][209] Loss_D: 0.002299 Loss_G: 0.001996 \n",
      "[1/10][26/100][210] Loss_D: 0.002673 Loss_G: 0.002364 \n",
      "[1/10][26/100][211] Loss_D: 0.002298 Loss_G: 0.002782 \n",
      "[1/10][26/100][212] Loss_D: 0.002053 Loss_G: 0.003208 \n",
      "[1/10][26/100][213] Loss_D: 0.001783 Loss_G: 0.002631 \n",
      "[1/10][26/100][214] Loss_D: 0.001985 Loss_G: 0.004534 \n",
      "[1/10][26/100][215] Loss_D: 0.002575 Loss_G: 0.002832 \n",
      "[1/10][26/100][216] Loss_D: 0.004498 Loss_G: 0.001807 \n",
      "[1/10][26/100][217] Loss_D: 0.002162 Loss_G: 0.004236 \n",
      "[1/10][26/100][218] Loss_D: 0.002409 Loss_G: 0.002494 \n",
      "[1/10][26/100][219] Loss_D: 0.004634 Loss_G: 0.003936 \n",
      "[1/10][26/100][220] Loss_D: 0.004444 Loss_G: 0.002922 \n",
      "[1/10][26/100][221] Loss_D: 0.002962 Loss_G: 0.002778 \n",
      "[1/10][26/100][222] Loss_D: 0.002599 Loss_G: 0.004413 \n",
      "[1/10][26/100][223] Loss_D: 0.002671 Loss_G: 0.002551 \n",
      "[1/10][26/100][224] Loss_D: 0.001244 Loss_G: 0.002201 \n",
      "[1/10][26/100][225] Loss_D: 0.004433 Loss_G: 0.001205 \n",
      "[1/10][26/100][226] Loss_D: 0.001613 Loss_G: 0.002192 \n",
      "[1/10][26/100][227] Loss_D: 0.004234 Loss_G: 0.004884 \n",
      "[1/10][26/100][228] Loss_D: 0.001616 Loss_G: 0.003499 \n",
      "[1/10][26/100][229] Loss_D: 0.002234 Loss_G: 0.005882 \n",
      "[1/10][26/100][230] Loss_D: 0.000924 Loss_G: 0.004721 \n",
      "[1/10][26/100][231] Loss_D: 0.005194 Loss_G: 0.001481 \n",
      "[1/10][26/100][232] Loss_D: 0.002352 Loss_G: 0.002378 \n",
      "[1/10][26/100][233] Loss_D: 0.005573 Loss_G: 0.000924 \n",
      "[1/10][26/100][234] Loss_D: 0.003757 Loss_G: 0.001566 \n",
      "[1/10][26/100][235] Loss_D: 0.002377 Loss_G: 0.006103 \n",
      "[1/10][26/100][236] Loss_D: 0.002979 Loss_G: 0.003421 \n",
      "[1/10][26/100][237] Loss_D: 0.003319 Loss_G: 0.006387 \n",
      "[1/10][26/100][238] Loss_D: 0.004300 Loss_G: 0.004726 \n",
      "[1/10][26/100][239] Loss_D: 0.003285 Loss_G: 0.001348 \n",
      "[1/10][26/100][240] Loss_D: 0.002083 Loss_G: 0.003060 \n",
      "[1/10][26/100][241] Loss_D: 0.002404 Loss_G: 0.002462 \n",
      "[1/10][26/100][242] Loss_D: 0.003452 Loss_G: 0.002585 \n",
      "[1/10][26/100][243] Loss_D: 0.002539 Loss_G: 0.001744 \n",
      "[1/10][26/100][244] Loss_D: 0.002923 Loss_G: 0.002627 \n",
      "[1/10][26/100][245] Loss_D: 0.005315 Loss_G: 0.003684 \n",
      "[1/10][26/100][246] Loss_D: 0.003647 Loss_G: 0.003987 \n",
      "[1/10][26/100][247] Loss_D: 0.002437 Loss_G: 0.003249 \n",
      "[1/10][26/100][248] Loss_D: 0.005681 Loss_G: 0.003081 \n",
      "[1/10][26/100][249] Loss_D: 0.003307 Loss_G: 0.005350 \n",
      "[1/10][26/100][250] Loss_D: 0.002642 Loss_G: 0.003091 \n",
      "[1/10][26/100][251] Loss_D: 0.004676 Loss_G: 0.005652 \n",
      "[1/10][26/100][252] Loss_D: 0.000795 Loss_G: 0.004412 \n",
      "[1/10][26/100][253] Loss_D: 0.002607 Loss_G: 0.003280 \n",
      "[1/10][26/100][254] Loss_D: 0.001146 Loss_G: 0.004980 \n",
      "[1/10][26/100][255] Loss_D: 0.003618 Loss_G: 0.003649 \n",
      "[1/10][26/100][256] Loss_D: 0.002007 Loss_G: 0.003202 \n",
      "[1/10][26/100][257] Loss_D: 0.004528 Loss_G: 0.002431 \n",
      "[1/10][26/100][258] Loss_D: 0.004551 Loss_G: 0.003967 \n",
      "[1/10][26/100][259] Loss_D: 0.002122 Loss_G: 0.002947 \n",
      "[1/10][26/100][260] Loss_D: 0.003622 Loss_G: 0.003896 \n",
      "[1/10][26/100][261] Loss_D: 0.003108 Loss_G: 0.003303 \n",
      "[1/10][26/100][262] Loss_D: 0.003135 Loss_G: 0.003625 \n",
      "[1/10][26/100][263] Loss_D: 0.002051 Loss_G: 0.002093 \n",
      "[1/10][26/100][264] Loss_D: 0.002043 Loss_G: 0.003377 \n",
      "[1/10][26/100][265] Loss_D: 0.003709 Loss_G: 0.003377 \n",
      "[1/10][26/100][266] Loss_D: 0.004488 Loss_G: 0.003488 \n",
      "[1/10][26/100][267] Loss_D: 0.001834 Loss_G: 0.001404 \n",
      "[1/10][26/100][268] Loss_D: 0.003553 Loss_G: 0.004317 \n",
      "[1/10][26/100][269] Loss_D: 0.004014 Loss_G: 0.002329 \n",
      "[1/10][26/100][270] Loss_D: 0.002708 Loss_G: 0.002615 \n",
      "[1/10][26/100][271] Loss_D: 0.003346 Loss_G: 0.004853 \n",
      "[1/10][26/100][272] Loss_D: 0.003114 Loss_G: 0.002695 \n",
      "[1/10][26/100][273] Loss_D: 0.004827 Loss_G: 0.003330 \n",
      "[1/10][26/100][274] Loss_D: 0.004011 Loss_G: 0.004526 \n",
      "[1/10][26/100][275] Loss_D: 0.004139 Loss_G: 0.003877 \n",
      "[1/10][26/100][276] Loss_D: 0.003228 Loss_G: 0.003256 \n",
      "[1/10][26/100][277] Loss_D: 0.001510 Loss_G: 0.001693 \n",
      "[1/10][26/100][278] Loss_D: 0.003229 Loss_G: 0.005033 \n",
      "[1/10][26/100][279] Loss_D: 0.001404 Loss_G: 0.004395 \n",
      "[1/10][26/100][280] Loss_D: 0.003605 Loss_G: 0.002738 \n",
      "[1/10][26/100][281] Loss_D: 0.002713 Loss_G: 0.001458 \n",
      "[1/10][26/100][282] Loss_D: 0.002705 Loss_G: 0.004668 \n",
      "[1/10][26/100][283] Loss_D: 0.001579 Loss_G: 0.002434 \n",
      "[1/10][26/100][284] Loss_D: 0.003209 Loss_G: 0.004568 \n",
      "[1/10][26/100][285] Loss_D: 0.003118 Loss_G: 0.003577 \n",
      "[1/10][26/100][286] Loss_D: 0.003715 Loss_G: 0.003164 \n",
      "[1/10][26/100][287] Loss_D: 0.002214 Loss_G: 0.005013 \n",
      "[1/10][26/100][288] Loss_D: 0.001208 Loss_G: 0.002877 \n",
      "[1/10][26/100][289] Loss_D: 0.002102 Loss_G: 0.003867 \n",
      "[1/10][26/100][290] Loss_D: 0.002925 Loss_G: 0.001822 \n",
      "[1/10][26/100][291] Loss_D: 0.005263 Loss_G: 0.002240 \n",
      "[1/10][26/100][292] Loss_D: 0.002623 Loss_G: 0.001771 \n",
      "[1/10][26/100][293] Loss_D: 0.003312 Loss_G: 0.000678 \n",
      "[1/10][26/100][294] Loss_D: 0.003020 Loss_G: 0.002848 \n",
      "[1/10][26/100][295] Loss_D: 0.001648 Loss_G: 0.002219 \n",
      "[1/10][26/100][296] Loss_D: 0.003846 Loss_G: 0.004864 \n",
      "[1/10][26/100][297] Loss_D: 0.005080 Loss_G: 0.001505 \n",
      "[1/10][26/100][298] Loss_D: 0.006741 Loss_G: 0.001830 \n",
      "[1/10][26/100][299] Loss_D: 0.003366 Loss_G: 0.004869 \n",
      "[1/10][26/100][300] Loss_D: 0.002302 Loss_G: 0.003956 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][26/100][301] Loss_D: 0.000032 Loss_G: 0.002480 \n",
      "[1/10][26/100][302] Loss_D: 0.004537 Loss_G: 0.005117 \n",
      "[1/10][26/100][303] Loss_D: 0.005361 Loss_G: 0.004975 \n",
      "[1/10][26/100][304] Loss_D: 0.002620 Loss_G: 0.003241 \n",
      "[1/10][26/100][305] Loss_D: 0.003712 Loss_G: 0.002655 \n",
      "[1/10][26/100][306] Loss_D: 0.002219 Loss_G: 0.002315 \n",
      "[1/10][26/100][307] Loss_D: 0.003433 Loss_G: 0.004483 \n",
      "[1/10][26/100][308] Loss_D: 0.003679 Loss_G: 0.003354 \n",
      "[1/10][26/100][309] Loss_D: 0.003732 Loss_G: 0.001947 \n",
      "[1/10][26/100][310] Loss_D: 0.001998 Loss_G: 0.003749 \n",
      "[1/10][26/100][311] Loss_D: 0.002352 Loss_G: 0.001317 \n",
      "[1/10][26/100][312] Loss_D: 0.001862 Loss_G: 0.001697 \n",
      "[1/10][26/100][313] Loss_D: 0.002955 Loss_G: 0.004951 \n",
      "[1/10][26/100][314] Loss_D: 0.002624 Loss_G: 0.002238 \n",
      "[1/10][26/100][315] Loss_D: 0.003031 Loss_G: 0.003324 \n",
      "[1/10][26/100][316] Loss_D: 0.002723 Loss_G: 0.002015 \n",
      "[1/10][26/100][317] Loss_D: 0.004062 Loss_G: 0.001314 \n",
      "[1/10][26/100][318] Loss_D: 0.002411 Loss_G: 0.003139 \n",
      "[1/10][26/100][319] Loss_D: 0.001479 Loss_G: 0.002102 \n",
      "[1/10][26/100][320] Loss_D: 0.003866 Loss_G: 0.002852 \n",
      "[1/10][26/100][321] Loss_D: 0.002909 Loss_G: 0.003657 \n",
      "[1/10][26/100][322] Loss_D: 0.004472 Loss_G: 0.004194 \n",
      "[1/10][26/100][323] Loss_D: 0.003300 Loss_G: 0.002468 \n",
      "[1/10][26/100][324] Loss_D: 0.004745 Loss_G: 0.003141 \n",
      "[1/10][26/100][325] Loss_D: 0.002559 Loss_G: 0.004989 \n",
      "[1/10][26/100][326] Loss_D: 0.002971 Loss_G: 0.004393 \n",
      "[1/10][26/100][327] Loss_D: 0.002671 Loss_G: 0.002417 \n",
      "[1/10][26/100][328] Loss_D: 0.002720 Loss_G: 0.004653 \n",
      "[1/10][26/100][329] Loss_D: 0.002529 Loss_G: 0.001744 \n",
      "[1/10][26/100][330] Loss_D: 0.001809 Loss_G: 0.003059 \n",
      "[1/10][26/100][331] Loss_D: 0.004052 Loss_G: 0.002678 \n",
      "[1/10][26/100][332] Loss_D: 0.002627 Loss_G: 0.002941 \n",
      "[1/10][26/100][333] Loss_D: 0.004170 Loss_G: 0.005081 \n",
      "[1/10][26/100][334] Loss_D: 0.003326 Loss_G: 0.002123 \n",
      "[1/10][26/100][335] Loss_D: 0.002990 Loss_G: 0.002259 \n",
      "[1/10][26/100][336] Loss_D: 0.003351 Loss_G: 0.004711 \n",
      "[1/10][26/100][337] Loss_D: 0.003509 Loss_G: 0.004803 \n",
      "[1/10][26/100][338] Loss_D: 0.003742 Loss_G: 0.002952 \n",
      "[1/10][26/100][339] Loss_D: 0.002449 Loss_G: 0.003339 \n",
      "[1/10][26/100][340] Loss_D: 0.003109 Loss_G: 0.004142 \n",
      "[1/10][26/100][341] Loss_D: 0.003213 Loss_G: 0.003516 \n",
      "[1/10][26/100][342] Loss_D: 0.001212 Loss_G: 0.002667 \n",
      "[1/10][26/100][343] Loss_D: 0.002181 Loss_G: 0.002196 \n",
      "[1/10][26/100][344] Loss_D: 0.002243 Loss_G: 0.003090 \n",
      "[1/10][26/100][345] Loss_D: 0.001201 Loss_G: 0.001526 \n",
      "[1/10][26/100][346] Loss_D: 0.003627 Loss_G: 0.003860 \n",
      "[1/10][26/100][347] Loss_D: 0.003714 Loss_G: 0.001922 \n",
      "[1/10][26/100][348] Loss_D: 0.002546 Loss_G: 0.006985 \n",
      "[1/10][26/100][349] Loss_D: 0.003359 Loss_G: 0.001908 \n",
      "[1/10][26/100][350] Loss_D: 0.002122 Loss_G: 0.003903 \n",
      "[1/10][26/100][351] Loss_D: 0.002454 Loss_G: 0.002068 \n",
      "[1/10][26/100][352] Loss_D: 0.003789 Loss_G: 0.004980 \n",
      "[1/10][26/100][353] Loss_D: 0.004585 Loss_G: 0.004072 \n",
      "[1/10][26/100][354] Loss_D: 0.002647 Loss_G: 0.001551 \n",
      "[1/10][26/100][355] Loss_D: 0.003916 Loss_G: 0.002615 \n",
      "[1/10][26/100][356] Loss_D: 0.000709 Loss_G: 0.001830 \n",
      "[1/10][26/100][357] Loss_D: 0.003447 Loss_G: 0.002638 \n",
      "[1/10][26/100][358] Loss_D: 0.003061 Loss_G: 0.002201 \n",
      "[1/10][26/100][359] Loss_D: 0.005054 Loss_G: 0.005861 \n",
      "[1/10][26/100][360] Loss_D: 0.002444 Loss_G: 0.003938 \n",
      "[1/10][26/100][361] Loss_D: 0.002706 Loss_G: 0.002663 \n",
      "[1/10][26/100][362] Loss_D: 0.003243 Loss_G: 0.002702 \n",
      "[1/10][26/100][363] Loss_D: 0.002739 Loss_G: 0.001668 \n",
      "[1/10][26/100][364] Loss_D: 0.003235 Loss_G: 0.004071 \n",
      "[1/10][26/100][365] Loss_D: 0.003751 Loss_G: 0.002950 \n",
      "[1/10][26/100][366] Loss_D: 0.003522 Loss_G: 0.003880 \n",
      "[1/10][26/100][367] Loss_D: 0.002454 Loss_G: 0.002529 \n",
      "[1/10][26/100][368] Loss_D: 0.002711 Loss_G: 0.004430 \n",
      "[1/10][26/100][369] Loss_D: 0.001607 Loss_G: 0.004020 \n",
      "[1/10][26/100][370] Loss_D: 0.002556 Loss_G: 0.001366 \n",
      "[1/10][26/100][371] Loss_D: 0.002321 Loss_G: 0.001712 \n",
      "[1/10][26/100][372] Loss_D: 0.002745 Loss_G: 0.002431 \n",
      "[1/10][26/100][373] Loss_D: 0.002165 Loss_G: 0.002382 \n",
      "[1/10][26/100][374] Loss_D: 0.003909 Loss_G: 0.003444 \n",
      "[1/10][26/100][375] Loss_D: 0.008734 Loss_G: 0.006268 \n",
      "[1/10][26/100][376] Loss_D: 0.002613 Loss_G: 0.002879 \n",
      "[1/10][26/100][377] Loss_D: 0.005638 Loss_G: 0.003118 \n",
      "[1/10][26/100][378] Loss_D: 0.002512 Loss_G: 0.001008 \n",
      "[1/10][26/100][379] Loss_D: 0.001226 Loss_G: 0.004676 \n",
      "[1/10][26/100][380] Loss_D: 0.002602 Loss_G: 0.002819 \n",
      "[1/10][26/100][381] Loss_D: 0.003865 Loss_G: 0.003782 \n",
      "[1/10][26/100][382] Loss_D: 0.001671 Loss_G: 0.002166 \n",
      "[1/10][26/100][383] Loss_D: 0.002319 Loss_G: 0.003254 \n",
      "[1/10][26/100][384] Loss_D: 0.002647 Loss_G: 0.002335 \n",
      "[1/10][26/100][385] Loss_D: 0.002119 Loss_G: 0.002328 \n",
      "[1/10][26/100][386] Loss_D: 0.003980 Loss_G: 0.002084 \n",
      "[1/10][26/100][387] Loss_D: 0.001874 Loss_G: 0.001974 \n",
      "[1/10][26/100][388] Loss_D: 0.004099 Loss_G: 0.004167 \n",
      "[1/10][26/100][389] Loss_D: 0.003648 Loss_G: 0.002134 \n",
      "[1/10][26/100][390] Loss_D: 0.002461 Loss_G: 0.002360 \n",
      "[1/10][26/100][391] Loss_D: 0.003027 Loss_G: 0.001940 \n",
      "[1/10][26/100][392] Loss_D: 0.002492 Loss_G: 0.002490 \n",
      "[1/10][26/100][393] Loss_D: 0.004002 Loss_G: 0.004664 \n",
      "[1/10][26/100][394] Loss_D: 0.002988 Loss_G: 0.004400 \n",
      "[1/10][26/100][395] Loss_D: 0.004551 Loss_G: 0.004444 \n",
      "[1/10][26/100][396] Loss_D: 0.002873 Loss_G: 0.003178 \n",
      "[1/10][26/100][397] Loss_D: 0.001537 Loss_G: 0.003616 \n",
      "[1/10][26/100][398] Loss_D: 0.001274 Loss_G: 0.002440 \n",
      "[1/10][26/100][399] Loss_D: 0.003463 Loss_G: 0.002162 \n",
      "[1/10][26/100][400] Loss_D: 0.003811 Loss_G: 0.001679 \n",
      "[2/10][26/100][401] Loss_D: 0.003141 Loss_G: 0.004373 \n",
      "[2/10][26/100][402] Loss_D: 0.004850 Loss_G: 0.005189 \n",
      "[2/10][26/100][403] Loss_D: 0.003978 Loss_G: 0.002492 \n",
      "[2/10][26/100][404] Loss_D: 0.001787 Loss_G: 0.003348 \n",
      "[2/10][26/100][405] Loss_D: 0.003410 Loss_G: 0.002773 \n",
      "[2/10][26/100][406] Loss_D: 0.003516 Loss_G: 0.003007 \n",
      "[2/10][26/100][407] Loss_D: 0.003704 Loss_G: 0.003819 \n",
      "[2/10][26/100][408] Loss_D: 0.002664 Loss_G: 0.002310 \n",
      "[2/10][26/100][409] Loss_D: 0.004940 Loss_G: 0.004626 \n",
      "[2/10][26/100][410] Loss_D: 0.001296 Loss_G: 0.005273 \n",
      "[2/10][26/100][411] Loss_D: 0.003448 Loss_G: 0.002354 \n",
      "[2/10][26/100][412] Loss_D: 0.002279 Loss_G: 0.004162 \n",
      "[2/10][26/100][413] Loss_D: 0.001340 Loss_G: 0.002157 \n",
      "[2/10][26/100][414] Loss_D: 0.003570 Loss_G: 0.003549 \n",
      "[2/10][26/100][415] Loss_D: 0.003330 Loss_G: 0.002624 \n",
      "[2/10][26/100][416] Loss_D: 0.002266 Loss_G: 0.001707 \n",
      "[2/10][26/100][417] Loss_D: 0.003305 Loss_G: 0.003010 \n",
      "[2/10][26/100][418] Loss_D: 0.002977 Loss_G: 0.003439 \n",
      "[2/10][26/100][419] Loss_D: 0.002737 Loss_G: 0.003312 \n",
      "[2/10][26/100][420] Loss_D: 0.004483 Loss_G: 0.001630 \n",
      "[2/10][26/100][421] Loss_D: 0.001867 Loss_G: 0.002356 \n",
      "[2/10][26/100][422] Loss_D: 0.001888 Loss_G: 0.001042 \n",
      "[2/10][26/100][423] Loss_D: 0.002120 Loss_G: 0.004004 \n",
      "[2/10][26/100][424] Loss_D: 0.002665 Loss_G: 0.001718 \n",
      "[2/10][26/100][425] Loss_D: 0.005135 Loss_G: 0.004462 \n",
      "[2/10][26/100][426] Loss_D: 0.004725 Loss_G: 0.004373 \n",
      "[2/10][26/100][427] Loss_D: 0.004562 Loss_G: 0.002647 \n",
      "[2/10][26/100][428] Loss_D: 0.004557 Loss_G: 0.002846 \n",
      "[2/10][26/100][429] Loss_D: 0.003845 Loss_G: 0.001911 \n",
      "[2/10][26/100][430] Loss_D: 0.002916 Loss_G: 0.003144 \n",
      "[2/10][26/100][431] Loss_D: 0.001048 Loss_G: 0.005869 \n",
      "[2/10][26/100][432] Loss_D: 0.005351 Loss_G: 0.003579 \n",
      "[2/10][26/100][433] Loss_D: 0.001297 Loss_G: 0.002879 \n",
      "[2/10][26/100][434] Loss_D: 0.003726 Loss_G: 0.002407 \n",
      "[2/10][26/100][435] Loss_D: 0.002475 Loss_G: 0.002130 \n",
      "[2/10][26/100][436] Loss_D: 0.003266 Loss_G: 0.003384 \n",
      "[2/10][26/100][437] Loss_D: 0.003468 Loss_G: 0.003362 \n",
      "[2/10][26/100][438] Loss_D: 0.003431 Loss_G: 0.002194 \n",
      "[2/10][26/100][439] Loss_D: 0.003231 Loss_G: 0.002174 \n",
      "[2/10][26/100][440] Loss_D: 0.001546 Loss_G: 0.003310 \n",
      "[2/10][26/100][441] Loss_D: 0.001865 Loss_G: 0.001745 \n",
      "[2/10][26/100][442] Loss_D: 0.003591 Loss_G: 0.002190 \n",
      "[2/10][26/100][443] Loss_D: 0.004809 Loss_G: 0.001571 \n",
      "[2/10][26/100][444] Loss_D: 0.004358 Loss_G: 0.001492 \n",
      "[2/10][26/100][445] Loss_D: 0.002026 Loss_G: 0.002239 \n",
      "[2/10][26/100][446] Loss_D: 0.002301 Loss_G: 0.002271 \n",
      "[2/10][26/100][447] Loss_D: 0.003254 Loss_G: 0.002911 \n",
      "[2/10][26/100][448] Loss_D: 0.003953 Loss_G: 0.003579 \n",
      "[2/10][26/100][449] Loss_D: 0.003574 Loss_G: 0.003551 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][26/100][450] Loss_D: 0.003610 Loss_G: 0.003299 \n",
      "[2/10][26/100][451] Loss_D: 0.003675 Loss_G: 0.004763 \n",
      "[2/10][26/100][452] Loss_D: 0.002241 Loss_G: 0.002938 \n",
      "[2/10][26/100][453] Loss_D: 0.002835 Loss_G: 0.004185 \n",
      "[2/10][26/100][454] Loss_D: 0.002765 Loss_G: 0.003533 \n",
      "[2/10][26/100][455] Loss_D: 0.003136 Loss_G: 0.001948 \n",
      "[2/10][26/100][456] Loss_D: 0.002970 Loss_G: 0.002815 \n",
      "[2/10][26/100][457] Loss_D: 0.002586 Loss_G: 0.001813 \n",
      "[2/10][26/100][458] Loss_D: 0.002609 Loss_G: 0.001417 \n",
      "[2/10][26/100][459] Loss_D: 0.003130 Loss_G: 0.001706 \n",
      "[2/10][26/100][460] Loss_D: 0.001854 Loss_G: 0.002468 \n",
      "[2/10][26/100][461] Loss_D: 0.002961 Loss_G: 0.002250 \n",
      "[2/10][26/100][462] Loss_D: 0.002596 Loss_G: 0.002183 \n",
      "[2/10][26/100][463] Loss_D: 0.002624 Loss_G: 0.002826 \n",
      "[2/10][26/100][464] Loss_D: 0.004170 Loss_G: 0.001324 \n",
      "[2/10][26/100][465] Loss_D: 0.003162 Loss_G: 0.005163 \n",
      "[2/10][26/100][466] Loss_D: 0.002802 Loss_G: 0.002031 \n",
      "[2/10][26/100][467] Loss_D: 0.003196 Loss_G: 0.002637 \n",
      "[2/10][26/100][468] Loss_D: 0.004129 Loss_G: 0.001133 \n",
      "[2/10][26/100][469] Loss_D: 0.003082 Loss_G: 0.002755 \n",
      "[2/10][26/100][470] Loss_D: 0.005065 Loss_G: 0.003591 \n",
      "[2/10][26/100][471] Loss_D: 0.003424 Loss_G: 0.004204 \n",
      "[2/10][26/100][472] Loss_D: 0.002854 Loss_G: 0.002216 \n",
      "[2/10][26/100][473] Loss_D: 0.003077 Loss_G: 0.003876 \n",
      "[2/10][26/100][474] Loss_D: 0.003207 Loss_G: 0.003399 \n",
      "[2/10][26/100][475] Loss_D: 0.004195 Loss_G: 0.002409 \n",
      "[2/10][26/100][476] Loss_D: 0.002603 Loss_G: 0.004017 \n",
      "[2/10][26/100][477] Loss_D: 0.003461 Loss_G: 0.003106 \n",
      "[2/10][26/100][478] Loss_D: 0.003100 Loss_G: 0.001589 \n",
      "[2/10][26/100][479] Loss_D: 0.003012 Loss_G: 0.002394 \n",
      "[2/10][26/100][480] Loss_D: 0.003790 Loss_G: 0.002664 \n",
      "[2/10][26/100][481] Loss_D: 0.002097 Loss_G: 0.003254 \n",
      "[2/10][26/100][482] Loss_D: 0.002142 Loss_G: 0.002777 \n",
      "[2/10][26/100][483] Loss_D: 0.002567 Loss_G: 0.002063 \n",
      "[2/10][26/100][484] Loss_D: 0.002879 Loss_G: 0.004023 \n",
      "[2/10][26/100][485] Loss_D: 0.002516 Loss_G: 0.004309 \n",
      "[2/10][26/100][486] Loss_D: 0.005084 Loss_G: 0.004474 \n",
      "[2/10][26/100][487] Loss_D: 0.003425 Loss_G: 0.002351 \n",
      "[2/10][26/100][488] Loss_D: 0.002821 Loss_G: 0.002870 \n",
      "[2/10][26/100][489] Loss_D: 0.004291 Loss_G: 0.001168 \n",
      "[2/10][26/100][490] Loss_D: 0.003977 Loss_G: 0.002668 \n",
      "[2/10][26/100][491] Loss_D: 0.003325 Loss_G: 0.003832 \n",
      "[2/10][26/100][492] Loss_D: 0.002206 Loss_G: 0.003279 \n",
      "[2/10][26/100][493] Loss_D: 0.005408 Loss_G: 0.003193 \n",
      "[2/10][26/100][494] Loss_D: 0.007591 Loss_G: 0.004631 \n",
      "[2/10][26/100][495] Loss_D: 0.002499 Loss_G: 0.002868 \n",
      "[2/10][26/100][496] Loss_D: 0.002577 Loss_G: 0.002665 \n",
      "[2/10][26/100][497] Loss_D: 0.001990 Loss_G: 0.002521 \n",
      "[2/10][26/100][498] Loss_D: 0.002610 Loss_G: 0.002228 \n",
      "[2/10][26/100][499] Loss_D: 0.003572 Loss_G: 0.003562 \n",
      "[2/10][26/100][500] Loss_D: 0.002818 Loss_G: 0.002497 \n",
      "[2/10][26/100][501] Loss_D: 0.002269 Loss_G: 0.001928 \n",
      "[2/10][26/100][502] Loss_D: 0.003420 Loss_G: 0.003330 \n",
      "[2/10][26/100][503] Loss_D: 0.001485 Loss_G: 0.001380 \n",
      "[2/10][26/100][504] Loss_D: 0.002851 Loss_G: 0.002687 \n",
      "[2/10][26/100][505] Loss_D: 0.002342 Loss_G: 0.003474 \n",
      "[2/10][26/100][506] Loss_D: 0.004433 Loss_G: 0.001520 \n",
      "[2/10][26/100][507] Loss_D: 0.006234 Loss_G: 0.003646 \n",
      "[2/10][26/100][508] Loss_D: 0.004165 Loss_G: 0.003790 \n",
      "[2/10][26/100][509] Loss_D: 0.003012 Loss_G: 0.002709 \n",
      "[2/10][26/100][510] Loss_D: 0.002771 Loss_G: 0.002214 \n",
      "[2/10][26/100][511] Loss_D: 0.003852 Loss_G: 0.001403 \n",
      "[2/10][26/100][512] Loss_D: 0.003089 Loss_G: 0.001099 \n",
      "[2/10][26/100][513] Loss_D: 0.003246 Loss_G: 0.002560 \n",
      "[2/10][26/100][514] Loss_D: 0.002097 Loss_G: 0.002859 \n",
      "[2/10][26/100][515] Loss_D: 0.002436 Loss_G: 0.002107 \n",
      "[2/10][26/100][516] Loss_D: 0.003337 Loss_G: 0.001208 \n",
      "[2/10][26/100][517] Loss_D: 0.002959 Loss_G: 0.003761 \n",
      "[2/10][26/100][518] Loss_D: 0.003403 Loss_G: 0.004791 \n",
      "[2/10][26/100][519] Loss_D: 0.006225 Loss_G: 0.002209 \n",
      "[2/10][26/100][520] Loss_D: 0.001898 Loss_G: 0.007370 \n",
      "[2/10][26/100][521] Loss_D: 0.005393 Loss_G: 0.003658 \n",
      "[2/10][26/100][522] Loss_D: 0.002207 Loss_G: 0.001438 \n",
      "[2/10][26/100][523] Loss_D: 0.002912 Loss_G: 0.003013 \n",
      "[2/10][26/100][524] Loss_D: 0.002687 Loss_G: 0.004007 \n",
      "[2/10][26/100][525] Loss_D: 0.003188 Loss_G: 0.003172 \n",
      "[2/10][26/100][526] Loss_D: 0.004059 Loss_G: 0.004036 \n",
      "[2/10][26/100][527] Loss_D: 0.003945 Loss_G: 0.002883 \n",
      "[2/10][26/100][528] Loss_D: 0.001716 Loss_G: 0.004080 \n",
      "[2/10][26/100][529] Loss_D: 0.002472 Loss_G: 0.005247 \n",
      "[2/10][26/100][530] Loss_D: 0.001250 Loss_G: 0.001833 \n",
      "[2/10][26/100][531] Loss_D: 0.001760 Loss_G: 0.001982 \n",
      "[2/10][26/100][532] Loss_D: 0.002673 Loss_G: 0.002335 \n",
      "[2/10][26/100][533] Loss_D: 0.001758 Loss_G: 0.002329 \n",
      "[2/10][26/100][534] Loss_D: 0.002847 Loss_G: 0.002355 \n",
      "[2/10][26/100][535] Loss_D: 0.002665 Loss_G: 0.001140 \n",
      "[2/10][26/100][536] Loss_D: 0.003087 Loss_G: 0.001683 \n",
      "[2/10][26/100][537] Loss_D: 0.003315 Loss_G: 0.004007 \n",
      "[2/10][26/100][538] Loss_D: 0.002560 Loss_G: 0.004146 \n",
      "[2/10][26/100][539] Loss_D: 0.002818 Loss_G: 0.001922 \n",
      "[2/10][26/100][540] Loss_D: 0.002529 Loss_G: 0.002553 \n",
      "[2/10][26/100][541] Loss_D: 0.005158 Loss_G: 0.003291 \n",
      "[2/10][26/100][542] Loss_D: 0.003135 Loss_G: 0.003023 \n",
      "[2/10][26/100][543] Loss_D: 0.005052 Loss_G: 0.003725 \n",
      "[2/10][26/100][544] Loss_D: 0.002187 Loss_G: 0.004139 \n",
      "[2/10][26/100][545] Loss_D: 0.002093 Loss_G: 0.003561 \n",
      "[2/10][26/100][546] Loss_D: 0.002429 Loss_G: 0.002694 \n",
      "[2/10][26/100][547] Loss_D: 0.001814 Loss_G: 0.002240 \n",
      "[2/10][26/100][548] Loss_D: 0.003054 Loss_G: 0.001197 \n",
      "[2/10][26/100][549] Loss_D: 0.002636 Loss_G: 0.001201 \n",
      "[2/10][26/100][550] Loss_D: 0.006160 Loss_G: 0.002287 \n",
      "[2/10][26/100][551] Loss_D: 0.004259 Loss_G: 0.002263 \n",
      "[2/10][26/100][552] Loss_D: 0.002534 Loss_G: 0.003192 \n",
      "[2/10][26/100][553] Loss_D: 0.003432 Loss_G: 0.001815 \n",
      "[2/10][26/100][554] Loss_D: 0.002818 Loss_G: 0.003460 \n",
      "[2/10][26/100][555] Loss_D: 0.004693 Loss_G: 0.005904 \n",
      "[2/10][26/100][556] Loss_D: 0.001750 Loss_G: 0.002281 \n",
      "[2/10][26/100][557] Loss_D: 0.003381 Loss_G: 0.005748 \n",
      "[2/10][26/100][558] Loss_D: 0.003254 Loss_G: 0.002378 \n",
      "[2/10][26/100][559] Loss_D: 0.004512 Loss_G: 0.002246 \n",
      "[2/10][26/100][560] Loss_D: 0.001896 Loss_G: 0.003630 \n",
      "[2/10][26/100][561] Loss_D: 0.003315 Loss_G: 0.003490 \n",
      "[2/10][26/100][562] Loss_D: 0.004311 Loss_G: 0.003259 \n",
      "[2/10][26/100][563] Loss_D: 0.003408 Loss_G: 0.005893 \n",
      "[2/10][26/100][564] Loss_D: 0.001967 Loss_G: 0.003066 \n",
      "[2/10][26/100][565] Loss_D: 0.001438 Loss_G: 0.001077 \n",
      "[2/10][26/100][566] Loss_D: 0.002322 Loss_G: 0.002996 \n",
      "[2/10][26/100][567] Loss_D: 0.002329 Loss_G: 0.001892 \n",
      "[2/10][26/100][568] Loss_D: 0.002659 Loss_G: 0.002143 \n",
      "[2/10][26/100][569] Loss_D: 0.001978 Loss_G: 0.001057 \n",
      "[2/10][26/100][570] Loss_D: 0.002680 Loss_G: 0.003078 \n",
      "[2/10][26/100][571] Loss_D: 0.003591 Loss_G: 0.003026 \n",
      "[2/10][26/100][572] Loss_D: 0.004211 Loss_G: 0.002929 \n",
      "[2/10][26/100][573] Loss_D: 0.002778 Loss_G: 0.001969 \n",
      "[2/10][26/100][574] Loss_D: 0.004585 Loss_G: 0.003917 \n",
      "[2/10][26/100][575] Loss_D: 0.006273 Loss_G: 0.005183 \n",
      "[2/10][26/100][576] Loss_D: 0.004423 Loss_G: 0.002694 \n",
      "[2/10][26/100][577] Loss_D: 0.004025 Loss_G: 0.003631 \n",
      "[2/10][26/100][578] Loss_D: 0.003054 Loss_G: 0.003605 \n",
      "[2/10][26/100][579] Loss_D: 0.002989 Loss_G: 0.003120 \n",
      "[2/10][26/100][580] Loss_D: 0.002509 Loss_G: 0.002397 \n",
      "[2/10][26/100][581] Loss_D: 0.004070 Loss_G: 0.004794 \n",
      "[2/10][26/100][582] Loss_D: 0.001054 Loss_G: 0.003548 \n",
      "[2/10][26/100][583] Loss_D: 0.002219 Loss_G: 0.002407 \n",
      "[2/10][26/100][584] Loss_D: 0.002292 Loss_G: 0.001345 \n",
      "[2/10][26/100][585] Loss_D: 0.002751 Loss_G: 0.001229 \n",
      "[2/10][26/100][586] Loss_D: 0.001896 Loss_G: 0.003172 \n",
      "[2/10][26/100][587] Loss_D: 0.005488 Loss_G: 0.001338 \n",
      "[2/10][26/100][588] Loss_D: 0.002974 Loss_G: 0.002515 \n",
      "[2/10][26/100][589] Loss_D: 0.002431 Loss_G: 0.002422 \n",
      "[2/10][26/100][590] Loss_D: 0.004495 Loss_G: 0.003721 \n",
      "[2/10][26/100][591] Loss_D: 0.003583 Loss_G: 0.003517 \n",
      "[2/10][26/100][592] Loss_D: 0.001106 Loss_G: 0.001321 \n",
      "[2/10][26/100][593] Loss_D: 0.003065 Loss_G: 0.003141 \n",
      "[2/10][26/100][594] Loss_D: 0.002271 Loss_G: 0.002813 \n",
      "[2/10][26/100][595] Loss_D: 0.003574 Loss_G: 0.002947 \n",
      "[2/10][26/100][596] Loss_D: 0.004337 Loss_G: 0.003304 \n",
      "[2/10][26/100][597] Loss_D: 0.003060 Loss_G: 0.001170 \n",
      "[2/10][26/100][598] Loss_D: 0.002504 Loss_G: 0.003480 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][26/100][599] Loss_D: 0.002520 Loss_G: 0.002468 \n",
      "[2/10][26/100][600] Loss_D: 0.002893 Loss_G: 0.003116 \n",
      "[3/10][26/100][601] Loss_D: 0.004093 Loss_G: 0.002176 \n",
      "[3/10][26/100][602] Loss_D: 0.003137 Loss_G: 0.003312 \n",
      "[3/10][26/100][603] Loss_D: 0.001907 Loss_G: 0.002240 \n",
      "[3/10][26/100][604] Loss_D: 0.002749 Loss_G: 0.003203 \n",
      "[3/10][26/100][605] Loss_D: 0.001670 Loss_G: 0.002336 \n",
      "[3/10][26/100][606] Loss_D: 0.004191 Loss_G: 0.002150 \n",
      "[3/10][26/100][607] Loss_D: 0.001085 Loss_G: 0.002404 \n",
      "[3/10][26/100][608] Loss_D: 0.004360 Loss_G: 0.001429 \n",
      "[3/10][26/100][609] Loss_D: 0.004606 Loss_G: 0.002639 \n",
      "[3/10][26/100][610] Loss_D: 0.003993 Loss_G: 0.002979 \n",
      "[3/10][26/100][611] Loss_D: 0.004061 Loss_G: 0.003927 \n",
      "[3/10][26/100][612] Loss_D: 0.003508 Loss_G: 0.003653 \n",
      "[3/10][26/100][613] Loss_D: 0.001862 Loss_G: 0.002002 \n",
      "[3/10][26/100][614] Loss_D: 0.002656 Loss_G: 0.002832 \n",
      "[3/10][26/100][615] Loss_D: 0.004895 Loss_G: 0.002247 \n",
      "[3/10][26/100][616] Loss_D: 0.001557 Loss_G: 0.003036 \n",
      "[3/10][26/100][617] Loss_D: 0.002080 Loss_G: 0.003839 \n",
      "[3/10][26/100][618] Loss_D: 0.003436 Loss_G: 0.002989 \n",
      "[3/10][26/100][619] Loss_D: 0.002337 Loss_G: 0.002161 \n",
      "[3/10][26/100][620] Loss_D: 0.001158 Loss_G: 0.003417 \n",
      "[3/10][26/100][621] Loss_D: 0.000817 Loss_G: 0.004558 \n",
      "[3/10][26/100][622] Loss_D: 0.002430 Loss_G: 0.001065 \n",
      "[3/10][26/100][623] Loss_D: 0.003682 Loss_G: 0.004168 \n",
      "[3/10][26/100][624] Loss_D: 0.003541 Loss_G: 0.003086 \n",
      "[3/10][26/100][625] Loss_D: 0.001140 Loss_G: 0.002436 \n",
      "[3/10][26/100][626] Loss_D: 0.002989 Loss_G: 0.004306 \n",
      "[3/10][26/100][627] Loss_D: 0.003594 Loss_G: 0.002650 \n",
      "[3/10][26/100][628] Loss_D: 0.003570 Loss_G: 0.002941 \n",
      "[3/10][26/100][629] Loss_D: 0.003325 Loss_G: 0.003007 \n",
      "[3/10][26/100][630] Loss_D: 0.004756 Loss_G: 0.002195 \n",
      "[3/10][26/100][631] Loss_D: 0.004155 Loss_G: 0.003004 \n",
      "[3/10][26/100][632] Loss_D: 0.002061 Loss_G: 0.002422 \n",
      "[3/10][26/100][633] Loss_D: 0.002970 Loss_G: 0.003628 \n",
      "[3/10][26/100][634] Loss_D: 0.001321 Loss_G: 0.001976 \n",
      "[3/10][26/100][635] Loss_D: 0.004388 Loss_G: 0.003234 \n",
      "[3/10][26/100][636] Loss_D: 0.002377 Loss_G: 0.004847 \n",
      "[3/10][26/100][637] Loss_D: 0.002662 Loss_G: 0.002556 \n",
      "[3/10][26/100][638] Loss_D: 0.002484 Loss_G: 0.002254 \n",
      "[3/10][26/100][639] Loss_D: 0.003054 Loss_G: 0.002034 \n",
      "[3/10][26/100][640] Loss_D: 0.000931 Loss_G: 0.004044 \n",
      "[3/10][26/100][641] Loss_D: 0.001356 Loss_G: 0.003290 \n",
      "[3/10][26/100][642] Loss_D: 0.005263 Loss_G: 0.003479 \n",
      "[3/10][26/100][643] Loss_D: 0.002743 Loss_G: 0.004137 \n",
      "[3/10][26/100][644] Loss_D: 0.002206 Loss_G: 0.003415 \n",
      "[3/10][26/100][645] Loss_D: 0.003459 Loss_G: 0.002582 \n",
      "[3/10][26/100][646] Loss_D: 0.001824 Loss_G: 0.003736 \n",
      "[3/10][26/100][647] Loss_D: 0.004667 Loss_G: 0.002946 \n",
      "[3/10][26/100][648] Loss_D: 0.002130 Loss_G: 0.003616 \n",
      "[3/10][26/100][649] Loss_D: 0.003194 Loss_G: 0.002551 \n",
      "[3/10][26/100][650] Loss_D: 0.003503 Loss_G: 0.003450 \n",
      "[3/10][26/100][651] Loss_D: 0.003165 Loss_G: 0.002218 \n",
      "[3/10][26/100][652] Loss_D: 0.002288 Loss_G: 0.000382 \n",
      "[3/10][26/100][653] Loss_D: 0.004559 Loss_G: 0.002322 \n",
      "[3/10][26/100][654] Loss_D: 0.003998 Loss_G: 0.002441 \n",
      "[3/10][26/100][655] Loss_D: 0.004807 Loss_G: 0.003493 \n",
      "[3/10][26/100][656] Loss_D: 0.000760 Loss_G: 0.003199 \n",
      "[3/10][26/100][657] Loss_D: 0.002462 Loss_G: 0.003669 \n",
      "[3/10][26/100][658] Loss_D: 0.004427 Loss_G: 0.003939 \n",
      "[3/10][26/100][659] Loss_D: 0.000457 Loss_G: 0.004873 \n",
      "[3/10][26/100][660] Loss_D: 0.001995 Loss_G: 0.002388 \n",
      "[3/10][26/100][661] Loss_D: 0.002056 Loss_G: 0.002613 \n",
      "[3/10][26/100][662] Loss_D: 0.002925 Loss_G: 0.002407 \n",
      "[3/10][26/100][663] Loss_D: 0.000856 Loss_G: 0.001248 \n",
      "[3/10][26/100][664] Loss_D: 0.003309 Loss_G: 0.003152 \n",
      "[3/10][26/100][665] Loss_D: 0.002809 Loss_G: 0.003077 \n",
      "[3/10][26/100][666] Loss_D: 0.004206 Loss_G: 0.003451 \n",
      "[3/10][26/100][667] Loss_D: 0.002800 Loss_G: 0.003568 \n",
      "[3/10][26/100][668] Loss_D: 0.001591 Loss_G: 0.001394 \n",
      "[3/10][26/100][669] Loss_D: 0.003477 Loss_G: 0.000573 \n",
      "[3/10][26/100][670] Loss_D: 0.001266 Loss_G: 0.003200 \n",
      "[3/10][26/100][671] Loss_D: 0.003037 Loss_G: 0.002442 \n",
      "[3/10][26/100][672] Loss_D: 0.001162 Loss_G: 0.002838 \n",
      "[3/10][26/100][673] Loss_D: 0.001148 Loss_G: 0.004382 \n",
      "[3/10][26/100][674] Loss_D: 0.003220 Loss_G: 0.004320 \n",
      "[3/10][26/100][675] Loss_D: 0.002430 Loss_G: 0.002373 \n",
      "[3/10][26/100][676] Loss_D: 0.000484 Loss_G: 0.003890 \n",
      "[3/10][26/100][677] Loss_D: 0.001655 Loss_G: 0.004213 \n",
      "[3/10][26/100][678] Loss_D: 0.002628 Loss_G: 0.003806 \n",
      "[3/10][26/100][679] Loss_D: 0.001987 Loss_G: 0.004032 \n",
      "[3/10][26/100][680] Loss_D: 0.003401 Loss_G: 0.004921 \n",
      "[3/10][26/100][681] Loss_D: 0.001775 Loss_G: 0.003378 \n",
      "[3/10][26/100][682] Loss_D: 0.001611 Loss_G: 0.002174 \n",
      "[3/10][26/100][683] Loss_D: 0.003128 Loss_G: 0.002937 \n",
      "[3/10][26/100][684] Loss_D: 0.005761 Loss_G: 0.001926 \n",
      "[3/10][26/100][685] Loss_D: 0.004088 Loss_G: 0.002789 \n",
      "[3/10][26/100][686] Loss_D: 0.003125 Loss_G: 0.002598 \n",
      "[3/10][26/100][687] Loss_D: 0.002005 Loss_G: 0.002061 \n",
      "[3/10][26/100][688] Loss_D: 0.003319 Loss_G: 0.002661 \n",
      "[3/10][26/100][689] Loss_D: 0.003571 Loss_G: 0.003245 \n",
      "[3/10][26/100][690] Loss_D: 0.002966 Loss_G: 0.000656 \n",
      "[3/10][26/100][691] Loss_D: 0.003972 Loss_G: 0.003503 \n",
      "[3/10][26/100][692] Loss_D: 0.003479 Loss_G: 0.002489 \n",
      "[3/10][26/100][693] Loss_D: 0.004066 Loss_G: 0.001375 \n",
      "[3/10][26/100][694] Loss_D: 0.002320 Loss_G: 0.000975 \n",
      "[3/10][26/100][695] Loss_D: 0.002845 Loss_G: 0.003036 \n",
      "[3/10][26/100][696] Loss_D: 0.003367 Loss_G: 0.003368 \n",
      "[3/10][26/100][697] Loss_D: 0.003217 Loss_G: 0.004622 \n",
      "[3/10][26/100][698] Loss_D: 0.002669 Loss_G: 0.001067 \n",
      "[3/10][26/100][699] Loss_D: 0.002316 Loss_G: 0.001283 \n",
      "[3/10][26/100][700] Loss_D: 0.000865 Loss_G: 0.003596 \n",
      "[3/10][26/100][701] Loss_D: 0.002916 Loss_G: 0.005878 \n",
      "[3/10][26/100][702] Loss_D: 0.004186 Loss_G: 0.000690 \n",
      "[3/10][26/100][703] Loss_D: 0.001876 Loss_G: 0.001980 \n",
      "[3/10][26/100][704] Loss_D: 0.003906 Loss_G: 0.001874 \n",
      "[3/10][26/100][705] Loss_D: 0.003323 Loss_G: 0.003120 \n",
      "[3/10][26/100][706] Loss_D: 0.004976 Loss_G: 0.006345 \n",
      "[3/10][26/100][707] Loss_D: 0.003968 Loss_G: 0.002122 \n",
      "[3/10][26/100][708] Loss_D: 0.003511 Loss_G: 0.003961 \n",
      "[3/10][26/100][709] Loss_D: 0.001256 Loss_G: 0.002693 \n",
      "[3/10][26/100][710] Loss_D: 0.002110 Loss_G: 0.003604 \n",
      "[3/10][26/100][711] Loss_D: 0.002506 Loss_G: 0.002392 \n",
      "[3/10][26/100][712] Loss_D: 0.002130 Loss_G: 0.003109 \n",
      "[3/10][26/100][713] Loss_D: 0.004190 Loss_G: 0.000466 \n",
      "[3/10][26/100][714] Loss_D: 0.003417 Loss_G: 0.001981 \n",
      "[3/10][26/100][715] Loss_D: 0.004176 Loss_G: 0.002688 \n",
      "[3/10][26/100][716] Loss_D: 0.002178 Loss_G: 0.001902 \n",
      "[3/10][26/100][717] Loss_D: 0.003913 Loss_G: 0.004942 \n",
      "[3/10][26/100][718] Loss_D: 0.004833 Loss_G: 0.002338 \n",
      "[3/10][26/100][719] Loss_D: 0.002456 Loss_G: 0.004217 \n",
      "[3/10][26/100][720] Loss_D: 0.001700 Loss_G: 0.002883 \n",
      "[3/10][26/100][721] Loss_D: 0.004705 Loss_G: 0.005275 \n",
      "[3/10][26/100][722] Loss_D: 0.001714 Loss_G: 0.002548 \n",
      "[3/10][26/100][723] Loss_D: 0.003740 Loss_G: 0.002227 \n",
      "[3/10][26/100][724] Loss_D: 0.003811 Loss_G: 0.001178 \n",
      "[3/10][26/100][725] Loss_D: 0.003105 Loss_G: 0.001713 \n",
      "[3/10][26/100][726] Loss_D: 0.001900 Loss_G: 0.002239 \n",
      "[3/10][26/100][727] Loss_D: 0.002375 Loss_G: 0.003513 \n",
      "[3/10][26/100][728] Loss_D: 0.004041 Loss_G: 0.003042 \n",
      "[3/10][26/100][729] Loss_D: 0.001790 Loss_G: 0.003669 \n",
      "[3/10][26/100][730] Loss_D: 0.003699 Loss_G: 0.004327 \n",
      "[3/10][26/100][731] Loss_D: 0.003832 Loss_G: 0.002463 \n",
      "[3/10][26/100][732] Loss_D: 0.000197 Loss_G: 0.002842 \n",
      "[3/10][26/100][733] Loss_D: 0.002612 Loss_G: 0.005720 \n",
      "[3/10][26/100][734] Loss_D: 0.003926 Loss_G: 0.002767 \n",
      "[3/10][26/100][735] Loss_D: 0.006043 Loss_G: 0.002193 \n",
      "[3/10][26/100][736] Loss_D: 0.002860 Loss_G: 0.002687 \n",
      "[3/10][26/100][737] Loss_D: 0.003172 Loss_G: 0.002221 \n",
      "[3/10][26/100][738] Loss_D: 0.001888 Loss_G: 0.003399 \n",
      "[3/10][26/100][739] Loss_D: 0.002183 Loss_G: 0.004157 \n",
      "[3/10][26/100][740] Loss_D: 0.002753 Loss_G: 0.002899 \n",
      "[3/10][26/100][741] Loss_D: 0.001743 Loss_G: 0.003732 \n",
      "[3/10][26/100][742] Loss_D: 0.003017 Loss_G: 0.003079 \n",
      "[3/10][26/100][743] Loss_D: 0.000033 Loss_G: 0.001499 \n",
      "[3/10][26/100][744] Loss_D: 0.002116 Loss_G: 0.001792 \n",
      "[3/10][26/100][745] Loss_D: 0.003219 Loss_G: 0.002835 \n",
      "[3/10][26/100][746] Loss_D: 0.002610 Loss_G: 0.003904 \n",
      "[3/10][26/100][747] Loss_D: 0.002567 Loss_G: 0.001964 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][26/100][748] Loss_D: 0.003058 Loss_G: 0.002309 \n",
      "[3/10][26/100][749] Loss_D: 0.002478 Loss_G: 0.001631 \n",
      "[3/10][26/100][750] Loss_D: 0.001417 Loss_G: 0.002758 \n",
      "[3/10][26/100][751] Loss_D: 0.002072 Loss_G: 0.002199 \n",
      "[3/10][26/100][752] Loss_D: 0.003244 Loss_G: 0.002624 \n",
      "[3/10][26/100][753] Loss_D: 0.000266 Loss_G: 0.001867 \n",
      "[3/10][26/100][754] Loss_D: 0.001712 Loss_G: 0.002381 \n",
      "[3/10][26/100][755] Loss_D: 0.001605 Loss_G: 0.002337 \n",
      "[3/10][26/100][756] Loss_D: 0.002321 Loss_G: 0.002074 \n",
      "[3/10][26/100][757] Loss_D: 0.002712 Loss_G: 0.001444 \n",
      "[3/10][26/100][758] Loss_D: 0.002429 Loss_G: 0.003667 \n",
      "[3/10][26/100][759] Loss_D: 0.003245 Loss_G: 0.002943 \n",
      "[3/10][26/100][760] Loss_D: 0.003304 Loss_G: 0.003131 \n",
      "[3/10][26/100][761] Loss_D: 0.003407 Loss_G: 0.002025 \n",
      "[3/10][26/100][762] Loss_D: 0.002675 Loss_G: 0.001929 \n",
      "[3/10][26/100][763] Loss_D: 0.003932 Loss_G: 0.001972 \n",
      "[3/10][26/100][764] Loss_D: 0.002406 Loss_G: 0.003742 \n",
      "[3/10][26/100][765] Loss_D: 0.003148 Loss_G: 0.002423 \n",
      "[3/10][26/100][766] Loss_D: 0.001815 Loss_G: 0.002592 \n",
      "[3/10][26/100][767] Loss_D: 0.002593 Loss_G: 0.002436 \n",
      "[3/10][26/100][768] Loss_D: 0.002064 Loss_G: 0.002851 \n",
      "[3/10][26/100][769] Loss_D: 0.003588 Loss_G: 0.003978 \n",
      "[3/10][26/100][770] Loss_D: 0.001717 Loss_G: 0.002896 \n",
      "[3/10][26/100][771] Loss_D: 0.001987 Loss_G: 0.001935 \n",
      "[3/10][26/100][772] Loss_D: 0.002389 Loss_G: 0.003532 \n",
      "[3/10][26/100][773] Loss_D: 0.002412 Loss_G: 0.003725 \n",
      "[3/10][26/100][774] Loss_D: 0.002994 Loss_G: 0.001540 \n",
      "[3/10][26/100][775] Loss_D: 0.003690 Loss_G: 0.002233 \n",
      "[3/10][26/100][776] Loss_D: 0.003191 Loss_G: 0.004501 \n",
      "[3/10][26/100][777] Loss_D: 0.004651 Loss_G: 0.003431 \n",
      "[3/10][26/100][778] Loss_D: 0.001871 Loss_G: 0.003011 \n",
      "[3/10][26/100][779] Loss_D: 0.005366 Loss_G: 0.001571 \n",
      "[3/10][26/100][780] Loss_D: 0.000114 Loss_G: 0.002920 \n",
      "[3/10][26/100][781] Loss_D: 0.001136 Loss_G: 0.003574 \n",
      "[3/10][26/100][782] Loss_D: 0.002217 Loss_G: 0.002930 \n",
      "[3/10][26/100][783] Loss_D: 0.002857 Loss_G: 0.003258 \n",
      "[3/10][26/100][784] Loss_D: 0.003689 Loss_G: 0.003225 \n",
      "[3/10][26/100][785] Loss_D: 0.003396 Loss_G: 0.001465 \n",
      "[3/10][26/100][786] Loss_D: 0.001945 Loss_G: 0.003297 \n",
      "[3/10][26/100][787] Loss_D: 0.002897 Loss_G: 0.001072 \n",
      "[3/10][26/100][788] Loss_D: 0.002719 Loss_G: 0.001133 \n",
      "[3/10][26/100][789] Loss_D: 0.002862 Loss_G: 0.003267 \n",
      "[3/10][26/100][790] Loss_D: 0.003559 Loss_G: 0.002552 \n",
      "[3/10][26/100][791] Loss_D: 0.002811 Loss_G: 0.004657 \n",
      "[3/10][26/100][792] Loss_D: 0.002491 Loss_G: 0.002215 \n",
      "[3/10][26/100][793] Loss_D: 0.004811 Loss_G: 0.003802 \n",
      "[3/10][26/100][794] Loss_D: 0.002743 Loss_G: 0.002917 \n",
      "[3/10][26/100][795] Loss_D: 0.001399 Loss_G: 0.002389 \n",
      "[3/10][26/100][796] Loss_D: 0.004191 Loss_G: 0.001997 \n",
      "[3/10][26/100][797] Loss_D: 0.003708 Loss_G: 0.000576 \n",
      "[3/10][26/100][798] Loss_D: 0.002585 Loss_G: 0.003490 \n",
      "[3/10][26/100][799] Loss_D: 0.001659 Loss_G: 0.001330 \n",
      "[3/10][26/100][800] Loss_D: 0.003135 Loss_G: 0.003385 \n",
      "[4/10][26/100][801] Loss_D: 0.000522 Loss_G: 0.002266 \n",
      "[4/10][26/100][802] Loss_D: 0.002834 Loss_G: 0.003812 \n",
      "[4/10][26/100][803] Loss_D: 0.002529 Loss_G: 0.001198 \n",
      "[4/10][26/100][804] Loss_D: 0.002111 Loss_G: 0.002312 \n",
      "[4/10][26/100][805] Loss_D: 0.005158 Loss_G: 0.002999 \n",
      "[4/10][26/100][806] Loss_D: 0.003815 Loss_G: 0.002184 \n",
      "[4/10][26/100][807] Loss_D: 0.003023 Loss_G: 0.003573 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-343-944a0ce5cb73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnetG_neg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# gen_losses, disc_losses = train_GAN(netD_neg, netG_neg, negative=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_GAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetD_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetG_neg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-340-60d54f04ce56>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[1;34m(netD, netG, negative, tr, steps_per_epoch, epochs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;31m# compute gradient, take step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mnetD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;31m#                 concated_real = torch.cat((real, condition), 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m#                 print(concated_real)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1014\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "netD_neg.train()\n",
    "netG_neg.train()\n",
    "# gen_losses, disc_losses = train_GAN(netD_neg, netG_neg, negative=True)\n",
    "train_GAN(netD_neg, netG_neg, tr=train_100k, epochs=10, negative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gen_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(disc_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to test without train\n",
    "netD_neg_test = NetD(train_100k.shape[1]).cuda()\n",
    "netG_neg_test = NetG(train_100k.shape[1]).cuda()\n",
    "\n",
    "netD_neg_test.eval()\n",
    "netG_neg_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetG(\n",
       "  (netGen): Sequential(\n",
       "    (0): Linear(in_features=1690, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=1682, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD_neg.eval()\n",
    "netG_neg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking accuraccy\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda() \n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k > 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake_accur_check = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without train\n",
    "fake_test_accur_check = netG_neg_test(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_accur_check_ = (fake_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_test_accur_check = (fake_test_accur_check.detach().cpu().numpy() > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24943, 13093)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_accur_check_ * negative_feedback).sum(), (fake_accur_check_ * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20943, 26242)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fake_test_accur_check * negative_feedback).sum(), (fake_test_accur_check * positive_feedback).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6133172686813052"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items\n",
    "(fake_accur_check_ * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5149622562639848"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on negative items - WITHOUT TRAIN \n",
    "(fake_test_accur_check * negative_feedback).sum() / negative_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7376204885673634"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items\n",
    "((1-fake_accur_check_) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47411875513516766"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47411875513516766"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy on positive items - WITHOUT TRAIN \n",
    "((1-fake_test_accur_check) * positive_feedback).sum() / positive_feedback.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12430"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD_neg.state_dict(), './netD_neg-100k')\n",
    "torch.save(netG_neg.state_dict(), './netG_neg-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fake_accur_check' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-362-f2d3f5fa5f9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mfake_accur_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mfake_test_accur_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0me_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fake_accur_check' is not defined"
     ]
    }
   ],
   "source": [
    "del fake_accur_check\n",
    "del fake_test_accur_check \n",
    "del e_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition, X, idxs = batch_generator(y_neg, to_add_negative, batch_size=256)\n",
    "# predicting on ngetavie feedbacks\n",
    "condition = torch.from_numpy(negative_feedback).float().cuda()\n",
    "# X = torch.from_numpy(to_add_negative).cuda()\n",
    "\n",
    "\n",
    "noise = torch.randn(condition.shape[0], nz).cuda()\n",
    "# e_mask = torch.Tensor(to_add_negative).cuda()\n",
    "e_mask = torch.Tensor(train_100k == 0).cuda()\n",
    "\n",
    "concated = torch.cat((noise, condition), 1)\n",
    "fake = netG_neg(e_mask, concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = (fake >0.9).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_augment_negative = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13777078364356143, 0.2542968846049817, 0.6079323317514569]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_probs = [(train_100k == 1).sum()/((train_100k > 0) & (train_100k < 4)).sum(), (train_100k == 2).sum()/(((train_100k > 0) & (train_100k < 4))).sum(), (train_100k == 3).sum()/((train_100k > 0) & (train_100k < 4)).sum()]\n",
    "p_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_100k = train_100k + to_augment_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.710139043178159, 5.734979440473204)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sparsity(train_100k), get_sparsity(augmented_train_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "\tcurrent iteration: 110\n",
      "\tcurrent iteration: 120\n",
      "\tcurrent iteration: 130\n",
      "\tcurrent iteration: 140\n",
      "\tcurrent iteration: 150\n",
      "\tcurrent iteration: 160\n",
      "\tcurrent iteration: 170\n",
      "\tcurrent iteration: 180\n",
      "\tcurrent iteration: 190\n",
      "\tcurrent iteration: 200\n",
      "\tcurrent iteration: 210\n",
      "\tcurrent iteration: 220\n",
      "\tcurrent iteration: 230\n",
      "\tcurrent iteration: 240\n",
      "\tcurrent iteration: 250\n",
      "\tcurrent iteration: 260\n",
      "\tcurrent iteration: 270\n",
      "\tcurrent iteration: 280\n",
      "\tcurrent iteration: 290\n",
      "\tcurrent iteration: 300\n",
      "Train mse: 0.24780276795013206\n",
      "Test mse: 0.9911321751624483\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-da219d16cefc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMF_SGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExplicitMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maugmented_train_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mMF_SGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_learning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_100k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mcalculate_learning_curve\u001b[1;34m(self, iter_array, test, learning_rate)\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Iteration: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0miter_diff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_iter, learning_rate)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpartial_train\u001b[1;34m(self, n_iter)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m             \u001b[0mctr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_col\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-241-1992e68e1d60>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, u, i)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sgd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_bias\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_vecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "MF_SGD.calculate_learning_curve([300], test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142822581632546\n",
      "Test mse: 1.179626365384201\n",
      "Iteration: 2\n",
      "Train mse: 1.0730309645422471\n",
      "Test mse: 1.1283647354414428\n",
      "Iteration: 5\n",
      "Train mse: 0.976704786728581\n",
      "Test mse: 1.0498895036356708\n",
      "Iteration: 10\n",
      "Train mse: 0.9191190735106731\n",
      "Test mse: 0.99882388507181\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8670975465312848\n",
      "Test mse: 0.9525039875303193\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8395297477072872\n",
      "Test mse: 0.9342662870213886\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7572607163519403\n",
      "Test mse: 0.917558302954112\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40646564364302945\n",
      "Test mse: 0.9047525011076776\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1583192976603522\n",
      "Test mse: 1.1778921160446154\n",
      "Iteration: 2\n",
      "Train mse: 1.0827823219708648\n",
      "Test mse: 1.124826924658217\n",
      "Iteration: 5\n",
      "Train mse: 0.9809962544391533\n",
      "Test mse: 1.0456674228179015\n",
      "Iteration: 10\n",
      "Train mse: 0.9217915122656181\n",
      "Test mse: 0.9960992361685521\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8695835693024129\n",
      "Test mse: 0.9521000726072203\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8421152097706776\n",
      "Test mse: 0.9345373660525688\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.755397379978269\n",
      "Test mse: 0.916590796822304\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "\tcurrent iteration: 60\n",
      "\tcurrent iteration: 70\n",
      "\tcurrent iteration: 80\n",
      "\tcurrent iteration: 90\n",
      "\tcurrent iteration: 100\n",
      "Train mse: 0.40575111332308983\n",
      "Test mse: 0.906762464167835\n"
     ]
    }
   ],
   "source": [
    "MF_SGD = ExplicitMF(augmented_train_100k, 40, learning='sgd', verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 200]\n",
    "MF_SGD.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n"
     ]
    }
   ],
   "source": [
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.35978032575701\n",
      "Test mse: 8.193157746225948\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359732338000212\n",
      "Test mse: 8.193099816595986\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359687859300425\n",
      "Test mse: 8.193046053932775\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359646525578646\n",
      "Test mse: 8.192996043819882\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3596080230681835\n",
      "Test mse: 8.192949426343956\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(augmented_train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393714646752196\n",
      "Test mse: 8.194362304310943\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393666122560651\n",
      "Test mse: 8.194304259396569\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39362115513897\n",
      "Test mse: 8.194250397772203\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393579374306474\n",
      "Test mse: 8.19420030178969\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393540461520644\n",
      "Test mse: 8.19415360957236\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "MF_ALS = ExplicitMF(train_100k, n_factors=10, learning='als',\n",
    "                    user_fact_reg=0.1, item_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200]\n",
    "MF_ALS.calculate_learning_curve(iter_array, test_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1583192976603522\n",
      "Test mse: 1.1778921160446154\n",
      "Iteration: 2\n",
      "Train mse: 1.0827823219708648\n",
      "Test mse: 1.124826924658217\n",
      "Iteration: 5\n",
      "Train mse: 0.9809962544391533\n",
      "Test mse: 1.0456674228179015\n",
      "Iteration: 10\n",
      "Train mse: 0.9217915122656181\n",
      "Test mse: 0.9960992361685521\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8695835693024129\n",
      "Test mse: 0.9521000726072203\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8421152097706776\n",
      "Test mse: 0.9345373660525688\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.755397379978269\n",
      "Test mse: 0.916590796822304\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6846501387489577\n",
      "Test mse: 0.9047955049119271\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6088625770108296\n",
      "Test mse: 0.8969742625646109\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5326639192133193\n",
      "Test mse: 0.8944780567537973\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4636710714200561\n",
      "Test mse: 0.8979252620915619\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.40575111332308983\n",
      "Test mse: 0.906762464167835\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.35905711689460085\n",
      "Test mse: 0.9198637997234861\n"
     ]
    }
   ],
   "source": [
    "# until test error starts to rise\n",
    "\n",
    "best_sgd_model = ExplicitMF(train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.1545119024961061\n",
      "Test mse: 1.178567399719999\n",
      "Iteration: 2\n",
      "Train mse: 1.0808310124661051\n",
      "Test mse: 1.1261795339578151\n",
      "Iteration: 5\n",
      "Train mse: 0.9808797891168647\n",
      "Test mse: 1.0474046524170364\n",
      "Iteration: 10\n",
      "Train mse: 0.9222637203091965\n",
      "Test mse: 0.997286882756945\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8708585227221491\n",
      "Test mse: 0.9523215310455245\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8464129951238863\n",
      "Test mse: 0.9346230958725521\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7999830140159173\n",
      "Test mse: 0.9226304802126559\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7510374101643228\n",
      "Test mse: 0.9117502158632841\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6909018150472902\n",
      "Test mse: 0.9010086763640088\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6230042394517625\n",
      "Test mse: 0.8922974818153959\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5493002333842844\n",
      "Test mse: 0.8866310890559449\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.47529433983651304\n",
      "Test mse: 0.8848626444854254\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4062577209484476\n",
      "Test mse: 0.8867158452927663\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(augmented_train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.158486007439644\n",
      "Test mse: 1.1779978613000384\n",
      "Iteration: 2\n",
      "Train mse: 1.083171581612149\n",
      "Test mse: 1.125028404345431\n",
      "Iteration: 5\n",
      "Train mse: 0.9817048627613273\n",
      "Test mse: 1.0459868491969369\n",
      "Iteration: 10\n",
      "Train mse: 0.9227319825521801\n",
      "Test mse: 0.9963652377560377\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8713373835757132\n",
      "Test mse: 0.9521701303030687\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.846936197066563\n",
      "Test mse: 0.934619043527751\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.8000609230973601\n",
      "Test mse: 0.9225265969757358\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7503523923902362\n",
      "Test mse: 0.9115515826764651\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.689505366026202\n",
      "Test mse: 0.9008884227444379\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6213034484620569\n",
      "Test mse: 0.8924318463805855\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5475532983376317\n",
      "Test mse: 0.8870274449200776\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4737341552445482\n",
      "Test mse: 0.8854985087833768\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4050262870207811\n",
      "Test mse: 0.8875516903462294\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(augmented_train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 1.142972933039822\n",
      "Test mse: 1.17972828223402\n",
      "Iteration: 2\n",
      "Train mse: 1.0733920132206576\n",
      "Test mse: 1.1285610630076768\n",
      "Iteration: 5\n",
      "Train mse: 0.9773881833559704\n",
      "Test mse: 1.0502187304294268\n",
      "Iteration: 10\n",
      "Train mse: 0.9200456767969902\n",
      "Test mse: 0.999134022874239\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 0.8688346580213652\n",
      "Test mse: 0.9526617108778075\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.8442347461991091\n",
      "Test mse: 0.934442842680263\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 0.7999403795934972\n",
      "Test mse: 0.9232297270102171\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.7525020754101615\n",
      "Test mse: 0.9130561640976463\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6925429566514093\n",
      "Test mse: 0.902326297805037\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.6242821764329887\n",
      "Test mse: 0.8932252838608207\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.5502016125007768\n",
      "Test mse: 0.8871597136187851\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.4759825330856412\n",
      "Test mse: 0.8851292592272328\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 0.40684917561304956\n",
      "Test mse: 0.8868481394223161\n"
     ]
    }
   ],
   "source": [
    "# until convergence\n",
    "best_sgd_model = ExplicitMF(train_100k, n_factors=80, learning='sgd', \\\n",
    "                            item_fact_reg=0.01, user_fact_reg=0.01, \\\n",
    "                            user_bias_reg=0.01, item_bias_reg=0.01, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_sgd_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.414043254856359\n",
      "Test mse: 10.597071517103002\n",
      "Iteration: 2\n",
      "Train mse: 5.729849556962303\n",
      "Test mse: 8.637501957776392\n",
      "Iteration: 5\n",
      "Train mse: 5.425216634306679\n",
      "Test mse: 8.224449939246286\n",
      "Iteration: 10\n",
      "Train mse: 5.416034494374579\n",
      "Test mse: 8.225911144030166\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.405244808568524\n",
      "Test mse: 8.212123871641975\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.394015977369497\n",
      "Test mse: 8.19475571062152\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.3937671853374605\n",
      "Test mse: 8.19442505144314\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393714646752196\n",
      "Test mse: 8.194362304310943\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393666122560651\n",
      "Test mse: 8.194304259396569\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39362115513897\n",
      "Test mse: 8.194250397772203\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393579374306474\n",
      "Test mse: 8.19420030178969\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.393540461520644\n",
      "Test mse: 8.19415360957236\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.39350414020554\n",
      "Test mse: 8.194110004824077\n"
     ]
    }
   ],
   "source": [
    "best_als_model = ExplicitMF(train_100k, n_factors=10, learning='als', \\\n",
    "                            item_fact_reg=0.1, user_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_als_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Train mse: 7.365331085064962\n",
      "Test mse: 10.594736049344187\n",
      "Iteration: 2\n",
      "Train mse: 5.692327274625398\n",
      "Test mse: 8.63601159502765\n",
      "Iteration: 5\n",
      "Train mse: 5.390926482500714\n",
      "Test mse: 8.223490099578223\n",
      "Iteration: 10\n",
      "Train mse: 5.382053601809805\n",
      "Test mse: 8.224694402460047\n",
      "Iteration: 25\n",
      "\tcurrent iteration: 10\n",
      "Train mse: 5.365488278184999\n",
      "Test mse: 8.201579412685142\n",
      "Iteration: 50\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3600334033203785\n",
      "Test mse: 8.193471712808986\n",
      "Iteration: 100\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "\tcurrent iteration: 30\n",
      "\tcurrent iteration: 40\n",
      "\tcurrent iteration: 50\n",
      "Train mse: 5.35983226130889\n",
      "Test mse: 8.193220337124336\n",
      "Iteration: 120\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.35978032575701\n",
      "Test mse: 8.193157746225948\n",
      "Iteration: 140\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359732338000212\n",
      "Test mse: 8.193099816595986\n",
      "Iteration: 160\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359687859300425\n",
      "Test mse: 8.193046053932775\n",
      "Iteration: 180\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359646525578646\n",
      "Test mse: 8.192996043819882\n",
      "Iteration: 200\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.3596080230681835\n",
      "Test mse: 8.192949426343956\n",
      "Iteration: 220\n",
      "\tcurrent iteration: 10\n",
      "\tcurrent iteration: 20\n",
      "Train mse: 5.359572079254732\n",
      "Test mse: 8.192905886804215\n"
     ]
    }
   ],
   "source": [
    "best_als_model = ExplicitMF(augmented_train_100k, n_factors=10, learning='als', \\\n",
    "                            item_fact_reg=0.1, user_fact_reg=0.1, verbose=True)\n",
    "iter_array = [1, 2, 5, 10, 25, 50, 100, 120, 140, 160, 180, 200, 220]\n",
    "best_als_model.calculate_learning_curve(iter_array, test_100k, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=10, max_iter=400, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "W = model.fit_transform(train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((943, 10), (10, 1682))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4260560791980288"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NMF(n_components=20, max_iter=800, init='nndsvdar', beta_loss='frobenius', alpha=0.1, random_state=seed)\n",
    "model = NMF(n_components=10, max_iter=400, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "W = model.fit_transform(augmented_train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4275072124477104"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NMF(n_components=20, max_iter=800, init='nndsvdar', beta_loss='frobenius', alpha=0.1, random_state=seed)\n",
    "model = NMF(n_components=10, max_iter=400, alpha=0.1, init='nndsvdar', random_state=seed)\n",
    "W = model.fit_transform(augmented_train_100k)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4263249840862073"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mse(test_100k, np.matmul(W,H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
